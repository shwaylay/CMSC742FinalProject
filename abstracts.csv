id,authors,title,category,abstract,versions,update_date,new_category,generated_abstract,prompt
2008.1275,"Dimitris Perdios, Manuel Vonlanthen, Florian Martinez, Marcel Arditi,
  Jean-Philippe Thiran",CNN-Based Image Reconstruction Method for Ultrafast Ultrasound Imaging,"['eess.IV', 'cs.CV', 'cs.LG']","  Ultrafast ultrasound (US) revolutionized biomedical imaging with its
capability of acquiring full-view frames at over 1 kHz, unlocking breakthrough
modalities such as shear-wave elastography and functional US neuroimaging. Yet,
it suffers from strong diffraction artifacts, mainly caused by grating lobes,
side lobes, or edge waves. Multiple acquisitions are typically required to
obtain a sufficient image quality, at the cost of a reduced frame rate. To
answer the increasing demand for high-quality imaging from single unfocused
acquisitions, we propose a two-step convolutional neural network (CNN)-based
image reconstruction method, compatible with real-time imaging. A low-quality
estimate is obtained by means of a backprojection-based operation, akin to
conventional delay-and-sum beamforming, from which a high-quality image is
restored using a residual CNN with multiscale and multichannel filtering
properties, trained specifically to remove the diffraction artifacts inherent
to ultrafast US imaging. To account for both the high dynamic range and the
oscillating properties of radio frequency US images, we introduce the mean
signed logarithmic absolute error (MSLAE) as a training loss function.
Experiments were conducted with a linear transducer array, in single plane-wave
(PW) imaging. Trainings were performed on a simulated dataset, crafted to
contain a wide diversity of structures and echogenicities. Extensive numerical
evaluations demonstrate that the proposed approach can reconstruct images from
single PWs with a quality similar to that of gold-standard synthetic aperture
imaging, on a dynamic range in excess of 60 dB. In vitro and in vivo
experiments show that trainings carried out on simulated data perform well in
experimental settings.
","[{'version': 'v1', 'created': 'Fri, 28 Aug 2020 17:15:37 GMT'}, {'version': 'v2', 'created': 'Wed, 26 Jan 2022 14:28:06 GMT'}, {'version': 'v3', 'created': 'Fri, 1 Apr 2022 17:45:10 GMT'}]",2022-04-04,"['Computer Vision and Pattern Recognition', 'Machine Learning']","This paper presents a novel CNN-based image reconstruction method for ultrafast ultrasound imaging. The proposed method combines the advantages of convolutional neural networks (CNNs) and ultrasound imaging to achieve high-quality images with low computational complexity. The proposed method is evaluated on a dataset of simulated ultrasound images and compared to two state-of-the-art methods. Results indicate that the proposed method outperforms the other methods in terms of image quality and computational complexity. Furthermore, the proposed method is able to capture subtle features in the images that are not visible in the other methods. The results of this paper demonstrate the potential of the proposed method for ultrafast ultrasound imaging.","Write an abstract for a paper called CNN-Based Image Reconstruction Method for Ultrafast Ultrasound Imaging about Computer Vision and Pattern Recognition, Machine Learning"
2207.14561,"Yuki Kadokawa, Lingwei Zhu, Yoshihisa Tsurumine, Takamitsu Matsubara","Cyclic Policy Distillation: Sample-Efficient Sim-to-Real Reinforcement
  Learning with Domain Randomization","['cs.RO', 'cs.LG']","  Deep reinforcement learning with domain randomization learns a control policy
in various simulations with randomized physical and sensor model parameters to
become transferable to the real world in a zero-shot setting. However, a huge
number of samples are often required to learn an effective policy when the
range of randomized parameters is extensive due to the instability of policy
updates. To alleviate this problem, we propose a sample-efficient method named
cyclic policy distillation (CPD). CPD divides the range of randomized
parameters into several small sub-domains and assigns a local policy to each
one. Then local policies are learned while cyclically transitioning to
sub-domains. CPD accelerates learning through knowledge transfer based on
expected performance improvements. Finally, all of the learned local policies
are distilled into a global policy for sim-to-real transfers. CPD's
effectiveness and sample efficiency are demonstrated through simulations with
four tasks (Pendulum from OpenAIGym and Pusher, Swimmer, and HalfCheetah from
Mujoco), and a real-robot, ball-dispersal task. We published code and videos
from our experiments at
https://github.com/yuki-kadokawa/cyclic-policy-distillation.
","[{'version': 'v1', 'created': 'Fri, 29 Jul 2022 09:22:53 GMT'}, {'version': 'v2', 'created': 'Mon, 10 Apr 2023 07:02:05 GMT'}]",2023-04-11,"['Robotics', 'Machine Learning']",", and Artificial Intelligence

This paper presents a novel reinforcement learning technique called Cyclic Policy Distillation (CPD) for sample-efficient sim-to-real transfer of robotic control policies. CPD is based on the concept of domain randomization, which randomly perturbs the simulation environment to generate a diverse set of training data. The proposed method uses a cyclic training process to transfer the learned policy from simulation to the real world. The effectiveness of CPD is demonstrated through experiments on a real-world robotic arm, where it is shown to outperform existing sim-to-real transfer techniques in terms of sample efficiency. Furthermore, the results show that CPD is able to generalize well to unseen environments, and is capable of learning complex manipulation tasks with a limited amount of data. The proposed method can be used as a powerful tool for robotic control, machine learning, and artificial intelligence research.","Write an abstract for a paper called Cyclic Policy Distillation: Sample-Efficient Sim-to-Real Reinforcement
  Learning with Domain Randomization about Robotics, Machine Learning"
2207.08389,"Amir H. Ashouri, Mostafa Elhoushi, Yuzhe Hua, Xiang Wang, Muhammad
  Asif Manzoor, Bryan Chan and Yaoqing Gao",MLGOPerf: An ML Guided Inliner to Optimize Performance,"['cs.PL', 'cs.AI', 'cs.LG', 'cs.NE', 'cs.PF']","  For the past 25 years, we have witnessed an extensive application of Machine
Learning to the Compiler space; the selection and the phase-ordering problem.
However, limited works have been upstreamed into the state-of-the-art
compilers, i.e., LLVM, to seamlessly integrate the former into the optimization
pipeline of a compiler to be readily deployed by the user. MLGO was among the
first of such projects and it only strives to reduce the code size of a binary
with an ML-based Inliner using Reinforcement Learning.
  This paper presents MLGOPerf; the first end-to-end framework capable of
optimizing performance using LLVM's ML-Inliner. It employs a secondary ML model
to generate rewards used for training a retargeted Reinforcement learning
agent, previously used as the primary model by MLGO. It does so by predicting
the post-inlining speedup of a function under analysis and it enables a fast
training framework for the primary model which otherwise wouldn't be practical.
The experimental results show MLGOPerf is able to gain up to 1.8% and 2.2% with
respect to LLVM's optimization at O3 when trained for performance on SPEC
CPU2006 and Cbench benchmarks, respectively. Furthermore, the proposed approach
provides up to 26% increased opportunities to autotune code regions for our
benchmarks which can be translated into an additional 3.7% speedup value.
","[{'version': 'v1', 'created': 'Mon, 18 Jul 2022 05:47:29 GMT'}, {'version': 'v2', 'created': 'Tue, 19 Jul 2022 15:07:17 GMT'}]",2023-01-18,"['Programming Languages', 'Artificial Intelligence', 'Machine Learning', 'Neural and Evolutionary Computing', 'Performance']","This paper introduces MLGOPerf, a novel machine learning-guided inliner for optimizing performance of programming languages. MLGOPerf leverages artificial intelligence, machine learning, neural and evolutionary computing to identify potential inlining opportunities and to determine which of them should be applied to enhance the performance of a given program. Our experimental results on a variety of programs demonstrate that MLGOPerf can significantly improve the performance of existing programs. Furthermore, we discuss the potential of MLGOPerf to be applied in other domains such as compilers, databases, and web services. Finally, we discuss the implications of our work and the potential for future research.","Write an abstract for a paper called MLGOPerf: An ML Guided Inliner to Optimize Performance about Programming Languages, Artificial Intelligence, Machine Learning, Neural and Evolutionary Computing, Performance"
2204.11428,"Prantika Chakraborty, Sudakshina Dutta, Debarshi Kumar Sanyal",Personal Research Knowledge Graphs,"['cs.IR', 'cs.HC']","  Maintaining research-related information in an organized manner can be
challenging for a researcher. In this paper, we envision personal research
knowledge graphs (PRKGs) as a means to represent structured information about
the research activities of a researcher. PRKGs can be used to power intelligent
personal assistants, and personalize various applications. We explore what
entities and relations could be potentially included in a PRKG, how to extract
them from various sources, and how to share a PRKG within a research group.
","[{'version': 'v1', 'created': 'Mon, 25 Apr 2022 04:31:33 GMT'}]",2022-04-26,"['Information Retrieval', 'Human-Computer Interaction']",", and Machine Learning

This paper investigates the potential of Personal Research Knowledge Graphs (PRKGs) for improving information retrieval, human-computer interaction, and machine learning. PRKGs are graph-based systems that capture the knowledge of an individual or organization, allowing them to easily access and share information. We discuss the advantages of PRKGs over traditional search engines, and explore how they can be used to improve information retrieval, human-computer interaction, and machine learning. We present a case study of a PRKG-based system used to improve the retrieval of relevant information from a large dataset. We then discuss the implications of PRKGs for the future of information retrieval, human-computer interaction, and machine learning. Finally, we provide recommendations for further research in the area of PRKGs.","Write an abstract for a paper called Personal Research Knowledge Graphs about Information Retrieval, Human-Computer Interaction"
2209.10117,"Wenqi Fan, Xiangyu Zhao, Xiao Chen, Jingran Su, Jingtong Gao, Lin
  Wang, Qidong Liu, Yiqi Wang, Han Xu, Lei Chen, Qing Li",A Comprehensive Survey on Trustworthy Recommender Systems,"['cs.IR', 'cs.AI', 'cs.CR', 'cs.LG']","  As one of the most successful AI-powered applications, recommender systems
aim to help people make appropriate decisions in an effective and efficient
way, by providing personalized suggestions in many aspects of our lives,
especially for various human-oriented online services such as e-commerce
platforms and social media sites. In the past few decades, the rapid
developments of recommender systems have significantly benefited human by
creating economic value, saving time and effort, and promoting social good.
However, recent studies have found that data-driven recommender systems can
pose serious threats to users and society, such as spreading fake news to
manipulate public opinion in social media sites, amplifying unfairness toward
under-represented groups or individuals in job matching services, or inferring
privacy information from recommendation results. Therefore, systems'
trustworthiness has been attracting increasing attention from various aspects
for mitigating negative impacts caused by recommender systems, so as to enhance
the public's trust towards recommender systems techniques. In this survey, we
provide a comprehensive overview of Trustworthy Recommender systems (TRec) with
a specific focus on six of the most important aspects; namely, Safety &
Robustness, Nondiscrimination & Fairness, Explainability, Privacy,
Environmental Well-being, and Accountability & Auditability. For each aspect,
we summarize the recent related technologies and discuss potential research
directions to help achieve trustworthy recommender systems in the future.
","[{'version': 'v1', 'created': 'Wed, 21 Sep 2022 04:34:17 GMT'}]",2022-09-22,"['Information Retrieval', 'Artificial Intelligence', 'Cryptography and Security', 'Machine Learning']","This paper presents a comprehensive survey of trustworthy recommender systems in the areas of information retrieval, artificial intelligence, cryptography and security, and machine learning. It begins by discussing the need for trustworthiness in recommender systems and the various techniques used to ensure trustworthiness. It then examines the roles of information retrieval, artificial intelligence, cryptography and security, and machine learning in the design and implementation of trustworthy recommender systems. The paper then reviews existing research and literature on the topic, including the various methods and techniques used to support trustworthiness in recommender systems. Finally, the paper provides a set of recommendations for further research in the area of trustworthy recommender systems. The paper concludes by summarizing the key findings and providing a set of future research directions.","Write an abstract for a paper called A Comprehensive Survey on Trustworthy Recommender Systems about Information Retrieval, Artificial Intelligence, Cryptography and Security, Machine Learning"
2001.03707,"Sen Na, Mihai Anitescu",Superconvergence of Online Optimization for Model Predictive Control,"['math.OC', 'cs.NA', 'cs.SY', 'eess.SY', 'math.DS', 'math.NA']","  We develop a one-Newton-step-per-horizon, online, lag-$L$, model predictive
control (MPC) algorithm for solving discrete-time, equality-constrained,
nonlinear dynamic programs. Based on recent sensitivity analysis results for
the target problems class, we prove that the approach exhibits a behavior that
we call superconvergence; that is, the tracking error with respect to the full
horizon solution is not only stable for successive horizon shifts, but also
decreases with increasing shift order to a minimum value that decays
exponentially in the length of the receding horizon. The key analytical step is
the decomposition of the one-step error recursion of our algorithm into
algorithmic error and perturbation error. We show that the perturbation error
decays exponentially with the lag between two consecutive receding horizons,
while~the algorithmic error, determined by Newton's method, achieves quadratic
convergence instead. Overall this approach induces our local exponential
convergence result in terms of the receding horizon length for suitable values
of $L$. Numerical experiments validate our theoretical findings.
","[{'version': 'v1', 'created': 'Sat, 11 Jan 2020 03:29:48 GMT'}, {'version': 'v2', 'created': 'Sat, 5 Feb 2022 19:01:00 GMT'}]",2022-02-08,"['Numerical Analysis', 'Systems and Control']","This paper investigates the superconvergence of online optimization for model predictive control (MPC) with numerical analysis, systems, and control. The paper provides a comprehensive overview of the state-of-the-art in online optimization for MPC, and presents a novel approach for superconvergence of the optimization process. The proposed approach is based on a combination of numerical analysis, systems and control theory, and machine learning. The paper further presents a numerical analysis of the proposed approach, and demonstrates its effectiveness in terms of convergence speed and accuracy. Finally, the paper provides a discussion of the potential applications of the proposed approach, and its implications for the field of model predictive control.","Write an abstract for a paper called Superconvergence of Online Optimization for Model Predictive Control about Numerical Analysis, Systems and Control"
2208.093,"William T. Ng, K. Siu, Albert C. Cheung, Michael K. Ng","Expressing Multivariate Time Series as Graphs with Time Series Attention
  Transformer","['cs.LG', 'cs.AI', 'math.DS', 'math.RT']","  A reliable and efficient representation of multivariate time series is
crucial in various downstream machine learning tasks. In multivariate time
series forecasting, each variable depends on its historical values and there
are inter-dependencies among variables as well. Models have to be designed to
capture both intra- and inter-relationships among the time series. To move
towards this goal, we propose the Time Series Attention Transformer (TSAT) for
multivariate time series representation learning. Using TSAT, we represent both
temporal information and inter-dependencies of multivariate time series in
terms of edge-enhanced dynamic graphs. The intra-series correlations are
represented by nodes in a dynamic graph; a self-attention mechanism is modified
to capture the inter-series correlations by using the super-empirical mode
decomposition (SMD) module. We applied the embedded dynamic graphs to times
series forecasting problems, including two real-world datasets and two
benchmark datasets. Extensive experiments show that TSAT clearly outerperforms
six state-of-the-art baseline methods in various forecasting horizons. We
further visualize the embedded dynamic graphs to illustrate the graph
representation power of TSAT. We share our code at
https://github.com/RadiantResearch/TSAT.
","[{'version': 'v1', 'created': 'Fri, 19 Aug 2022 12:25:56 GMT'}]",2022-08-22,"['Machine Learning', 'Artificial Intelligence']","This paper presents a novel approach to expressing multivariate time series as graphs with Time Series Attention Transformer (TSAT), a machine learning and artificial intelligence model. TSAT is a deep learning model that is capable of capturing the temporal dependencies of multivariate time series and learning to predict future values. The model is composed of an encoder-decoder architecture, with an attention mechanism that allows the model to focus on important features and reduce the amount of time series data needed for training. Our experiments demonstrate that the proposed TSAT model outperforms existing methods in terms of accuracy and efficiency, and is able to accurately capture the temporal dependencies in multivariate time series. The results of our experiments also show that the model can accurately capture complex patterns in time series data, and can be used for forecasting and anomaly detection.","Write an abstract for a paper called Expressing Multivariate Time Series as Graphs with Time Series Attention
  Transformer about Machine Learning, Artificial Intelligence"
2211.09081,"Hamid Reza Hashempour, Hamed Bastami, Majid Moradikia, Seyed
  A.Zekavat, Hamid Behroozi, and A. Lee Swindlehurst","Secure SWIPT in STAR-RIS Aided Downlink MISO Rate-Splitting Multiple
  Access Networks","['cs.IT', 'eess.SP', 'math.IT']","  Recently, simultaneously transmitting and reflecting reconfigurable
intelligent surfaces (STAR-RISs) have emerged as a novel technology that
facilitates sustainable communication by providing 360 coverage and new
degrees-of-freedom (DoF) for manipulating signal propagation as well as
simultaneous wireless information and power transfer (SWIPT). Inspired by these
applications, this paper presents a novel STAR-RIS-aided secure SWIPT system
for downlink multiple input single output (MISO) Rate-Splitting multiple access
(RSMA) networks. The transmitter concurrently communicates with the information
receivers (IRs) and sends energy to untrusted energy receivers (UERs). UERs are
also able to wiretap the IR streams. The paper assumes that the channel state
information (CSI) of the IRs is known at the transmitter. However, only
imperfect CSI (ICSI) for the UERs is available at the transmitter. The paper
aims to maximize the achievable worst-case sum secrecy rate (WCSSR) of the IRs
under a total transmit power constraint, a sum energy constraint for the UERs,
and constraints on the transmission and reflection coefficients by jointly
optimizing the precoders and the transmission and reflection beamforming at the
STAR-RIS. The formulated problem is non-convex with intricately coupled
variables, and to tackle this challenge a suboptimal two-step iterative
algorithm based on the sequential parametric convex approximation (SPCA) method
is proposed. Specifically, the precoders and the transmission and reflection
beamforming vectors are optimized alternatingly. Simulations are conducted to
show that the proposed RSMA-based algorithm in a STAR-RIS aided network can
improve the secrecy of the confidential information and the overall spectral
efficiency.
","[{'version': 'v1', 'created': 'Wed, 16 Nov 2022 18:02:53 GMT'}]",2022-11-17,['Information Theory'],"This paper explores the application of secure simultaneous wireless information and power transfer (SWIPT) in a multiple-input single-output (MISO) rate-splitting multiple access (RSMA) network aided by a simultaneous transmission and reception (STAR-RIS) system. We analyze the secure SWIPT in the rate-splitting multiple access (RSMA) network, and derive the optimal power allocation and rate-splitting coefficients to maximize the achievable secrecy rate. We further investigate the impact of the number of antennas and the number of users on the secrecy rate of the system. Our simulation results reveal that the secrecy rate is improved by increasing the number of antennas, and the maximum secrecy rate is achieved when the number of users is equal to the number of antennas. The results of this paper provide insights into the secure SWIPT in RSMA networks with STAR-RIS systems and can be used to design secure communication systems.","Write an abstract for a paper called Secure SWIPT in STAR-RIS Aided Downlink MISO Rate-Splitting Multiple
  Access Networks about Information Theory"
2102.08026,"Nabil Ibtehaz, Muhammad E. H. Chowdhury, Amith Khandakar, Serkan
  Kiranyaz, M. Sohel Rahman, Anas Tahir, Yazan Qiblawey, and Tawsifur Rahman","EDITH :ECG biometrics aided by Deep learning for reliable Individual
  auTHentication","['cs.LG', 'cs.AI', 'cs.CR']","  In recent years, physiological signal based authentication has shown great
promises,for its inherent robustness against forgery. Electrocardiogram (ECG)
signal, being the most widely studied biosignal, has also received the highest
level of attention in this regard. It has been proven with numerous studies
that by analyzing ECG signals from different persons, it is possible to
identify them, with acceptable accuracy. In this work, we present, EDITH, a
deep learning-based framework for ECG biometrics authentication system.
Moreover, we hypothesize and demonstrate that Siamese architectures can be used
over typical distance metrics for improved performance. We have evaluated EDITH
using 4 commonly used datasets and outperformed the prior works using less
number of beats. EDITH performs competitively using just a single heartbeat
(96-99.75% accuracy) and can be further enhanced by fusing multiple beats (100%
accuracy from 3 to 6 beats). Furthermore, the proposed Siamese architecture
manages to reduce the identity verification Equal Error Rate (EER) to 1.29%. A
limited case study of EDITH with real-world experimental data also suggests its
potential as a practical authentication system.
","[{'version': 'v1', 'created': 'Tue, 16 Feb 2021 08:45:17 GMT'}, {'version': 'v2', 'created': 'Sun, 14 Nov 2021 01:37:03 GMT'}]",2022-01-24,"['Machine Learning', 'Artificial Intelligence', 'Cryptography and Security']","This paper examines the use of deep learning techniques to aid in the authentication of individuals using electrocardiogram (ECG) biometrics. The paper explores the potential of using machine learning, artificial intelligence, cryptography and security to improve the reliability of ECG biometrics. We present a novel approach which combines ECG biometrics with deep learning to improve authentication accuracy and reliability. We evaluate our approach using a publicly available ECG dataset and demonstrate that our method outperforms existing approaches in terms of accuracy, reliability and security. Finally, we discuss the implications and potential applications of our proposed approach.","Write an abstract for a paper called EDITH :ECG biometrics aided by Deep learning for reliable Individual
  auTHentication about Machine Learning, Artificial Intelligence, Cryptography and Security"
2203.1286,"Felix S. Campbell, Bahareh Sadat Arab, Boris Glavic",Efficient Answering of Historical What-if Queries,['cs.DB'],"  We introduce historical what-if queries, a novel type of what-if analysis
that determines the effect of a hypothetical change to the transactional
history of a database. For example, ""how would revenue be affected if we would
have charged an additional $6 for shipping?"" Such queries may lead to more
actionable insights than traditional what-if queries as their results can be
used to inform future actions, e.g., increasing shipping fees. We develop
efficient techniques for answering historical what-if queries, i.e.,
determining how a modified history affects the current database state. Our
techniques are based on reenactment, a replay technique for transactional
histories. We optimize this process using program and data slicing techniques
that determine which updates and what data can be excluded from reenactment
without affecting the result. Using an implementation of our techniques in
Mahif (a Middleware for Answering Historical what-IF queries) we demonstrate
their effectiveness experimentally.
","[{'version': 'v1', 'created': 'Thu, 24 Mar 2022 05:52:10 GMT'}]",2022-03-25,['Databases'],"This paper presents an efficient method for answering what-if queries about historical databases. We propose a novel approach to efficiently answer queries about databases that have been modified over time. Our method uses a combination of temporal logic, database query optimization techniques, and historical data analysis to answer queries that involve changes to the database over time. We demonstrate the effectiveness of our method through experiments on a real-world dataset. Our results show that our method can answer queries about historical databases quickly and accurately. In addition, we discuss potential applications of our method, such as providing support for data-driven decision-making.",Write an abstract for a paper called Efficient Answering of Historical What-if Queries about Databases
2203.14191,"Aron Brenner, Manxi Wu, and Saurabh Amin","Interpretable Machine Learning Models for Modal Split Prediction in
  Transportation Systems",['cs.LG'],"  Modal split prediction in transportation networks has the potential to
support network operators in managing traffic congestion and improving transit
service reliability. We focus on the problem of hourly prediction of the
fraction of travelers choosing one mode of transportation over another using
high-dimensional travel time data. We use logistic regression as base model and
employ various regularization techniques for variable selection to prevent
overfitting and resolve multicollinearity issues. Importantly, we interpret the
prediction accuracy results with respect to the inherent variability of modal
splits and travelers' aggregate responsiveness to changes in travel time. By
visualizing model parameters, we conclude that the subset of segments found
important for predictive accuracy changes from hour-to-hour and include
segments that are topologically central and/or highly congested. We apply our
approach to the San Francisco Bay Area freeway and rapid transit network and
demonstrate superior prediction accuracy and interpretability of our method
compared to pre-specified variable selection methods.
","[{'version': 'v1', 'created': 'Sun, 27 Mar 2022 02:59:00 GMT'}, {'version': 'v2', 'created': 'Sat, 24 Sep 2022 15:44:05 GMT'}]",2023-03-17,['Machine Learning'],"This paper presents a novel approach to predicting modal split in transportation systems using interpretable machine learning models. Modal split is an important concept in transportation planning, as it describes the proportion of trips taken by different modes of transportation. Currently, there is a lack of interpretable models that can accurately predict modal split. This paper proposes a new method for predicting modal split using a combination of interpretable machine learning models. Specifically, a novel combination of decision trees, random forests, and gradient boosting algorithms is used to predict modal split. The performance of the proposed model is compared to that of existing models, and the results demonstrate that the proposed model is more accurate and interpretable than existing models. The paper also provides insights into how the proposed model can be used to inform transportation planning decisions. The results of this work provide a valuable contribution to the field of transportation planning and demonstrate the potential of interpretable machine learning models in this domain.","Write an abstract for a paper called Interpretable Machine Learning Models for Modal Split Prediction in
  Transportation Systems about Machine Learning"
2211.01282,"Valeria Banica, Georg Maierhofer, Katharina Schratz","Numerical integration of Schr\""odinger maps via the Hasimoto transform","['math.NA', 'cs.NA']","  We introduce a numerical approach to computing the Schr\""odinger map (SM)
based on the Hasimoto transform which relates the SM flow to a cubic nonlinear
Schr\""odinger (NLS) equation. In exploiting this nonlinear transform we are
able to introduce the first fully explicit unconditionally stable symmetric
integrators for the SM equation. Our approach consists of two parts: an
integration of the NLS equation followed by the numerical evaluation of the
Hasimoto transform. Motivated by the desire to study rough solutions to the SM
equation, we also introduce a new symmetric low-regularity integrator for the
NLS equation. This is combined with our novel fast low-regularity Hasimoto
(FLowRH) transform, based on a tailored analysis of the resonance structures in
the Magnus expansion and a fast realisation based on block-Toeplitz partitions,
to yield an efficient low-regularity integrator for the SM equation. This
scheme in particular allows us to obtain approximations to the SM in a more
general regime (i.e. under lower regularity assumptions) than previously
proposed methods. The favorable properties of our methods are exhibited both in
theoretical convergence analysis and in numerical experiments.
","[{'version': 'v1', 'created': 'Wed, 2 Nov 2022 17:05:24 GMT'}]",2022-11-03,['Numerical Analysis'],"This paper presents a numerical integration method for Schr\""odinger maps on two-dimensional tori via the Hasimoto transform. The Hasimoto transform is used to construct a numerical scheme for solving the Schr\""odinger map equation, which is a nonlinear partial differential equation that arises in the study of geometric optics. The numerical scheme is tested on a variety of two-dimensional tori, and the results demonstrate that the Hasimoto transform is an effective numerical tool for solving Schr\""odinger maps. The numerical scheme is shown to be accurate, efficient, and robust, and is suitable for use in a wide range of applications.","Write an abstract for a paper called Numerical integration of Schr\""odinger maps via the Hasimoto transform about Numerical Analysis"
2207.11814,"Md Mohaiminul Islam, Gedas Bertasius","Object State Change Classification in Egocentric Videos using the
  Divided Space-Time Attention Mechanism",['cs.CV'],"  This report describes our submission called ""TarHeels"" for the Ego4D: Object
State Change Classification Challenge. We use a transformer-based video
recognition model and leverage the Divided Space-Time Attention mechanism for
classifying object state change in egocentric videos. Our submission achieves
the second-best performance in the challenge. Furthermore, we perform an
ablation study to show that identifying object state change in egocentric
videos requires temporal modeling ability. Lastly, we present several positive
and negative examples to visualize our model's predictions. The code is
publicly available at: https://github.com/md-mohaiminul/ObjectStateChange
","[{'version': 'v1', 'created': 'Sun, 24 Jul 2022 20:53:36 GMT'}, {'version': 'v2', 'created': 'Wed, 4 Jan 2023 12:04:20 GMT'}]",2023-01-05,['Computer Vision and Pattern Recognition'],"This paper presents a novel approach to classify object state changes in egocentric videos using the Divided Space-Time Attention Mechanism. The proposed approach is based on a two-level attention mechanism, which integrates both spatial and temporal information for recognizing object state changes. The first level of attention focuses on the spatial information, which is used to detect the objects and estimate their locations. The second level of attention focuses on the temporal information, which is used to recognize the object state changes. The Divided Space-Time Attention Mechanism is evaluated on a challenging real-world egocentric video dataset. The experimental results demonstrate the effectiveness of the proposed approach in accurately recognizing object state changes. The proposed approach outperforms the state-of-the-art methods and achieves an accuracy of 95.9%, demonstrating its potential for real-world applications.","Write an abstract for a paper called Object State Change Classification in Egocentric Videos using the
  Divided Space-Time Attention Mechanism about Computer Vision and Pattern Recognition"
2212.08489,"Esa\'u Villatoro-Tello, Srikanth Madikeri, Juan Zuluaga-Gomez, Bidisha
  Sharma, Seyyed Saeed Sarfjoo, Iuliia Nigmatulina, Petr Motlicek, Alexei V.
  Ivanov, Aravind Ganapathiraju","Effectiveness of Text, Acoustic, and Lattice-based representations in
  Spoken Language Understanding tasks","['cs.CL', 'cs.AI', 'cs.SD', 'eess.AS']","  In this paper, we perform an exhaustive evaluation of different
representations to address the intent classification problem in a Spoken
Language Understanding (SLU) setup. We benchmark three types of systems to
perform the SLU intent detection task: 1) text-based, 2) lattice-based, and a
novel 3) multimodal approach. Our work provides a comprehensive analysis of
what could be the achievable performance of different state-of-the-art SLU
systems under different circumstances, e.g., automatically- vs.
manually-generated transcripts. We evaluate the systems on the publicly
available SLURP spoken language resource corpus. Our results indicate that
using richer forms of Automatic Speech Recognition (ASR) outputs, namely
word-consensus-networks, allows the SLU system to improve in comparison to the
1-best setup (5.5% relative improvement). However, crossmodal approaches, i.e.,
learning from acoustic and text embeddings, obtains performance similar to the
oracle setup, a relative improvement of 17.8% over the 1-best configuration,
being a recommended alternative to overcome the limitations of working with
automatically generated transcripts.
","[{'version': 'v1', 'created': 'Fri, 16 Dec 2022 14:01:42 GMT'}, {'version': 'v2', 'created': 'Fri, 17 Mar 2023 13:26:14 GMT'}]",2023-03-20,"['Computation and Language', 'Artificial Intelligence', 'Sound']","This paper studies the effectiveness of text, acoustic, and lattice-based representations in spoken language understanding (SLU) tasks for computation and language, artificial intelligence, and sound. We compare the performance of these three representations on a range of SLU tasks, including intent classification, slot tagging, and semantic parsing. We evaluate the models using a variety of metrics, including accuracy, precision, recall, and F1 score. We also analyze the impact of the choice of representation on the performance of the models. Our results indicate that the text-based representations outperform acoustic and lattice-based representations in most SLU tasks, while acoustic and lattice-based representations are more effective in some tasks. We also find that the choice of representation has a significant impact on the performance of the models. Our findings provide insights into the effectiveness of different representations in SLU tasks and can help inform the development of more effective models for natural language processing.","Write an abstract for a paper called Effectiveness of Text, Acoustic, and Lattice-based representations in
  Spoken Language Understanding tasks about Computation and Language, Artificial Intelligence, Sound"
2002.01563,"Kirill Shmilovich, Rachael A. Mansbach, Hythem Sidky, Olivia E. Dunne,
  Sayak Subhra Panda, John D. Tovar, Andrew L. Ferguson","Discovery of Self-Assembling $\pi$-Conjugated Peptides by Active
  Learning-Directed Coarse-Grained Molecular Simulation","['q-bio.BM', 'cond-mat.soft', 'cs.LG']","  Electronically-active organic molecules have demonstrated great promise as
novel soft materials for energy harvesting and transport. Self-assembled
nanoaggregates formed from $\pi$-conjugated oligopeptides composed of an
aromatic core flanked by oligopeptide wings offer emergent optoelectronic
properties within a water soluble and biocompatible substrate. Nanoaggregate
properties can be controlled by tuning core chemistry and peptide composition,
but the sequence-structure-function relations remain poorly characterized. In
this work, we employ coarse-grained molecular dynamics simulations within an
active learning protocol employing deep representational learning and Bayesian
optimization to efficiently identify molecules capable of assembling pseudo-1D
nanoaggregates with good stacking of the electronically-active $\pi$-cores. We
consider the DXXX-OPV3-XXXD oligopeptide family, where D is an Asp residue and
OPV3 is an oligophenylene vinylene oligomer (1,4-distyrylbenzene), to identify
the top performing XXX tripeptides within all 20$^3$ = 8,000 possible
sequences. By direct simulation of only 2.3% of this space, we identify
molecules predicted to exhibit superior assembly relative to those reported in
prior work. Spectral clustering of the top candidates reveals new design rules
governing assembly. This work establishes new understanding of DXXX-OPV3-XXXD
assembly, identifies promising new candidates for experimental testing, and
presents a computational design platform that can be generically extended to
other peptide-based and peptide-like systems.
","[{'version': 'v1', 'created': 'Mon, 27 Jan 2020 00:01:21 GMT'}]",2022-05-12,['Machine Learning'],"This paper presents a novel approach to the discovery of self-assembling $\pi$-conjugated peptides using active learning-directed coarse-grained molecular simulation. In this approach, a machine learning algorithm is used to identify the most promising peptide sequences from a pool of candidate peptides, based on their propensity to self-assemble. The identified peptides are then subjected to coarse-grained molecular simulation to determine their self-assembly behavior. The results demonstrate that the active learning-directed coarse-grained molecular simulation approach can identify self-assembling $\pi$-conjugated peptides with high efficiency. This approach has the potential to significantly reduce the time and cost associated with the discovery of self-assembling peptides, and could be extended to other peptide systems.","Write an abstract for a paper called Discovery of Self-Assembling $\pi$-Conjugated Peptides by Active
  Learning-Directed Coarse-Grained Molecular Simulation about Machine Learning"
2110.04175,"Thijs Vogels and Lie He and Anastasia Koloskova and Tao Lin and Sai
  Praneeth Karimireddy and Sebastian U. Stich and Martin Jaggi",RelaySum for Decentralized Deep Learning on Heterogeneous Data,"['cs.LG', 'cs.DC', 'math.OC', 'stat.ML']","  In decentralized machine learning, workers compute model updates on their
local data. Because the workers only communicate with few neighbors without
central coordination, these updates propagate progressively over the network.
This paradigm enables distributed training on networks without all-to-all
connectivity, helping to protect data privacy as well as to reduce the
communication cost of distributed training in data centers. A key challenge,
primarily in decentralized deep learning, remains the handling of differences
between the workers' local data distributions. To tackle this challenge, we
introduce the RelaySum mechanism for information propagation in decentralized
learning. RelaySum uses spanning trees to distribute information exactly
uniformly across all workers with finite delays depending on the distance
between nodes. In contrast, the typical gossip averaging mechanism only
distributes data uniformly asymptotically while using the same communication
volume per step as RelaySum. We prove that RelaySGD, based on this mechanism,
is independent of data heterogeneity and scales to many workers, enabling
highly accurate decentralized deep learning on heterogeneous data. Our code is
available at http://github.com/epfml/relaysgd.
","[{'version': 'v1', 'created': 'Fri, 8 Oct 2021 14:55:32 GMT'}, {'version': 'v2', 'created': 'Mon, 31 Jan 2022 13:00:46 GMT'}]",2022-02-01,"['Machine Learning', 'Distributed, Parallel, and Cluster Computing']","This paper presents RelaySum, a decentralized deep learning framework for heterogeneous data that combines machine learning, distributed, parallel, and cluster computing. RelaySum enables the distributed and parallel training of deep learning models on large-scale datasets by leveraging a set of data-partitioned nodes. It provides a distributed deep learning platform with scalability, fault tolerance, and security. The paper presents the design principles and implementation of RelaySum and evaluates its performance on a variety of datasets. We demonstrate that RelaySum outperforms existing distributed deep learning frameworks in terms of scalability, accuracy, and speed. Furthermore, we provide insights into the trade-offs between scalability, accuracy, and speed when using RelaySum.","Write an abstract for a paper called RelaySum for Decentralized Deep Learning on Heterogeneous Data about Machine Learning, Distributed, Parallel, and Cluster Computing"
2209.09078,"Shizhe Ding, Boyang Xia, Milong Ren, Dongbo Bu","NIERT: Accurate Numerical Interpolation through Unifying Scattered Data
  Representations using Transformer Encoder",['cs.LG'],"  Interpolation for scattered data is a classical problem in numerical
analysis, with a long history of theoretical and practical contributions.
Recent advances have utilized deep neural networks to construct interpolators,
exhibiting excellent and generalizable performance. However, they still fall
short in two aspects: \textbf{1) inadequate representation learning}, resulting
from separate embeddings of observed and target points in popular
encoder-decoder frameworks and \textbf{2) limited generalization power}, caused
by overlooking prior interpolation knowledge shared across different domains.
To overcome these limitations, we present a \textbf{N}umerical
\textbf{I}nterpolation approach using \textbf{E}ncoder \textbf{R}epresentation
of \textbf{T}ransformers (called \textbf{NIERT}). On one hand, NIERT utilizes
an encoder-only framework rather than the encoder-decoder structure. This way,
NIERT can embed observed and target points into a unified encoder
representation space, thus effectively exploiting the correlations among them
and obtaining more precise representations. On the other hand, we propose to
pre-train NIERT on large-scale synthetic mathematical functions to acquire
prior interpolation knowledge, and transfer it to multiple interpolation
domains with consistent performance gain. On both synthetic and real-world
datasets, NIERT outperforms the existing approaches by a large margin, i.e.,
4.3$\sim$14.3$\times$ lower MAE on TFRD subsets, and 1.7/1.8/8.7$\times$ lower
MSE on Mathit/PhysioNet/PTV datasets. The source code of NIERT is available at
https://github.com/DingShizhe/NIERT.
","[{'version': 'v1', 'created': 'Mon, 19 Sep 2022 15:12:47 GMT'}, {'version': 'v2', 'created': 'Fri, 7 Oct 2022 15:42:21 GMT'}, {'version': 'v3', 'created': 'Tue, 14 Mar 2023 15:13:06 GMT'}]",2023-03-15,['Machine Learning'],"This paper presents NIERT (Numerical Interpolation through Unifying Scattered Data Representations using Transformer Encoder), a novel machine learning architecture based on Transformer Encoder for accurate numerical interpolation of scattered data. NIERT is designed to unify multiple representations of scattered data, such as point clouds, images, and graphs, into a unified representation that can be used for interpolation tasks. We demonstrate the effectiveness of NIERT on a variety of datasets, including point clouds, images, and graphs. Results show that NIERT outperforms existing methods in terms of accuracy and speed. In addition, NIERT is able to interpolate data with missing values, providing a more robust solution for numerical interpolation tasks.","Write an abstract for a paper called NIERT: Accurate Numerical Interpolation through Unifying Scattered Data
  Representations using Transformer Encoder about Machine Learning"
2110.01123,"James Zhu, Nathan J. Kong, George Council, Aaron M. Johnson",Hybrid Event Shaping to Stabilize Periodic Hybrid Orbits,['cs.RO'],"  Many controllers for legged robotic systems leverage open- or closed-loop
control at discrete hybrid events to enhance stability. These controllers
appear in several well studied phenomena such as the Raibert stepping
controller, paddle juggling and swing leg retraction. This work introduces
hybrid event shaping (HES): a generalized method for analyzing and producing
stable hybrid event controllers. HES utilizes the saltation matrix, which gives
a closed-form equation for the effect that hybrid events have on stability. We
also introduce shape parameters, which are higher order terms that can be tuned
completely independently from the system dynamics to promote stability.
Optimization methods are used to produce values of these parameters that
optimize a stability measure. Hybrid event shaping captures previously
developed control methods while also producing new optimally stable
trajectories without the need for continuous-domain feedback.
","[{'version': 'v1', 'created': 'Sun, 3 Oct 2021 23:17:16 GMT'}, {'version': 'v2', 'created': 'Tue, 5 Oct 2021 03:49:12 GMT'}, {'version': 'v3', 'created': 'Thu, 24 Feb 2022 16:21:02 GMT'}, {'version': 'v4', 'created': 'Sun, 3 Jul 2022 14:10:46 GMT'}]",2022-07-05,['Robotics'],"This paper presents a hybrid event shaping (HES) approach for stabilizing periodic hybrid orbits in robotics. Hybrid orbits are a type of motion that combines both continuous and discrete motion. The HES approach is based on the concept of event-based control, and is combined with a Lyapunov-based stabilizing controller to achieve a periodic hybrid orbit. The HES approach is tested on a robotic manipulator in simulations and experiments, and the results demonstrate the effectiveness of the approach in stabilizing periodic hybrid orbits. This paper provides an important contribution to the field of robotics, as it presents a novel approach to addressing the challenging problem of stabilizing periodic hybrid orbits.",Write an abstract for a paper called Hybrid Event Shaping to Stabilize Periodic Hybrid Orbits about Robotics
2101.12001,"Thanasis Vergoulis, Ilias Kanellos, Claudio Atzori, Andrea Mannocci,
  Serafeim Chatzopoulos, Sandro La Bruzzo, Natalia Manola, Paolo Manghi",BIP! DB: A Dataset of Impact Measures for Scientific Publications,"['cs.DL', 'cs.IR']","  The growth rate of the number of scientific publications is constantly
increasing, creating important challenges in the identification of valuable
research and in various scholarly data management applications, in general. In
this context, measures which can effectively quantify the scientific impact
could be invaluable. In this work, we present BIP! DB, an open dataset that
contains a variety of impact measures calculated for a large collection of more
than 100 million scientific publications from various disciplines.
","[{'version': 'v1', 'created': 'Thu, 28 Jan 2021 13:59:55 GMT'}, {'version': 'v2', 'created': 'Fri, 6 May 2022 13:03:19 GMT'}]",2022-05-09,"['Digital Libraries', 'Information Retrieval']",", and Data Mining

This paper introduces BIP! DB, a new dataset of impact measures for scientific publications in the fields of digital libraries, information retrieval, and data mining. BIP! DB is the first dataset to provide comprehensive coverage of impact measures such as citations, downloads, and mentions in social media for publications in these fields. The dataset is composed of more than 3,000 publications from the past 10 years. The paper describes the methodologies used for data collection and analysis, and discusses the implications of the dataset for research and practice in the fields of digital libraries, information retrieval, and data mining. Finally, the paper presents the results of an analysis of the dataset and provides recommendations for future research.","Write an abstract for a paper called BIP! DB: A Dataset of Impact Measures for Scientific Publications about Digital Libraries, Information Retrieval"
2211.00311,"Youfang Han, Chunping Li",Entity Matching by Pool-based Active Learning,"['cs.LG', 'cs.DB']","  The goal of entity matching is to find the corresponding records representing
the same real-world entity from different data sources. At present, in the
mainstream methods, rule-based entity matching methods need tremendous domain
knowledge. The machine-learning based or deep-learning based entity matching
methods need a large number of labeled samples to build the model, which is
difficult to achieve in some applications. In addition, learning-based methods
are easy to over-fitting, so the quality requirements of training samples are
very high. In this paper, we present an active learning method ALMatcher for
the entity matching tasks. This method needs to manually label only a small
number of valuable samples, and use these samples to build a model with high
quality. This paper proposes a hybrid uncertainty as query strategy to find
those valuable samples for labeling, which can minimize the number of labeled
training samples meanwhile meet the task requirements. The proposed method has
been validated on seven data sets in different fields. The experiment shows
that ALMatcher uses only a small number of labeled samples and achieves better
results compared to existing approaches.
","[{'version': 'v1', 'created': 'Tue, 1 Nov 2022 07:31:43 GMT'}]",2022-11-02,"['Machine Learning', 'Databases']","This paper presents a novel approach to entity matching using pool-based active learning. Entity matching is a challenging task in the realm of machine learning and databases, which involves finding similar or related entities from different data sources. The proposed method is based on a pool-based active learning framework that combines traditional supervised learning with active learning techniques. The method is evaluated on a real-world dataset from the restaurant domain. Results show that the proposed approach improves the accuracy of entity matching compared to the traditional supervised learning approach. Furthermore, the proposed method is also computationally efficient and requires less labeled data than traditional methods. The paper provides a comprehensive analysis of the proposed approach and its performance, and discusses the potential applications of the proposed method in related fields.","Write an abstract for a paper called Entity Matching by Pool-based Active Learning about Machine Learning, Databases"
2207.03571,"Alain Raymond-Saez, Julio Hurtado, Alvaro Soto",A Study on the Predictability of Sample Learning Consistency,"['cs.LG', 'cs.AI']","  Curriculum Learning is a powerful training method that allows for faster and
better training in some settings. This method, however, requires having a
notion of which examples are difficult and which are easy, which is not always
trivial to provide. A recent metric called C-Score acts as a proxy for example
difficulty by relating it to learning consistency. Unfortunately, this method
is quite compute intensive which limits its applicability for alternative
datasets. In this work, we train models through different methods to predict
C-Score for CIFAR-100 and CIFAR-10. We find, however, that these models
generalize poorly both within the same distribution as well as out of
distribution. This suggests that C-Score is not defined by the individual
characteristics of each sample but rather by other factors. We hypothesize that
a sample's relation to its neighbours, in particular, how many of them share
the same labels, can help in explaining C-Scores. We plan to explore this in
future work.
","[{'version': 'v1', 'created': 'Thu, 7 Jul 2022 21:05:53 GMT'}]",2022-07-11,"['Machine Learning', 'Artificial Intelligence']","This paper examines the predictability of sample learning consistency in the context of machine learning and artificial intelligence. It looks at the various techniques used to determine the consistency of learning samples, such as cross-validation and bootstrapping, and how these techniques can be used to measure the effectiveness of machine learning and AI models. The paper also discusses the challenges of measuring the predictability of sample learning consistency and how to overcome them. Finally, the paper provides an analysis of the results obtained from the study and the implications for machine learning and AI.","Write an abstract for a paper called A Study on the Predictability of Sample Learning Consistency about Machine Learning, Artificial Intelligence"
2302.09018,"Yujie Zhou, Haodong Duan, Anyi Rao, Bing Su, Jiaqi Wang","Self-supervised Action Representation Learning from Partial
  Spatio-Temporal Skeleton Sequences",['cs.CV'],"  Self-supervised learning has demonstrated remarkable capability in
representation learning for skeleton-based action recognition. Existing methods
mainly focus on applying global data augmentation to generate different views
of the skeleton sequence for contrastive learning. However, due to the rich
action clues in the skeleton sequences, existing methods may only take a global
perspective to learn to discriminate different skeletons without thoroughly
leveraging the local relationship between different skeleton joints and video
frames, which is essential for real-world applications. In this work, we
propose a Partial Spatio-Temporal Learning (PSTL) framework to exploit the
local relationship from a partial skeleton sequences built by a unique
spatio-temporal masking strategy. Specifically, we construct a
negative-sample-free triplet steam structure that is composed of an anchor
stream without any masking, a spatial masking stream with Central Spatial
Masking (CSM), and a temporal masking stream with Motion Attention Temporal
Masking (MATM). The feature cross-correlation matrix is measured between the
anchor stream and the other two masking streams, respectively. (1) Central
Spatial Masking discards selected joints from the feature calculation process,
where the joints with a higher degree of centrality have a higher possibility
of being selected. (2) Motion Attention Temporal Masking leverages the motion
of action and remove frames that move faster with a higher possibility. Our
method achieves SOTA performance on NTU-60, NTU-120 and PKU-MMD under various
downstream tasks. A practical evaluation is performed where some skeleton
joints are lost in downstream tasks. In contrast to previous methods that
suffer from large performance drops, our PSTL can still achieve remarkable
results, validating the robustness of our method. Code:
https://github.com/YujieOuO/PSTL.git.
","[{'version': 'v1', 'created': 'Fri, 17 Feb 2023 17:35:05 GMT'}, {'version': 'v2', 'created': 'Wed, 22 Feb 2023 07:47:39 GMT'}]",2023-02-23,['Computer Vision and Pattern Recognition'],"This paper presents a self-supervised action representation learning method from partial spatio-temporal skeleton sequences. The proposed method utilizes a self-supervised learning approach to learn action representations from partial skeleton sequences with missing joints. The learned action representations are then used for action recognition tasks. Experiments are conducted on two public datasets, NTU RGB+D and Human 3.6M, to evaluate the effectiveness of the proposed method. Results show that the proposed method outperforms state-of-the-art methods in action recognition tasks.","Write an abstract for a paper called Self-supervised Action Representation Learning from Partial
  Spatio-Temporal Skeleton Sequences about Computer Vision and Pattern Recognition"
2211.14122,"Chaojun Chen, Khashayar Namdar, Yujie Wu, Shahob Hosseinpour, Manohar
  Shroff, Andrea S. Doria, Farzad Khalvati","Automating Cobb Angle Measurement for Adolescent Idiopathic Scoliosis
  using Instance Segmentation","['eess.IV', 'cs.CV', 'cs.LG']","  Scoliosis is a three-dimensional deformity of the spine, most often diagnosed
in childhood. It affects 2-3% of the population, which is approximately seven
million people in North America. Currently, the reference standard for
assessing scoliosis is based on the manual assignment of Cobb angles at the
site of the curvature center. This manual process is time consuming and
unreliable as it is affected by inter- and intra-observer variance. To overcome
these inaccuracies, machine learning (ML) methods can be used to automate the
Cobb angle measurement process. This paper proposes to address the Cobb angle
measurement task using YOLACT, an instance segmentation model. The proposed
method first segments the vertebrae in an X-Ray image using YOLACT, then it
tracks the important landmarks using the minimum bounding box approach. Lastly,
the extracted landmarks are used to calculate the corresponding Cobb angles.
The model achieved a Symmetric Mean Absolute Percentage Error (SMAPE) score of
10.76%, demonstrating the reliability of this process in both vertebra
localization and Cobb angle measurement.
","[{'version': 'v1', 'created': 'Fri, 25 Nov 2022 14:04:06 GMT'}]",2022-11-28,"['Computer Vision and Pattern Recognition', 'Machine Learning']","This paper presents a novel approach to automated Cobb angle measurement for adolescent idiopathic scoliosis (AIS) using instance segmentation in computer vision and pattern recognition, and machine learning. The proposed approach is based on a convolutional neural network (CNN) trained with manually labeled images. The CNN model is used to segment the vertebrae from the X-ray images, and then the Cobb angle is calculated based on the segmented vertebrae. The results demonstrate that the proposed approach is able to accurately measure the Cobb angle with a mean absolute error of 0.7° and a standard deviation of 0.4°. Furthermore, the proposed approach is able to reduce the measurement time from 20 minutes to 5 minutes. This paper provides a potential solution for the automated measurement of the Cobb angle for AIS, which can help to improve the diagnosis and treatment of AIS patients.","Write an abstract for a paper called Automating Cobb Angle Measurement for Adolescent Idiopathic Scoliosis
  using Instance Segmentation about Computer Vision and Pattern Recognition, Machine Learning"
2210.16349,"Katherine Baker, Lehel Banjai, Mariya Ptashnyk","Numerical analysis of a time-stepping method for the Westervelt equation
  with time-fractional damping","['math.NA', 'cs.NA', 'math.AP']","  We develop a numerical method for the Westervelt equation, an important
equation in nonlinear acoustics, in the form where the attenuation is
represented by a class of non-local in time operators. A semi-discretisation in
time based on the trapezoidal rule and A-stable convolution quadrature is
stated and analysed. Existence and regularity analysis of the continuous
equations informs the stability and error analysis of the semi-discrete system.
The error analysis includes the consideration of the singularity at $t = 0$
which is addressed by the use of a correction in the numerical scheme.
Extensive numerical experiments confirm the theory.
","[{'version': 'v1', 'created': 'Fri, 28 Oct 2022 18:24:13 GMT'}]",2022-11-01,['Numerical Analysis'],"This paper presents a numerical analysis of a time-stepping method for the Westervelt equation with time-fractional damping. The numerical method is based on a time-splitting approach, which is a widely used technique for solving time-dependent equations. The numerical analysis of the time-stepping method is conducted by examining the stability, accuracy, and convergence of the numerical solution. The numerical results are compared to the exact solution, and the accuracy of the numerical solution is evaluated. The numerical results show that the time-stepping method is accurate and stable for a wide range of parameters. The results of this study provide insight into the numerical solution of the Westervelt equation with time-fractional damping.","Write an abstract for a paper called Numerical analysis of a time-stepping method for the Westervelt equation
  with time-fractional damping about Numerical Analysis"
2208.02991,"Sandipan Banerjee, Walter Scheirer, Kevin Bowyer, Patrick Flynn","Analyzing the Impact of Shape & Context on the Face Recognition
  Performance of Deep Networks",['cs.CV'],"  In this article, we analyze how changing the underlying 3D shape of the base
identity in face images can distort their overall appearance, especially from
the perspective of deep face recognition. As done in popular training data
augmentation schemes, we graphically render real and synthetic face images with
randomly chosen or best-fitting 3D face models to generate novel views of the
base identity. We compare deep features generated from these images to assess
the perturbation these renderings introduce into the original identity. We
perform this analysis at various degrees of facial yaw with the base identities
varying in gender and ethnicity. Additionally, we investigate if adding some
form of context and background pixels in these rendered images, when used as
training data, further improves the downstream performance of a face
recognition model. Our experiments demonstrate the significance of facial shape
in accurate face matching and underpin the importance of contextual data for
network training.
","[{'version': 'v1', 'created': 'Fri, 5 Aug 2022 05:32:07 GMT'}]",2022-08-08,['Computer Vision and Pattern Recognition'],"This paper presents a comprehensive analysis of the impact of shape and context on the performance of deep networks for face recognition. The research focuses on the effect of various shape and context features on the accuracy of deep networks for face recognition tasks. The research is based on a deep learning model that has been trained on a large dataset of face images. The performance of the deep networks is evaluated on a test set of face images. In addition, the paper discusses the potential of shape and context features for improving the accuracy of deep networks for face recognition. Finally, the results of the study are discussed and the implications of the findings are presented.","Write an abstract for a paper called Analyzing the Impact of Shape & Context on the Face Recognition
  Performance of Deep Networks about Computer Vision and Pattern Recognition"
2303.10396,"Xiaoqi Zhao, Youwei Pang, Lihe Zhang, Huchuan Lu, Lei Zhang","Towards Diverse Binary Segmentation via A Simple yet General Gated
  Network",['cs.CV'],"  In many binary segmentation tasks, most CNNs-based methods use a U-shape
encoder-decoder network as their basic structure. They ignore two key problems
when the encoder exchanges information with the decoder: one is the lack of
interference control mechanism between them, the other is without considering
the disparity of the contributions from different encoder levels. In this work,
we propose a simple yet general gated network (GateNet) to tackle them all at
once. With the help of multi-level gate units, the valuable context information
from the encoder can be selectively transmitted to the decoder. In addition, we
design a gated dual branch structure to build the cooperation among the
features of different levels and improve the discrimination ability of the
network. Furthermore, we introduce a ``Fold'' operation to improve the atrous
convolution and form a novel folded atrous convolution, which can be flexibly
embedded in ASPP or DenseASPP to accurately localize foreground objects of
various scales. GateNet can be easily generalized to many binary segmentation
tasks, including general and specific object segmentation and multi-modal
segmentation. Without bells and whistles, our network consistently performs
favorably against the state-of-the-art methods under 10 metrics on 33 datasets
of 10 binary segmentation tasks.
","[{'version': 'v1', 'created': 'Sat, 18 Mar 2023 11:26:36 GMT'}]",2023-03-21,['Computer Vision and Pattern Recognition'],"This paper presents a novel gated network for binary segmentation in computer vision and pattern recognition. The proposed network is simple yet general, and can be used to achieve diverse segmentation results. The network consists of a combination of convolutional layers, batch normalization layers, and a gated convolutional layer. Experiments on two different datasets demonstrate the effectiveness of the proposed network in producing accurate segmentation results. Furthermore, the network is able to generate diverse segmentation results by varying the gated convolutional layer parameters. The results show that the proposed network is a promising approach for binary segmentation tasks.","Write an abstract for a paper called Towards Diverse Binary Segmentation via A Simple yet General Gated
  Network about Computer Vision and Pattern Recognition"
2303.00573,"Yani Feng, Kejun Tang, Xiaoliang Wan, Qifeng Liao","Dimension-reduced KRnet maps for high-dimensional Bayesian inverse
  problems","['stat.ML', 'cs.LG']","  We present a dimension-reduced KRnet map approach (DR-KRnet) for
high-dimensional Bayesian inverse problems, which is based on an explicit
construction of a map that pushes forward the prior measure to the posterior
measure in the latent space. Our approach consists of two main components:
data-driven VAE prior and density approximation of the posterior of the latent
variable. In reality, it may not be trivial to initialize a prior distribution
that is consistent with available prior data; in other words, the complex prior
information is often beyond simple hand-crafted priors. We employ variational
autoencoder (VAE) to approximate the underlying distribution of the prior
dataset, which is achieved through a latent variable and a decoder. Using the
decoder provided by the VAE prior, we reformulate the problem in a
low-dimensional latent space. In particular, we seek an invertible transport
map given by KRnet to approximate the posterior distribution of the latent
variable. Moreover, an efficient physics-constrained surrogate model without
any labeled data is constructed to reduce the computational cost of solving
both forward and adjoint problems involved in likelihood computation. With
numerical experiments, we demonstrate the accuracy and efficiency of DR-KRnet
for high-dimensional Bayesian inverse problems.
","[{'version': 'v1', 'created': 'Wed, 1 Mar 2023 15:16:27 GMT'}, {'version': 'v2', 'created': 'Wed, 8 Mar 2023 09:03:51 GMT'}]",2023-03-09,['Machine Learning'],"This paper presents a novel dimension-reduced KRnet (Kernel Regularized Neural Network) map for high-dimensional Bayesian inverse problems in Machine Learning. The proposed map is based on a combination of kernel regularized neural networks and dimensionality reduction techniques. The proposed map is used to efficiently estimate the posterior distribution of high-dimensional Bayesian inverse problems. The proposed map is evaluated on a variety of benchmark datasets and compared to existing state-of-the-art methods. Results show that the proposed map performs better than existing methods in terms of accuracy and computational efficiency. The paper also discusses the advantages and limitations of the proposed map, as well as potential applications.","Write an abstract for a paper called Dimension-reduced KRnet maps for high-dimensional Bayesian inverse
  problems about Machine Learning"
2108.06779,Wenbo Wang and Amir Leshem,"Non-convex Generalized Nash Games for Energy Efficient Power Allocation
  and Beamforming in mmWave Networks",['cs.NI'],"  Network management is a fundamental ingredient for efficient operation of
wireless networks. With increasing bandwidth, number of antennas and number of
users, the amount of information required for network management increases
significantly. Therefore, distributed network management is a key to efficient
operation of future networks. This paper focuses on the problem of distributed
joint beamforming control and power allocation in ad-hoc mmWave networks. Over
the shared spectrum, a number of multi-input-multi-output links attempt to
minimize their supply power by simultaneously finding the locally optimal power
allocation and beamformers in a self-organized manner. Our design considers a
family of non-convex quality-of-service constraint and utility functions
characterized by monotonicity in the strategies of the various users. We
propose a two-stage, decentralized optimization scheme, where the adaptation of
power levels and beamformer coefficients are iteratively performed by each
link. We first prove that given a set of receive beamformers, the power
allocation stage converges to an optimal generalized Nash equilibrium of the
generalized power allocation game. Then we prove that iterative
minimum-mean-square-error adaptation of the receive beamformer results in an
overall converging scheme. Several transmit beamforming schemes requiring
different levels of information exchange are also compared in the proposed
allocation framework. Our simulation results show that allowing each link to
optimize its transmit filters using the direct channel results in a near
optimum performance with very low computational complexity, even though the
problem is highly non-convex.
","[{'version': 'v1', 'created': 'Sun, 15 Aug 2021 17:06:04 GMT'}, {'version': 'v2', 'created': 'Tue, 31 Aug 2021 15:08:56 GMT'}, {'version': 'v3', 'created': 'Wed, 19 Jan 2022 18:26:00 GMT'}, {'version': 'v4', 'created': 'Mon, 30 May 2022 18:56:21 GMT'}]",2022-07-20,['Networking and Internet Architecture'],"This paper presents a novel non-convex generalized Nash game formulation for energy efficient power allocation and beamforming in mmWave networks. The proposed formulation is based on a three-dimensional (3D) joint optimization of power allocation and beamforming, which is formulated as a non-cooperative game among the transmitters in the network. We propose a distributed algorithm to solve this game, which is based on a projected subgradient method. Simulation results demonstrate that the proposed approach outperforms existing convex formulations in terms of energy efficiency and beamforming performance. Moreover, the proposed approach is shown to converge to a stable equilibrium in a distributed manner.","Write an abstract for a paper called Non-convex Generalized Nash Games for Energy Efficient Power Allocation
  and Beamforming in mmWave Networks about Networking and Internet Architecture"
2206.00285,"Abdurakhmon Sadiev, Aleksandr Beznosikov, Abdulla Jasem Almansoori,
  Dmitry Kamzolov, Rachael Tappenden, Martin Tak\'a\v{c}",Stochastic Gradient Methods with Preconditioned Updates,"['math.OC', 'cs.LG']","  This work considers non-convex finite sum minimization. There are a number of
algorithms for such problems, but existing methods often work poorly when the
problem is badly scaled and/or ill-conditioned, and a primary goal of this work
is to introduce methods that alleviate this issue. Thus, here we include a
preconditioner that is based upon Hutchinson's approach to approximating the
diagonal of the Hessian, and couple it with several gradient based methods to
give new `scaled' algorithms: {\tt Scaled SARAH} and {\tt Scaled L-SVRG}.
Theoretical complexity guarantees under smoothness assumptions are presented,
and we prove linear convergence when both smoothness and the PL-condition is
assumed. Because our adaptively scaled methods use approximate partial second
order curvature information, they are better able to mitigate the impact of
badly scaled problems, and this improved practical performance is demonstrated
in the numerical experiments that are also presented in this work.
","[{'version': 'v1', 'created': 'Wed, 1 Jun 2022 07:38:08 GMT'}]",2022-06-02,['Machine Learning'],"This paper focuses on the use of stochastic gradient methods with preconditioned updates for machine learning applications. We introduce a new preconditioned update algorithm for stochastic gradient descent and discuss its performance on a variety of machine learning tasks. We discuss the advantages of using preconditioned updates over traditional stochastic gradient descent and analyze the effects of the proposed preconditioned updates on the convergence rate of the algorithm. We then present experimental results that demonstrate the effectiveness of the proposed algorithm on a range of machine learning tasks. Finally, we discuss the implications of our results and provide insight into the potential of preconditioned updates for machine learning applications.",Write an abstract for a paper called Stochastic Gradient Methods with Preconditioned Updates about Machine Learning
2209.02064,Adel Javanmard and Mohammad Mehrabi,GRASP: A Goodness-of-Fit Test for Classification Learning,"['stat.ME', 'cs.LG', 'math.ST', 'stat.ML', 'stat.TH']","  Performance of classifiers is often measured in terms of average accuracy on
test data. Despite being a standard measure, average accuracy fails in
characterizing the fit of the model to the underlying conditional law of labels
given the features vector ($Y|X$), e.g. due to model misspecification, over
fitting, and high-dimensionality. In this paper, we consider the fundamental
problem of assessing the goodness-of-fit for a general binary classifier. Our
framework does not make any parametric assumption on the conditional law $Y|X$,
and treats that as a black box oracle model which can be accessed only through
queries. We formulate the goodness-of-fit assessment problem as a tolerance
hypothesis testing of the form \[ H_0: \mathbb{E}\Big[D_f\Big({\sf
Bern}(\eta(X))\|{\sf Bern}(\hat{\eta}(X))\Big)\Big]\leq \tau\,, \] where $D_f$
represents an $f$-divergence function, and $\eta(x)$, $\hat{\eta}(x)$
respectively denote the true and an estimate likelihood for a feature vector
$x$ admitting a positive label. We propose a novel test, called \grasp for
testing $H_0$, which works in finite sample settings, no matter the features
(distribution-free). We also propose model-X \grasp designed for model-X
settings where the joint distribution of the features vector is known. Model-X
\grasp uses this distributional information to achieve better power. We
evaluate the performance of our tests through extensive numerical experiments.
","[{'version': 'v1', 'created': 'Mon, 5 Sep 2022 17:18:43 GMT'}]",2022-09-07,['Machine Learning'],"This paper introduces GRASP, a novel goodness-of-fit test for classification learning about machine learning. GRASP is designed to evaluate the performance of a machine learning model on a given dataset. It uses a combination of measures including accuracy, precision, recall, AUC, and F1 score to determine the model's goodness-of-fit. Additionally, GRASP is capable of detecting overfitting and underfitting in the model and provides a visualization of the model's performance. The paper also presents a case study of GRASP on a real-world dataset to demonstrate its effectiveness. The results show that GRASP is an effective tool for assessing the performance of a machine learning model.",Write an abstract for a paper called GRASP: A Goodness-of-Fit Test for Classification Learning about Machine Learning
2211.07389,"Andrea Martin, Luca Furieri, Florian D\""orfler, John Lygeros,
  Giancarlo Ferrari-Trecate","Follow the Clairvoyant: an Imitation Learning Approach to Optimal
  Control","['eess.SY', 'cs.LG', 'cs.SY']","  We consider control of dynamical systems through the lens of competitive
analysis. Most prior work in this area focuses on minimizing regret, that is,
the loss relative to an ideal clairvoyant policy that has noncausal access to
past, present, and future disturbances. Motivated by the observation that the
optimal cost only provides coarse information about the ideal closed-loop
behavior, we instead propose directly minimizing the tracking error relative to
the optimal trajectories in hindsight, i.e., imitating the clairvoyant policy.
By embracing a system level perspective, we present an efficient
optimization-based approach for computing follow-the-clairvoyant (FTC) safe
controllers. We prove that these attain minimal regret if no constraints are
imposed on the noncausal benchmark. In addition, we present numerical
experiments to show that our policy retains the hallmark of competitive
algorithms of interpolating between classical $\mathcal{H}_2$ and
$\mathcal{H}_\infty$ control laws - while consistently outperforming regret
minimization methods in constrained scenarios thanks to the superior ability to
chase the clairvoyant.
","[{'version': 'v1', 'created': 'Mon, 14 Nov 2022 14:15:12 GMT'}]",2022-11-15,"['Machine Learning', 'Systems and Control']",This paper presents a novel imitation learning approach to optimal control for machine learning systems and control. We propose a method which combines a clairvoyant controller with a reinforcement learning agent. The clairvoyant controller is used to generate a set of optimal trajectories for the agent to imitate. The agent is trained using a combination of supervised and reinforcement learning techniques to learn the optimal control policy. We evaluate our method on a simulated robotic arm environment and demonstrate that our approach outperforms the baseline reinforcement learning approach. The results show that our proposed method is able to learn an optimal control policy with significantly less training data and computational resources. Our results suggest that our imitation learning approach is a promising approach to optimal control for machine learning systems and control.,"Write an abstract for a paper called Follow the Clairvoyant: an Imitation Learning Approach to Optimal
  Control about Machine Learning, Systems and Control"
2204.12155,Danny Wood and Tingting Mu and Gavin Brown,Bias-Variance Decompositions for Margin Losses,"['stat.ML', 'cs.LG']","  We introduce a novel bias-variance decomposition for a range of strictly
convex margin losses, including the logistic loss (minimized by the classic
LogitBoost algorithm), as well as the squared margin loss and canonical
boosting loss. Furthermore, we show that, for all strictly convex margin
losses, the expected risk decomposes into the risk of a ""central"" model and a
term quantifying variation in the functional margin with respect to variations
in the training data. These decompositions provide a diagnostic tool for
practitioners to understand model overfitting/underfitting, and have
implications for additive ensemble models -- for example, when our
bias-variance decomposition holds, there is a corresponding ""ambiguity""
decomposition, which can be used to quantify model diversity.
","[{'version': 'v1', 'created': 'Tue, 26 Apr 2022 08:45:43 GMT'}]",2022-04-27,['Machine Learning'],"This paper presents a novel approach to machine learning which decomposes the bias-variance trade-off for margin losses. We introduce the concept of bias-variance decompositions for margin losses and discuss their implications for model selection, training, and evaluation. We then discuss several methods for computing bias-variance decompositions for margin losses, including the use of bootstrapping and cross-validation. We also provide a detailed comparison of these methods and their respective strengths and weaknesses. Finally, we discuss the potential applications of these decompositions, such as improving model selection and evaluation, and compare the performance of our approach to existing methods.",Write an abstract for a paper called Bias-Variance Decompositions for Margin Losses about Machine Learning
2205.14056,"Site Bai, Chuyang Ke, Jean Honorio",Dual Convexified Convolutional Neural Networks,"['cs.LG', 'stat.ML']","  We propose the framework of dual convexified convolutional neural networks
(DCCNNs). In this framework, we first introduce a primal learning problem
motivated by convexified convolutional neural networks (CCNNs), and then
construct the dual convex training program through careful analysis of the
Karush-Kuhn-Tucker (KKT) conditions and Fenchel conjugates. Our approach
reduces the computational overhead of constructing a large kernel matrix and
more importantly, eliminates the ambiguity of factorizing the matrix. Due to
the low-rank structure in CCNNs and the related subdifferential of nuclear
norms, there is no closed-form expression to recover the primal solution from
the dual solution. To overcome this, we propose a highly novel weight recovery
algorithm, which takes the dual solution and the kernel information as the
input, and recovers the linear weight and the output of convolutional layer,
instead of weight parameter. Furthermore, our recovery algorithm exploits the
low-rank structure and imposes a small number of filters indirectly, which
reduces the parameter size. As a result, DCCNNs inherit all the statistical
benefits of CCNNs, while enjoying a more formal and efficient workflow.
","[{'version': 'v1', 'created': 'Fri, 27 May 2022 15:45:08 GMT'}, {'version': 'v2', 'created': 'Thu, 8 Dec 2022 03:58:46 GMT'}]",2022-12-09,['Machine Learning'],This paper presents a novel approach to machine learning called Dual Convexified Convolutional Neural Networks (DC-CNN). DC-CNN is a combination of two well-known machine learning techniques: convolutional neural networks (CNN) and convex optimization. The paper demonstrates how DC-CNN can be used to improve the accuracy of machine learning models while also reducing computational complexity. Experiments are conducted on a variety of datasets to demonstrate the effectiveness of DC-CNN. Results show that DC-CNN can achieve better accuracy and faster training time than existing methods. The paper also provides a discussion of the advantages and disadvantages of DC-CNN compared to existing methods.,Write an abstract for a paper called Dual Convexified Convolutional Neural Networks about Machine Learning
2202.07931,"Guochen Yu, Andong Li, Hui Wang, Yutian Wang, Yuxuan Ke, and Chengshi
  Zheng","DBT-Net: Dual-branch federative magnitude and phase estimation with
  attention-in-attention transformer for monaural speech enhancement","['cs.SD', 'eess.AS']","  The decoupling-style concept begins to ignite in the speech enhancement area,
which decouples the original complex spectrum estimation task into multiple
easier sub-tasks i.e., magnitude-only recovery and the residual complex
spectrum estimation)}, resulting in better performance and easier
interpretability. In this paper, we propose a dual-branch federative magnitude
and phase estimation framework, dubbed DBT-Net, for monaural speech
enhancement, aiming at recovering the coarse- and fine-grained regions of the
overall spectrum in parallel. From the complementary perspective, the magnitude
estimation branch is designed to filter out dominant noise components in the
magnitude domain, while the complex spectrum purification branch is elaborately
designed to inpaint the missing spectral details and implicitly estimate the
phase information in the complex-valued spectral domain. To facilitate the
information flow between each branch, interaction modules are introduced to
leverage features learned from one branch, so as to suppress the undesired
parts and recover the missing components of the other branch. Instead of
adopting the conventional RNNs and temporal convolutional networks for sequence
modeling, we employ a novel attention-in-attention transformer-based network
within each branch for better feature learning. More specially, it is composed
of several adaptive spectro-temporal attention transformer-based modules and an
adaptive hierarchical attention module, aiming to capture long-term
time-frequency dependencies and further aggregate intermediate hierarchical
contextual information. Comprehensive evaluations on the WSJ0-SI84 +
DNS-Challenge and VoiceBank + DEMAND dataset demonstrate that the proposed
approach consistently outperforms previous advanced systems and yields
state-of-the-art performance in terms of speech quality and intelligibility.
","[{'version': 'v1', 'created': 'Wed, 16 Feb 2022 08:44:38 GMT'}, {'version': 'v2', 'created': 'Sat, 30 Jul 2022 12:27:38 GMT'}]",2022-08-02,['Sound'],"In this paper, we propose DBT-Net, a Dual-branch Federative Magnitude and Phase Estimation with Attention-in-Attention Transformer for monaural speech enhancement. DBT-Net combines the federative magnitude and phase estimation (FMPE) and attention-in-attention (AIA) transformer to extract the magnitude and phase of the sound signal, and then reconstruct a clean speech signal by combining both. The FMPE module can extract the magnitude and phase of the sound signal from the noisy input, while the AIA transformer can further refine the magnitude and phase estimation. Experiments on the TIMIT and CHiME-4 datasets show that DBT-Net outperforms the state-of-the-art speech enhancement methods in terms of speech quality and intelligibility.","Write an abstract for a paper called DBT-Net: Dual-branch federative magnitude and phase estimation with
  attention-in-attention transformer for monaural speech enhancement about Sound"
2111.10915,Buddhika Jayawardana and Tomoki Ohsawa,"Semiexplicit Symplectic Integrators for Non-separable Hamiltonian
  Systems","['math.NA', 'cs.NA', 'math.DS', 'physics.comp-ph']","  We construct a symplectic integrator for non-separable Hamiltonian systems
combining an extended phase space approach of Pihajoki and the symmetric
projection method. The resulting method is semiexplicit in the sense that the
main time evolution step is explicit whereas the symmetric projection step is
implicit. The symmetric projection binds potentially diverging copies of
solutions, thereby remedying the main drawback of the extended phase space
approach. Moreover, our semiexplicit method is symplectic in the original phase
space. This is in contrast to existing extended phase space integrators, which
are symplectic only in the extended phase space. We demonstrate that our method
exhibits an excellent long-time preservation of invariants, and also that it
tends to be as fast as and can be faster than Tao's explicit modified extended
phase space integrator particularly for small enough time steps and with
higher-order implementations and for higher-dimensional problems.
","[{'version': 'v1', 'created': 'Sun, 21 Nov 2021 22:50:52 GMT'}, {'version': 'v2', 'created': 'Thu, 23 Mar 2023 07:07:12 GMT'}]",2023-03-24,['Numerical Analysis'],This paper presents a new semiexplicit symplectic integrator for non-separable Hamiltonian systems. The proposed integrator is based on a modified Lie-Trotter splitting method. It is demonstrated that the proposed integrator can achieve high order accuracy and preserve the symplectic structure of the Hamiltonian system. Numerical experiments are conducted to verify the effectiveness and accuracy of the proposed integrator. Results show that the proposed integrator is capable of accurately and efficiently solving non-separable Hamiltonian systems.,"Write an abstract for a paper called Semiexplicit Symplectic Integrators for Non-separable Hamiltonian
  Systems about Numerical Analysis"
2302.01977,"Yotam Yaniv, Osman Asif Malik, Pieter Ghysels, Xiaoye S. Li","Construction of Hierarchically Semi-Separable matrix Representation
  using Adaptive Johnson-Lindenstrauss Sketching","['math.NA', 'cs.NA']","  We extend an adaptive partially matrix-free Hierarchically Semi-Separable
(HSS) matrix construction algorithm by Gorman et al. [SIAM J. Sci. Comput.
41(5), 2019] which uses Gaussian sketching operators to a broader class of
Johnson--Lindenstrauss (JL) sketching operators. We present theoretical work
which justifies this extension. In particular, we extend the earlier
concentration bounds to all JL sketching operators and examine this bound for
specific classes of such operators including the original Gaussian sketching
operators, subsampled randomized Hadamard transform (SRHT) and the sparse
Johnson--Lindenstrauss transform (SJLT). We discuss the implementation details
of applying SJLT efficiently and demonstrate experimentally that using SJLT
instead of Gaussian sketching operators leads to 1.5--2.5x speedups of the HSS
construction implementation in the STRUMPACK C++ library. The generalized
algorithm allows users to select their own JL sketching operators with
theoretical lower bounds on the size of the operators which may lead to faster
run time with similar HSS construction accuracy.
","[{'version': 'v1', 'created': 'Fri, 3 Feb 2023 19:53:45 GMT'}]",2023-02-07,['Numerical Analysis'],"This paper presents an efficient and robust method for constructing a hierarchically semi-separable (HSS) matrix representation using adaptive Johnson-Lindenstrauss sketching for numerical analysis. The proposed method is based on a combination of the Johnson-Lindenstrauss lemma and a novel adaptive sketching technique. The proposed method is able to construct a HSS matrix representation that is well suited for numerical analysis, while preserving the numerical properties of the original matrix. The proposed method is tested on various numerical problems, including linear systems, eigenvalue problems, and singular value decomposition. The numerical results show that the proposed method is more accurate and efficient than existing methods.","Write an abstract for a paper called Construction of Hierarchically Semi-Separable matrix Representation
  using Adaptive Johnson-Lindenstrauss Sketching about Numerical Analysis"
2303.09613,J. S. C. Prentice,"Stepwise global error control in Euler's method using the DP853 triple
  and the Taylor remainder term","['math.NA', 'cs.NA']","  We report on a novel algorithm for controlling global error in a step-by-step
(stepwise) sense, in the numerical solution of a scalar, autonomous, nonstiff
or weakly stiff problem. The algorithm exploits the remainder term of a Taylor
expansion of the solution. It requires the use of the DP853 triple to solve an
auxiliary problem which, in turn, enables the remainder term to be determined.
A quenching process then allows the solution generated by Euler's method to be
controlled. We have achieved tolerances on the relative global error as strict
as 1e-10.
","[{'version': 'v1', 'created': 'Thu, 16 Mar 2023 19:39:03 GMT'}]",2023-03-20,['Numerical Analysis'],This paper examines the use of stepwise global error control in Euler's method using the DP853 triple and the Taylor remainder term in numerical analysis. The DP853 triple is a three-term recurrence formula for computing the numerical derivatives of arbitrary order of a function. This paper will analyze the accuracy of Euler's method when combined with the DP853 triple and the Taylor remainder term. The accuracy of the numerical solution will be measured in terms of the global error. The results of this paper will provide insight into the effectiveness of the combination of the DP853 triple and the Taylor remainder term in controlling the global error in Euler's method.,"Write an abstract for a paper called Stepwise global error control in Euler's method using the DP853 triple
  and the Taylor remainder term about Numerical Analysis"
2212.05537,"Zengyang Li, Yilin Peng, Peng Liang, Apostolos Ampatzoglou, Ran Mo,
  Hui Liu, Xiaoxiao Qi",Technical Debt Management in OSS Projects: An Empirical Study on GitHub,['cs.SE'],"  Technical debt (TD) refers to delayed tasks and immature artifacts that may
bring short-term benefits but incur extra costs of change during maintenance
and evolution in the long term. TD has been extensively studied in the past
decade, and numerous open source software (OSS) projects were used to explore
specific aspects of TD and validate various approaches for TD management (TDM).
However, there still lacks a comprehensive understanding on the practice of TDM
in OSS development, which penetrates the OSS community's perception of the TD
concept and how TD is managed in OSS development. To this end, we conducted an
empirical study on the whole GitHub to explore the adoption and execution of
TDM based on issues in OSS projects. We collected 35,278 issues labeled as TD
(TD issues) distributed over 3,598 repositories in total from the issue
tracking system of GitHub between 2009 and 2020. The findings are that: (1) the
OSS community is embracing the TD concept; (2) the analysis of TD instances
shows that TD may affect both internal and external quality of software
systems; (3) only one TD issue was identified in 31.1% of the repositories and
all TD issues were identified by only one developer in 69.0% of the
repositories; (4) TDM was ignored in 27.3% of the repositories after TD issues
were identified; and (5) among the repositories with TD labels, 32.9% have
abandoned TDM while only 8.2% adopt TDM as a consistent practice. These
findings provide valuable insights for practitioners in TDM and promising
research directions for further investigation.
","[{'version': 'v1', 'created': 'Sun, 11 Dec 2022 16:31:39 GMT'}]",2022-12-13,['Software Engineering'],"This paper presents an empirical study on the management of technical debt in open source software (OSS) projects hosted on GitHub. The study focuses on the identification of the main factors that influence the management of technical debt in OSS projects. The study is based on an exploratory analysis of the main technical debt management practices used by OSS projects. The analysis is based on a large dataset of OSS projects hosted on GitHub, with the objective of understanding the technical debt management practices used by OSS projects and their impact on the quality of the software. The results of the study show that OSS projects use various technical debt management practices and that the use of these practices has a significant impact on the quality of the software. The results of the study also provide insights into the factors that influence the management of technical debt in OSS projects.",Write an abstract for a paper called Technical Debt Management in OSS Projects: An Empirical Study on GitHub about Software Engineering
2208.00354,"Lei Huang, Jiawang Nie, Ya-Xiang Yuan",Generalized truncated moment problems with unbounded sets,"['math.NA', 'cs.NA', 'math.OC']","  This paper studies generalized truncated moment problems with unbounded sets.
First, we study geometric properties of the truncated moment cone and its dual
cone of nonnegative polynomials. By the technique of homogenization, we give a
convergent hierarchy of Moment-SOS relaxations for approximating these cones.
With them, we give a Moment-SOS method for solving generalized truncated moment
problems with unbounded sets. Finitely atomic representing measures, or
certificates for their nonexistence, can be obtained by the proposed method.
Numerical experiments and applications are also given.
","[{'version': 'v1', 'created': 'Sun, 31 Jul 2022 03:53:13 GMT'}]",2022-08-02,['Numerical Analysis'],"This paper presents a novel approach to solving truncated moment problems with unbounded sets in numerical analysis. A generalized truncated moment problem is formulated and the existence of a solution is established. Further, a numerical algorithm is proposed to compute the solution of the problem. The proposed algorithm is based on the concept of convex duality and is proved to converge to the exact solution of the problem. Numerical experiments are conducted to demonstrate the effectiveness of the proposed algorithm. The results obtained in the numerical experiments are compared with those of existing methods and the superiority of the proposed algorithm is demonstrated.",Write an abstract for a paper called Generalized truncated moment problems with unbounded sets about Numerical Analysis
2209.116,"Nicola Piovesan, David Lopez-Perez, Antonio De Domenico, Xinli Geng,
  Harvey Bao, Merouane Debbah","Machine Learning and Analytical Power Consumption Models for 5G Base
  Stations","['cs.NI', 'cs.LG']","  The energy consumption of the fifth generation(5G) of mobile networks is one
of the major concerns of the telecom industry. However, there is not currently
an accurate and tractable approach to evaluate 5G base stations (BSs) power
consumption. In this article, we propose a novel model for a realistic
characterisation of the power consumption of 5G multi-carrier BSs, which builds
on a large data collection campaign. At first, we define a machine learning
architecture that allows modelling multiple 5G BS products. Then, we exploit
the knowledge gathered by this framework to derive a realistic and analytically
tractable power consumption model, which can help driving both theoretical
analyses as well as feature standardisation, development and optimisation
frameworks. Notably, we demonstrate that such model has high precision, and it
is able of capturing the benefits of energy saving mechanisms. We believe this
analytical model represents a fundamental tool for understanding 5G BSs power
consumption, and accurately optimising the network energy efficiency.
","[{'version': 'v1', 'created': 'Fri, 23 Sep 2022 14:07:36 GMT'}]",2022-09-26,"['Networking and Internet Architecture', 'Machine Learning']",", and Power Consumption

This paper presents a novel approach to create analytical power consumption models for 5G base stations using machine learning techniques. It introduces a framework for network and internet architecture that encompasses machine learning algorithms and power consumption models. The proposed framework is applied to a real-world 5G base station system, and the results demonstrate the effectiveness of the proposed approach. The paper also provides insights into the development of analytical power consumption models for 5G base stations and the potential for further optimization of power consumption. Finally, the paper discusses the implications of the proposed approach for future research in the field of networking and internet architecture, machine learning, and power consumption.","Write an abstract for a paper called Machine Learning and Analytical Power Consumption Models for 5G Base
  Stations about Networking and Internet Architecture, Machine Learning"
2209.14008,"Piotr P\k{e}zik, Agnieszka Miko{\l}ajczyk-Bare{\l}a, Adam
  Wawrzy\'nski, Bart{\l}omiej Nito\'n, Maciej Ogrodniczuk","Keyword Extraction from Short Texts with a Text-To-Text Transfer
  Transformer",['cs.CL'],"  The paper explores the relevance of the Text-To-Text Transfer Transformer
language model (T5) for Polish (plT5) to the task of intrinsic and extrinsic
keyword extraction from short text passages. The evaluation is carried out on
the new Polish Open Science Metadata Corpus (POSMAC), which is released with
this paper: a collection of 216,214 abstracts of scientific publications
compiled in the CURLICAT project. We compare the results obtained by four
different methods, i.e. plT5kw, extremeText, TermoPL, KeyBERT and conclude that
the plT5kw model yields particularly promising results for both frequent and
sparsely represented keywords. Furthermore, a plT5kw keyword generation model
trained on the POSMAC also seems to produce highly useful results in
cross-domain text labelling scenarios. We discuss the performance of the model
on news stories and phone-based dialog transcripts which represent text genres
and domains extrinsic to the dataset of scientific abstracts. Finally, we also
attempt to characterize the challenges of evaluating a text-to-text model on
both intrinsic and extrinsic keyword extraction.
","[{'version': 'v1', 'created': 'Wed, 28 Sep 2022 11:31:43 GMT'}, {'version': 'v2', 'created': 'Mon, 17 Oct 2022 11:04:18 GMT'}]",2022-10-18,['Computation and Language'],"This paper presents a novel approach to keyword extraction from short texts using a text-to-text transfer transformer. We focus on the application of this technique to the field of Computation and Language, where the ability to accurately extract keywords from a text is essential for many tasks. We propose a transformer-based model that leverages a large-scale language model to capture the semantic relationships between words in a text. We evaluate the model on two datasets, one for keyword extraction from research papers and one for keyword extraction from news articles. Our results show that the proposed model outperforms existing methods in terms of both accuracy and efficiency. Furthermore, we analyze the model's performance on different text lengths, and show that it is able to accurately extract keywords from short texts. Overall, our results demonstrate the effectiveness of the proposed approach and its potential for use in Computation and Language applications.","Write an abstract for a paper called Keyword Extraction from Short Texts with a Text-To-Text Transfer
  Transformer about Computation and Language"
2207.11559,"Qinghua Tao, Francesco Tonin, Panagiotis Patrinos, Johan A.K. Suykens",Tensor-based Multi-view Spectral Clustering via Shared Latent Space,['cs.LG'],"  Multi-view Spectral Clustering (MvSC) attracts increasing attention due to
diverse data sources. However, most existing works are prohibited in
out-of-sample predictions and overlook model interpretability and exploration
of clustering results. In this paper, a new method for MvSC is proposed via a
shared latent space from the Restricted Kernel Machine framework. Through the
lens of conjugate feature duality, we cast the weighted kernel principal
component analysis problem for MvSC and develop a modified weighted conjugate
feature duality to formulate dual variables. In our method, the dual variables,
playing the role of hidden features, are shared by all views to construct a
common latent space, coupling the views by learning projections from
view-specific spaces. Such single latent space promotes well-separated clusters
and provides straightforward data exploration, facilitating visualization and
interpretation. Our method requires only a single eigendecomposition, whose
dimension is independent of the number of views. To boost higher-order
correlations, tensor-based modelling is introduced without increasing
computational complexity. Our method can be flexibly applied with out-of-sample
extensions, enabling greatly improved efficiency for large-scale data with
fixed-size kernel schemes. Numerical experiments verify that our method is
effective regarding accuracy, efficiency, and interpretability, showing a sharp
eigenvalue decay and distinct latent variable distributions.
","[{'version': 'v1', 'created': 'Sat, 23 Jul 2022 17:30:54 GMT'}]",2022-07-26,['Machine Learning'],"This paper presents a novel tensor-based multi-view spectral clustering (TMVSC) framework for multi-view data. The proposed TMVSC framework is based on a shared latent space, which is constructed by a tensor-based learning model. The shared latent space is then used to perform spectral clustering on multi-view data. Experiments on several benchmark datasets demonstrate that the proposed TMVSC framework can achieve better clustering performance than existing multi-view spectral clustering methods. Furthermore, the proposed framework is also shown to be robust to noise and missing data. The results of this paper show that the proposed TMVSC framework is an effective and efficient tool for multi-view data clustering.",Write an abstract for a paper called Tensor-based Multi-view Spectral Clustering via Shared Latent Space about Machine Learning
2211.16209,Hengshuai Yao,"The Vanishing Decision Boundary Complexity and the Strong First
  Component","['cs.LG', 'cs.AI']","  We show that unlike machine learning classifiers, there are no complex
boundary structures in the decision boundaries for well-trained deep models.
However, we found that the complicated structures do appear in training but
they vanish shortly after shaping. This is a pessimistic news if one seeks to
capture different levels of complexity in the decision boundary for
understanding generalization, which works well in machine learning.
Nonetheless, we found that the decision boundaries of predecessor models on the
training data are reflective of the final model's generalization. We show how
to use the predecessor decision boundaries for studying the generalization of
deep models. We have three major findings. One is on the strength of the first
principle component of deep models, another about the singularity of
optimizers, and the other on the effects of the skip connections in ResNets.
Code is at https://github.com/hengshu1/decision_boundary_github.
","[{'version': 'v1', 'created': 'Fri, 25 Nov 2022 21:39:08 GMT'}]",2022-11-30,"['Machine Learning', 'Artificial Intelligence']","This paper explores the implications of the vanishing decision boundary complexity and the strong first component of machine learning and artificial intelligence. We analyze the effects of these two phenomena on the development of intelligent systems and their ability to learn from data. We discuss the implications of the vanishing decision boundary complexity on the complexity of the problem and the implications of the strong first component on the ability of the system to generalize. We also discuss the implications of both phenomena on the scalability of the system, and the potential for the system to be used in real-world applications. Finally, we discuss the implications of these two phenomena on the development of intelligent systems and their ability to learn from data.","Write an abstract for a paper called The Vanishing Decision Boundary Complexity and the Strong First
  Component about Machine Learning, Artificial Intelligence"
2105.00922,"Lucas Chesnel, J\'er\'emy Heleine, Sergei A. Nazarov",Acoustic passive cloaking using thin outer resonators,"['math.AP', 'cs.NA', 'math.NA']","  We consider the propagation of acoustic waves in a 2D waveguide unbounded in
one direction and containing a compact obstacle. The wavenumber is fixed so
that only one mode can propagate. The goal of this work is to propose a method
to cloak the obstacle. More precisely, we add to the geometry thin outer
resonators of width $\varepsilon$ and we explain how to choose their positions
as well as their lengths to get a transmission coefficient approximately equal
to one as if there were no obstacle. In the process we also investigate several
related problems. In particular, we explain how to get zero transmission and
how to design phase shifters. The approach is based on asymptotic analysis in
presence of thin resonators. An essential point is that we work around
resonance lengths of the resonators. This allows us to obtain effects of order
one with geometrical perturbations of width $\varepsilon$. Various numerical
experiments illustrate the theory.
","[{'version': 'v1', 'created': 'Mon, 3 May 2021 14:59:36 GMT'}]",2022-04-27,['Numerical Analysis'],"This paper presents a numerical analysis of acoustic passive cloaking using thin outer resonators. The analysis is based on the Helmholtz equation, which governs the propagation of sound waves in a medium. The cloaking technique is based on the concept of resonance, where a thin outer resonator is placed around an inner region, creating a resonant frequency that is higher than the frequency of the sound wave. The numerical analysis is performed using a finite-difference time-domain (FDTD) method, which is used to simulate the wave propagation in the medium. The results of the analysis show that the cloaking technique is effective in reducing the sound wave intensity in the inner region, thus providing acoustic cloaking. Furthermore, the numerical analysis reveals the effects of various parameters, such as the thickness of the outer resonator and the frequency of the sound wave, on the effectiveness of the cloaking technique. The results of this study provide insight into the design of acoustic passive cloaking systems using thin outer resonators.",Write an abstract for a paper called Acoustic passive cloaking using thin outer resonators about Numerical Analysis
1910.06299,"Gamal Sallam, Zizhan Zheng, Bo Ji","Placement and Allocation of Virtual Network Functions: Multi-dimensional
  Case",['cs.NI'],"  Network function virtualization (NFV) is an emerging design paradigm that
replaces physical middlebox devices with software modules running on general
purpose commodity servers. While gradually transitioning to NFV, Internet
service providers face the problem of where to introduce NFV in order to make
the most benefit of that; here, we measure the benefit by the amount of traffic
that can be served in an NFV-enabled network. This problem is non-trivial as it
is composed of two challenging subproblems: 1) placement of nodes to support
virtual network functions (referred to as VNF-nodes); 2) allocation of the
VNF-nodes' resources to network flows. This problem has been studied for the
one-dimensional setting, where all network flows require one network function,
which requires a unit of resource to process a unit of flow. In this work, we
consider the multi-dimensional setting, where flows must be processed by
multiple network functions, which require a different amount of each resource
to process a unit of flow. The multi-dimensional setting introduces new
challenges in addition to those of the one-dimensional setting (e.g.,
NP-hardness and non-submodularity) and also makes the resource allocation
subproblem a multi-dimensional generalization of the generalized assignment
problem with assignment restrictions. To address these difficulties, we propose
a novel two-level relaxation method that allows us to draw a connection to the
sequence submodular theory and utilize the property of sequence submodularity
along with the primal-dual technique to design two approximation algorithms. We
further prove that the proposed algorithms have a non-trivial approximation
ratio that depends on the number of VNF-nodes, resources, and a measure of the
available resource compared to flow demand. Finally, we perform trace-driven
simulations to show the effectiveness of the proposed algorithms.
","[{'version': 'v1', 'created': 'Mon, 14 Oct 2019 17:27:59 GMT'}, {'version': 'v2', 'created': 'Sun, 31 May 2020 22:39:43 GMT'}, {'version': 'v3', 'created': 'Sat, 19 Feb 2022 18:41:15 GMT'}]",2022-02-22,['Networking and Internet Architecture'],"This paper presents a novel approach to the placement and allocation of virtual network functions (VNFs) in a multi-dimensional network environment. It provides a comprehensive overview of the challenges associated with the placement and allocation of VNFs in a multi-dimensional network environment. The paper proposes a novel approach to the placement and allocation of VNFs based on a combination of graph theory, optimization techniques, and heuristics. The proposed approach is evaluated through extensive simulations on various network topologies and traffic patterns. The results show that the proposed approach provides better performance in terms of network resource utilization, delay, and throughput compared to existing approaches. The paper also provides insights into the challenges associated with the placement and allocation of VNFs in a multi-dimensional network environment.","Write an abstract for a paper called Placement and Allocation of Virtual Network Functions: Multi-dimensional
  Case about Networking and Internet Architecture"
2302.0364,"Junwen Huang, Alexey Artemov, Yujin Chen, Shuaifeng Zhi, Kai Xu,
  Matthias Nie{\ss}ner",S4R: Self-Supervised Semantic Scene Reconstruction from RGB-D Scans,['cs.CV'],"  Most deep learning approaches to comprehensive semantic modeling of 3D indoor
spaces require costly dense annotations in the 3D domain. In this work, we
explore a central 3D scene modeling task, namely, semantic scene
reconstruction, using a fully self-supervised approach. To this end, we design
a trainable model that employs both incomplete 3D reconstructions and their
corresponding source RGB-D images, fusing cross-domain features into volumetric
embeddings to predict complete 3D geometry, color, and semantics. Our key
technical innovation is to leverage differentiable rendering of color and
semantics, using the observed RGB images and a generic semantic segmentation
model as color and semantics supervision, respectively. We additionally develop
a method to synthesize an augmented set of virtual training views complementing
the original real captures, enabling more efficient self-supervision for
semantics. In this work we propose an end-to-end trainable solution jointly
addressing geometry completion, colorization, and semantic mapping from a few
RGB-D images, without 3D or 2D ground-truth. Our method is the first, to our
knowledge, fully self-supervised method addressing completion and semantic
segmentation of real-world 3D scans. It performs comparably well with the 3D
supervised baselines, surpasses baselines with 2D supervision on real datasets,
and generalizes well to unseen scenes.
","[{'version': 'v1', 'created': 'Tue, 7 Feb 2023 17:47:52 GMT'}, {'version': 'v2', 'created': 'Tue, 21 Feb 2023 20:50:33 GMT'}]",2023-02-23,['Computer Vision and Pattern Recognition'],"This paper presents S4R, a novel self-supervised framework for semantic scene reconstruction from RGB-D scans. S4R utilizes a 3D convolutional neural network to extract semantic features from RGB-D images. The proposed framework is then used to reconstruct the 3D scene in a semantically meaningful way. Experiments on the NYU Depth V2 dataset demonstrate that S4R achieves state-of-the-art performance in terms of both reconstruction accuracy and semantic segmentation accuracy. The results show that S4R is an effective approach for semantic scene reconstruction from RGB-D scans and can serve as a powerful tool for computer vision and pattern recognition applications.",Write an abstract for a paper called S4R: Self-Supervised Semantic Scene Reconstruction from RGB-D Scans about Computer Vision and Pattern Recognition
2209.1036,Wei Jiang and Hans Dieter Schotten,"Performance Impact of Channel Aging and Phase Noise on Intelligent
  Reflecting Surface","['cs.IT', 'eess.SP', 'math.IT']","  This letter aims to clarify the impact of channel aging and phase noise on
the performance of intelligent reflecting surface-aided wireless systems. We
first model mathematically the outdated channel state information (CSI) due to
Doppler shifts and phase noise stemming from hardware impairment. Then, a
closed-form expression of achievable spectral efficiency under noisy and aged
CSI is theoretically derived. Some typical simulation results to numerically
demonstrate the performance impact are illustrated.
","[{'version': 'v1', 'created': 'Wed, 21 Sep 2022 13:53:56 GMT'}]",2022-09-22,['Information Theory'],"This paper examines the performance impact of channel aging and phase noise on an intelligent reflecting surface (IRS) from an information-theoretic perspective. We analyze the capacity of the IRS-assisted communication system in the presence of channel aging and phase noise. We consider the performance of different modulation schemes, such as binary phase shift keying (BPSK) and quadrature phase shift keying (QPSK), when the IRS is subject to the effects of channel aging and phase noise. We also propose an optimal power allocation strategy for maximizing the capacity of the IRS-assisted communication system. Finally, numerical results are presented to validate the effectiveness of the proposed power allocation strategy.","Write an abstract for a paper called Performance Impact of Channel Aging and Phase Noise on Intelligent
  Reflecting Surface about Information Theory"
2203.01404,"Ryan K. Cosner, Ivan D. Jimenez Rodriguez, Tamas G. Molnar, Wyatt
  Ubellacker, Yisong Yue, Aaron D. Ames, and Katherine L. Bouman","Self-Supervised Online Learning for Safety-Critical Control using Stereo
  Vision",['cs.RO'],"  With the increasing prevalence of complex vision-based sensing methods for
use in obstacle identification and state estimation, characterizing
environment-dependent measurement errors has become a difficult and essential
part of modern robotics. This paper presents a self-supervised learning
approach to safety-critical control. In particular, the uncertainty associated
with stereo vision is estimated, and adapted online to new visual environments,
wherein this estimate is leveraged in a safety-critical controller in a robust
fashion. To this end, we propose an algorithm that exploits the structure of
stereo-vision to learn an uncertainty estimate without the need for
ground-truth data. We then robustify existing Control Barrier Function-based
controllers to provide safety in the presence of this uncertainty estimate. We
demonstrate the efficacy of our method on a quadrupedal robot in a variety of
environments. When not using our method safety is violated. With offline
training alone we observe the robot is safe, but overly-conservative. With our
online method the quadruped remains safe and conservatism is reduced.
","[{'version': 'v1', 'created': 'Wed, 2 Mar 2022 21:01:13 GMT'}]",2022-03-04,['Robotics'],"This paper presents a novel self-supervised online learning approach for safety-critical control of robots using stereo vision. The proposed method uses a deep convolutional neural network to learn a mapping from an image pair to a control action. The learning algorithm is designed to be robust to changes in the environment, including changes in the robot's task, and is able to adapt to new situations without requiring expensive retraining. Additionally, the proposed method is able to cope with incomplete data, as well as with large amounts of data. Experiments on a simulated robotic platform demonstrate the effectiveness of the proposed approach, providing a promising solution for safety-critical control of robots using stereo vision.","Write an abstract for a paper called Self-Supervised Online Learning for Safety-Critical Control using Stereo
  Vision about Robotics"
2210.05168,Andrei V. Konstantinov and Lev V. Utkin,"LARF: Two-level Attention-based Random Forests with a Mixture of
  Contamination Models","['cs.LG', 'cs.AI', 'stat.ML']","  New models of the attention-based random forests called LARF (Leaf
Attention-based Random Forest) are proposed. The first idea behind the models
is to introduce a two-level attention, where one of the levels is the ""leaf""
attention and the attention mechanism is applied to every leaf of trees. The
second level is the tree attention depending on the ""leaf"" attention. The
second idea is to replace the softmax operation in the attention with the
weighted sum of the softmax operations with different parameters. It is
implemented by applying a mixture of the Huber's contamination models and can
be regarded as an analog of the multi-head attention with ""heads"" defined by
selecting a value of the softmax parameter. Attention parameters are simply
trained by solving the quadratic optimization problem. To simplify the tuning
process of the models, it is proposed to make the tuning contamination
parameters to be training and to compute them by solving the quadratic
optimization problem. Many numerical experiments with real datasets are
performed for studying LARFs. The code of proposed algorithms can be found in
https://github.com/andruekonst/leaf-attention-forest.
","[{'version': 'v1', 'created': 'Tue, 11 Oct 2022 06:14:12 GMT'}]",2022-10-12,"['Machine Learning', 'Artificial Intelligence']","This paper presents LARF, a two-level attention-based random forest with a mixture of contamination models for machine learning and artificial intelligence applications. The random forest is composed of two levels of attention-based decision trees: a base-level decision tree and a top-level decision tree. The base-level decision tree is trained on the original dataset, while the top-level decision tree is trained on the output of the base-level decision tree. The mixture of contamination models is used to identify and filter out outlier samples in the training data. Experiments conducted on several datasets demonstrate that LARF achieves better accuracy than existing methodologies. We also present an analysis of the impact of the contamination models on the performance of the random forest.","Write an abstract for a paper called LARF: Two-level Attention-based Random Forests with a Mixture of
  Contamination Models about Machine Learning, Artificial Intelligence"
2210.11791,"Eric Chan, Marek Chrobak, Mohsen Lesani",Cross-chain Swaps with Preferences,"['cs.DC', 'cs.GT']","  Extreme valuation and volatility of cryptocurrencies require investors to
diversify often which demands secure exchange protocols. A cross-chain swap
protocol allows distrusting parties to securely exchange their assets. However,
the current models and protocols assume predefined user preferences for
acceptable outcomes. This paper presents a generalized model of swaps that
allows each party to specify its preferences on the subsets of its incoming and
outgoing assets. It shows that the existing swap protocols are not necessarily
a strong Nash equilibrium in this model. It characterizes the class of swap
graphs that have protocols that are safe, live and a strong Nash equilibrium,
and presents such a protocol for this class. Further, it shows that deciding
whether a swap is in this class is NP-hard through a reduction from 3SAT, and
further is $\Sigma_2^{\mathsf{P}}$-complete through a reduction from
$\exists\forall\mathsf{DNF}$.
","[{'version': 'v1', 'created': 'Fri, 21 Oct 2022 08:01:05 GMT'}]",2022-10-24,"['Distributed, Parallel, and Cluster Computing', 'Computer Science and Game Theory']","This paper explores the use of cross-chain swaps with preferences in distributed, parallel, and cluster computing, computer science, and game theory. We discuss the benefits and drawbacks of using such swaps and how they can be used to improve the efficiency of distributed, parallel, and cluster computing systems. We also examine how the preferences of the participants in the swap can be used to optimize the system’s performance. Finally, we discuss how game theory can be used to model the interactions between participants in the swap and how this can be used to improve the system’s efficiency. Our results show that cross-chain swaps with preferences can be used to improve the efficiency of distributed, parallel, and cluster computing systems and can be used to optimize the performance of game theory models.","Write an abstract for a paper called Cross-chain Swaps with Preferences about Distributed, Parallel, and Cluster Computing, Computer Science and Game Theory"
2301.13142,"Szabolcs Cs\'efalvay, James Imber",Self-Compressing Neural Networks,"['cs.LG', 'cs.AI']","  This work focuses on reducing neural network size, which is a major driver of
neural network execution time, power consumption, bandwidth, and memory
footprint. A key challenge is to reduce size in a manner that can be exploited
readily for efficient training and inference without the need for specialized
hardware. We propose Self-Compression: a simple, general method that
simultaneously achieves two goals: (1) removing redundant weights, and (2)
reducing the number of bits required to represent the remaining weights. This
is achieved using a generalized loss function to minimize overall network size.
In our experiments we demonstrate floating point accuracy with as few as 3% of
the bits and 18% of the weights remaining in the network.
","[{'version': 'v1', 'created': 'Mon, 30 Jan 2023 18:22:28 GMT'}, {'version': 'v2', 'created': 'Tue, 31 Jan 2023 10:28:52 GMT'}]",2023-02-01,"['Machine Learning', 'Artificial Intelligence']",This paper explores the concept of self-compressing neural networks in the context of machine learning and artificial intelligence. Self-compressing neural networks are a type of artificial neural network (ANN) that can reduce the number of parameters used in the network without sacrificing accuracy. The paper discusses the potential benefits of self-compressing neural networks and how they can be used in machine learning and artificial intelligence applications. The paper also reviews existing approaches to self-compressing neural networks and discusses potential challenges and future research directions. The paper concludes by providing an overview of the potential benefits of self-compressing neural networks and the implications for machine learning and artificial intelligence.,"Write an abstract for a paper called Self-Compressing Neural Networks about Machine Learning, Artificial Intelligence"
2301.11706,"Mang Ning, Enver Sangineto, Angelo Porrello, Simone Calderara, Rita
  Cucchiara",Input Perturbation Reduces Exposure Bias in Diffusion Models,"['cs.LG', 'cs.AI', 'cs.CV']","  Denoising Diffusion Probabilistic Models have shown an impressive generation
quality, although their long sampling chain leads to high computational costs.
In this paper, we observe that a long sampling chain also leads to an error
accumulation phenomenon, which is similar to the exposure bias problem in
autoregressive text generation. Specifically, we note that there is a
discrepancy between training and testing, since the former is conditioned on
the ground truth samples, while the latter is conditioned on the previously
generated results. To alleviate this problem, we propose a very simple but
effective training regularization, consisting in perturbing the ground truth
samples to simulate the inference time prediction errors. We empirically show
that the proposed input perturbation leads to a significant improvement of the
sample quality while reducing both the training and the inference times. For
instance, on CelebA 64$\times$64, we achieve a new state-of-the-art FID score
of 1.27, while saving 37.5% of the training time. The code is publicly
available at https://github.com/forever208/DDPM-IP
","[{'version': 'v1', 'created': 'Fri, 27 Jan 2023 13:34:54 GMT'}, {'version': 'v2', 'created': 'Thu, 16 Feb 2023 19:29:45 GMT'}]",2023-02-20,"['Machine Learning', 'Artificial Intelligence', 'Computer Vision and Pattern Recognition']","This paper examines the use of input perturbation to reduce exposure bias for diffusion models in the context of machine learning, artificial intelligence, computer vision, and pattern recognition. Exposure bias is a phenomenon in which a machine learning model is unable to generalize beyond the training data, leading to poor performance when applied to new data. Input perturbation is a technique in which the input data is randomly modified to create a new, perturbed dataset with which to train the model. This paper explores how input perturbation can reduce exposure bias and improve the performance of diffusion models. The results of this study show that input perturbation can reduce exposure bias in diffusion models and improve the accuracy of the models when applied to new data. Additionally, the paper discusses the implications of these findings for the fields of machine learning, artificial intelligence, computer vision, and pattern recognition.","Write an abstract for a paper called Input Perturbation Reduces Exposure Bias in Diffusion Models about Machine Learning, Artificial Intelligence, Computer Vision and Pattern Recognition"
2112.07673,"Sean Craven, Djuna Croon, Daniel Cutting, Rachel Houtz",Machine learning a manifold,"['hep-ph', 'cs.LG']","  We propose a simple method to identify a continuous Lie algebra symmetry in a
dataset through regression by an artificial neural network. Our proposal takes
advantage of the $ \mathcal{O}(\epsilon^2)$ scaling of the output variable
under infinitesimal symmetry transformations on the input variables. As
symmetry transformations are generated post-training, the methodology does not
rely on sampling of the full representation space or binning of the dataset,
and the possibility of false identification is minimised. We demonstrate our
method in the SU(3)-symmetric (non-) linear $\Sigma$ model.
","[{'version': 'v1', 'created': 'Tue, 14 Dec 2021 19:00:00 GMT'}, {'version': 'v2', 'created': 'Tue, 31 May 2022 10:51:34 GMT'}]",2022-06-01,['Machine Learning'],"This paper explores the concept of machine learning a manifold and its potential applications. Machine learning a manifold is a technique that uses machine learning to learn a manifold, a mathematical structure that describes the relationship between multiple variables. It is an important tool in the field of machine learning, as it can be used to identify patterns, uncover hidden relationships, and make predictions. The paper will discuss the concept of machine learning a manifold, its advantages, and its potential applications. In particular, the paper will focus on its potential applications in the fields of computer vision, natural language processing, and robotics. The paper will also discuss the challenges associated with machine learning a manifold and potential solutions. Finally, the paper will present a case study of the use of machine learning a manifold in a real-world application.",Write an abstract for a paper called Machine learning a manifold about Machine Learning
2301.03724,"Guangyuan Hu, Zecheng He, Ruby Lee",SoK: Hardware Defenses Against Speculative Execution Attacks,"['cs.CR', 'cs.AR']","  Speculative execution attacks leverage the speculative and out-of-order
execution features in modern computer processors to access secret data or
execute code that should not be executed. Secret information can then be leaked
through a covert channel. While software patches can be installed for
mitigation on existing hardware, these solutions can incur big performance
overhead. Hardware mitigation is being studied extensively by the computer
architecture community. It has the benefit of preserving software compatibility
and the potential for much smaller performance overhead than software
solutions.
  This paper presents a systematization of the hardware defenses against
speculative execution attacks that have been proposed. We show that speculative
execution attacks consist of 6 critical attack steps. We propose defense
strategies, each of which prevents a critical attack step from happening, thus
preventing the attack from succeeding. We then summarize 20 hardware defenses
and overhead-reducing features that have been proposed. We show that each
defense proposed can be classified under one of our defense strategies, which
also explains why it can thwart the attack from succeeding. We discuss the
scope of the defenses, their performance overhead, and the security-performance
trade-offs that can be made.
","[{'version': 'v1', 'created': 'Mon, 9 Jan 2023 23:58:25 GMT'}]",2023-02-03,"['Cryptography and Security', 'Hardware Architecture']","This paper explores the use of hardware-based defenses against speculative execution attacks in the context of cryptography and security. It provides an overview of the different types of hardware-based defenses and their effectiveness in mitigating the risks associated with such attacks. It also discusses the implications of these defenses on hardware architecture, including the potential trade-offs between security and performance. Finally, the paper provides an analysis of the current state of research and highlights potential areas of improvement. The aim of this paper is to provide an up-to-date overview of the current state of hardware-based defenses against speculative execution attacks and to identify potential future directions for research.","Write an abstract for a paper called SoK: Hardware Defenses Against Speculative Execution Attacks about Cryptography and Security, Hardware Architecture"
2205.03257,"James Jordon, Lukasz Szpruch, Florimond Houssiau, Mirko Bottarelli,
  Giovanni Cherubin, Carsten Maple, Samuel N. Cohen, Adrian Weller","Synthetic Data -- what, why and how?",['cs.LG'],"  This explainer document aims to provide an overview of the current state of
the rapidly expanding work on synthetic data technologies, with a particular
focus on privacy. The article is intended for a non-technical audience, though
some formal definitions have been given to provide clarity to specialists. This
article is intended to enable the reader to quickly become familiar with the
notion of synthetic data, as well as understand some of the subtle intricacies
that come with it. We do believe that synthetic data is a very useful tool, and
our hope is that this report highlights that, while drawing attention to
nuances that can easily be overlooked in its deployment.
","[{'version': 'v1', 'created': 'Fri, 6 May 2022 14:27:45 GMT'}]",2022-05-09,['Machine Learning'],"This paper explores the concept of Synthetic Data and its use in Machine Learning. It begins by examining the definition of Synthetic Data and its purpose in Machine Learning. It then looks at the advantages and disadvantages of using Synthetic Data in Machine Learning, as well as the different methods of generating Synthetic Data. Finally, the paper discusses the potential implications of using Synthetic Data in Machine Learning and the ethical considerations that must be taken into account when using it. The paper concludes by providing a framework for how Synthetic Data can be used effectively in Machine Learning.","Write an abstract for a paper called Synthetic Data -- what, why and how? about Machine Learning"
2002.05545,Martin Morin and Pontus Giselsson,"Sampling and Update Frequencies in Proximal Variance-Reduced Stochastic
  Gradient Methods","['math.OC', 'cs.LG']","  Variance-reduced stochastic gradient methods have gained popularity in recent
times. Several variants exist with different strategies for the storing and
sampling of gradients and this work concerns the interactions between these two
aspects. We present a general proximal variance-reduced gradient method and
analyze it under strong convexity assumptions. Special cases of the algorithm
include SAGA, L-SVRG and their proximal variants. Our analysis sheds light on
epoch-length selection and the need to balance the convergence of the iterates
with how often gradients are stored. The analysis improves on other convergence
rates found in the literature and produces a new and faster converging sampling
strategy for SAGA. Problem instances for which the predicted rates are the same
as the practical rates are presented together with problems based on real world
data.
","[{'version': 'v1', 'created': 'Thu, 13 Feb 2020 14:56:05 GMT'}, {'version': 'v2', 'created': 'Tue, 25 Feb 2020 19:58:04 GMT'}, {'version': 'v3', 'created': 'Tue, 18 Oct 2022 11:10:35 GMT'}]",2022-10-19,['Machine Learning'],"This paper presents a study of the impact of sampling and update frequencies on the performance of Proximal Variance-Reduced Stochastic Gradient (PVR-SG) methods in machine learning. We analyze the influence of different sampling and update frequencies on the convergence rate and accuracy of PVR-SG methods and compare them to other state-of-the-art methods. We also present a novel sampling strategy to improve the performance of PVR-SG and demonstrate its effectiveness on real-world datasets. Our results show that our proposed sampling strategy is more effective than existing methods, resulting in faster convergence and higher accuracy.","Write an abstract for a paper called Sampling and Update Frequencies in Proximal Variance-Reduced Stochastic
  Gradient Methods about Machine Learning"
2303.17252,Venus Pasandi and Daniele Pucci,Torque Control with Joints Position and Velocity Limits Avoidance,"['cs.RO', 'math.DS']","  The design of a control architecture for providing the desired motion along
with the realization of the joint limitation of a robotic system is still an
open challenge in control and robotics. This paper presents a torque control
architecture for fully actuated manipulators for tracking the desired
time-varying trajectory while ensuring the joints position and velocity limits.
The presented architecture stems from the parametrization of the feasible
joints position and velocity space by exogenous states. The proposed
parametrization transforms the control problem with constrained states to an
un-constrained one by replacing the joints position and velocity with the
exogenous states. With the help of Lyapunov-based arguments, we prove that the
proposed control architecture ensures the stability and convergence of the
desired joint trajectory along with the joints position and velocity limits
avoidance. We validate the performance of proposed architecture through various
simulations on a simple two-degree-of-freedom manipulator and the humanoid
robot iCub.
","[{'version': 'v1', 'created': 'Thu, 30 Mar 2023 09:30:26 GMT'}]",2023-03-31,['Robotics'],"This paper presents a torque control approach for robotic arms with joint position and velocity limits avoidance. The proposed approach is based on a nonlinear dynamic model of the robot and uses a combination of a sliding mode controller and a fuzzy controller. The sliding mode controller is used to ensure the joint position and velocity limits are not exceeded, while the fuzzy controller is used to regulate the torque of the robot. The performance of the proposed approach is evaluated in simulation and experimentally on a robotic arm. The results show that the proposed approach is robust and able to track desired joint trajectories with joint position and velocity limits avoidance.",Write an abstract for a paper called Torque Control with Joints Position and Velocity Limits Avoidance about Robotics
2301.04272,"Noveen Sachdeva, Julian McAuley",Data Distillation: A Survey,"['cs.LG', 'cs.CV', 'cs.IR']","  The popularity of deep learning has led to the curation of a vast number of
massive and multifarious datasets. Despite having close-to-human performance on
individual tasks, training parameter-hungry models on large datasets poses
multi-faceted problems such as (a) high model-training time; (b) slow research
iteration; and (c) poor eco-sustainability. As an alternative, data
distillation approaches aim to synthesize terse data summaries, which can serve
as effective drop-in replacements of the original dataset for scenarios like
model training, inference, architecture search, etc. In this survey, we present
a formal framework for data distillation, along with providing a detailed
taxonomy of existing approaches. Additionally, we cover data distillation
approaches for different data modalities, namely images, graphs, and user-item
interactions (recommender systems), while also identifying current challenges
and future research directions.
","[{'version': 'v1', 'created': 'Wed, 11 Jan 2023 02:25:10 GMT'}]",2023-01-12,"['Machine Learning', 'Computer Vision and Pattern Recognition', 'Information Retrieval']","This paper surveys the current state of data distillation techniques in machine learning, computer vision and pattern recognition, and information retrieval. It provides a comprehensive overview of the various techniques, their advantages and disadvantages, and their potential applications. It also examines the challenges and opportunities that data distillation presents for the future of these fields. The paper concludes with a discussion of the potential implications of data distillation for the development of intelligent systems.","Write an abstract for a paper called Data Distillation: A Survey about Machine Learning, Computer Vision and Pattern Recognition, Information Retrieval"
2303.02957,"Atsushi Nitanda, Kazusato Oko, Denny Wu, Nobuhito Takenouchi, Taiji
  Suzuki","Primal and Dual Analysis of Entropic Fictitious Play for Finite-sum
  Problems","['stat.ML', 'cs.LG', 'math.OC']","  The entropic fictitious play (EFP) is a recently proposed algorithm that
minimizes the sum of a convex functional and entropy in the space of measures
-- such an objective naturally arises in the optimization of a two-layer neural
network in the mean-field regime. In this work, we provide a concise
primal-dual analysis of EFP in the setting where the learning problem exhibits
a finite-sum structure. We establish quantitative global convergence guarantees
for both the continuous-time and discrete-time dynamics based on properties of
a proximal Gibbs measure introduced in Nitanda et al. (2022). Furthermore, our
primal-dual framework entails a memory-efficient particle-based implementation
of the EFP update, and also suggests a connection to gradient boosting methods.
We illustrate the efficiency of our novel implementation in experiments
including neural network optimization and image synthesis.
","[{'version': 'v1', 'created': 'Mon, 6 Mar 2023 08:05:08 GMT'}]",2023-03-07,['Machine Learning'],"This paper presents a novel approach to machine learning using primal and dual analysis of entropic fictitious play for finite-sum problems. We introduce a new technique for solving finite-sum games using artificial intelligence. We prove the convergence of the primal and dual algorithms and show that the entropic fictitious play method is more efficient than traditional approaches. We also demonstrate the effectiveness of the proposed approach by applying it to a variety of finite-sum problems. Finally, we discuss the implications of our results and the potential for further research in this area.","Write an abstract for a paper called Primal and Dual Analysis of Entropic Fictitious Play for Finite-sum
  Problems about Machine Learning"
2205.12854,"Liyan Tang, Tanya Goyal, Alexander R. Fabbri, Philippe Laban, Jiacheng
  Xu, Semih Yahvuz, Wojciech Kry\'sci\'nski, Justin F. Rousseau, Greg Durrett","Understanding Factual Errors in Summarization: Errors, Summarizers,
  Datasets, Error Detectors","['cs.CL', 'cs.AI']","  The propensity of abstractive summarization systems to make factual errors
has been the subject of significant study, including work on models to detect
factual errors and annotation of errors in current systems' outputs. However,
the ever-evolving nature of summarization systems, error detectors, and
annotated benchmarks make factuality evaluation a moving target; it is hard to
get a clear picture of how techniques compare. In this work, we collect labeled
factuality errors from across nine datasets of annotated summary outputs and
stratify them in a new way, focusing on what kind of base summarization model
was used. To support finer-grained analysis, we unify the labeled error types
into a single taxonomy and project each of the datasets' errors into this
shared labeled space. We then contrast five state-of-the-art error detection
methods on this benchmark. Our findings show that benchmarks built on modern
summary outputs (those from pre-trained models) show significantly different
results than benchmarks using pre-Transformer models. Furthermore, no one
factuality technique is superior in all settings or for all error types,
suggesting that system developers should take care to choose the right system
for their task at hand.
","[{'version': 'v1', 'created': 'Wed, 25 May 2022 15:26:48 GMT'}]",2022-05-26,"['Computation and Language', 'Artificial Intelligence']","This paper explores the various challenges associated with identifying and correcting factual errors in summarization. The paper begins by discussing the types of errors that summarizers make, including both errors of omission and errors of commission. It then examines the different summarization techniques and datasets used in summarization research. Next, the paper looks at the various error detectors that have been developed to detect factual errors in summarization. Finally, the paper discusses the implications of these findings for the field of computation and language, and artificial intelligence. The paper concludes by discussing potential directions for future research in this area.","Write an abstract for a paper called Understanding Factual Errors in Summarization: Errors, Summarizers,
  Datasets, Error Detectors about Computation and Language, Artificial Intelligence"
2209.12699,"Gangwei Xu, Yun Wang, Junda Cheng, Jinhui Tang, Xin Yang","Accurate and Efficient Stereo Matching via Attention Concatenation
  Volume",['cs.CV'],"  Stereo matching is a fundamental building block for many vision and robotics
applications. An informative and concise cost volume representation is vital
for stereo matching of high accuracy and efficiency. In this paper, we present
a novel cost volume construction method, named attention concatenation volume
(ACV), which generates attention weights from correlation clues to suppress
redundant information and enhance matching-related information in the
concatenation volume. The ACV can be seamlessly embedded into most stereo
matching networks, the resulting networks can use a more lightweight
aggregation network and meanwhile achieve higher accuracy. We further design a
fast version of ACV to enable real-time performance, named Fast-ACV, which
generates high likelihood disparity hypotheses and the corresponding attention
weights from low-resolution correlation clues to significantly reduce
computational and memory cost and meanwhile maintain a satisfactory accuracy.
The core idea of our Fast-ACV is volume attention propagation (VAP) which can
automatically select accurate correlation values from an upsampled correlation
volume and propagate these accurate values to the surroundings pixels with
ambiguous correlation clues. Furthermore, we design a highly accurate network
ACVNet and a real-time network Fast-ACVNet based on our ACV and Fast-ACV
respectively, which achieve the state-of-the-art performance on several
benchmarks (i.e., our ACVNet ranks the 2nd on KITTI 2015 and Scene Flow, and
the 3rd on KITTI 2012 and ETH3D among all the published methods; our
Fast-ACVNet outperforms almost all state-of-the-art real-time methods on Scene
Flow, KITTI 2012 and 2015 and meanwhile has better generalization ability)
","[{'version': 'v1', 'created': 'Fri, 23 Sep 2022 08:14:30 GMT'}, {'version': 'v2', 'created': 'Tue, 27 Sep 2022 01:32:39 GMT'}]",2022-09-29,['Computer Vision and Pattern Recognition'],"This paper presents a novel stereo matching algorithm that is both accurate and efficient. It is based on a novel attention concatenation architecture that combines multiple feature extractors and attention modules in order to accurately identify matching pixels in stereo images. The proposed approach is evaluated on several popular stereo datasets and shows improved accuracy and efficiency compared to existing state-of-the-art methods. Furthermore, the proposed approach is shown to be robust to varying image conditions and illumination levels. The results demonstrate that the proposed method can be used to accurately and efficiently perform stereo matching in real-world applications.","Write an abstract for a paper called Accurate and Efficient Stereo Matching via Attention Concatenation
  Volume about Computer Vision and Pattern Recognition"
2211.11803,"Chinonso Nwankwo, Nneka Umeorah, Tony Ware, Weizhong Dai",Deep learning and American options via free boundary framework,"['q-fin.CP', 'cs.NA', 'math.AP', 'math.NA', 'q-fin.MF', 'q-fin.PR']","  We propose a deep learning method for solving the American options model with
a free boundary feature. To extract the free boundary known as the early
exercise boundary from our proposed method, we introduce the Landau
transformation. For efficient implementation of our proposed method, we further
construct a dual solution framework consisting of a novel auxiliary function
and free boundary equations. The auxiliary function is formulated to include
the feed forward deep neural network (DNN) output and further mimic the far
boundary behaviour, smooth pasting condition, and remaining boundary conditions
due to the second-order space derivative and first-order time derivative.
Because the early exercise boundary and its derivative are not a priori known,
the boundary values mimicked by the auxiliary function are in approximate form.
Concurrently, we then establish equations that approximate the early exercise
boundary and its derivative directly from the DNN output based on some linear
relationships at the left boundary. Furthermore, the option Greeks are obtained
from the derivatives of this auxiliary function. We test our implementation
with several examples and compare them with the existing numerical methods. All
indicators show that our proposed deep learning method presents an efficient
and alternative way of pricing options with early exercise features.
","[{'version': 'v1', 'created': 'Mon, 21 Nov 2022 19:15:24 GMT'}, {'version': 'v2', 'created': 'Fri, 9 Dec 2022 21:13:50 GMT'}]",2022-12-13,['Numerical Analysis'],"This paper presents a numerical analysis of deep learning and American options using a free boundary framework. The proposed framework is based on the deep learning technique, which is a type of artificial intelligence that is used to identify patterns in data and to make predictions. The framework is used to analyze the pricing of American options using a free boundary approach. This approach is based on the idea that the option's price should be determined by the underlying asset's price and the option's exercise boundary. The numerical analysis of the proposed framework is conducted using a simulated dataset and an empirical dataset. The results of the analysis demonstrate that the proposed framework is capable of accurately pricing American options. Furthermore, the results also suggest that deep learning is a viable approach for pricing American options.",Write an abstract for a paper called Deep learning and American options via free boundary framework about Numerical Analysis
2206.00944,"Shingo Yashima, Teppei Suzuki, Kohta Ishikawa, Ikuro Sato, Rei
  Kawakami",Feature Space Particle Inference for Neural Network Ensembles,"['cs.LG', 'cs.CV', 'stat.ML']","  Ensembles of deep neural networks demonstrate improved performance over
single models. For enhancing the diversity of ensemble members while keeping
their performance, particle-based inference methods offer a promising approach
from a Bayesian perspective. However, the best way to apply these methods to
neural networks is still unclear: seeking samples from the weight-space
posterior suffers from inefficiency due to the over-parameterization issues,
while seeking samples directly from the function-space posterior often results
in serious underfitting. In this study, we propose optimizing particles in the
feature space where the activation of a specific intermediate layer lies to
address the above-mentioned difficulties. Our method encourages each member to
capture distinct features, which is expected to improve ensemble prediction
robustness. Extensive evaluation on real-world datasets shows that our model
significantly outperforms the gold-standard Deep Ensembles on various metrics,
including accuracy, calibration, and robustness. Code is available at
https://github.com/DensoITLab/featurePI .
","[{'version': 'v1', 'created': 'Thu, 2 Jun 2022 09:16:26 GMT'}]",2022-06-03,"['Machine Learning', 'Computer Vision and Pattern Recognition']","This paper presents a novel approach to machine learning, computer vision and pattern recognition using feature space particle inference for neural network ensembles. We propose a method that combines particle filtering and neural networks to infer feature space from a given input image. This method is used to generate a set of neural network ensembles, each with a different set of features. We then evaluate the performance of each ensemble and combine the results to obtain a more accurate prediction. We demonstrate the effectiveness of our approach on a variety of datasets, including MNIST, CIFAR-10 and ImageNet. Our results show that our proposed method outperforms existing methods in terms of accuracy and computational efficiency.","Write an abstract for a paper called Feature Space Particle Inference for Neural Network Ensembles about Machine Learning, Computer Vision and Pattern Recognition"
2205.10174,"Avinash Sudhodanan, Andrew Paverd","Pre-hijacked accounts: An Empirical Study of Security Failures in User
  Account Creation on the Web",['cs.CR'],"  The ubiquity of user accounts in websites and online services makes account
hijacking a serious security concern. Although previous research has studied
various techniques through which an attacker can gain access to a victim's
account, relatively little attention has been directed towards the process of
account creation. The current trend towards federated authentication (e.g.,
Single Sign-On) adds an additional layer of complexity because many services
now support both the classic approach in which the user directly sets a
password, and the federated approach in which the user authenticates via an
identity provider.
  Inspired by previous work on preemptive account hijacking [Ghasemisharif et
al., USENIX SEC 2018], we show that there exists a whole class of account
pre-hijacking attacks. The distinctive feature of these attacks is that the
attacker performs some action before the victim creates an account, which makes
it trivial for the attacker to gain access after the victim has
created/recovered the account. Assuming a realistic attacker who knows only the
victim's email address, we identify and discuss five different types of account
pre-hijacking attacks.
  To ascertain the prevalence of such vulnerabilities in the wild, we analyzed
75 popular services and found that at least 35 of these were vulnerable to one
or more account pre-hijacking attacks. Whilst some of these may be noticed by
attentive users, others were completely undetectable from the victim's
perspective. Finally, we investigated the root cause of these vulnerabilities
and present a set of security requirements to prevent such vulnerabilities
arising in future.
","[{'version': 'v1', 'created': 'Fri, 20 May 2022 13:27:37 GMT'}]",2022-05-23,['Cryptography and Security'],"This paper presents an empirical study of security failures in user account creation on the web. The study focuses on pre-hijacked accounts, which are created by malicious actors before legitimate users can sign up for an account. The paper examines the security vulnerabilities that enable pre-hijacked accounts, the methods used to hijack the accounts, and the ways in which the accounts can be used to launch attacks. The paper also explores the cryptographic and security measures that can be implemented to protect against pre-hijacked accounts. The findings of the study suggest that security vulnerabilities in user account creation can be exploited by malicious actors, and that these vulnerabilities can be mitigated by implementing cryptographic and security measures. The paper seeks to inform security practitioners and developers about the dangers of pre-hijacked accounts and the need to secure user accounts.","Write an abstract for a paper called Pre-hijacked accounts: An Empirical Study of Security Failures in User
  Account Creation on the Web about Cryptography and Security"
2012.00289,"Travis Greene, Galit Shmueli, Jan Fell, Ching-Fu Lin, Han-Wei Liu","Forks Over Knives: Predictive Inconsistency in Criminal Justice
  Algorithmic Risk Assessment Tools",['cs.CY'],"  Big data and algorithmic risk prediction tools promise to improve criminal
justice systems by reducing human biases and inconsistencies in decision
making. Yet different, equally-justifiable choices when developing, testing,
and deploying these sociotechnical tools can lead to disparate predicted risk
scores for the same individual. Synthesizing diverse perspectives from machine
learning, statistics, sociology, criminology, law, philosophy and economics, we
conceptualize this phenomenon as predictive inconsistency. We describe sources
of predictive inconsistency at different stages of algorithmic risk assessment
tool development and deployment and consider how future technological
developments may amplify predictive inconsistency. We argue, however, that in a
diverse and pluralistic society we should not expect to completely eliminate
predictive inconsistency. Instead, to bolster the legal, political, and
scientific legitimacy of algorithmic risk prediction tools, we propose
identifying and documenting relevant and reasonable ""forking paths"" to enable
quantifiable, reproducible multiverse and specification curve analyses of
predictive inconsistency at the individual level.
","[{'version': 'v1', 'created': 'Tue, 1 Dec 2020 06:12:30 GMT'}, {'version': 'v2', 'created': 'Thu, 22 Sep 2022 14:42:03 GMT'}]",2022-09-23,['Computers and Society'],"This paper examines the predictive inconsistency of algorithmic risk assessment tools in the criminal justice system. Algorithmic risk assessment tools are increasingly being used to inform decisions about pretrial detention, sentencing, and parole. However, there is growing evidence that these tools can lead to disparate outcomes based on race, gender, and other demographic factors. This paper will review the existing literature on algorithmic risk assessment tools and their impact on criminal justice outcomes. It will also discuss the ethical implications of using these tools and the potential for algorithmic bias to perpetuate existing inequalities in the criminal justice system. Finally, the paper will present an argument for why algorithmic risk assessment tools should not be used to make decisions in the criminal justice system.","Write an abstract for a paper called Forks Over Knives: Predictive Inconsistency in Criminal Justice
  Algorithmic Risk Assessment Tools about Computers and Society"
2206.08932,"Claire Stevenson, Iris Smal, Matthijs Baas, Raoul Grasman and Han van
  der Maas",Putting GPT-3's Creativity to the (Alternative Uses) Test,"['cs.AI', 'cs.CL', 'cs.HC']","  AI large language models have (co-)produced amazing written works from
newspaper articles to novels and poetry. These works meet the standards of the
standard definition of creativity: being original and useful, and sometimes
even the additional element of surprise. But can a large language model
designed to predict the next text fragment provide creative, out-of-the-box,
responses that still solve the problem at hand? We put Open AI's generative
natural language model, GPT-3, to the test. Can it provide creative solutions
to one of the most commonly used tests in creativity research? We assessed
GPT-3's creativity on Guilford's Alternative Uses Test and compared its
performance to previously collected human responses on expert ratings of
originality, usefulness and surprise of responses, flexibility of each set of
ideas as well as an automated method to measure creativity based on the
semantic distance between a response and the AUT object in question. Our
results show that -- on the whole -- humans currently outperform GPT-3 when it
comes to creative output. But, we believe it is only a matter of time before
GPT-3 catches up on this particular task. We discuss what this work reveals
about human and AI creativity, creativity testing and our definition of
creativity.
","[{'version': 'v1', 'created': 'Fri, 10 Jun 2022 15:36:45 GMT'}]",2022-06-22,"['Artificial Intelligence', 'Computation and Language', 'Human-Computer Interaction']","This paper explores the potential of GPT-3, a state-of-the-art natural language processing (NLP) model, to generate creative ideas. We investigate how GPT-3 can be used to generate alternative uses for everyday items. We evaluate GPT-3's performance using a qualitative analysis of the alternative uses generated. We also compare GPT-3's performance to that of humans in a user study. Our results show that GPT-3 can produce creative ideas, but it is not yet on par with human performance. We discuss the implications of our findings for the use of GPT-3 in creative applications and suggest potential areas of improvement.","Write an abstract for a paper called Putting GPT-3's Creativity to the (Alternative Uses) Test about Artificial Intelligence, Computation and Language, Human-Computer Interaction"
2210.1366,"Giovanni Apruzzese, Mauro Conti, Ying Yuan","SpacePhish: The Evasion-space of Adversarial Attacks against Phishing
  Website Detectors using Machine Learning","['cs.CR', 'cs.LG', 'cs.NI']","  Existing literature on adversarial Machine Learning (ML) focuses either on
showing attacks that break every ML model, or defenses that withstand most
attacks. Unfortunately, little consideration is given to the actual
\textit{cost} of the attack or the defense. Moreover, adversarial samples are
often crafted in the ""feature-space"", making the corresponding evaluations of
questionable value. Simply put, the current situation does not allow to
estimate the actual threat posed by adversarial attacks, leading to a lack of
secure ML systems.
  We aim to clarify such confusion in this paper. By considering the
application of ML for Phishing Website Detection (PWD), we formalize the
""evasion-space"" in which an adversarial perturbation can be introduced to fool
a ML-PWD -- demonstrating that even perturbations in the ""feature-space"" are
useful. Then, we propose a realistic threat model describing evasion attacks
against ML-PWD that are cheap to stage, and hence intrinsically more attractive
for real phishers. Finally, we perform the first statistically validated
assessment of state-of-the-art ML-PWD against 12 evasion attacks. Our
evaluation shows (i) the true efficacy of evasion attempts that are more likely
to occur; and (ii) the impact of perturbations crafted in different
evasion-spaces. Our realistic evasion attempts induce a statistically
significant degradation (3-10% at $p\!<$0.05), and their cheap cost makes them
a subtle threat. Notably, however, some ML-PWD are immune to our most realistic
attacks ($p$=0.22). Our contribution paves the way for a much needed
re-assessment of adversarial attacks against ML systems for cybersecurity.
","[{'version': 'v1', 'created': 'Mon, 24 Oct 2022 23:45:09 GMT'}]",2022-10-26,"['Cryptography and Security', 'Machine Learning', 'Networking and Internet Architecture']","SpacePhish is a research paper that investigates the evasion-space of adversarial attacks against phishing website detectors using machine learning. This paper focuses on the application of cryptography and security, machine learning, networking and internet architecture to the problem of phishing website detection. It examines the limitations of current phishing website detectors and proposes novel methods for detecting malicious websites. Furthermore, the paper evaluates the effectiveness of these methods and discusses the implications of their use in the field of network security. Finally, this paper provides insights into the future direction of research in the field of phishing website detection.","Write an abstract for a paper called SpacePhish: The Evasion-space of Adversarial Attacks against Phishing
  Website Detectors using Machine Learning about Cryptography and Security, Machine Learning, Networking and Internet Architecture"
2304.03779,"Yilin Ning, Victor Volovici, Marcus Eng Hock Ong, Benjamin Alan
  Goldstein, Nan Liu","A roadmap to fair and trustworthy prediction model validation in
  healthcare","['cs.LG', 'cs.AI', 'cs.CY']","  A prediction model is most useful if it generalizes beyond the development
data with external validations, but to what extent should it generalize remains
unclear. In practice, prediction models are externally validated using data
from very different settings, including populations from other health systems
or countries, with predictably poor results. This may not be a fair reflection
of the performance of the model which was designed for a specific target
population or setting, and may be stretching the expected model
generalizability. To address this, we suggest to externally validate a model
using new data from the target population to ensure clear implications of
validation performance on model reliability, whereas model generalizability to
broader settings should be carefully investigated during model development
instead of explored post-hoc. Based on this perspective, we propose a roadmap
that facilitates the development and application of reliable, fair, and
trustworthy artificial intelligence prediction models.
","[{'version': 'v1', 'created': 'Fri, 7 Apr 2023 04:24:19 GMT'}]",2023-04-11,"['Machine Learning', 'Artificial Intelligence', 'Computers and Society']","This paper presents a roadmap to facilitate the development of fair and trustworthy prediction model validation in healthcare using machine learning and artificial intelligence. It begins by discussing the role of computers and society in healthcare, including the need for ethical use of data and the development of predictive models. It then examines the current state of prediction model validation in healthcare, including the challenges of developing models that are fair, reliable, and trustworthy. Finally, the paper outlines a roadmap for the development of prediction model validation in healthcare, including steps to ensure model fairness, accuracy, and trustworthiness. The roadmap provides a comprehensive framework for the development of fair and trustworthy prediction models in healthcare and sets the stage for further research in this field.","Write an abstract for a paper called A roadmap to fair and trustworthy prediction model validation in
  healthcare about Machine Learning, Artificial Intelligence, Computers and Society"
2206.0285,"Fang Xu and Yilei Shi and Patrick Ebel and Lei Yu and Gui-Song Xia and
  Wen Yang and Xiao Xiang Zhu",GLF-CR: SAR-Enhanced Cloud Removal with Global-Local Fusion,"['cs.CV', 'eess.IV']","  The challenge of the cloud removal task can be alleviated with the aid of
Synthetic Aperture Radar (SAR) images that can penetrate cloud cover. However,
the large domain gap between optical and SAR images as well as the severe
speckle noise of SAR images may cause significant interference in SAR-based
cloud removal, resulting in performance degeneration. In this paper, we propose
a novel global-local fusion based cloud removal (GLF-CR) algorithm to leverage
the complementary information embedded in SAR images. Exploiting the power of
SAR information to promote cloud removal entails two aspects. The first, global
fusion, guides the relationship among all local optical windows to maintain the
structure of the recovered region consistent with the remaining cloud-free
regions. The second, local fusion, transfers complementary information embedded
in the SAR image that corresponds to cloudy areas to generate reliable texture
details of the missing regions, and uses dynamic filtering to alleviate the
performance degradation caused by speckle noise. Extensive evaluation
demonstrates that the proposed algorithm can yield high quality cloud-free
images and outperform state-of-the-art cloud removal algorithms with a gain
about 1.7dB in terms of PSNR on SEN12MS-CR dataset.
","[{'version': 'v1', 'created': 'Mon, 6 Jun 2022 18:53:19 GMT'}, {'version': 'v2', 'created': 'Mon, 8 Aug 2022 10:21:25 GMT'}, {'version': 'v3', 'created': 'Tue, 9 Aug 2022 09:24:27 GMT'}]",2022-08-10,['Computer Vision and Pattern Recognition'],"This paper presents GLF-CR, a novel SAR-enhanced cloud removal approach with global-local fusion for computer vision and pattern recognition. GLF-CR leverages the complementary properties of global and local features to accurately identify cloud-covered areas in SAR images. Specifically, a global feature extractor is used to capture the overall context of the image, while a local feature extractor is used to capture the detailed information of the image. The global and local features are then fused together to generate a cloud mask, which is used to remove the cloud-covered areas. Experiments on real-world SAR images demonstrate the effectiveness of GLF-CR in accurately removing cloud-covered areas and preserving the details of the underlying objects.",Write an abstract for a paper called GLF-CR: SAR-Enhanced Cloud Removal with Global-Local Fusion about Computer Vision and Pattern Recognition
2206.06255,"Hugo Tessier, Vincent Gripon, Mathieu L\'eonardon, Matthieu Arzel,
  David Bertrand, Thomas Hannagan","Energy Consumption Analysis of pruned Semantic Segmentation Networks on
  an Embedded GPU",['cs.NE'],"  Deep neural networks are the state of the art in many computer vision tasks.
Their deployment in the context of autonomous vehicles is of particular
interest, since their limitations in terms of energy consumption prohibit the
use of very large networks, that typically reach the best performance. A common
method to reduce the complexity of these architectures, without sacrificing
accuracy, is to rely on pruning, in which the least important portions are
eliminated. There is a large literature on the subject, but interestingly few
works have measured the actual impact of pruning on energy. In this work, we
are interested in measuring it in the specific context of semantic segmentation
for autonomous driving, using the Cityscapes dataset. To this end, we analyze
the impact of recently proposed structured pruning methods when trained
architectures are deployed on a Jetson Xavier embedded GPU.
","[{'version': 'v1', 'created': 'Mon, 13 Jun 2022 15:36:39 GMT'}]",2022-12-13,['Neural and Evolutionary Computing'],"This paper presents an analysis of energy consumption of pruned semantic segmentation networks on an embedded GPU. We compare the energy consumption of different pruning strategies, such as magnitude pruning, filter pruning, and layer pruning, as well as evolutionary computing algorithms, such as genetic algorithms and evolutionary strategies, for optimizing the pruning of the networks. We also analyze the impact of pruning on the accuracy of the networks. Our results show that the energy consumption of pruned networks can be significantly reduced without compromising the accuracy of the networks. Furthermore, we demonstrate that evolutionary computing algorithms can be used to optimize the pruning of the networks, resulting in further energy savings.","Write an abstract for a paper called Energy Consumption Analysis of pruned Semantic Segmentation Networks on
  an Embedded GPU about Neural and Evolutionary Computing"
2008.11289,"Krishna Somandepalli, Rajat Hebbar, Shrikanth Narayanan","Robust Character Labeling in Movie Videos: Data Resources and
  Self-supervised Feature Adaptation","['cs.CV', 'eess.IV']","  Robust face clustering is a vital step in enabling computational
understanding of visual character portrayal in media. Face clustering for
long-form content is challenging because of variations in appearance and lack
of supporting large-scale labeled data. Our work in this paper focuses on two
key aspects of this problem: the lack of domain-specific training or benchmark
datasets, and adapting face embeddings learned on web images to long-form
content, specifically movies. First, we present a dataset of over 169,000 face
tracks curated from 240 Hollywood movies with weak labels on whether a pair of
face tracks belong to the same or a different character. We propose an offline
algorithm based on nearest-neighbor search in the embedding space to mine
hard-examples from these tracks. We then investigate triplet-loss and multiview
correlation-based methods for adapting face embeddings to hard-examples. Our
experimental results highlight the usefulness of weakly labeled data for
domain-specific feature adaptation. Overall, we find that multiview
correlation-based adaptation yields more discriminative and robust face
embeddings. Its performance on downstream face verification and clustering
tasks is comparable to that of the state-of-the-art results in this domain. We
also present the SAIL-Movie Character Benchmark corpus developed to augment
existing benchmarks. It consists of racially diverse actors and provides
face-quality labels for subsequent error analysis. We hope that the large-scale
datasets developed in this work can further advance automatic character
labeling in videos. All resources are available freely at
https://sail.usc.edu/~ccmi/multiface.
","[{'version': 'v1', 'created': 'Tue, 25 Aug 2020 22:07:41 GMT'}, {'version': 'v2', 'created': 'Fri, 25 Feb 2022 23:18:30 GMT'}]",2022-03-01,['Computer Vision and Pattern Recognition'],"This paper presents a comprehensive overview of robust character labeling in movie videos. It reviews data resources and self-supervised feature adaptation methods for computer vision and pattern recognition applications. It examines existing deep learning-based approaches and their limitations, and proposes a novel self-supervised feature adaptation method for character labeling. The proposed method is evaluated on a publicly available dataset of movie videos, and experimental results demonstrate its effectiveness and robustness. Finally, the paper discusses future directions for research in this field.","Write an abstract for a paper called Robust Character Labeling in Movie Videos: Data Resources and
  Self-supervised Feature Adaptation about Computer Vision and Pattern Recognition"
2212.09242,"Jun Takamatsu, Kazuhiro Sasabuchi, Naoki Wake, Atsushi Kanehira, and
  Katsushi Ikeuchi",Learning-from-Observation System Considering Hardware-Level Reusability,['cs.RO'],"  Robot developers develop various types of robots for satisfying users'
various demands. Users' demands are related to their backgrounds and robots
suitable for users may vary. If a certain developer would offer a robot that is
different from the usual to a user, the robot-specific software has to be
changed. On the other hand, robot-software developers would like to reuse their
developed software as much as possible to reduce their efforts. We propose the
system design considering hardware-level reusability. For this purpose, we
begin with the learning-from-observation framework. This framework represents a
target task in robot-agnostic representation, and thus the represented task
description can be shared with various robots. When executing the task, it is
necessary to convert the robot-agnostic description into commands of a target
robot. To increase the reusability, first, we implement the skill library,
robot motion primitives, only considering a robot hand and we regarded that a
robot was just a carrier to move the hand on the target trajectory. The skill
library is reusable if we would like to the same robot hand. Second, we employ
the generic IK solver to quickly swap a robot. We verify the hardware-level
reusability by applying two task descriptions to two different robots, Nextage
and Fetch.
","[{'version': 'v1', 'created': 'Mon, 19 Dec 2022 04:28:52 GMT'}]",2022-12-20,['Robotics'],"This paper presents a Learning-from-Observation (LfO) system that considers hardware-level reusability for robotics applications. The proposed system enables robots to acquire skills from observation of human demonstrations, while considering the physical constraints of the robot’s hardware. The system is based on a model-free reinforcement learning (RL) approach, and relies on a set of features extracted from the observations. The features are then used to generate a policy that can be used to control the robot. We evaluate the proposed system on a simulated robotic arm and demonstrate its ability to acquire skills from human demonstrations, while considering the physical constraints of the robot’s hardware. The results show that the proposed system is able to generate policies that are more accurate and efficient than those generated by a traditional RL approach that does not consider the physical constraints of the robot’s hardware.",Write an abstract for a paper called Learning-from-Observation System Considering Hardware-Level Reusability about Robotics
2110.0935,"Paolo Rocca, Pietro Da R\`u, Nicola Anselmi, Marco Salucci, Giacomo
  Oliveri, Danilo Erricolo, and Andrea Massa","On the Design of Modular Reflecting EM Skins for Enhanced Urban Wireless
  Coverage","['eess.SY', 'cs.SY']","  The design of modular, passive, and static artificial metasurfaces to be used
as electromagnetic skins (EMSs) of buildings for improving the coverage in
urban millimeter-wave communication scenarios is addressed. Towards this end,
an ad-hoc design strategy is presented to determine optimal trade-off
implementative solutions that assure a suitable coverage of the areas of
interest, where the signal from the base station is too weak, with the minimum
complexity. More specifically, the admissible surface in the building facade is
first partitioned into tiles, which are the minimum-size elements of the
artificial coating (i.e., the building block of an EMS). Then, the search for
the optimal EMS layout (i.e., the minimum number and the positions of the tiles
to be installed) is carried out with a binary multi-objective optimization
method. Representative numerical results are reported and discussed to point
out the features and the potentialities of the EMS solution in the smart
electromagnetic environment (SEME) as well as the effectiveness of the proposed
design method.
","[{'version': 'v1', 'created': 'Mon, 18 Oct 2021 14:23:30 GMT'}]",2022-11-23,['Systems and Control'],"This paper presents a novel design of modular reflecting electromagnetic (EM) skins for enhanced urban wireless coverage. The design is based on the principles of systems and control theory, and takes into account the physical and environmental constraints of urban environments. The proposed design uses modular reflecting EM skins to create a distributed reflecting surface that can be used to increase the coverage of wireless communication systems in urban areas. The paper discusses the design principles of the proposed system, its implementation, and the potential benefits of the proposed system. Furthermore, the paper provides a detailed analysis of the system and its performance in different urban settings. Finally, the paper presents the results of simulations and experiments that illustrate the effectiveness of the proposed design in improving urban wireless coverage.","Write an abstract for a paper called On the Design of Modular Reflecting EM Skins for Enhanced Urban Wireless
  Coverage about Systems and Control"
2210.17224,"Moshe Eliasof, Lars Ruthotto, Eran Treister","$\omega$GNNs: Deep Graph Neural Networks Enhanced by Multiple
  Propagation Operators",['cs.LG'],"  Graph Neural Networks (GNNs) are limited in their propagation operators.
These operators often contain non-negative elements only and are shared across
channels and layers, limiting the expressiveness of GNNs. Moreover, some GNNs
suffer from over-smoothing, limiting their depth. On the other hand,
Convolutional Neural Networks (CNNs) can learn diverse propagation filters, and
phenomena like over-smoothing are typically not apparent in CNNs. In this
paper, we bridge this gap by incorporating trainable channel-wise weighting
factors $\omega$ to learn and mix multiple smoothing and sharpening propagation
operators at each layer. Our generic method is called $\omega$GNN, and we study
two variants: $\omega$GCN and $\omega$GAT. For $\omega$GCN, we theoretically
analyse its behaviour and the impact of $\omega$ on the obtained node features.
Our experiments confirm these findings, demonstrating and explaining how both
variants do not over-smooth. Additionally, we experiment with 15 real-world
datasets on node- and graph-classification tasks, where our $\omega$GCN and
$\omega$GAT perform better or on par with state-of-the-art methods.
","[{'version': 'v1', 'created': 'Mon, 31 Oct 2022 11:08:04 GMT'}]",2022-11-01,['Machine Learning'],"This paper presents a novel approach to graph neural networks (GNNs) called $\omega$GNNs. $\omega$GNNs are an enhanced version of GNNs, where multiple propagation operators are used to capture different kinds of relationships between nodes in a graph. We propose a new method to combine the operators in an iterative manner and use it to propagate information throughout the graph. We evaluate our proposed method on several benchmark datasets and show that it outperforms existing GNNs in terms of accuracy, efficiency, and scalability. The results demonstrate that $\omega$GNNs are a promising approach for machine learning tasks on graphs.","Write an abstract for a paper called $\omega$GNNs: Deep Graph Neural Networks Enhanced by Multiple
  Propagation Operators about Machine Learning"
2210.1546,"Hyunsik Jeon, Jun-Gi Jang, Taehun Kim, U Kang","Accurate Bundle Matching and Generation via Multitask Learning with
  Partially Shared Parameters","['cs.IR', 'cs.AI', 'cs.LG']","  How can we recommend existing bundles to users accurately? How can we
generate new tailored bundles for users? Recommending a bundle, or a group of
various items, has attracted widespread attention in e-commerce owing to the
increased satisfaction of both users and providers. Bundle matching and bundle
generation are two representative tasks in bundle recommendation. The bundle
matching task is to correctly match existing bundles to users while the bundle
generation is to generate new bundles that users would prefer. Although many
recent works have developed bundle recommendation models, they fail to achieve
high accuracy since they do not handle heterogeneous data effectively and do
not learn a method for customized bundle generation. In this paper, we propose
BundleMage, an accurate approach for bundle matching and generation. BundleMage
effectively mixes user preferences of items and bundles using an adaptive gate
technique to achieve high accuracy for the bundle matching. BundleMage also
generates a personalized bundle by learning a generation module that exploits a
user preference and the characteristic of a given incomplete bundle to be
completed. BundleMage further improves its performance using multi-task
learning with partially shared parameters. Through extensive experiments, we
show that BundleMage achieves up to 6.6% higher nDCG in bundle matching and
6.3x higher nDCG in bundle generation than the best competitors. We also
provide qualitative analysis that BundleMage effectively generates bundles
considering both the tastes of users and the characteristics of target bundles.
","[{'version': 'v1', 'created': 'Wed, 19 Oct 2022 09:46:20 GMT'}, {'version': 'v2', 'created': 'Fri, 28 Oct 2022 11:20:20 GMT'}]",2022-10-31,"['Information Retrieval', 'Artificial Intelligence', 'Machine Learning']","This paper presents a novel multitask learning approach to accurately generate and match bundles of information in the context of information retrieval. We introduce a partially shared parameter model that can be effectively used to learn multiple tasks simultaneously. Our model is based on a deep neural network with a shared backbone, allowing for task-specific parameters to be learned. We evaluate our model on a real-world dataset of search engine queries and demonstrate that our model outperforms existing methods in terms of accuracy. Moreover, our model can learn from fewer labeled examples than traditional methods, making it more suitable for real-world applications. Our results suggest that multitask learning with partially shared parameters is a promising approach for accurate bundle matching and generation in the context of information retrieval.","Write an abstract for a paper called Accurate Bundle Matching and Generation via Multitask Learning with
  Partially Shared Parameters about Information Retrieval, Artificial Intelligence, Machine Learning"
2208.071,"Dingmin Wang, Przemys{\l}aw Andrzej Wa{\l}\k{e}ga, and Bernardo Cuenca
  Grau",Seminaive Materialisation in DatalogMTL,"['cs.DB', 'cs.AI']","  DatalogMTL is an extension of Datalog with metric temporal operators that has
found applications in temporal ontology-based data access and query answering,
as well as in stream reasoning. Practical algorithms for DatalogMTL are reliant
on materialisation-based reasoning, where temporal facts are derived in a
forward chaining manner in successive rounds of rule applications. Current
materialisation-based procedures are, however, based on a naive evaluation
strategy, where the main source of inefficiency stems from redundant
computations.
  In this paper, we propose a materialisation-based procedure which,
analogously to the classical seminaive algorithm in Datalog, aims at minimising
redundant computation by ensuring that each temporal rule instance is
considered at most once during the execution of the algorithm. Our experiments
show that our optimised seminaive strategy for DatalogMTL is able to
significantly reduce materialisation times.
","[{'version': 'v1', 'created': 'Mon, 15 Aug 2022 10:04:44 GMT'}, {'version': 'v2', 'created': 'Tue, 27 Sep 2022 16:13:35 GMT'}]",2022-09-28,"['Databases', 'Artificial Intelligence']","This paper presents Seminaive Materialisation, a novel approach to query evaluation in DatalogMTL, a database query language based on the language of monadic temporal logic. The approach combines the use of artificial intelligence techniques such as machine learning and deep learning with classic database techniques such as semi-naive evaluation. Our approach improves the scalability of query evaluation in DatalogMTL and allows for efficient handling of complex queries. We present a prototype implementation and evaluate its performance on a real-world dataset. The results show that the proposed approach is more efficient than the existing semi-naive evaluation method. Additionally, we discuss the potential for further improvements and future research directions.","Write an abstract for a paper called Seminaive Materialisation in DatalogMTL about Databases, Artificial Intelligence"
2111.08611,"Eduard Gorbunov, Hugo Berard, Gauthier Gidel, Nicolas Loizou",Stochastic Extragradient: General Analysis and Improved Rates,"['math.OC', 'cs.LG']","  The Stochastic Extragradient (SEG) method is one of the most popular
algorithms for solving min-max optimization and variational inequalities
problems (VIP) appearing in various machine learning tasks. However, several
important questions regarding the convergence properties of SEG are still open,
including the sampling of stochastic gradients, mini-batching, convergence
guarantees for the monotone finite-sum variational inequalities with possibly
non-monotone terms, and others. To address these questions, in this paper, we
develop a novel theoretical framework that allows us to analyze several
variants of SEG in a unified manner. Besides standard setups, like Same-Sample
SEG under Lipschitzness and monotonicity or Independent-Samples SEG under
uniformly bounded variance, our approach allows us to analyze variants of SEG
that were never explicitly considered in the literature before. Notably, we
analyze SEG with arbitrary sampling which includes importance sampling and
various mini-batching strategies as special cases. Our rates for the new
variants of SEG outperform the current state-of-the-art convergence guarantees
and rely on less restrictive assumptions.
","[{'version': 'v1', 'created': 'Tue, 16 Nov 2021 16:49:31 GMT'}, {'version': 'v2', 'created': 'Mon, 27 Dec 2021 18:24:16 GMT'}, {'version': 'v3', 'created': 'Tue, 22 Feb 2022 11:17:08 GMT'}]",2022-02-23,['Machine Learning'],"This paper presents an analysis of the stochastic extragradient method in machine learning. We provide a general convergence rate analysis of the method and show that the rate is improved when the variance of the stochastic gradients is bounded. We also discuss the application of the method to a variety of problems, such as non-convex optimization, online learning, and reinforcement learning. Our results demonstrate that the stochastic extragradient method can be a powerful tool for solving machine learning problems.",Write an abstract for a paper called Stochastic Extragradient: General Analysis and Improved Rates about Machine Learning
2205.02568,"Mihir Durve, Adriano Tiribocchi, Fabio Bonaccorso, Andrea Montessori,
  Marco Lauricella, Michal Bogdan, Jan Guzowski, Sauro Succi","DropTrack -- automatic droplet tracking using deep learning for
  microfluidic applications","['cs.CV', 'physics.comp-ph']","  Deep neural networks are rapidly emerging as data analysis tools, often
outperforming the conventional techniques used in complex microfluidic systems.
One fundamental analysis frequently desired in microfluidic experiments is
counting and tracking the droplets. Specifically, droplet tracking in dense
emulsions is challenging as droplets move in tightly packed configurations.
Sometimes the individual droplets in these dense clusters are hard to resolve,
even for a human observer. Here, two deep learning-based cutting-edge
algorithms for object detection (YOLO) and object tracking (DeepSORT) are
combined into a single image analysis tool, DropTrack, to track droplets in
microfluidic experiments. DropTrack analyzes input videos, extracts droplets'
trajectories, and infers other observables of interest, such as droplet
numbers. Training an object detector network for droplet recognition with
manually annotated images is a labor-intensive task and a persistent
bottleneck. This work partly resolves this problem by training object detector
networks (YOLOv5) with hybrid datasets containing real and synthetic images. We
present an analysis of a double emulsion experiment as a case study to measure
DropTrack's performance. For our test case, the YOLO networks trained with 60%
synthetic images show similar performance in droplet counting as with the one
trained using 100% real images, meanwhile saving the image annotation work by
60%. DropTrack's performance is measured in terms of mean average precision
(mAP), mean square error in counting the droplets, and inference speed. The
fastest configuration of DropTrack runs inference at about 30 frames per
second, well within the standards for real-time image analysis.
","[{'version': 'v1', 'created': 'Thu, 5 May 2022 11:03:32 GMT'}]",2022-08-17,['Computer Vision and Pattern Recognition'],"This paper presents DropTrack, a novel deep learning based computer vision system for automated droplet tracking in microfluidic applications. DropTrack utilizes a convolutional neural network (CNN) to detect and track droplets in real-time. The system is evaluated on a variety of microfluidic datasets and shows superior performance compared to existing methods. The paper also provides insights into the design and implementation of the system, as well as its potential applications in microfluidics. The results demonstrate that DropTrack is a promising tool for automated droplet tracking in microfluidic applications.","Write an abstract for a paper called DropTrack -- automatic droplet tracking using deep learning for
  microfluidic applications about Computer Vision and Pattern Recognition"
2009.12871,"Francisco Benita, Vittorio Bil\`o, Barnab\'e Monnot, Georgios
  Piliouras and Cosimo Vinci","Data-Driven Models of Selfish Routing: Why Price of Anarchy Does Depend
  on Network Topology","['cs.GT', 'econ.TH']","  We investigate traffic routing both from the perspective of theory as well as
real world data. First, we introduce a new type of games: $\theta$-free flow
games. Here, commuters only consider, in their strategy sets, paths whose
free-flow costs (informally their lengths) are within a small multiplicative
$(1+\theta)$ constant of the optimal free-flow cost path connecting their
source and destination, where $\theta\geq0$. We provide an exhaustive analysis
of tight bounds on PoA($\theta$) for arbitrary classes of cost functions, both
in the case of general congestion/routing games as well as in the special case
of path-disjoint networks. Second, by using a large mobility dataset in
Singapore, we inspect minute-by-minute decision-making of thousands of
commuters, and find that $\theta=1$ is a good estimate of agents' route
(pre)selection mechanism. In contrast, in Pigou networks, the ratio of the
free-flow costs of the routes, and thus $\theta$, is \textit{infinite}; so,
although such worst case networks are mathematically simple, they correspond to
artificial routing scenarios with little resemblance to real world conditions,
opening the possibility of proving much stronger Price of Anarchy guarantees by
explicitly studying their dependency on $\theta$. For example, in the case of
the standard Bureau of Public Roads (BPR) cost model, where$c_e(x)= a_e
x^4+b_e$, and for quartic cost functions in general, the standard PoA bound for
$\theta=\infty$ is $2.1505$, and this is tight both for general networks as
well as path-disjoint and even parallel-edge networks. In comparison, for
$\theta=1$, the PoA in the case of general networks is only $1.6994$, whereas
for path-disjoint/parallel-edge networks is even smaller ($1.3652$), showing
that both the route geometries as captured by the parameter $\theta$ as well as
the network topology have significant effects on PoA.
","[{'version': 'v1', 'created': 'Sun, 27 Sep 2020 15:22:33 GMT'}, {'version': 'v2', 'created': 'Sat, 19 Mar 2022 13:33:21 GMT'}]",2022-03-22,['Computer Science and Game Theory'],"This paper will explore the concept of data-driven models of selfish routing, and how the price of anarchy is dependent on network topology. Through a combination of computer science and game theory, this paper will analyze how different network topologies can affect the efficiency of selfish routing. We will discuss how the topology of the network can be used to improve the performance of selfish routing, and how different network topologies can affect the price of anarchy. We will also analyze how the topology of the network can be manipulated to achieve better efficiency, and how the price of anarchy can be reduced. Finally, we will discuss the implications of these findings, and how they can be used to improve the efficiency of selfish routing in various network topologies.","Write an abstract for a paper called Data-Driven Models of Selfish Routing: Why Price of Anarchy Does Depend
  on Network Topology about Computer Science and Game Theory"
2208.071,"Dingmin Wang, Przemys{\l}aw Andrzej Wa{\l}\k{e}ga, and Bernardo Cuenca
  Grau",Seminaive Materialisation in DatalogMTL,"['cs.DB', 'cs.AI']","  DatalogMTL is an extension of Datalog with metric temporal operators that has
found applications in temporal ontology-based data access and query answering,
as well as in stream reasoning. Practical algorithms for DatalogMTL are reliant
on materialisation-based reasoning, where temporal facts are derived in a
forward chaining manner in successive rounds of rule applications. Current
materialisation-based procedures are, however, based on a naive evaluation
strategy, where the main source of inefficiency stems from redundant
computations.
  In this paper, we propose a materialisation-based procedure which,
analogously to the classical seminaive algorithm in Datalog, aims at minimising
redundant computation by ensuring that each temporal rule instance is
considered at most once during the execution of the algorithm. Our experiments
show that our optimised seminaive strategy for DatalogMTL is able to
significantly reduce materialisation times.
","[{'version': 'v1', 'created': 'Mon, 15 Aug 2022 10:04:44 GMT'}, {'version': 'v2', 'created': 'Tue, 27 Sep 2022 16:13:35 GMT'}]",2022-09-28,"['Databases', 'Artificial Intelligence']","and Machine Learning

This paper examines the concept of seminaive materialisation in DatalogMTL, a database query language based on Datalog and the Monadic Temporal Logic (MTL). Seminaive materialisation is a technique that allows for the efficient evaluation of recursive queries in databases. The paper examines how seminaive materialisation can be used in combination with artificial intelligence and machine learning techniques to improve query evaluation efficiency. The paper evaluates the performance of seminaive materialisation in comparison to other query evaluation techniques, and discusses the potential applications of seminaive materialisation in the fields of artificial intelligence and machine learning. The paper concludes by discussing the implications of seminaive materialisation for the future of database query evaluation.","Write an abstract for a paper called Seminaive Materialisation in DatalogMTL about Databases, Artificial Intelligence"
2111.08611,"Eduard Gorbunov, Hugo Berard, Gauthier Gidel, Nicolas Loizou",Stochastic Extragradient: General Analysis and Improved Rates,"['math.OC', 'cs.LG']","  The Stochastic Extragradient (SEG) method is one of the most popular
algorithms for solving min-max optimization and variational inequalities
problems (VIP) appearing in various machine learning tasks. However, several
important questions regarding the convergence properties of SEG are still open,
including the sampling of stochastic gradients, mini-batching, convergence
guarantees for the monotone finite-sum variational inequalities with possibly
non-monotone terms, and others. To address these questions, in this paper, we
develop a novel theoretical framework that allows us to analyze several
variants of SEG in a unified manner. Besides standard setups, like Same-Sample
SEG under Lipschitzness and monotonicity or Independent-Samples SEG under
uniformly bounded variance, our approach allows us to analyze variants of SEG
that were never explicitly considered in the literature before. Notably, we
analyze SEG with arbitrary sampling which includes importance sampling and
various mini-batching strategies as special cases. Our rates for the new
variants of SEG outperform the current state-of-the-art convergence guarantees
and rely on less restrictive assumptions.
","[{'version': 'v1', 'created': 'Tue, 16 Nov 2021 16:49:31 GMT'}, {'version': 'v2', 'created': 'Mon, 27 Dec 2021 18:24:16 GMT'}, {'version': 'v3', 'created': 'Tue, 22 Feb 2022 11:17:08 GMT'}]",2022-02-23,['Machine Learning'],"This paper presents a comprehensive analysis of the stochastic extragradient method for machine learning. We provide a general convergence rate and prove that the method converges linearly under certain conditions. Furthermore, we propose a new variant of the stochastic extragradient method and prove that it converges faster than the original algorithm. Our numerical experiments demonstrate the effectiveness of the proposed method.",Write an abstract for a paper called Stochastic Extragradient: General Analysis and Improved Rates about Machine Learning
2205.02568,"Mihir Durve, Adriano Tiribocchi, Fabio Bonaccorso, Andrea Montessori,
  Marco Lauricella, Michal Bogdan, Jan Guzowski, Sauro Succi","DropTrack -- automatic droplet tracking using deep learning for
  microfluidic applications","['cs.CV', 'physics.comp-ph']","  Deep neural networks are rapidly emerging as data analysis tools, often
outperforming the conventional techniques used in complex microfluidic systems.
One fundamental analysis frequently desired in microfluidic experiments is
counting and tracking the droplets. Specifically, droplet tracking in dense
emulsions is challenging as droplets move in tightly packed configurations.
Sometimes the individual droplets in these dense clusters are hard to resolve,
even for a human observer. Here, two deep learning-based cutting-edge
algorithms for object detection (YOLO) and object tracking (DeepSORT) are
combined into a single image analysis tool, DropTrack, to track droplets in
microfluidic experiments. DropTrack analyzes input videos, extracts droplets'
trajectories, and infers other observables of interest, such as droplet
numbers. Training an object detector network for droplet recognition with
manually annotated images is a labor-intensive task and a persistent
bottleneck. This work partly resolves this problem by training object detector
networks (YOLOv5) with hybrid datasets containing real and synthetic images. We
present an analysis of a double emulsion experiment as a case study to measure
DropTrack's performance. For our test case, the YOLO networks trained with 60%
synthetic images show similar performance in droplet counting as with the one
trained using 100% real images, meanwhile saving the image annotation work by
60%. DropTrack's performance is measured in terms of mean average precision
(mAP), mean square error in counting the droplets, and inference speed. The
fastest configuration of DropTrack runs inference at about 30 frames per
second, well within the standards for real-time image analysis.
","[{'version': 'v1', 'created': 'Thu, 5 May 2022 11:03:32 GMT'}]",2022-08-17,['Computer Vision and Pattern Recognition'],"This paper presents DropTrack, a novel approach to automatically track droplets in microfluidic applications using deep learning. DropTrack leverages advances in computer vision and pattern recognition to accurately detect, track, and classify droplets in microfluidic systems. The system is composed of a convolutional neural network (CNN) and a tracking algorithm to accurately identify and track droplets. The CNN is trained on a large dataset of droplet images, while the tracking algorithm uses the CNN output to identify and track droplets. The evaluation of DropTrack is performed using microfluidic images of droplets in different scenarios. The results show that DropTrack outperforms existing methods in terms of accuracy, speed, and robustness. The paper also discusses potential applications of DropTrack in microfluidic systems, such as automated droplet sorting and droplet manipulation.","Write an abstract for a paper called DropTrack -- automatic droplet tracking using deep learning for
  microfluidic applications about Computer Vision and Pattern Recognition"
2009.12871,"Francisco Benita, Vittorio Bil\`o, Barnab\'e Monnot, Georgios
  Piliouras and Cosimo Vinci","Data-Driven Models of Selfish Routing: Why Price of Anarchy Does Depend
  on Network Topology","['cs.GT', 'econ.TH']","  We investigate traffic routing both from the perspective of theory as well as
real world data. First, we introduce a new type of games: $\theta$-free flow
games. Here, commuters only consider, in their strategy sets, paths whose
free-flow costs (informally their lengths) are within a small multiplicative
$(1+\theta)$ constant of the optimal free-flow cost path connecting their
source and destination, where $\theta\geq0$. We provide an exhaustive analysis
of tight bounds on PoA($\theta$) for arbitrary classes of cost functions, both
in the case of general congestion/routing games as well as in the special case
of path-disjoint networks. Second, by using a large mobility dataset in
Singapore, we inspect minute-by-minute decision-making of thousands of
commuters, and find that $\theta=1$ is a good estimate of agents' route
(pre)selection mechanism. In contrast, in Pigou networks, the ratio of the
free-flow costs of the routes, and thus $\theta$, is \textit{infinite}; so,
although such worst case networks are mathematically simple, they correspond to
artificial routing scenarios with little resemblance to real world conditions,
opening the possibility of proving much stronger Price of Anarchy guarantees by
explicitly studying their dependency on $\theta$. For example, in the case of
the standard Bureau of Public Roads (BPR) cost model, where$c_e(x)= a_e
x^4+b_e$, and for quartic cost functions in general, the standard PoA bound for
$\theta=\infty$ is $2.1505$, and this is tight both for general networks as
well as path-disjoint and even parallel-edge networks. In comparison, for
$\theta=1$, the PoA in the case of general networks is only $1.6994$, whereas
for path-disjoint/parallel-edge networks is even smaller ($1.3652$), showing
that both the route geometries as captured by the parameter $\theta$ as well as
the network topology have significant effects on PoA.
","[{'version': 'v1', 'created': 'Sun, 27 Sep 2020 15:22:33 GMT'}, {'version': 'v2', 'created': 'Sat, 19 Mar 2022 13:33:21 GMT'}]",2022-03-22,['Computer Science and Game Theory'],"This paper examines the implications of network topology on the Price of Anarchy (PoA) in selfish routing models. We analyze the impact of various topologies, such as complete graphs, star graphs, and trees, on the PoA in two-player games. We develop data-driven models to capture the behavior of players in these games and study the impact of network topology on the PoA. Our results show that the PoA does indeed depend on the network topology, and that network topology can be used to design strategies that reduce the PoA. We discuss the implications of our work for the design of networks and routing policies.","Write an abstract for a paper called Data-Driven Models of Selfish Routing: Why Price of Anarchy Does Depend
  on Network Topology about Computer Science and Game Theory"
2204.04397,Yunfan Hu and Zhaopeng Qiu and Xian Wu,"Denoising Neural Network for News Recommendation with Positive and
  Negative Implicit Feedback","['cs.IR', 'cs.CL']","  News recommendation is different from movie or e-commercial recommendation as
people usually do not grade the news. Therefore, user feedback for news is
always implicit (click behavior, reading time, etc). Inevitably, there are
noises in implicit feedback. On one hand, the user may exit immediately after
clicking the news as he dislikes the news content, leaving the noise in his
positive implicit feedback; on the other hand, the user may be recommended
multiple interesting news at the same time and only click one of them,
producing the noise in his negative implicit feedback. Opposite implicit
feedback could construct more integrated user preferences and help each other
to minimize the noise influence. Previous works on news recommendation only
used positive implicit feedback and suffered from the noise impact. In this
paper, we propose a denoising neural network for news recommendation with
positive and negative implicit feedback, named DRPN. DRPN utilizes both
feedback for recommendation with a module to denoise both positive and negative
implicit feedback to further enhance the performance. Experiments on the
real-world large-scale dataset demonstrate the state-of-the-art performance of
DRPN.
","[{'version': 'v1', 'created': 'Sat, 9 Apr 2022 05:47:17 GMT'}]",2022-04-12,"['Information Retrieval', 'Computation and Language']","This paper presents a denoising neural network for news recommendation with positive and negative implicit feedback about information retrieval, computation and language. The proposed model is a deep learning-based method that combines user-level and item-level representations for news recommendation. It employs an attention mechanism to capture the user’s preferences and a denoising autoencoder to learn the latent features of news items. The model is evaluated on two real-world datasets and compared with several state-of-the-art recommendation models. The results show that the proposed model outperforms the baselines in terms of precision, recall, and F1-score. Furthermore, the model can effectively capture the user’s preferences and provide better recommendations.","Write an abstract for a paper called Denoising Neural Network for News Recommendation with Positive and
  Negative Implicit Feedback about Information Retrieval, Computation and Language"
2210.11774,"Martino Borello, Paolo Santonastaso, Ferdinando Zullo","Left ideal LRPC codes and a ROLLO-type cryptosystem based on group
  algebras","['cs.IT', 'cs.CR', 'math.IT', 'math.RA']","  In this paper we introduce left ideal low-rank parity-check codes by using
group algebras and we finally use them to extend ROLLO-I KEM.
","[{'version': 'v1', 'created': 'Fri, 21 Oct 2022 07:19:58 GMT'}]",2022-10-24,"['Information Theory', 'Cryptography and Security']","This paper presents a new approach to the study of Information Theory, Cryptography and Security by introducing Left ideal LRPC codes and a ROLLO-type cryptosystem based on group algebras. The paper discusses the properties of Left ideal LRPC codes and their applications in cryptography. It then presents the ROLLO cryptosystem, which is based on group algebras, and explains how it can be used to construct secure cryptographic systems. Finally, the paper provides an analysis of the security of the proposed cryptosystem and discusses the implications of the results.","Write an abstract for a paper called Left ideal LRPC codes and a ROLLO-type cryptosystem based on group
  algebras about Information Theory, Cryptography and Security"
2303.11681,"Weijia Wu, Yuzhong Zhao, Mike Zheng Shou, Hong Zhou, Chunhua Shen","DiffuMask: Synthesizing Images with Pixel-level Annotations for Semantic
  Segmentation Using Diffusion Models",['cs.CV'],"  Collecting and annotating images with pixel-wise labels is time-consuming and
laborious. In contrast, synthetic data can be freely available using a
generative model (e.g., DALL-E, Stable Diffusion). In this paper, we show that
it is possible to automatically obtain accurate semantic masks of synthetic
images generated by the Off-the-shelf Stable Diffusion model, which uses only
text-image pairs during training. Our approach, called DiffuMask, exploits the
potential of the cross-attention map between text and image, which is natural
and seamless to extend the text-driven image synthesis to semantic mask
generation. DiffuMask uses text-guided cross-attention information to localize
class/word-specific regions, which are combined with practical techniques to
create a novel high-resolution and class-discriminative pixel-wise mask. The
methods help to reduce data collection and annotation costs obviously.
Experiments demonstrate that the existing segmentation methods trained on
synthetic data of DiffuMask can achieve a competitive performance over the
counterpart of real data (VOC 2012, Cityscapes). For some classes (e.g., bird),
DiffuMask presents promising performance, close to the stateof-the-art result
of real data (within 3% mIoU gap). Moreover, in the open-vocabulary
segmentation (zero-shot) setting, DiffuMask achieves a new SOTA result on
Unseen class of VOC 2012. The project website can be found at
https://weijiawu.github.io/DiffusionMask/.
","[{'version': 'v1', 'created': 'Tue, 21 Mar 2023 08:43:15 GMT'}]",2023-03-22,['Computer Vision and Pattern Recognition'],"This paper presents DiffuMask, a novel approach for synthesizing images with pixel-level annotations for semantic segmentation using diffusion models. DiffuMask is based on the idea of using a diffusion process to propagate annotations from a source image to a target image, while preserving the appearance of the target image. This approach is demonstrated through experiments on several datasets, including PASCAL VOC, Cityscapes, and COCO-Stuff. The results show that DiffuMask outperforms existing methods in terms of both accuracy and speed, and is suitable for a wide range of applications in computer vision and pattern recognition.","Write an abstract for a paper called DiffuMask: Synthesizing Images with Pixel-level Annotations for Semantic
  Segmentation Using Diffusion Models about Computer Vision and Pattern Recognition"
2111.0201,"M.A. Ganaie, M. Tanveer, P.N. Suganthan, V. Snasel",Oblique and rotation double random forest,"['cs.LG', 'cs.AI']","  Random Forest is an ensemble of decision trees based on the bagging and
random subspace concepts. As suggested by Breiman, the strength of unstable
learners and the diversity among them are the ensemble models' core strength.
In this paper, we propose two approaches known as oblique and rotation double
random forests. In the first approach, we propose rotation based double random
forest. In rotation based double random forests, transformation or rotation of
the feature space is generated at each node. At each node different random
feature subspace is chosen for evaluation, hence the transformation at each
node is different. Different transformations result in better diversity among
the base learners and hence, better generalization performance. With the double
random forest as base learner, the data at each node is transformed via two
different transformations namely, principal component analysis and linear
discriminant analysis. In the second approach, we propose oblique double random
forest. Decision trees in random forest and double random forest are
univariate, and this results in the generation of axis parallel split which
fails to capture the geometric structure of the data. Also, the standard random
forest may not grow sufficiently large decision trees resulting in suboptimal
performance. To capture the geometric properties and to grow the decision trees
of sufficient depth, we propose oblique double random forest. The oblique
double random forest models are multivariate decision trees. At each non-leaf
node, multisurface proximal support vector machine generates the optimal plane
for better generalization performance. Also, different regularization
techniques are employed for tackling the small sample size problems in the
decision trees of oblique double random forest.
","[{'version': 'v1', 'created': 'Wed, 3 Nov 2021 04:19:41 GMT'}, {'version': 'v2', 'created': 'Sat, 6 Nov 2021 06:54:59 GMT'}, {'version': 'v3', 'created': 'Tue, 9 Aug 2022 19:02:09 GMT'}]",2022-08-11,"['Machine Learning', 'Artificial Intelligence']","This paper presents a novel Machine Learning and Artificial Intelligence approach called Oblique and Rotation Double Random Forest (ORDER). ORDER is an ensemble learning method combining the oblique tree and rotation forest methods. The oblique tree utilizes orthogonal splits to reduce the number of dimensions and improve the accuracy of the model, while the rotation forest uses random rotations of the feature space to increase the diversity of the trees in the ensemble. The two methods are combined to form a double random forest, which is then evaluated on a variety of benchmark datasets. Results show that ORDER consistently outperforms the standard random forest and oblique tree methods, providing a more accurate and robust model for Machine Learning and Artificial Intelligence applications.","Write an abstract for a paper called Oblique and rotation double random forest about Machine Learning, Artificial Intelligence"
2207.01583,"Ashkan Mirzaei, Yash Kant, Jonathan Kelly, and Igor Gilitschenski",LaTeRF: Label and Text Driven Object Radiance Fields,['cs.CV'],"  Obtaining 3D object representations is important for creating photo-realistic
simulations and for collecting AR and VR assets. Neural fields have shown their
effectiveness in learning a continuous volumetric representation of a scene
from 2D images, but acquiring object representations from these models with
weak supervision remains an open challenge. In this paper we introduce LaTeRF,
a method for extracting an object of interest from a scene given 2D images of
the entire scene, known camera poses, a natural language description of the
object, and a set of point-labels of object and non-object points in the input
images. To faithfully extract the object from the scene, LaTeRF extends the
NeRF formulation with an additional `objectness' probability at each 3D point.
Additionally, we leverage the rich latent space of a pre-trained CLIP model
combined with our differentiable object renderer, to inpaint the occluded parts
of the object. We demonstrate high-fidelity object extraction on both synthetic
and real-world datasets and justify our design choices through an extensive
ablation study.
","[{'version': 'v1', 'created': 'Mon, 4 Jul 2022 17:07:57 GMT'}, {'version': 'v2', 'created': 'Tue, 5 Jul 2022 14:32:57 GMT'}, {'version': 'v3', 'created': 'Mon, 18 Jul 2022 18:27:31 GMT'}]",2022-09-05,['Computer Vision and Pattern Recognition'],"This paper presents LaTeRF, a novel object detection method based on Label and Text Driven Object Radiance Fields. LaTeRF is a supervised learning approach that combines object labels and text descriptions to create a Radiance Field representation of an object. This representation is then used to detect objects in images. Experiments on the PASCAL VOC dataset show that LaTeRF can achieve competitive results compared to state-of-the-art methods for object detection. The results demonstrate the effectiveness of LaTeRF in recognizing objects in images, and thus could have potential applications in the fields of computer vision and pattern recognition.",Write an abstract for a paper called LaTeRF: Label and Text Driven Object Radiance Fields about Computer Vision and Pattern Recognition
2302.10666,Jan Komenda and Tom\'a\v{s} Masopust,"Supervisory Control of Modular Discrete-Event Systems under Partial
  Observation: Normality",['cs.FL'],"  Complex systems are often composed of many small communicating components
called modules. We investigate the synthesis of supervisory controllers for
modular systems under partial observation that, as the closed-loop system,
realize the supremal normal sublanguage of the specification. We call such
controllers maximally permissive normal supervisors. The challenge in modular
systems is to find conditions under which the global nonblocking and maximally
permissive normal supervisor can be achieved locally as the parallel
composition of local normal supervisors. We show that a structural concept of
hierarchical supervisory control called modified observation consistency (MOC)
is such a condition. However, the algorithmic verification of MOC is an open
problem, and therefore it is necessary to find easily-verifiable conditions
that ensure MOC. We show that the condition that all shared events are
observable is such a condition. Considering specifications, we examine both
local specifications, where each module has its own specification, and global
specifications. We combine our results for normality with the existing results
for controllability to locally synthesize the nonblocking and maximally
permissive controllable and normal supervisor. Finally, we illustrate the
results on an industrial case study of the patient table of an MRI scanner.
","[{'version': 'v1', 'created': 'Tue, 21 Feb 2023 13:26:44 GMT'}]",2023-02-22,['Formal Languages and Automata Theory'],"This paper explores the use of formal language theory and automata theory to develop a supervisory control system for modular discrete-event systems under partial observation. It begins by introducing the concept of modular discrete-event systems and discussing their importance in modern industrial automation. It then examines the use of formal language theory and automata theory to develop a supervisory control system for modular discrete-event systems under partial observation. The paper then presents a formal language model for the modular discrete-event system, which is used to develop a supervisory control system. Finally, the paper discusses the implications of the results and provides a summary of the findings. This paper provides an important contribution to the field of industrial automation, as it demonstrates how to use formal language theory and automata theory to develop a supervisory control system for modular discrete-event systems under partial observation.","Write an abstract for a paper called Supervisory Control of Modular Discrete-Event Systems under Partial
  Observation: Normality about Formal Languages and Automata Theory"
2211.14091,"Junbo Zhang, Guofan Fan, Guanghan Wang, Zhengyuan Su, Kaisheng Ma, Li
  Yi",Language-Assisted 3D Feature Learning for Semantic Scene Understanding,['cs.CV'],"  Learning descriptive 3D features is crucial for understanding 3D scenes with
diverse objects and complex structures. However, it is usually unknown whether
important geometric attributes and scene context obtain enough emphasis in an
end-to-end trained 3D scene understanding network. To guide 3D feature learning
toward important geometric attributes and scene context, we explore the help of
textual scene descriptions. Given some free-form descriptions paired with 3D
scenes, we extract the knowledge regarding the object relationships and object
attributes. We then inject the knowledge to 3D feature learning through three
classification-based auxiliary tasks. This language-assisted training can be
combined with modern object detection and instance segmentation methods to
promote 3D semantic scene understanding, especially in a label-deficient
regime. Moreover, the 3D feature learned with language assistance is better
aligned with the language features, which can benefit various 3D-language
multimodal tasks. Experiments on several benchmarks of 3D-only and 3D-language
tasks demonstrate the effectiveness of our language-assisted 3D feature
learning. Code is available at
https://github.com/Asterisci/Language-Assisted-3D.
","[{'version': 'v1', 'created': 'Fri, 25 Nov 2022 13:21:59 GMT'}, {'version': 'v2', 'created': 'Sun, 11 Dec 2022 03:35:39 GMT'}]",2022-12-13,['Computer Vision and Pattern Recognition'],"This paper presents a novel language-assisted 3D feature learning approach for semantic scene understanding in computer vision and pattern recognition. The proposed method is based on a deep learning architecture that combines 3D convolutional neural networks with natural language processing techniques. Experiments conducted on the NYU Depth dataset demonstrate that the proposed approach achieves state-of-the-art results in semantic scene understanding tasks, such as object recognition, material recognition, and scene segmentation. Furthermore, the proposed method is shown to be robust to different types of noise and to be able to generalize to unseen data. The paper also discusses the potential applications of the proposed approach in the field of computer vision and pattern recognition.",Write an abstract for a paper called Language-Assisted 3D Feature Learning for Semantic Scene Understanding about Computer Vision and Pattern Recognition
2108.11254,"Dong Li, Chaoyu Quan and Jiao Xu","Stability and convergence of Strang splitting. Part II: tensorial
  Allen-Cahn equations","['math.NA', 'cs.NA', 'math.AP']","  We consider the second-order in time Strang-splitting approximation for
vector-valued and matrix-valued Allen-Cahn equations. Both the linear
propagator and the nonlinear propagator are computed explicitly. For the
vector-valued case, we prove the maximum principle and unconditional energy
dissipation for a judiciously modified energy functional. The modified energy
functional is close to the classical energy up to $\mathcal O(\tau)$ where
$\tau$ is the splitting step. For the matrix-valued case, we prove a sharp
maximum principle in the matrix Frobenius norm. We show modified energy
dissipation under very mild splitting step constraints. We exhibit several
numerical examples to show the efficiency of the method as well as the
sharpness of the results.
","[{'version': 'v1', 'created': 'Wed, 25 Aug 2021 14:25:28 GMT'}, {'version': 'v2', 'created': 'Tue, 1 Mar 2022 03:42:00 GMT'}]",2022-03-02,['Numerical Analysis'],"This paper presents a numerical analysis of the stability and convergence of Strang splitting for a system of tensorial Allen-Cahn equations. We derive a numerical scheme based on Strang splitting and prove its stability and convergence using a combination of energy estimates and Fourier analysis. We also discuss the numerical implementation of the scheme and present numerical results to illustrate the accuracy and efficiency of the proposed method. Finally, we discuss the implications of our results and its applications in the numerical analysis of a wide range of physical phenomena.","Write an abstract for a paper called Stability and convergence of Strang splitting. Part II: tensorial
  Allen-Cahn equations about Numerical Analysis"
2205.1544,"Jacob Azoulay, Nico Carballal","Lithium-Ion Battery Charging Schedule Optimization to Balance Battery
  Usage and Degradation","['math.OC', 'cs.PF']","  This work optimizes a lithium-ion battery charging schedule while considering
a joint revenue and battery degradation model. The study extends the work of
Meheswari et. al. to encourage battery usage/charging at optimal intervals
depending on energy cost forecasts. This paper utilizes central difference
Nesterov momentum gradient descent to come to optimal charging strategies and
deal with the non-linearities of the battery degradation model. This
optimization strategy is tested against constant, random varied price forecasts
and a novel Gaussian process cost forecasting model. Contrary to many other
papers regarding battery charging, formulating schedule optimization as a
multivariate optimization problem provides meaningful insight to the inherent
balance between these two competing objectives.
","[{'version': 'v1', 'created': 'Mon, 30 May 2022 21:35:57 GMT'}]",2022-06-01,['Performance'],"This paper presents a study on Lithium-Ion Battery Charging Schedule Optimization to Balance Battery Usage and Degradation about Performance. The study focuses on developing a charging schedule that will optimize the battery performance while minimizing the battery degradation. The study is conducted by using a simulation model of a Lithium-Ion battery and its charging system. The study also considers the influence of different charging parameters, such as charge rate and temperature, on the battery performance and degradation. The results of the study demonstrate that the proposed charging schedule can effectively balance the battery usage and degradation, and improve the battery performance. The proposed charging schedule can also be applied to other types of batteries, such as Nickel-Metal Hydride and Lead-Acid batteries.","Write an abstract for a paper called Lithium-Ion Battery Charging Schedule Optimization to Balance Battery
  Usage and Degradation about Performance"
2207.07703,"Hossam Farag, Cedomir Stefanovic and Mikael Gidlund","Distributed Backlog-Aware D2D Communication for Heterogeneous IIoT
  Applications",['cs.NI'],"  Delay and Age-of-Information (AoI) are two crucial performance metrics for
emerging time-sensitive applications in Industrial Internet of Things (IIoT).
In order to achieve optimal performance, studying the inherent interplay
between these two parameters in non-trivial task. In this work, we consider a
Device-to-Device (D2D)-based heterogeneous IIoT network that supports two types
of traffic flows, namely AoI-orientated. First, we introduce a distributed
backlog-aware random access protocol that allows the AoI-orientated nodes to
opportunistically access the channel based on the queue occupancy of the
delay-oriented node. Then, we develop an analytical framework to evaluate the
average delay and the average AoI, and formulate an optimization problem to
minimize the AoI under a given delay constraint. Finally, we provide numerical
results to demonstrate the impact of different network parameters on the
performance in terms of the average delay and the average AoI. We also give the
numerical solutions of the optimal parameters that minimize the AoI subject to
a delay constraint.
","[{'version': 'v1', 'created': 'Fri, 15 Jul 2022 18:57:19 GMT'}]",2022-07-19,['Networking and Internet Architecture'],"This paper presents an innovative distributed backlog-aware device-to-device (D2D) communication approach for heterogeneous Industrial Internet of Things (IIoT) applications. The proposed approach is based on a distributed network architecture and leverages the capabilities of the underlying devices to provide efficient and reliable communication. The proposed approach reduces communication latency by proactively identifying the backlogged devices and providing them with a higher priority for communication. Furthermore, the proposed approach reduces communication overhead by leveraging the capabilities of the underlying devices to prioritize traffic and provide an efficient communication path. Simulation results demonstrate that the proposed approach outperforms existing approaches in terms of latency and communication overhead. This paper provides an effective and efficient solution for IIoT applications that require low latency and low communication overhead.","Write an abstract for a paper called Distributed Backlog-Aware D2D Communication for Heterogeneous IIoT
  Applications about Networking and Internet Architecture"
2111.03009,"Sampath Kumar Mulagaleti, Alberto Bemporad, Mario Zanon","Computation of Input Disturbance Sets for Constrained Output
  Reachability","['math.OC', 'cs.SY', 'eess.SY']","  Linear models with additive unknown-but-bounded input disturbances are
extensively used to model uncertainty in robust control systems design.
Typically, the disturbance set is either assumed to be known a priori or
estimated from data through set-membership identification. However, the problem
of computing a suitable input disturbance set in case the set of possible
output values is assigned a priori has received relatively little attention.
This problem arises in many contexts, such as in supervisory control, actuator
design, decentralized control, and others. In this paper, we propose a method
to compute input disturbance sets (and the corresponding set of states) such
that the resulting set of outputs matches as closely as possible a given set of
outputs, while additionally satisfying strict (inner or outer) inclusion
constraints. We formulate the problem as an optimization problem by relying on
the concept of robust invariance. The effectiveness of the approach is
demonstrated in numerical examples that illustrate how to solve safe reference
set and input-constraint set computation problems.
","[{'version': 'v1', 'created': 'Thu, 4 Nov 2021 17:07:42 GMT'}]",2022-08-22,['Systems and Control'],This paper investigates the computation of input disturbance sets for constrained output reachability of dynamical systems. The proposed approach utilizes a set-based approach to compute the input disturbance sets for a given system and constraint. The paper presents the theoretical background and an efficient computational approach for computing the reachable sets for a given system and constraint. The paper also presents an illustrative example to demonstrate the proposed approach and its effectiveness in computing the input disturbance sets for constrained output reachability. The results obtained from the example show that the proposed approach is effective and efficient in computing the input disturbance sets for constrained output reachability.,"Write an abstract for a paper called Computation of Input Disturbance Sets for Constrained Output
  Reachability about Systems and Control"
2303.14828,"Dina Bashkirova, Samarth Mishra, Diala Lteif, Piotr Teterwak, Donghyun
  Kim, Fadi Alladkani, James Akl, Berk Calli, Sarah Adel Bargal, Kate Saenko,
  Daehan Kim, Minseok Seo, YoungJin Jeon, Dong-Geol Choi, Shahaf Ettedgui, Raja
  Giryes, Shady Abu-Hussein, Binhui Xie, Shuang Li",VisDA 2022 Challenge: Domain Adaptation for Industrial Waste Sorting,['cs.CV'],"  Label-efficient and reliable semantic segmentation is essential for many
real-life applications, especially for industrial settings with high visual
diversity, such as waste sorting. In industrial waste sorting, one of the
biggest challenges is the extreme diversity of the input stream depending on
factors like the location of the sorting facility, the equipment available in
the facility, and the time of year, all of which significantly impact the
composition and visual appearance of the waste stream. These changes in the
data are called ``visual domains'', and label-efficient adaptation of models to
such domains is needed for successful semantic segmentation of industrial
waste. To test the abilities of computer vision models on this task, we present
the VisDA 2022 Challenge on Domain Adaptation for Industrial Waste Sorting. Our
challenge incorporates a fully-annotated waste sorting dataset, ZeroWaste,
collected from two real material recovery facilities in different locations and
seasons, as well as a novel procedurally generated synthetic waste sorting
dataset, SynthWaste. In this competition, we aim to answer two questions: 1)
can we leverage domain adaptation techniques to minimize the domain gap? and 2)
can synthetic data augmentation improve performance on this task and help adapt
to changing data distributions? The results of the competition show that
industrial waste detection poses a real domain adaptation problem, that domain
generalization techniques such as augmentations, ensembling, etc., improve the
overall performance on the unlabeled target domain examples, and that
leveraging synthetic data effectively remains an open problem. See
https://ai.bu.edu/visda-2022/
","[{'version': 'v1', 'created': 'Sun, 26 Mar 2023 21:38:38 GMT'}]",2023-03-28,['Computer Vision and Pattern Recognition'],"This paper presents the VisDA 2022 Challenge, a domain adaptation task for industrial waste sorting using computer vision and pattern recognition. The challenge presents a real-world problem in which images of industrial waste need to be accurately and quickly identified and sorted into different categories. The paper describes the motivation behind the challenge and provides details on the dataset, evaluation metrics, and submission guidelines. The paper further discusses the potential benefits and applications of the challenge, such as improved accuracy and speed of industrial waste sorting, and improved understanding of domain adaptation techniques. The paper concludes with a discussion of the expected outcomes of the challenge and future directions for research.",Write an abstract for a paper called VisDA 2022 Challenge: Domain Adaptation for Industrial Waste Sorting about Computer Vision and Pattern Recognition
2203.11997,"Meng Feng, Chieh-Chi Kao, Qingming Tang, Ming Sun, Viktor Rozgic,
  Spyros Matsoukas, Chao Wang",Federated Self-Supervised Learning for Acoustic Event Classification,"['cs.SD', 'cs.LG', 'eess.AS']","  Standard acoustic event classification (AEC) solutions require large-scale
collection of data from client devices for model optimization. Federated
learning (FL) is a compelling framework that decouples data collection and
model training to enhance customer privacy. In this work, we investigate the
feasibility of applying FL to improve AEC performance while no customer data
can be directly uploaded to the server. We assume no pseudo labels can be
inferred from on-device user inputs, aligning with the typical use cases of
AEC. We adapt self-supervised learning to the FL framework for on-device
continual learning of representations, and it results in improved performance
of the downstream AEC classifiers without labeled/pseudo-labeled data
available. Compared to the baseline w/o FL, the proposed method improves
precision up to 20.3\% relatively while maintaining the recall. Our work
differs from prior work in FL in that our approach does not require
user-generated learning targets, and the data we use is collected from our Beta
program and is de-identified, to maximally simulate the production settings.
","[{'version': 'v1', 'created': 'Tue, 22 Mar 2022 18:49:52 GMT'}]",2022-03-24,"['Sound', 'Machine Learning']",", and AI

This paper presents a novel federated self-supervised learning approach for acoustic event classification. The proposed approach is based on the application of a convolutional neural network (CNN) model to classify sound events from raw audio signals. The federated learning framework is used to enable the collaborative learning of the model from multiple distributed sound sources. The self-supervised learning approach is used to learn representations from unlabeled data without relying on manual annotations. The performance of the proposed approach is evaluated on two publicly available datasets and compared against a baseline supervised learning approach. Results show that the proposed approach achieves comparable performance to the baseline approach while reducing the need for manual annotations and allowing for distributed learning of the model. The proposed approach is a promising solution for the development of AI systems for acoustic event classification in real-world scenarios.","Write an abstract for a paper called Federated Self-Supervised Learning for Acoustic Event Classification about Sound, Machine Learning"
2203.1009,"Xiaotian Yu, Yifan Yang, Aibo Wang, Ling Xing, Hanling Yi, Guangming
  Lu, Xiaoyu Wang",FaceMap: Towards Unsupervised Face Clustering via Map Equation,"['cs.CV', 'cs.LG']","  Face clustering is an essential task in computer vision due to the explosion
of related applications such as augmented reality or photo album management.
The main challenge of this task lies in the imperfectness of similarities among
image feature representations. Given an existing feature extraction model, it
is still an unresolved problem that how can the inherent characteristics of
similarities of unlabelled images be leveraged to improve the clustering
performance. Motivated by answering the question, we develop an effective
unsupervised method, named as FaceMap, by formulating face clustering as a
process of non-overlapping community detection, and minimizing the entropy of
information flows on a network of images. The entropy is denoted by the map
equation and its minimum represents the least description of paths among images
in expectation. Inspired by observations on the ranked transition probabilities
in the affinity graph constructed from facial images, we develop an outlier
detection strategy to adaptively adjust transition probabilities among images.
Experiments with ablation studies demonstrate that FaceMap significantly
outperforms existing methods and achieves new state-of-the-arts on three
popular large-scale datasets for face clustering, e.g., an absolute improvement
of more than $10\%$ and $4\%$ comparing with prior unsupervised and supervised
methods respectively in terms of average of Pairwise F-score. Our code is
publicly available on github.
","[{'version': 'v1', 'created': 'Mon, 21 Mar 2022 03:23:09 GMT'}]",2022-03-22,"['Computer Vision and Pattern Recognition', 'Machine Learning']","This paper presents FaceMap, a novel unsupervised face clustering algorithm based on the Map Equation. FaceMap utilizes a combination of computer vision and pattern recognition techniques, as well as machine learning, to accurately group facial images into clusters. The algorithm is evaluated on a large-scale face dataset, and results show that FaceMap outperforms existing state-of-the-art methods for unsupervised face clustering. The proposed algorithm is also compared to supervised methods, and results demonstrate that FaceMap is competitive with supervised methods in terms of accuracy and speed. The paper also discusses the potential applications of FaceMap in facial recognition and facial verification tasks.","Write an abstract for a paper called FaceMap: Towards Unsupervised Face Clustering via Map Equation about Computer Vision and Pattern Recognition, Machine Learning"
2103.03407,Alexander D. Gilbert and Robert Scheichl,"Multilevel quasi-Monte Carlo for random elliptic eigenvalue problems II:
  Efficient algorithms and numerical results","['math.NA', 'cs.NA']","  Stochastic PDE eigenvalue problems often arise in the field of uncertainty
quantification, whereby one seeks to quantify the uncertainty in an eigenvalue,
or its eigenfunction. In this paper we present an efficient multilevel
quasi-Monte Carlo (MLQMC) algorithm for computing the expectation of the
smallest eigenvalue of an elliptic eigenvalue problem with stochastic
coefficients. Each sample evaluation requires the solution of a PDE eigenvalue
problem, and so tackling this problem in practice is notoriously
computationally difficult. We speed up the approximation of this expectation in
four ways: we use a multilevel variance reduction scheme to spread the work
over a hierarchy of FE meshes and truncation dimensions; we use QMC methods to
efficiently compute the expectations on each level; we exploit the smoothness
in parameter space and reuse the eigenvector from a nearby QMC point to reduce
the number of iterations of the eigensolver; and we utilise a two-grid
discretisation scheme to obtain the eigenvalue on the fine mesh with a single
linear solve. The full error analysis of a basic MLQMC algorithm is given in
the companion paper [Gilbert and Scheichl, 2022], and so in this paper we focus
on how to further improve the efficiency and provide theoretical justification
for using nearby QMC points and two-grid methods. Numerical results are
presented that show the efficiency of our algorithm, and also show that the
four strategies we employ are complementary.
","[{'version': 'v1', 'created': 'Fri, 5 Mar 2021 00:42:09 GMT'}, {'version': 'v2', 'created': 'Mon, 17 Jan 2022 00:14:30 GMT'}, {'version': 'v3', 'created': 'Thu, 6 Oct 2022 05:48:39 GMT'}]",2022-10-07,['Numerical Analysis'],"This paper presents a novel multilevel quasi-Monte Carlo approach for random elliptic eigenvalue problems. The proposed approach is based on a combination of low-discrepancy sequences and hierarchical tensor-product grids. We present a detailed description of the proposed approach, including efficient algorithms for the numerical solution of the problem. We then provide numerical results for a range of test problems, demonstrating the effectiveness of the proposed approach. Finally, we discuss the implications of our results for the field of numerical analysis.","Write an abstract for a paper called Multilevel quasi-Monte Carlo for random elliptic eigenvalue problems II:
  Efficient algorithms and numerical results about Numerical Analysis"
2209.06826,Thom Neuteboom and Tim van Erven,"Modifying Squint for Prediction with Expert Advice in a Changing
  Environment",['cs.LG'],"  We provide a new method for online learning, specifically prediction with
expert advice, in a changing environment. In a non-changing environment the
Squint algorithm has been designed to always function at least as well as other
known algorithms and in specific cases it functions much better. However, when
using a conventional black-box algorithm to make Squint suitable for a changing
environment, it loses its beneficial properties. Hence, we provide a new
algorithm, Squint-CE, which is suitable for a changing environment and
preserves the properties of Squint.
","[{'version': 'v1', 'created': 'Wed, 14 Sep 2022 11:55:15 GMT'}]",2022-09-16,['Machine Learning'],"This paper presents a novel approach to machine learning that uses expert advice to modify a squint algorithm for prediction in a changing environment. The squint algorithm is an unsupervised learning technique that adapts to changes in the environment, but is limited to predicting only one variable at a time. The proposed approach combines expert advice with the squint algorithm to improve prediction accuracy and robustness in a changing environment. Experiments are conducted on a real-world dataset to evaluate the effectiveness of the proposed approach. Results show that the proposed approach outperforms the original squint algorithm in terms of prediction accuracy and robustness, and is able to adapt to changes in the environment more quickly. The proposed approach is a promising technique for machine learning in dynamic environments.","Write an abstract for a paper called Modifying Squint for Prediction with Expert Advice in a Changing
  Environment about Machine Learning"
1810.00803,"Florian Hirschberger, Dennis Forster, J\""org L\""ucke",Large Scale Clustering with Variational EM for Gaussian Mixture Models,"['stat.ML', 'cs.LG']","  This paper represents a preliminary (pre-reviewing) version of a sublinear
variational algorithm for isotropic Gaussian mixture models (GMMs). Further
developments of the algorithm for GMMs with diagonal covariance matrices
(instead of isotropic clusters) and their corresponding benchmarking results
have been published by TPAMI (doi:10.1109/TPAMI.2021.3133763) in the paper ""A
Variational EM Acceleration for Efficient Clustering at Very Large Scales"". We
kindly refer the reader to the TPAMI paper instead of this much earlier arXiv
version (the TPAMI paper is also open access). Publicly available source code
accompanies the paper (see
https://github.com/variational-sublinear-clustering). Please note that the
TPAMI paper does not contain the benchmark on the 80 Million Tiny Images
dataset anymore because we followed the call of the dataset creators to
discontinue the use of that dataset.
  The aim of the project (which resulted in this arXiv version and the later
TPAMI paper) is the exploration of the current efficiency and large-scale
limits in fitting a parametric model for clustering to data distributions. To
reduce computational complexity, we used a clustering objective based on
truncated variational EM (which reduces complexity for many clusters) in
combination with coreset objectives (which reduce complexity for many data
points). We used efficient coreset construction and efficient seeding to
translate the theoretical sublinear complexity gains into an efficient
algorithm. In applications to standard large-scale benchmarks for clustering,
we then observed substantial wall-clock speedups compared to already highly
efficient clustering approaches. To demonstrate that the observed efficiency
enables applications previously considered unfeasible, we clustered the entire
and unscaled 80 Million Tiny Images dataset into up to 32,000 clusters.
","[{'version': 'v1', 'created': 'Mon, 1 Oct 2018 16:34:51 GMT'}, {'version': 'v2', 'created': 'Tue, 12 Feb 2019 12:09:15 GMT'}, {'version': 'v3', 'created': 'Fri, 7 Jun 2019 16:12:38 GMT'}, {'version': 'v4', 'created': 'Tue, 21 Jun 2022 17:53:00 GMT'}]",2022-06-22,['Machine Learning'],"This paper presents a new approach to large scale clustering using variational EM for Gaussian Mixture Models. We propose a novel algorithm that employs a distributed optimization procedure to scale up the existing variational EM method. Our algorithm is based on a divide-and-conquer strategy, which divides the data into smaller subsets and distributes the computations across multiple machines. We analyze the theoretical properties of the algorithm and demonstrate its scalability and effectiveness on a variety of real-world datasets. We also compare our results with existing methods and show that our approach is more efficient and effective for large scale clustering.",Write an abstract for a paper called Large Scale Clustering with Variational EM for Gaussian Mixture Models about Machine Learning
2206.00893,"Jiachen Kang, Wenjing Jia and Xiangjian He",Leveraging Systematic Knowledge of 2D Transformations,"['cs.CV', 'cs.LG']","  The existing deep learning models suffer from out-of-distribution (o.o.d.)
performance drop in computer vision tasks. In comparison, humans have a
remarkable ability to interpret images, even if the scenes in the images are
rare, thanks to the systematicity of acquired knowledge. This work focuses on
1) the acquisition of systematic knowledge of 2D transformations, and 2)
architectural components that can leverage the learned knowledge in image
classification tasks in an o.o.d. setting. With a new training methodology
based on synthetic datasets that are constructed under the causal framework,
the deep neural networks acquire knowledge from semantically different domains
(e.g. even from noise), and exhibit certain level of systematicity in parameter
estimation experiments. Based on this, a novel architecture is devised
consisting of a classifier, an estimator and an identifier (abbreviated as
""CED""). By emulating the ""hypothesis-verification"" process in human visual
perception, CED improves the classification accuracy significantly on test sets
under covariate shift.
","[{'version': 'v1', 'created': 'Thu, 2 Jun 2022 06:46:12 GMT'}]",2022-06-03,"['Computer Vision and Pattern Recognition', 'Machine Learning']","This paper presents a novel approach to computer vision and pattern recognition using systematic knowledge of 2D transformations. By leveraging the inherent symmetries in 2D transformations, the proposed method can be used to detect and recognize patterns in images with high accuracy. Additionally, the method utilizes machine learning techniques to further improve the accuracy of the pattern recognition. The results of the experiments demonstrate that the proposed method outperforms existing methods in terms of accuracy and speed. The paper also discusses the potential applications of the proposed method in various fields such as medical imaging, autonomous driving, and surveillance.","Write an abstract for a paper called Leveraging Systematic Knowledge of 2D Transformations about Computer Vision and Pattern Recognition, Machine Learning"
2011.03667,"Yunhao Yang, Andrew Whinston","Identifying Mislabeled Images in Supervised Learning Utilizing
  Autoencoder","['cs.CV', 'cs.LG']","  Supervised learning is based on the assumption that the ground truth in the
training data is accurate. However, this may not be guaranteed in real-world
settings. Inaccurate training data will result in some unexpected predictions.
In image classification, incorrect labels may cause the classification model to
be inaccurate as well. In this paper, I am going to apply unsupervised
techniques to the training data before training the classification network. A
convolutional autoencoder is applied to encode and reconstruct images. The
encoder will project the image data on to latent space. In the latent space,
image features are preserved in a lower dimension. The assumption is that data
samples with similar features are likely to have the same label. Noised samples
can be classified in the latent space by the Density-Base Scan (DBSCAN)
clustering algorithm. These incorrectly labeled data are visualized as outliers
in the latent space. Therefore, the outliers identified by the DBSCAN algorithm
can be classified as incorrectly labeled samples. After the outliers are
detected, all the outliers are treated as mislabeled data samples and removed
from the dataset. Thus the training data can be directly used in training the
supervised learning network. The algorithm can detect and remove above 67\% of
mislabeled data in the experimental dataset.
","[{'version': 'v1', 'created': 'Sat, 7 Nov 2020 03:09:34 GMT'}, {'version': 'v2', 'created': 'Mon, 18 Jan 2021 22:59:44 GMT'}]",2022-01-06,"['Computer Vision and Pattern Recognition', 'Machine Learning']","This paper presents a novel approach to identify mislabeled images in supervised learning utilizing an autoencoder. Autoencoders are an unsupervised learning technique used to extract features from data and can be used to detect outliers in data. In this paper, we propose a novel approach to identify mislabeled images in supervised learning using an autoencoder. We use the autoencoder to extract features from the data and then use a clustering algorithm to group the data into clusters. We then use the clusters to identify mislabeled images. We evaluate our approach on a publicly available dataset and show that our approach is able to identify mislabeled images with high accuracy. Our results demonstrate the potential of our proposed approach for identifying mislabeled images in supervised learning.","Write an abstract for a paper called Identifying Mislabeled Images in Supervised Learning Utilizing
  Autoencoder about Computer Vision and Pattern Recognition, Machine Learning"
2302.00049,"Simon Geisler, Yujia Li, Daniel Mankowitz, Ali Taylan Cemgil, Stephan
  G\""unnemann, Cosmin Paduraru",Transformers Meet Directed Graphs,['cs.LG'],"  Transformers were originally proposed as a sequence-to-sequence model for
text but have become vital for a wide range of modalities, including images,
audio, video, and undirected graphs. However, transformers for directed graphs
are a surprisingly underexplored topic, despite their applicability to
ubiquitous domains including source code and logic circuits. In this work, we
propose two direction- and structure-aware positional encodings for directed
graphs: (1) the eigenvectors of the Magnetic Laplacian - a direction-aware
generalization of the combinatorial Laplacian; (2) directional random walk
encodings. Empirically, we show that the extra directionality information is
useful in various downstream tasks, including correctness testing of sorting
networks and source code understanding. Together with a data-flow-centric graph
construction, our model outperforms the prior state of the art on the Open
Graph Benchmark Code2 relatively by 14.7%.
","[{'version': 'v1', 'created': 'Tue, 31 Jan 2023 19:33:14 GMT'}]",2023-02-02,['Machine Learning'],"This paper examines the application of directed graph-based machine learning algorithms to the transformer architecture. We discuss the advantages of using directed graphs to represent the transformer architecture, and how these graphs can be used to improve model accuracy and reduce training time. We then discuss the use of directed graph-based algorithms such as graph convolutional networks (GCN) and graph attention networks (GAT) to train the transformer models. We evaluate the performance of these algorithms on several different datasets and compare the results to existing transformer models. Finally, we discuss how these algorithms can be further improved to better suit the transformer architecture.",Write an abstract for a paper called Transformers Meet Directed Graphs about Machine Learning
1810.10738,"Thomas Tseng, Laxman Dhulipala, Guy Blelloch",Batch-Parallel Euler Tour Trees,['cs.DS'],"  The dynamic trees problem is to maintain a forest undergoing edge insertions
and deletions while supporting queries for information such as connectivity.
There are many existing data structures for this problem, but few of them are
capable of exploiting parallelism in the batch-setting, in which large batches
of edges are inserted or deleted from the forest at once. In this paper, we
demonstrate that the Euler tour tree, an existing sequential dynamic trees data
structure, can be parallelized in the batch setting. For a batch of $k$ updates
over a forest of $n$ vertices, our parallel Euler tour trees perform $O(k \log
(1 + n/k))$ expected work with $O(\log n)$ depth with high probability. Our
work bound is asymptotically optimal, and we improve on the depth bound
achieved by Acar et al. for the batch-parallel dynamic trees problem.
  The main building block for parallelizing Euler tour trees is a
batch-parallel skip list data structure, which we believe may be of independent
interest. Euler tour trees require a sequence data structure capable of joins
and splits. Sequentially, balanced binary trees are used, but they are
difficult to join or split in parallel. We show that skip lists, on the other
hand, support batches of joins or splits of size $k$ over $n$ elements with
$O(k \log (1 + n/k))$ work in expectation and $O(\log n)$ depth with high
probability. We also achieve the same efficiency bounds for augmented skip
lists, which allows us to augment our Euler tour trees to support subtree
queries.
  Our data structures achieve between 67--96x self-relative speedup on 72 cores
with hyper-threading on large batch sizes. Our data structures also outperform
the fastest existing sequential dynamic trees data structures empirically.
","[{'version': 'v1', 'created': 'Thu, 25 Oct 2018 06:30:18 GMT'}, {'version': 'v2', 'created': 'Fri, 28 Dec 2018 08:46:50 GMT'}, {'version': 'v3', 'created': 'Sat, 5 Mar 2022 16:26:51 GMT'}]",2022-03-08,['Data Structures and Algorithms'],"This paper discusses the design and implementation of a new batch-parallel algorithm for constructing Euler tour trees from a given set of edges. The algorithm is based on a divide-and-conquer approach, which allows for efficient parallelization of the construction process. We also discuss several optimizations that can be applied to further improve the performance of the algorithm. Finally, we present an experimental evaluation of the proposed algorithm, which shows significant performance improvements over existing algorithms.",Write an abstract for a paper called Batch-Parallel Euler Tour Trees about Data Structures and Algorithms
2204.10983,"Yawen Wu, Dewen Zeng, Zhepeng Wang, Yiyu Shi, Jingtong Hu",Federated Contrastive Learning for Volumetric Medical Image Segmentation,"['eess.IV', 'cs.CV', 'cs.LG']","  Supervised deep learning needs a large amount of labeled data to achieve high
performance. However, in medical imaging analysis, each site may only have a
limited amount of data and labels, which makes learning ineffective. Federated
learning (FL) can help in this regard by learning a shared model while keeping
training data local for privacy. Traditional FL requires fully-labeled data for
training, which is inconvenient or sometimes infeasible to obtain due to high
labeling cost and the requirement of expertise. Contrastive learning (CL), as a
self-supervised learning approach, can effectively learn from unlabeled data to
pre-train a neural network encoder, followed by fine-tuning for downstream
tasks with limited annotations. However, when adopting CL in FL, the limited
data diversity on each client makes federated contrastive learning (FCL)
ineffective. In this work, we propose an FCL framework for volumetric medical
image segmentation with limited annotations. More specifically, we exchange the
features in the FCL pre-training process such that diverse contrastive data are
provided to each site for effective local CL while keeping raw data private.
Based on the exchanged features, global structural matching further leverages
the structural similarity to align local features to the remote ones such that
a unified feature space can be learned among different sites. Experiments on a
cardiac MRI dataset show the proposed framework substantially improves the
segmentation performance compared with state-of-the-art techniques.
","[{'version': 'v1', 'created': 'Sat, 23 Apr 2022 03:47:23 GMT'}]",2022-04-26,"['Computer Vision and Pattern Recognition', 'Machine Learning']",This paper presents a novel federated contrastive learning framework for volumetric medical image segmentation. The proposed method employs a federated learning approach to address the privacy and data fragmentation issues of medical image datasets. The proposed framework is based on contrastive learning and utilizes a Siamese network to learn the similarity of medical images from different sources. The framework is evaluated on a medical image segmentation task using a benchmark dataset and achieves promising results compared to existing methods. The results demonstrate the effectiveness of the proposed federated contrastive learning framework in volumetric medical image segmentation.,"Write an abstract for a paper called Federated Contrastive Learning for Volumetric Medical Image Segmentation about Computer Vision and Pattern Recognition, Machine Learning"
2208.11335,"Djibrilla Amadou Kountche, Fatma Raissi, Mandimby Ranaivo
  Rakotondravelona, Edoardo Bonetto, Daniele Brevi, Angel Martin, Oihana
  Otaegui, Gorka Velez","Monetisation of and Access to in-Vehicle data and resources: the 5GMETA
  approach","['cs.CY', 'cs.CV', 'cs.NI']","  Today's vehicles are increasingly embedded with computers and sensors which
produce huge amount of data. The data are exploited for internal purposes and
with the development of connected infrastructures and smart cities, the
vehicles interact with each other as well as with road users generating other
types of data. The access to these data and in-vehicle resources and their
monetisation faces many challenges which are presented in this paper.
Furthermore, the most important commercial solution compared to the open and
novel approach faced in the H2020 5GMETA project.
","[{'version': 'v1', 'created': 'Wed, 24 Aug 2022 07:16:42 GMT'}]",2022-08-25,"['Computers and Society', 'Computer Vision and Pattern Recognition', 'Networking and Internet Architecture']","This paper presents the 5GMETA approach to monetising and providing access to in-vehicle data and resources. The paper discusses the challenges of monetising and providing access to the in-vehicle data and resources, including security, privacy and data ownership. The 5GMETA approach is based on a 5G platform that provides a secure and reliable environment for monetising and providing access to in-vehicle data and resources. The paper further discusses the architecture of the 5GMETA approach, its components and the technologies used. Finally, the paper presents the results of the evaluation of the 5GMETA approach in terms of scalability, performance and security. The paper concludes by discussing the potential applications and implications of the 5GMETA approach for the future of connected vehicles.","Write an abstract for a paper called Monetisation of and Access to in-Vehicle data and resources: the 5GMETA
  approach about Computers and Society, Computer Vision and Pattern Recognition, Networking and Internet Architecture"
2208.14535,"Sadananda Behera, Tania Panayiotou, Georgios Ellinas","Modeling Soft-Failure Evolution for Triggering Timely Repair with Low
  QoT Margins",['cs.LG'],"  In this work, the capabilities of an encoder-decoder learning framework are
leveraged to predict soft-failure evolution over a long future horizon. This
enables the triggering of timely repair actions with low
quality-of-transmission (QoT) margins before a costly hard-failure occurs,
ultimately reducing the frequency of repair actions and associated operational
expenses. Specifically, it is shown that the proposed scheme is capable of
triggering a repair action several days prior to the expected day of a
hard-failure, contrary to soft-failure detection schemes utilizing rule-based
fixed QoT margins, that may lead either to premature repair actions (i.e.,
several months before the event of a hard-failure) or to repair actions that
are taken too late (i.e., after the hard failure has occurred). Both frameworks
are evaluated and compared for a lightpath established in an elastic optical
network, where soft-failure evolution can be modeled by analyzing
bit-error-rate information monitored at the coherent receivers.
","[{'version': 'v1', 'created': 'Tue, 30 Aug 2022 20:45:19 GMT'}]",2022-09-01,['Machine Learning'],This paper explores the use of machine learning to model the evolution of soft-failures in order to trigger timely repair with low Quality of Service (QoS) margins. Soft-failure evolution is a complex process that is difficult to predict and may lead to unexpected system downtime. Machine learning algorithms can be used to identify patterns in the evolution of soft-failures and predict when a repair should be triggered. This paper evaluates the effectiveness of machine learning algorithms in predicting the evolution of soft-failures and triggering timely repairs with low QoS margins. The results of this study provide insights into the potential of machine learning to improve the performance and reliability of systems by predicting and preventing soft-failures.,"Write an abstract for a paper called Modeling Soft-Failure Evolution for Triggering Timely Repair with Low
  QoT Margins about Machine Learning"
2105.06903,"Weipeng Huang, Tin Lok James Ng, Nishma Laitonjam, Neil J. Hurley",Posterior Regularization on Bayesian Hierarchical Mixture Clustering,"['stat.ML', 'cs.AI', 'cs.LG']","  Bayesian hierarchical mixture clustering (BHMC) improves on the traditional
Bayesian hierarchical clustering by, with regard to the parent-to-child
diffusion in the generative process, replacing the conventional
Gaussian-to-Gaussian (G2G) kernels with a Hierarchical Dirichlet Process
Mixture Model (HDPMM). However, the drawback of the BHMC lies in the
possibility of obtaining trees with comparatively high nodal variance in the
higher levels (i.e., those closer to the root node). This can be interpreted as
that the separation between the nodes, particularly those in the higher levels,
might be weak. We attempt to overcome this drawback through a recent
inferential framework named posterior regularization, which facilitates a
simple manner to impose extra constraints on a Bayesian model to address its
weakness. To enhance the separation of clusters, we apply posterior
regularization to impose max-margin constraints on the nodes at every level of
the hierarchy. In this paper, we illustrate the modeling detail of applying the
PR on BHMC and show that this solution achieves the desired improvements over
the BHMC model.
","[{'version': 'v1', 'created': 'Fri, 14 May 2021 15:41:15 GMT'}, {'version': 'v2', 'created': 'Mon, 17 May 2021 17:03:57 GMT'}, {'version': 'v3', 'created': 'Tue, 7 Dec 2021 12:33:15 GMT'}, {'version': 'v4', 'created': 'Fri, 10 Dec 2021 04:41:41 GMT'}, {'version': 'v5', 'created': 'Mon, 18 Jul 2022 04:55:08 GMT'}, {'version': 'v6', 'created': 'Tue, 22 Nov 2022 10:53:23 GMT'}]",2022-11-23,"['Artificial Intelligence', 'Machine Learning']","This paper presents a novel method of posterior regularization for Bayesian hierarchical mixture clustering in artificial intelligence and machine learning. We propose a new approach to incorporate a prior distribution over the model parameters, which allows for better parameter estimation and improved generalization performance. We then evaluate our method on a variety of datasets and compare it to existing approaches. Our results show that posterior regularization improves the accuracy of Bayesian hierarchical mixture clustering models, leading to better clustering performance. Additionally, we discuss the implications of our approach and its potential applications in artificial intelligence and machine learning.","Write an abstract for a paper called Posterior Regularization on Bayesian Hierarchical Mixture Clustering about Artificial Intelligence, Machine Learning"
2104.1501,"J. C. Schoeman, C. E. van Daalen, J. A. du Preez",Degenerate Gaussian factors for probabilistic inference,"['cs.LG', 'stat.ML']","  In this paper, we propose a parametrised factor that enables inference on
Gaussian networks where linear dependencies exist among the random variables.
Our factor representation is effectively a generalisation of traditional
Gaussian parametrisations where the positive-definite constraint of the
covariance matrix has been relaxed. For this purpose, we derive various
statistical operations and results (such as marginalisation, multiplication and
affine transformations of random variables) that extend the capabilities of
Gaussian factors to these degenerate settings. By using this principled factor
definition, degeneracies can be accommodated accurately and automatically at
little additional computational cost. As illustration, we apply our methodology
to a representative example involving recursive state estimation of cooperative
mobile robots.
","[{'version': 'v1', 'created': 'Fri, 30 Apr 2021 13:58:29 GMT'}, {'version': 'v2', 'created': 'Thu, 4 Aug 2022 15:30:36 GMT'}]",2022-08-05,['Machine Learning'],"This paper explores the use of degenerate Gaussian factors for probabilistic inference in the context of Machine Learning. We propose a novel approach for probabilistic inference which combines the advantages of using degenerate Gaussian factors with the powerful capabilities of machine learning algorithms. Our approach is based on the idea of using the degenerate Gaussian factors to represent the prior distribution of a given model, and then using the machine learning algorithms to optimize the posterior distribution. We demonstrate that our approach is able to accurately capture the uncertainty in the model and provide accurate probabilistic inference. We also discuss the advantages and drawbacks of our approach and provide a comparison to existing approaches. Finally, we provide a set of experiments which demonstrate the effectiveness of our proposed approach.",Write an abstract for a paper called Degenerate Gaussian factors for probabilistic inference about Machine Learning
2111.02291,"Yuan Gao, Hamidreza Kamkari, Andreas Karrenbauer, Kurt Mehlhorn,
  Mohammadamin Sharifi",Physarum Inspired Dynamics to Solve Semi-Definite Programs,"['cs.DS', 'math.OC']","  Physarum Polycephalum is a slime mold that can solve shortest path problems.
A mathematical model based on Physarum's behavior, known as the Physarum
Directed Dynamics, can solve positive linear programs. In this paper, we
present a family of Physarum-based dynamics extending the previous work and
introduce a new algorithm to solve positive Semi-Definite Programs (SDP). The
Physarum dynamics are governed by orthogonal projections (w.r.t. time-dependent
scalar products) on the affine subspace defined by the linear constraints. We
present a natural generalization of the scalar products used in the LP case to
the matrix space for SDPs, which boils down to the linear case when all
matrices in the SDP are diagonal, thus, representing an LP. We investigate the
behavior of the induced dynamics theoretically and experimentally, highlight
challenges arising from the non-commutative nature of matrix products, and
prove soundness and convergence under mild conditions. Moreover, we consider a
more abstract view on the dynamics that suggests a slight variation to
guarantee unconditional soundness and convergence-to-optimality. By simulating
these dynamics using suitable discretizations, one obtains numerical algorithms
for solving positive SDPs, which have applications in discrete optimization,
e.g., for computing the Goemans-Williamson approximation for MaxCut or the
Lovasz theta number for determining the clique/chromatic number in perfect
graphs.
","[{'version': 'v1', 'created': 'Wed, 3 Nov 2021 15:23:31 GMT'}, {'version': 'v2', 'created': 'Fri, 15 Jul 2022 13:01:12 GMT'}, {'version': 'v3', 'created': 'Mon, 18 Jul 2022 15:49:19 GMT'}]",2022-07-19,['Data Structures and Algorithms'],"This paper presents a novel approach to solving semi-definite programs (SDPs) using Physarum inspired dynamics. SDPs have traditionally been solved using interior point methods, which are computationally expensive. We propose a biologically inspired approach, which utilizes the foraging behavior of the slime mold Physarum polycephalum to solve SDPs. We use the Physarum dynamics to construct a network of nodes and edges that represent the SDP solution. We then apply an optimization algorithm to the constructed network to find the optimal solution. We compare the performance of our approach with that of existing interior point methods and show that our approach is more efficient in terms of both time and memory requirements. This paper contributes to the field of data structures and algorithms by providing a new approach to solving SDPs.",Write an abstract for a paper called Physarum Inspired Dynamics to Solve Semi-Definite Programs about Data Structures and Algorithms
2212.02985,"Yun-Wei Chu, Seyyedali Hosseinalipour, Elizabeth Tenorio, Laura Cruz,
  Kerrie Douglas, Andrew Lan, Christopher Brinton","Multi-Layer Personalized Federated Learning for Mitigating Biases in
  Student Predictive Analytics","['cs.LG', 'cs.AI', 'cs.CY']","  Traditional learning-based approaches to student modeling (e.g., predicting
grades based on measured activities) generalize poorly to
underrepresented/minority student groups due to biases in data availability. In
this paper, we propose a Multi-Layer Personalized Federated Learning (MLPFL)
methodology which optimizes inference accuracy over different layers of student
grouping criteria, such as by course and by demographic subgroups within each
course. In our approach, personalized models for individual student subgroups
are derived from a global model, which is trained in a distributed fashion via
meta-gradient updates that account for subgroup heterogeneity while preserving
modeling commonalities that exist across the full dataset. To evaluate our
methodology, we consider case studies of two popular downstream student
modeling tasks, knowledge tracing and outcome prediction, which leverage
multiple modalities of student behavior (e.g., visits to lecture videos and
participation on forums) in model training. Experiments on three real-world
datasets from online courses demonstrate that our approach obtains substantial
improvements over existing student modeling baselines in terms of increasing
the average and decreasing the variance of prediction quality across different
student subgroups. Visual analysis of the resulting students' knowledge state
embeddings confirm that our personalization methodology extracts activity
patterns which cluster into different student subgroups, consistent with the
performance enhancements we obtain over the baselines.
","[{'version': 'v1', 'created': 'Mon, 5 Dec 2022 17:27:28 GMT'}]",2022-12-07,"['Machine Learning', 'Artificial Intelligence', 'Computers and Society']","This paper presents a multi-layer personalized federated learning approach to mitigate biases in student predictive analytics about machine learning, artificial intelligence, and computers and society. The proposed approach is based on the federated learning framework, which enables distributed learning without the need to share raw data. To address the issue of data bias, the proposed approach uses a multi-layer personalized federated learning approach that combines personalized federated learning and transfer learning. The proposed approach is evaluated on a real-world dataset and results show that it can effectively reduce biases in the student predictive analytics. The proposed approach is expected to be a promising tool for mitigating data biases in predictive analytics and can be extended to other machine learning tasks.","Write an abstract for a paper called Multi-Layer Personalized Federated Learning for Mitigating Biases in
  Student Predictive Analytics about Machine Learning, Artificial Intelligence, Computers and Society"
2210.08981,"Petra J\""a\""askel\""ainen, Daniel Pargman and Andr\'e Holzapfel","Towards sustainability assessment of artificial intelligence in artistic
  practices","['cs.CY', 'cs.AI', 'cs.SE']","  An increasing number of artists use Ai in their creative practices
(Creative-Ai) and their works have by now become visible at prominent art
venues. The research community has, on the other hand, recognized that there
are sustainability concerns of using Ai technologies related to, for instance,
energy consumption and the increasing size and complexity of models. These two
conflicting trajectories constitute the starting point of our research. Here,
we discuss insights from our currently on-going fieldwork research and outline
considerations for drawing various limitations in sustainability assessment
studies of Ai art. We provide ground for further, more specific sustainability
assessments in the domain, as well as knowledge on the state of sustainability
assessments in this domain.
","[{'version': 'v1', 'created': 'Mon, 3 Oct 2022 15:02:22 GMT'}]",2022-10-18,"['Computers and Society', 'Artificial Intelligence', 'Software Engineering']","This paper presents a study towards the sustainability assessment of artificial intelligence (AI) in artistic practices. AI-based technologies are increasingly used in the creative arts, from music composition to video production. However, these technologies have yet to be evaluated for their impact on the environment and society. This paper examines the current state of AI-based art and evaluates the potential for sustainability assessment of AI-based art. It looks at the ethical implications of AI-based art and the need for sustainability assessment of AI-based art. It also presents a framework for assessing AI-based art for sustainability. The paper concludes by discussing the implications of this research and potential directions for future work.","Write an abstract for a paper called Towards sustainability assessment of artificial intelligence in artistic
  practices about Computers and Society, Artificial Intelligence, Software Engineering"
2206.11118,Jixiang Chen and Fu Luo and Zhenkun Wang,"Dynamic Multi-objective Ensemble of Acquisition Functions in Batch
  Bayesian Optimization",['cs.NE'],"  Bayesian optimization (BO) is a typical approach to solve expensive
optimization problems. In each iteration of BO, a Gaussian process(GP) model is
trained using the previously evaluated solutions; then next candidate solutions
for expensive evaluation are recommended by maximizing a cheaply-evaluated
acquisition function on the trained surrogate model. The acquisition function
plays a crucial role in the optimization process. However, each acquisition
function has its own strengths and weaknesses, and no single acquisition
function can consistently outperform the others on all kinds of problems. To
better leverage the advantages of different acquisition functions, we propose a
new method for batch BO. In each iteration, three acquisition functions are
dynamically selected from a set based on their current and historical
performance to form a multi-objective optimization problem (MOP). Using an
evolutionary multi-objective algorithm to optimize such a MOP, a set of
non-dominated solutions can be obtained. To select batch candidate solutions,
we rank these non-dominated solutions into several layers according to their
relative performance on the three acquisition functions. The empirical results
show that the proposed method is competitive with the state-of-the-art methods
on different problems.
","[{'version': 'v1', 'created': 'Wed, 22 Jun 2022 14:09:18 GMT'}]",2022-06-23,['Neural and Evolutionary Computing'],"This paper presents a dynamic multi-objective ensemble of acquisition functions for batch Bayesian optimization (BO) using neural and evolutionary computing. In BO, acquisition functions are used to select the next set of points to evaluate in order to optimize a given objective. We propose a novel ensemble of acquisition functions that can adapt the exploration-exploitation trade-off in BO. The ensemble is composed of both a neural network and evolutionary algorithm, allowing it to dynamically adjust the trade-off in order to find an optimal set of points for evaluation. We evaluate the performance of the proposed approach on a set of benchmark problems and compare it to existing BO methods. Results show that the proposed approach significantly outperforms existing methods in terms of both accuracy and convergence speed.","Write an abstract for a paper called Dynamic Multi-objective Ensemble of Acquisition Functions in Batch
  Bayesian Optimization about Neural and Evolutionary Computing"
2303.06863,Dongfang Zhao,"Semantically Secure Private Set Intersection over Outsourced Multi-Owner
  Secret-Shared Databases","['cs.CR', 'cs.DB']","  Private set intersection (PSI) aims to allow users to find out the commonly
shared items among the users without revealing other membership information.
The most recently proposed approach to PSI in the database community was Prism,
which is built upon secret sharing and the assumption that multiple
non-colluding servers are available. One limitation of Prism lies in its
semantic security: the encoding on the servers is deterministic, implying that
the scheme cannot be indistinguishable under a chosen-plaintext attack
(IND-CPA). This paper extends the original PSI scheme of Prism by two
orthogonal primitives, namely Kaleido-RND and Kaleido-AES: the former exhibits
highly efficient performance with randomized encoding and the latter is
provably secure under CPA attacks with more computational overhead. A system
prototype is implemented and deployed on a 34-node cluster of SQLite instances.
Extensive experiments on the TPC-H benchmark and three real-world applications
confirm the effectiveness of the proposed Kaleido primitives.
","[{'version': 'v1', 'created': 'Mon, 13 Mar 2023 05:15:31 GMT'}]",2023-03-14,"['Cryptography and Security', 'Databases']","This paper examines the problem of securely computing private set intersection (PSI) over outsourced multi-owner secret-shared databases. We present a novel protocol that is semantically secure under the honest-but-curious adversary model. The protocol is designed to be resistant to malicious data modification and leakage. We also analyze the security of our protocol and discuss its performance. Our experimental results show that our protocol is efficient and secure in practice. We also discuss the implications of our work for cryptography and security, as well as databases.","Write an abstract for a paper called Semantically Secure Private Set Intersection over Outsourced Multi-Owner
  Secret-Shared Databases about Cryptography and Security, Databases"
2207.11728,"Taeho Shin, Dongjun Lee, Dongwhee Kim, Gaeryun Sung, Wookjin Shin,
  Yunseong Jo, Hyungjoo Park, Jaeduk Han","A Custom IC Layout Generation Engine Based on Dynamic Templates and
  Grids","['eess.SP', 'cs.AR']","  This paper presents an automatic layout generation framework in advanced CMOS
technologies. The framework extends the template-and-grid-based layout
generation methodology with the following additional techniques applied to
produce optimal layouts more effectively. First, layout templates and grids are
dynamically created and adjusted during runtime to serve various structural,
functional, and design requirements. Virtual instances support the dynamic
template-and-grid-based layout generation process. The framework also
implements various post-processing functions to handle process-specific
requirements efficiently. The post-processing functions include cut/dummy
pattern generation and multiple-patterning adjustment. The generator
description capability is enhanced with circular grid indexing/slicing and
conditional conversion operators. The layout generation framework is applied to
various design examples and generates DRC/LVS clean layouts automatically in
multiple CMOS technologies.
","[{'version': 'v1', 'created': 'Sun, 24 Jul 2022 12:15:19 GMT'}]",2022-07-26,['Hardware Architecture'],"This paper presents a custom IC layout generation engine based on dynamic templates and grids. The engine is designed to address the challenges of hardware design complexity and time-to-market. It is based on an innovative approach that combines dynamic templates and grids, allowing the designer to quickly configure the layout of an integrated circuit (IC). The engine is built on a modular framework that supports the creation of a wide range of ICs with different sizes and architectures. The engine is also capable of automatically generating a layout from a given set of parameters, reducing the time and effort required for manual layout. The proposed engine is evaluated on a number of benchmarks and the results demonstrate its effectiveness in terms of design time and quality.","Write an abstract for a paper called A Custom IC Layout Generation Engine Based on Dynamic Templates and
  Grids about Hardware Architecture"
2302.07411,"Dong Jiang, Zhen Yuan, Wen-xin Li, Liang-liang Lu","Real-time chaotic video encryption based on multithreaded parallel
  confusion and diffusion","['cs.CR', 'cs.DC']","  Due to the strong correlation between adjacent pixels, most image encryption
schemes perform multiple rounds of confusion and diffusion to protect the image
against attacks. Such operations, however, are time-consuming, cannot meet the
real-time requirements of video encryption. Existing works, therefore, realize
video encryption by simplifying the encryption process or encrypting specific
parts of video frames, which results in lower security compared to image
encryption. To solve the problem, this paper proposes a real-time chaotic video
encryption strategy based on multithreaded parallel confusion and diffusion. It
takes a video as the input, splits the frame into subframes, creates a set of
threads to simultaneously perform five rounds of confusion and diffusion
operations on corresponding subframes, and efficiently outputs the encrypted
frames. The encryption speed evaluation shows that our method significantly
improves the confusion and diffusion speed, realizes real-time 480x480,
576x576, and 768x768 24FPS video encryption using Intel Core i5-1135G7, Intel
Core i7-8700, and Intel Xeon Gold 6226R, respectively. The statistical and
security analysis prove that the deployed cryptosystems have outstanding
statistical properties, can resist attacks, channel noise, and data loss.
Compared with existing works, to the best of our knowledge, the proposed
strategy achieves the fastest encryption speed, and realizes the first
real-time chaotic video encryption that reaches the security level of image
encryption. In addition, it is suitable for many confusion, diffusion
algorithms and can be easily deployed with both hardware and software.
","[{'version': 'v1', 'created': 'Wed, 15 Feb 2023 00:26:27 GMT'}]",2023-02-16,"['Cryptography and Security', 'Distributed, Parallel, and Cluster Computing']","This paper presents a novel real-time chaotic video encryption algorithm based on multithreaded parallel confusion and diffusion. The proposed algorithm takes advantage of chaotic systems, cryptography, and distributed, parallel, and cluster computing to provide an effective and efficient solution to secure video data. The proposed algorithm includes an encryption phase, a decryption phase, and a key management phase. The encryption phase uses chaotic systems and cryptography to generate a pseudorandom keystream, which is then used to encrypt the video data. The decryption phase is based on distributed, parallel, and cluster computing and provides a secure and efficient way to decrypt the encrypted video data. Finally, the key management phase ensures secure key distribution and secure key storage. The proposed algorithm is evaluated using various metrics such as security, speed, and quality of the encrypted video data. The results show that the proposed algorithm is secure, fast, and provides high-quality encrypted video data.","Write an abstract for a paper called Real-time chaotic video encryption based on multithreaded parallel
  confusion and diffusion about Cryptography and Security, Distributed, Parallel, and Cluster Computing"
2205.06267,"Shivam Duggal, Deepak Pathak",Topologically-Aware Deformation Fields for Single-View 3D Reconstruction,"['cs.CV', 'cs.AI', 'cs.GR', 'cs.LG']","  We present a framework for learning 3D object shapes and dense cross-object
3D correspondences from just an unaligned category-specific image collection.
The 3D shapes are generated implicitly as deformations to a category-specific
signed distance field and are learned in an unsupervised manner solely from
unaligned image collections and their poses without any 3D supervision.
Generally, image collections on the internet contain several intra-category
geometric and topological variations, for example, different chairs can have
different topologies, which makes the task of joint shape and correspondence
estimation much more challenging. Because of this, prior works either focus on
learning each 3D object shape individually without modeling cross-instance
correspondences or perform joint shape and correspondence estimation on
categories with minimal intra-category topological variations. We overcome
these restrictions by learning a topologically-aware implicit deformation field
that maps a 3D point in the object space to a higher dimensional point in the
category-specific canonical space. At inference time, given a single image, we
reconstruct the underlying 3D shape by first implicitly deforming each 3D point
in the object space to the learned category-specific canonical space using the
topologically-aware deformation field and then reconstructing the 3D shape as a
canonical signed distance field. Both canonical shape and deformation field are
learned end-to-end in an inverse-graphics fashion using a learned recurrent ray
marcher (SRN) as a differentiable rendering module. Our approach, dubbed TARS,
achieves state-of-the-art reconstruction fidelity on several datasets:
ShapeNet, Pascal3D+, CUB, and Pix3D chairs. Result videos and code at
https://shivamduggal4.github.io/tars-3D/
","[{'version': 'v1', 'created': 'Thu, 12 May 2022 17:59:59 GMT'}, {'version': 'v2', 'created': 'Sat, 21 May 2022 01:33:21 GMT'}]",2022-05-24,"['Computer Vision and Pattern Recognition', 'Artificial Intelligence', 'Graphics', 'Machine Learning']","This paper presents a novel approach to single-view 3D reconstruction using topologically-aware deformation fields. By leveraging the principles of computer vision, pattern recognition, artificial intelligence, graphics, and machine learning, our method is capable of accurately reconstructing 3D objects from a single image. We demonstrate our approach on a variety of datasets and show that it outperforms existing methods in terms of accuracy and robustness. Additionally, we discuss the potential applications of our method and provide insight into how it could be used to improve the accuracy of existing 3D reconstruction algorithms.","Write an abstract for a paper called Topologically-Aware Deformation Fields for Single-View 3D Reconstruction about Computer Vision and Pattern Recognition, Artificial Intelligence, Graphics, Machine Learning"
2301.10936,"Ningxin Zheng, Huiqiang Jiang, Quanlu Zhang, Zhenhua Han, Yuqing Yang,
  Lingxiao Ma, Fan Yang, Lili Qiu, Mao Yang, Lidong Zhou","SparDA: Accelerating Dynamic Sparse Deep Neural Networks via
  Sparse-Dense Transformation","['cs.LG', 'cs.NE']","  Due to its high cost-effectiveness, sparsity has become the most important
approach for building efficient deep-learning models. However, commodity
accelerators are built mainly for efficient dense computation, creating a huge
gap for general sparse computation to leverage. Existing solutions have to use
time-consuming compiling to improve the efficiency of sparse kernels in an
ahead-of-time manner and thus are limited to static sparsity. A wide range of
dynamic sparsity opportunities is missed because their sparsity patterns are
only known at runtime. This limits the future of building more biological
brain-like neural networks that should be dynamically and sparsely activated.
  In this paper, we bridge the gap between sparse computation and commodity
accelerators by proposing a system, called Spider, for efficiently executing
deep learning models with dynamic sparsity. We identify an important property
called permutation invariant that applies to most deep-learning computations.
The property enables Spider (1) to extract dynamic sparsity patterns of tensors
that are only known at runtime with little overhead; and (2) to transform the
dynamic sparse computation into an equivalent dense computation which has been
extremely optimized on commodity accelerators. Extensive evaluation on diverse
models shows Spider can extract and transform dynamic sparsity with negligible
overhead but brings up to 9.4x speedup over state-of-art solutions.
","[{'version': 'v1', 'created': 'Thu, 26 Jan 2023 04:50:14 GMT'}]",2023-01-27,"['Machine Learning', 'Neural and Evolutionary Computing']","This paper proposes SparDA, a novel technique for accelerating dynamic sparse deep neural networks (D-SDNNs) via sparse-dense transformation. SparDA leverages the sparse-dense transformation to reduce the complexity of D-SDNNs and improve their performance. The proposed technique applies the sparse-dense transformation to the input data, which is then used to train the D-SDNNs. The sparse-dense transformation is based on evolutionary computing, which enables the D-SDNNs to learn more efficiently. The experimental results demonstrate that SparDA achieves significant speedups in training and inference for D-SDNNs compared to the baseline. Furthermore, the proposed technique can be easily integrated into existing deep learning frameworks. The results of this paper show that the proposed SparDA technique is a promising approach for accelerating dynamic sparse deep neural networks.","Write an abstract for a paper called SparDA: Accelerating Dynamic Sparse Deep Neural Networks via
  Sparse-Dense Transformation about Machine Learning, Neural and Evolutionary Computing"
2210.08181,"Keyu Yan and Man Zhou and Jie Huang and Feng Zhao and Chengjun Xie and
  Chongyi Li and Danfeng Hong","Panchromatic and Multispectral Image Fusion via Alternating Reverse
  Filtering Network","['cs.CV', 'eess.IV']","  Panchromatic (PAN) and multi-spectral (MS) image fusion, named
Pan-sharpening, refers to super-resolve the low-resolution (LR) multi-spectral
(MS) images in the spatial domain to generate the expected high-resolution (HR)
MS images, conditioning on the corresponding high-resolution PAN images. In
this paper, we present a simple yet effective \textit{alternating reverse
filtering network} for pan-sharpening. Inspired by the classical reverse
filtering that reverses images to the status before filtering, we formulate
pan-sharpening as an alternately iterative reverse filtering process, which
fuses LR MS and HR MS in an interpretable manner. Different from existing
model-driven methods that require well-designed priors and degradation
assumptions, the reverse filtering process avoids the dependency on pre-defined
exact priors. To guarantee the stability and convergence of the iterative
process via contraction mapping on a metric space, we develop the learnable
multi-scale Gaussian kernel module, instead of using specific filters. We
demonstrate the theoretical feasibility of such formulations. Extensive
experiments on diverse scenes to thoroughly verify the performance of our
method, significantly outperforming the state of the arts.
","[{'version': 'v1', 'created': 'Sat, 15 Oct 2022 03:56:05 GMT'}]",2022-10-18,['Computer Vision and Pattern Recognition'],"This paper presents a novel approach to image fusion of panchromatic and multispectral images using an Alternating Reverse Filtering Network (ARFN). The ARFN is a convolutional neural network (CNN) that is trained to combine the spectral information of a multispectral image with the spatial information of a panchromatic image. The ARFN uses a series of alternating convolutional layers to reverse the process of image decomposition, resulting in a fused image with both spectral and spatial information. Experiments were conducted to evaluate the performance of the ARFN on two publicly available datasets. The results demonstrate that the proposed method achieves superior performance compared to state-of-the-art fusion methods. The proposed method is also computationally efficient, making it suitable for real-time applications in computer vision and pattern recognition.","Write an abstract for a paper called Panchromatic and Multispectral Image Fusion via Alternating Reverse
  Filtering Network about Computer Vision and Pattern Recognition"
2202.0286,Farhad Shirani and Hamidreza Aghasi,"MIMO Systems with One-bit ADCs: Capacity Gains using Nonlinear Analog
  Operations","['cs.IT', 'eess.SP', 'math.IT']","  Analog to Digital Converters (ADCs) are a major contributor to the energy
consumption on the receiver side of millimeter-wave multiple-input
multiple-output (MIMO) systems with large antenna arrays. Consequently, there
has been significant interest in using low-resolution ADCs along with hybrid
beam-forming at MIMO receivers for energy efficiency. However, decreasing the
ADC resolution results in performance loss -- in terms of achievable rates --
due to increased quantization error. In this work, we study the application of
practically implementable nonlinear analog operations, prior to sampling and
quantization at the ADCs, as a way to mitigate the aforementioned rate-loss. A
receiver architecture consisting of linear analog combiners, implementable
nonlinear analog operators, and one-bit threshold ADCs is designed. The
fundamental information theoretic performance limits of the resulting
communication system, in terms of achievable rates, are investigated under
various assumptions on the set of implementable nonlinear analog functions. In
order to justify the feasibility of the nonlinear operations in the proposed
receiver architecture, an analog circuit is introduced, and circuit simulations
exhibiting the generation of the desired nonlinear analog operations are
provided.
","[{'version': 'v1', 'created': 'Sun, 6 Feb 2022 21:37:15 GMT'}]",2022-02-08,['Information Theory'],This paper investigates the capacity gains of MIMO systems with one-bit analog-to-digital converters (ADCs) when nonlinear analog operations are applied. We introduce a new approach to optimize the performance of one-bit ADC MIMO systems by incorporating nonlinear analog operations. We analyze the capacity gains of the proposed approach by deriving closed-form expressions for the mutual information and capacity of the system. Simulation results show that our approach provides significant capacity gains compared to traditional linear systems. We also analyze the effect of different system parameters on the capacity gains. The results of this paper provide insight into the design of one-bit ADC MIMO systems with nonlinear analog operations and can be used to significantly improve the performance of communication systems.,"Write an abstract for a paper called MIMO Systems with One-bit ADCs: Capacity Gains using Nonlinear Analog
  Operations about Information Theory"
2208.09009,"Xupeng Ai, Victor Santamaria, Isirame Babajide Omofuma, and Sunil K.
  Agrawal","Evaluation of Postural Muscle Synergies during a Complex Motor Task in a
  Virtual Reality Environment","['cs.RO', 'physics.data-an']","  In this study, we investigate how the central nervous system (CNS) organizes
postural control synergies when individuals perform a complex catch-and-throw
task in a virtual reality (VR) environment. A Robotic Upright Stand Trainer
(RobUST) platform, including surface electromyography and kinematics, was used
to investigate how the CNS fine-tunes postural synergies with perturbative and
assist-as-needed force fields. A control group without assistive forces was
recruited to elucidate the effect of force fields on motor performance and
postural synergy organization after the perturbation and during the VR reaching
task. We found that the application of assistive forces significantly improved
reaching and balance control. The group receiving assistive forces displayed
four postural control synergies characterized by higher complexity (i.e.,
greater number of muscles involved). However, control subjects displayed eight
synergies that recruited less number of muscles. In conclusion, assistive
forces reduce the number of postural synergies while increasing the complexity
of muscle module composition.
","[{'version': 'v1', 'created': 'Thu, 18 Aug 2022 18:07:11 GMT'}]",2022-08-22,['Robotics'],"This paper evaluates postural muscle synergies during a complex motor task in a virtual reality environment for robotics. We used a humanoid robot with a human-like body structure and kinematics to perform a complex task of reaching and grasping in a virtual environment. We evaluated the postural muscle synergies of the robot during the task and compared them to those of a human. We found that the postural muscle synergies of the robot during the task were similar to those of a human, suggesting that the robot was able to perform the task in the same manner as a human. The results of this study provide insight into how robots can be used in virtual reality environments for complex motor tasks.","Write an abstract for a paper called Evaluation of Postural Muscle Synergies during a Complex Motor Task in a
  Virtual Reality Environment about Robotics"
2203.05437,"Aman Kumar, Himani Shrotriya, Prachi Sahu, Raj Dabre, Ratish
  Puduppully, Anoop Kunchukuttan, Amogh Mishra, Mitesh M. Khapra, Pratyush
  Kumar","IndicNLG Benchmark: Multilingual Datasets for Diverse NLG Tasks in Indic
  Languages","['cs.CL', 'cs.AI']","  Natural Language Generation (NLG) for non-English languages is hampered by
the scarcity of datasets in these languages. In this paper, we present the
IndicNLG Benchmark, a collection of datasets for benchmarking NLG for 11 Indic
languages. We focus on five diverse tasks, namely, biography generation using
Wikipedia infoboxes, news headline generation, sentence summarization,
paraphrase generation and, question generation. We describe the created
datasets and use them to benchmark the performance of several monolingual and
multilingual baselines that leverage pre-trained sequence-to-sequence models.
Our results exhibit the strong performance of multilingual language-specific
pre-trained models, and the utility of models trained on our dataset for other
related NLG tasks. Our dataset creation methods can be easily applied to
modest-resource languages as they involve simple steps such as scraping news
articles and Wikipedia infoboxes, light cleaning, and pivoting through machine
translation data. To the best of our knowledge, the IndicNLG Benchmark is the
first NLG benchmark for Indic languages and the most diverse multilingual NLG
dataset, with approximately 8M examples across 5 tasks and 11 languages. The
datasets and models are publicly available at
https://ai4bharat.iitm.ac.in/indicnlg-suite.
","[{'version': 'v1', 'created': 'Thu, 10 Mar 2022 15:53:58 GMT'}, {'version': 'v2', 'created': 'Thu, 27 Oct 2022 02:33:39 GMT'}]",2022-10-28,"['Computation and Language', 'Artificial Intelligence']","This paper presents IndicNLG Benchmark, a collection of multilingual datasets for a variety of Natural Language Generation (NLG) tasks in Indic languages. The datasets cover a range of topics related to Computation and Language, Artificial Intelligence and Machine Learning. The datasets are created with a focus on providing a broad range of language resources for NLG research and development in Indic languages. The datasets are designed to be suitable for both supervised and unsupervised learning and are available for download. The paper also discusses the challenges of creating such datasets and presents a detailed analysis of the datasets. Finally, the paper provides a comparison of the datasets with existing datasets in the same domain.","Write an abstract for a paper called IndicNLG Benchmark: Multilingual Datasets for Diverse NLG Tasks in Indic
  Languages about Computation and Language, Artificial Intelligence"
1910.08918,"Tadahiro Taniguchi, Tomoaki Nakamura, Masahiro Suzuki, Ryo Kuniyasu,
  Kaede Hayashi, Akira Taniguchi, Takato Horii, Takayuki Nagai","Neuro-SERKET: Development of Integrative Cognitive System through the
  Composition of Deep Probabilistic Generative Models","['cs.LG', 'cs.AI']","  This paper describes a framework for the development of an integrative
cognitive system based on probabilistic generative models (PGMs) called
Neuro-SERKET. Neuro-SERKET is an extension of SERKET, which can compose
elemental PGMs developed in a distributed manner and provide a scheme that
allows the composed PGMs to learn throughout the system in an unsupervised way.
In addition to the head-to-tail connection supported by SERKET, Neuro-SERKET
supports tail-to-tail and head-to-head connections, as well as neural
network-based modules, i.e., deep generative models. As an example of a
Neuro-SERKET application, an integrative model was developed by composing a
variational autoencoder (VAE), a Gaussian mixture model (GMM), latent Dirichlet
allocation (LDA), and automatic speech recognition (ASR). The model is called
VAE+GMM+LDA+ASR. The performance of VAE+GMM+LDA+ASR and the validity of
Neuro-SERKET were demonstrated through a multimodal categorization task using
image data and a speech signal of numerical digits.
","[{'version': 'v1', 'created': 'Sun, 20 Oct 2019 07:35:39 GMT'}, {'version': 'v2', 'created': 'Thu, 30 Jan 2020 04:41:41 GMT'}]",2023-01-18,"['Machine Learning', 'Artificial Intelligence']",", and Neuroscience

This paper presents Neuro-SERKET, an integrative cognitive system developed through the composition of deep probabilistic generative models. Neuro-SERKET is based on advances in machine learning, artificial intelligence, and neuroscience, and provides a framework for understanding the relationship between the three disciplines. This paper will discuss the development of the system, its components, and how they interact to form a unified cognitive system. The paper will also cover the application of Neuro-SERKET to various tasks, such as image recognition, language processing, and decision making. Finally, the paper will discuss the potential implications of Neuro-SERKET for the future of cognitive systems and artificial intelligence.","Write an abstract for a paper called Neuro-SERKET: Development of Integrative Cognitive System through the
  Composition of Deep Probabilistic Generative Models about Machine Learning, Artificial Intelligence"
2209.02062,"Utkarsh Patel, Animesh Mukherjee, Mainack Mondal","""Dummy Grandpa, do you know anything?"": Identifying and Characterizing
  Ad hominem Fallacy Usage in the Wild",['cs.CL'],"  Today, participating in discussions on online forums is extremely commonplace
and these discussions have started rendering a strong influence on the overall
opinion of online users. Naturally, twisting the flow of the argument can have
a strong impact on the minds of naive users, which in the long run might have
socio-political ramifications, for example, winning an election or spreading
targeted misinformation. Thus, these platforms are potentially highly
vulnerable to malicious players who might act individually or as a cohort to
breed fallacious arguments with a motive to sway public opinion. Ad hominem
arguments are one of the most effective forms of such fallacies. Although a
simple fallacy, it is effective enough to sway public debates in offline world
and can be used as a precursor to shutting down the voice of opposition by
slander.
  In this work, we take a first step in shedding light on the usage of ad
hominem fallacies in the wild. First, we build a powerful ad hominem detector
with high accuracy (F1 more than 83%, showing a significant improvement over
prior work), even for datasets for which annotated instances constitute a very
small fraction. We then used our detector on 265k arguments collected from the
online debate forum - CreateDebate. Our crowdsourced surveys validate our
in-the-wild predictions on CreateDebate data (94% match with manual
annotation). Our analysis revealed that a surprising 31.23% of CreateDebate
content contains ad hominem fallacy, and a cohort of highly active users post
significantly more ad hominem to suppress opposing views. Then, our temporal
analysis revealed that ad hominem argument usage increased significantly since
the 2016 US Presidential election, not only for topics like Politics, but also
for Science and Law. We conclude by discussing important implications of our
work to detect and defend against ad hominem fallacies.
","[{'version': 'v1', 'created': 'Mon, 5 Sep 2022 17:16:44 GMT'}]",2022-09-07,['Computation and Language'],"This paper examines the use of the ad hominem fallacy in discussions about computation and language in the wild. We identify and characterize the various forms of ad hominem fallacy usage in online forums, such as Reddit and Twitter. We present a survey of the different types of ad hominem fallacies used, such as attacking the person rather than the argument, and discuss the implications of such usage. We also provide a detailed analysis of how the ad hominem fallacy is used in the context of computation and language. Our work provides insight into how ad hominem fallacy usage is used in online conversations and can help inform future research on the topic.","Write an abstract for a paper called ""Dummy Grandpa, do you know anything?"": Identifying and Characterizing
  Ad hominem Fallacy Usage in the Wild about Computation and Language"
2303.11407,"Mohammad Khajenejad, Scott Brown, Sonia Martinez","Distributed Resilient Interval Observers for Bounded-Error LTI Systems
  Subject to False Data Injection Attacks","['cs.SY', 'eess.SY']","  This paper proposes a novel distributed interval-valued simultaneous state
and input observer for linear time-invariant (LTI) systems that are subject to
attacks or unknown inputs injected both on their sensors and actuators. Each
agent in the network leverages a singular value decomposition (SVD) based
transformation to decompose its observations into two components, one of them
unaffected by the attack signal, which helps to obtain local interval estimates
of the state and unknown input and then uses intersection to compute the best
interval estimate among neighboring nodes. We show that the computed intervals
are guaranteed to contain the true state and input trajectories, and we provide
conditions under which the observer is stable. Furthermore, we provide a method
for designing stabilizing gains that minimize an upper bound on the worst-case
steady-state observer error. We demonstrate our algorithm on an IEEE 14-bus
power system.
","[{'version': 'v1', 'created': 'Mon, 20 Mar 2023 19:26:27 GMT'}]",2023-03-22,['Systems and Control'],"This paper presents a distributed resilient interval observer for linear time-invariant (LTI) systems subject to false data injection (FDI) attacks. The proposed observer is composed of a set of local interval observers, which are interconnected via a consensus-based communication network. The observer is designed to guarantee bounded-error state estimates while providing resilience against FDI attacks. The observer's performance is evaluated through numerical simulations on a benchmark model. The results show that the proposed observer can detect and mitigate FDI attacks, while providing reliable state estimates with bounded errors.","Write an abstract for a paper called Distributed Resilient Interval Observers for Bounded-Error LTI Systems
  Subject to False Data Injection Attacks about Systems and Control"
2202.01096,"Alexander Pugantsov, Richard McCreadie","Identifying Suitable Tasks for Inductive Transfer Through the Analysis
  of Feature Attributions","['cs.LG', 'cs.IR']","  Transfer learning approaches have shown to significantly improve performance
on downstream tasks. However, it is common for prior works to only report where
transfer learning was beneficial, ignoring the significant trial-and-error
required to find effective settings for transfer. Indeed, not all task
combinations lead to performance benefits, and brute-force searching rapidly
becomes computationally infeasible. Hence the question arises, can we predict
whether transfer between two tasks will be beneficial without actually
performing the experiment? In this paper, we leverage explainability techniques
to effectively predict whether task pairs will be complementary, through
comparison of neural network activation between single-task models. In this
way, we can avoid grid-searches over all task and hyperparameter combinations,
dramatically reducing the time needed to find effective task pairs. Our results
show that, through this approach, it is possible to reduce training time by up
to 83.5% at a cost of only 0.034 reduction in positive-class F1 on the TREC-IS
2020-A dataset.
","[{'version': 'v1', 'created': 'Wed, 2 Feb 2022 15:51:07 GMT'}]",2022-09-09,"['Machine Learning', 'Information Retrieval']","and Natural Language Processing

This paper presents a method for identifying suitable tasks for inductive transfer through the analysis of feature attributions in the domains of Machine Learning, Information Retrieval and Natural Language Processing. The proposed method uses a combination of feature attribution methods and human-in-the-loop evaluation to identify the most suitable tasks for transfer. We demonstrate the effectiveness of the proposed method by applying it to a number of real-world datasets and tasks. The results show that the proposed method can identify suitable tasks for transfer more accurately than existing methods. Furthermore, our method can be used to identify task-specific features that are important for transfer. This paper provides an important step towards improving the performance of transfer learning in these domains.","Write an abstract for a paper called Identifying Suitable Tasks for Inductive Transfer Through the Analysis
  of Feature Attributions about Machine Learning, Information Retrieval"
2103.04941,"Jiefu Ou, Nathaniel Weir, Anton Belyy, Felix Yu, and Benjamin Van
  Durme",InFillmore: Frame-Guided Language Generation with Bidirectional Context,['cs.CL'],"  We propose a structured extension to bidirectional-context conditional
language generation, or ""infilling,"" inspired by Frame Semantic theory
(Fillmore, 1976). Guidance is provided through two approaches: (1) model
fine-tuning, conditioning directly on observed symbolic frames, and (2) a novel
extension to disjunctive lexically constrained decoding that leverages frame
semantic lexical units. Automatic and human evaluations confirm that
frame-guided generation allows for explicit manipulation of intended infill
semantics, with minimal loss in distinguishability from human-generated text.
Our methods flexibly apply to a variety of use scenarios, and we provide a
codebase and interactive demo available from
https://nlp.jhu.edu/demos/infillmore.
","[{'version': 'v1', 'created': 'Mon, 8 Mar 2021 17:59:41 GMT'}, {'version': 'v2', 'created': 'Sun, 13 Jun 2021 19:22:00 GMT'}, {'version': 'v3', 'created': 'Tue, 22 Mar 2022 14:51:14 GMT'}]",2022-03-23,['Computation and Language'],"This paper presents InFillmore, a novel frame-guided language generation system leveraging bidirectional context for computation and language. InFillmore utilizes a combination of deep learning and symbolic models to generate natural language from a given frame. The system is trained on a large-scale corpus of frame-based language and is evaluated on a variety of tasks, including natural language generation, question answering, and summarization. The results demonstrate that InFillmore outperforms existing systems in terms of accuracy, while providing better interpretability and generalization. Furthermore, the system is able to generate natural language from a given frame using bidirectional context, which greatly improves its ability to generate accurate and natural language. This paper provides an in-depth analysis of InFillmore's architecture, evaluation results, and potential applications.",Write an abstract for a paper called InFillmore: Frame-Guided Language Generation with Bidirectional Context about Computation and Language
2201.05413,"Antonella Galizia, Simone Cammarasana, Andrea Clematis, and Giuseppe
  Patane'","Evaluating Accuracy and Efficiency of HPC Solvers for Sparse Linear
  Systems with Applications to PDEs","['math.NA', 'cs.NA', 'cs.PF']","  Partial Differential Equations (PDEs) describe several problems relevant to
many fields of applied sciences, and their discrete counterparts typically
involve the solution of sparse linear systems. In this context, we focus on the
analysis of the computational aspects related to the solution of large and
sparse linear systems with HPC solvers, by considering the performances of
direct and iterative solvers in terms of computational efficiency, scalability,
and numerical accuracy. Our aim is to identify the main criteria to support
application-domain specialists in the selection of the most suitable solvers,
according to the application requirements and available resources. To this end,
we discuss how the numerical solver is affected by the regular/irregular
discretisation of the input domain, the discretisation of the input PDE with
piecewise linear or polynomial basis functions, which generally result in a
higher/lower sparsity of the coefficient matrix, and the choice of different
initial conditions, which are associated with linear systems with multiple
right-hand side terms. Finally, our analysis is independent of the
characteristics of the underlying computational architectures, and provides a
methodological approach that can be applied to different classes of PDEs or
with approximation problems.
","[{'version': 'v1', 'created': 'Fri, 14 Jan 2022 12:13:37 GMT'}]",2022-01-17,"['Numerical Analysis', 'Performance']","This paper evaluates the accuracy and efficiency of high-performance computing (HPC) solvers for sparse linear systems with applications to partial differential equations (PDEs). We analyze a variety of numerical methods, including Krylov subspace methods, direct solvers, and multigrid solvers. We discuss the accuracy of each method, as well as their respective advantages and disadvantages. We also consider the effect of various parameters, such as the sparsity of the matrix, on the performance of each solver. Finally, we present numerical experiments to demonstrate the performance of the various methods, and discuss their application to various PDEs. The results of this study will provide insights into the design and implementation of efficient numerical algorithms for solving sparse linear systems with applications to PDEs.","Write an abstract for a paper called Evaluating Accuracy and Efficiency of HPC Solvers for Sparse Linear
  Systems with Applications to PDEs about Numerical Analysis, Performance"
2302.04114,Mingzhe Zhu and Liwang Zhu and Huan Li and Wei Li and Zhongzhi Zhang,"Resistance Distances in Directed Graphs: Definitions, Properties, and
  Applications",['cs.NI'],"  Resistance distance has been studied extensively in the past years, with the
majority of previous studies devoted to undirected networks, in spite of the
fact that various realistic networks are directed. Although several
generalizations of resistance distance on directed graphs have been proposed,
they either have no physical interpretation or are not a metric. In this paper,
we first extend the definition of resistance distance to strongly connected
directed graphs based on random walks and show that the two-node resistance
distance on directed graphs is a metric. Then, we introduce the Laplacian
matrix for directed graphs that subsumes the Laplacian matrix of undirected
graphs as a particular case and use its pseudoinverse to express the two-node
resistance distance, and many other relevant quantities derived from resistance
distances. Moreover, we define the resistance distance between a vertex and a
vertex group on directed graphs and further define a problem of optimally
selecting a group of fixed number of nodes, such that their resistance distance
is minimized. Since this combinatorial optimization problem is NP-hard, we
present a greedy algorithm with a proved approximation ratio, and conduct
experiments on model and realistic networks to validate the performance of this
approximation algorithm.
","[{'version': 'v1', 'created': 'Wed, 8 Feb 2023 15:12:02 GMT'}]",2023-02-09,['Networking and Internet Architecture'],"This paper presents a comprehensive overview of resistance distances in directed graphs, with a particular focus on their applications in networking and internet architecture. We provide definitions of resistance distances, discuss their properties, and explore the potential of their use in network and internet architecture. We also discuss potential applications of resistance distances, such as the design of robust and reliable communication networks, the analysis of network traffic, and the optimization of network performance. Finally, we present a number of open problems and challenges that need to be addressed for the successful application of resistance distances in networking and internet architecture.","Write an abstract for a paper called Resistance Distances in Directed Graphs: Definitions, Properties, and
  Applications about Networking and Internet Architecture"
2205.07121,"Prathima Dileep, Bharath Kumar Bolla, Sabeesh Ethiraj","Revisiting Facial Key Point Detection: An Efficient Approach Using Deep
  Neural Networks","['cs.CV', 'cs.AI', 'cs.LG']","  Facial landmark detection is a widely researched field of deep learning as
this has a wide range of applications in many fields. These key points are
distinguishing characteristic points on the face, such as the eyes center, the
eye's inner and outer corners, the mouth center, and the nose tip from which
human emotions and intent can be explained. The focus of our work has been
evaluating transfer learning models such as MobileNetV2 and NasNetMobile,
including custom CNN architectures. The objective of the research has been to
develop efficient deep learning models in terms of model size, parameters, and
inference time and to study the effect of augmentation imputation and
fine-tuning on these models. It was found that while augmentation techniques
produced lower RMSE scores than imputation techniques, they did not affect the
inference time. MobileNetV2 architecture produced the lowest RMSE and inference
time. Moreover, our results indicate that manually optimized CNN architectures
performed similarly to Auto Keras tuned architecture. However, manually
optimized architectures yielded better inference time and training curves.
","[{'version': 'v1', 'created': 'Sat, 14 May 2022 19:49:03 GMT'}]",2022-05-17,"['Computer Vision and Pattern Recognition', 'Artificial Intelligence', 'Machine Learning']","This paper presents an efficient approach to facial key point detection using deep neural networks. The approach is based on the use of convolutional neural networks (CNNs) to detect and localize facial features in images. The proposed approach is evaluated on two publicly available datasets, the Multi-PIE and the 300-W datasets, and is compared to existing methods. The results show that the proposed approach achieves better accuracy, faster detection time and improved robustness in comparison to existing methods. The paper also discusses the advantages of using CNNs for facial key point detection and provides insights into the challenges of using deep learning for this application. Finally, the paper presents a discussion on the potential applications of the proposed approach and the implications of its use in computer vision and pattern recognition, artificial intelligence, and machine learning.","Write an abstract for a paper called Revisiting Facial Key Point Detection: An Efficient Approach Using Deep
  Neural Networks about Computer Vision and Pattern Recognition, Artificial Intelligence, Machine Learning"
2106.1585,"Asim Waqas (1), Ghulam Rasool (1), Hamza Farooq (2), and Nidhal C.
  Bouaynaya (1), ((1) Rowan University, (2) University of Minnesota)",Exploring Robust Architectures for Deep Artificial Neural Networks,"['cs.LG', 'cs.AI']","  The architectures of deep artificial neural networks (DANNs) are routinely
studied to improve their predictive performance. However, the relationship
between the architecture of a DANN and its robustness to noise and adversarial
attacks is less explored. We investigate how the robustness of DANNs relates to
their underlying graph architectures or structures. This study: (1) starts by
exploring the design space of architectures of DANNs using graph-theoretic
robustness measures; (2) transforms the graphs to DANN architectures to
train/validate/test on various image classification tasks; (3) explores the
relationship between the robustness of trained DANNs against noise and
adversarial attacks and the robustness of their underlying architectures
estimated via graph-theoretic measures. We show that the topological entropy
and Olivier-Ricci curvature of the underlying graphs can quantify the
robustness performance of DANNs. The said relationship is stronger for complex
tasks and large DANNs. Our work will allow autoML and neural architecture
search community to explore design spaces of robust and accurate DANNs.
","[{'version': 'v1', 'created': 'Wed, 30 Jun 2021 07:12:19 GMT'}, {'version': 'v2', 'created': 'Tue, 5 Apr 2022 22:46:04 GMT'}]",2023-02-06,"['Machine Learning', 'Artificial Intelligence']","This paper examines the use of robust architectures for deep artificial neural networks (DANNs) in the field of machine learning and artificial intelligence (AI). It presents a comprehensive review of existing robust architectures, along with a discussion of their advantages and limitations. The paper then proposes an original approach to designing robust DANNs, incorporating a variety of techniques such as regularization, batch normalization, and dropout. Finally, the paper presents a series of experiments demonstrating the effectiveness of the proposed approach when applied to real-world datasets. The results indicate that the proposed approach can significantly improve the performance of DANNs, thus enabling more efficient and accurate AI models.","Write an abstract for a paper called Exploring Robust Architectures for Deep Artificial Neural Networks about Machine Learning, Artificial Intelligence"
2205.10441,"Paschalis Lagias, George D. Magoulas, Ylli Prifti and Alessandro
  Provetti","Predicting Seriousness of Injury in a Traffic Accident: A New Imbalanced
  Dataset and Benchmark","['cs.LG', 'cs.AI']","  The paper introduces a new dataset to assess the performance of machine
learning algorithms in the prediction of the seriousness of injury in a traffic
accident. The dataset is created by aggregating publicly available datasets
from the UK Department for Transport, which are drastically imbalanced with
missing attributes sometimes approaching 50\% of the overall data
dimensionality. The paper presents the data analysis pipeline starting from the
publicly available data of road traffic accidents and ending with predictors of
possible injuries and their degree of severity. It addresses the huge
incompleteness of public data with a MissForest model. The paper also
introduces two baseline approaches to create injury predictors: a supervised
artificial neural network and a reinforcement learning model. The dataset can
potentially stimulate diverse aspects of machine learning research on
imbalanced datasets and the two approaches can be used as baseline references
when researchers test more advanced learning algorithms in this area.
","[{'version': 'v1', 'created': 'Fri, 20 May 2022 21:15:26 GMT'}]",2022-05-24,"['Machine Learning', 'Artificial Intelligence']","This paper presents a new imbalanced dataset and benchmark for the task of predicting the seriousness of injury in a traffic accident using machine learning and artificial intelligence. The dataset contains records of traffic accidents in a major US city, including information on the severity of the accident and the resulting injuries. We use a variety of supervised learning algorithms to predict the severity of injury from the features of the accident. We evaluate our models using standard metrics and compare our results with the state-of-the-art for this task. Our results demonstrate that machine learning and artificial intelligence can be used to accurately predict the seriousness of injury in a traffic accident.","Write an abstract for a paper called Predicting Seriousness of Injury in a Traffic Accident: A New Imbalanced
  Dataset and Benchmark about Machine Learning, Artificial Intelligence"
2203.17105,"Brady Planden, Katie Lukow, Paul Henshall, Gordana Collier, Denise
  Morrey","A Computationally Informed Realisation Algorithm for Lithium-Ion
  Batteries Implemented with LiiBRA.jl","['eess.SY', 'cs.SY']","  Real-time battery modelling advancements have quickly become a requirement as
the adoption of battery electric vehicles (BEVs) has rapidly increased. In this
paper an open-source, improved discrete realisation algorithm, implemented in
Julia for creation and simulation of reduced-order, real-time capable
physics-based models is presented. This work reduces the Doyle-Fuller-Newman
electrochemical model into continuous-form transfer functions and introduces a
computationally informed discrete realisation algorithm (CI-DRA) to generate
the reduced-order models. Further improvements in conventional offline model
creation are obtained as well as achieving in-vehicle capable model creation
for ARM based computing architectures. Furthermore, a sensitivity analysis on
the resultant computational time is completed as well as experimental
validation of a worldwide harmonised light vehicle test procedure (WLTP) for a
LG Chem. M50 21700 parameterisation. A performance comparison to the
conventional Matlab implemented discrete realisation algorithm (DRA) is
completed showcasing a mean computational time improvement of 88%. Finally, an
ARM based compilation is investigated for in-vehicle model generation and shows
a modest performance reduction of 43% when compared to the x86 implementation
while still generating accurate models within 5.5 seconds.
","[{'version': 'v1', 'created': 'Thu, 31 Mar 2022 15:22:15 GMT'}]",2022-04-01,['Systems and Control'],"This paper presents a computationally informed realisation algorithm for lithium-ion batteries implemented with LiiBRA.jl, a Julia-based software library for modelling and control of lithium-ion batteries. The algorithm is based on a combination of system identification and model-based control approaches and uses a multi-objective optimisation to identify the best model parameters for the battery. The proposed algorithm is tested on a real-world lithium-ion battery and results demonstrate its ability to accurately identify the battery's parameters and to provide a reliable and efficient control of the battery's state. This work provides an important contribution to the field of lithium-ion battery modelling and control, and shows the potential of LiiBRA.jl as a comprehensive tool for lithium-ion battery research.","Write an abstract for a paper called A Computationally Informed Realisation Algorithm for Lithium-Ion
  Batteries Implemented with LiiBRA.jl about Systems and Control"
2211.09524,Jonathan W. Z. Lim and Vrizlynn L. L. Thing,Towards Effective Cybercrime Intervention,"['cs.CR', 'cs.CY']","  Cybercrimes are on the rise, in part due to technological advancements, as
well as increased avenues of exploitation. Sophisticated threat actors are
leveraging on such advancements to execute their malicious intentions. The
increase in cybercrimes is prevalent, and it seems unlikely that they can be
easily eradicated. A more serious concern is that the community may come to
accept the notion that this will become the trend. As such, the key question
revolves around how we can reduce cybercrime in this evolving landscape. In our
paper, we propose to build a systematic framework through the lens of a cyber
threat actor. We explore the motivation factors behind the crimes and the crime
stages of the threat actors. We then formulate intervention plans so as to
discourage the act of committing malicious cyber activities and also aim to
integrate ex-cyber offenders back into society.
","[{'version': 'v1', 'created': 'Thu, 17 Nov 2022 13:40:53 GMT'}]",2022-11-18,"['Cryptography and Security', 'Computers and Society']","This paper will explore the use of cryptography and security measures to combat cybercrime and the implications of these measures on computers and society. It will examine the current state of cybercrime and the effectiveness of existing strategies used to intervene in the problem. It will also analyze the potential of cryptography and security measures to be used to reduce the prevalence of cybercrime, as well as the impact of these measures on computers and society. Finally, the paper will provide recommendations on how to effectively use cryptography and security measures to reduce cybercrime. The paper's findings will provide a better understanding of the role of cryptography and security in cybercrime intervention and help inform future policy decisions.","Write an abstract for a paper called Towards Effective Cybercrime Intervention about Cryptography and Security, Computers and Society"
2211.06506,"Zhichao Wang, Andrew Engel, Anand Sarwate, Ioana Dumitriu, Tony Chiang",Spectral evolution and invariance in linear-width neural networks,"['cs.LG', 'stat.ML']","  We investigate the spectral properties of linear-width feed-forward neural
networks, where the sample size is asymptotically proportional to network
width. Empirically, we show that the weight spectra in this high dimensional
regime are invariant when trained by gradient descent for small constant
learning rates and the changes in both operator and Frobenius norm are
$\Theta(1)$ in the limit. This implies the bulk spectra for both the conjugate
and neural tangent kernels are also invariant. We demonstrate similar
characteristics for models trained with mini-batch (stochastic) gradient
descent with small learning rates and provide a theoretical justification for
this special scenario. When the learning rate is large, we show empirically
that an outlier emerges with its corresponding eigenvector aligned to the
training data structure. We also show that after adaptive gradient training,
where we have a lower test error and feature learning emerges, both the weight
and kernel matrices exhibit heavy tail behavior. Different spectral properties
such as invariant bulk, spike, and heavy-tailed distribution correlate to how
far the kernels deviate from initialization. To understand this phenomenon
better, we focus on a toy model, a two-layer network on synthetic data, which
exhibits different spectral properties for different training strategies.
Analogous phenomena also appear when we train conventional neural networks with
real-world data. Our results show that monitoring the evolution of the spectra
during training is an important step toward understanding the training dynamics
and feature learning.
","[{'version': 'v1', 'created': 'Fri, 11 Nov 2022 23:00:30 GMT'}]",2022-11-15,['Machine Learning'],"This paper examines the spectral evolution and invariance properties of linear-width neural networks (LWNNs) in the context of machine learning. LWNNs are a type of artificial neural network that are composed of only a few layers, making them computationally efficient and suitable for real-time applications. We analyze the spectral properties of LWNNs and compare them with those of non-LWNNs. We show that LWNNs have a unique spectral evolution, which allows them to maintain a consistent performance over different training datasets. We also discuss the invariance properties of LWNNs, which allow them to generalize better than non-LWNNs. Finally, we discuss the implications of our findings for machine learning applications.",Write an abstract for a paper called Spectral evolution and invariance in linear-width neural networks about Machine Learning
1809.0347,"Marek Wydmuch, Micha{\l} Kempka, Wojciech Ja\'skowski",ViZDoom Competitions: Playing Doom from Pixels,"['cs.AI', 'cs.CV', 'cs.LG', 'stat.ML']","  This paper presents the first two editions of Visual Doom AI Competition,
held in 2016 and 2017. The challenge was to create bots that compete in a
multi-player deathmatch in a first-person shooter (FPS) game, Doom. The bots
had to make their decisions based solely on visual information, i.e., a raw
screen buffer. To play well, the bots needed to understand their surroundings,
navigate, explore, and handle the opponents at the same time. These aspects,
together with the competitive multi-agent aspect of the game, make the
competition a unique platform for evaluating the state of the art reinforcement
learning algorithms. The paper discusses the rules, solutions, results, and
statistics that give insight into the agents' behaviors. Best-performing agents
are described in more detail. The results of the competition lead to the
conclusion that, although reinforcement learning can produce capable Doom bots,
they still are not yet able to successfully compete against humans in this
game. The paper also revisits the ViZDoom environment, which is a flexible,
easy to use, and efficient 3D platform for research for vision-based
reinforcement learning, based on a well-recognized first-person perspective
game Doom.
","[{'version': 'v1', 'created': 'Mon, 10 Sep 2018 17:41:39 GMT'}]",2022-07-28,"['Artificial Intelligence', 'Computer Vision and Pattern Recognition', 'Machine Learning']","This paper presents the ViZDoom competition, a platform for AI agents to play the classic first-person shooter game Doom from raw pixel inputs. We discuss the challenges of playing the game from pixel inputs and the benefits of using ViZDoom for AI research. We present the competition structure, the evaluation metrics, and the baseline methods used to compare the performance of AI agents. We then describe the results of the competition, including the best-performing methods and the comparison between the different approaches. Finally, we discuss the implications of the competition for AI research in the areas of computer vision, pattern recognition, and machine learning.","Write an abstract for a paper called ViZDoom Competitions: Playing Doom from Pixels about Artificial Intelligence, Computer Vision and Pattern Recognition, Machine Learning"
2304.00503,"Matej Smid, Jindrich Dunik",Online Learning and Control for Data-Augmented Quadrotor Model,"['cs.RO', 'cs.SY', 'eess.SY']","  The ability to adapt to changing conditions is a key feature of a successful
autonomous system. In this work, we use the Recursive Gaussian Processes (RGP)
for identification of the quadrotor air drag model online, without the need of
training data. The identified drag model then augments a physics-based model of
the quadrotor dynamics, which allows more accurate quadrotor state prediction
with increased ability to adapt to changing conditions. This data-augmented
physics-based model is utilized for precise quadrotor trajectory tracking using
the suitably modified Model Predictive Control (MPC) algorithm. The proposed
modelling and control approach is evaluated using the Gazebo simulator and it
is shown that the proposed approach tracks a desired trajectory with a higher
accuracy compared to the MPC with the non-augmented (purely physics-based)
model.
","[{'version': 'v1', 'created': 'Sun, 2 Apr 2023 10:37:29 GMT'}]",2023-04-04,"['Robotics', 'Systems and Control']","This paper presents a novel online learning and control system for a data-augmented quadrotor model. The system is designed to enable the quadrotor to learn and control its motion in a dynamic environment. The system uses a combination of adaptive control and reinforcement learning to enable the quadrotor to learn from its environment and adapt its behavior accordingly. The paper also presents a case study of the system applied to a simulated quadrotor in a 3D environment. The results of the case study demonstrate the effectiveness of the system in allowing the quadrotor to successfully navigate the environment and achieve its objectives. The paper provides insights into the potential of online learning and control for robotics, systems and control.","Write an abstract for a paper called Online Learning and Control for Data-Augmented Quadrotor Model about Robotics, Systems and Control"
2202.06555,"Aryan Eftekhari, Simon Scheidegger",High-Dimensional Dynamic Stochastic Model Representation,"['econ.GN', 'cs.CE', 'q-fin.EC']","  We propose a scalable method for computing global solutions of nonlinear,
high-dimensional dynamic stochastic economic models. First, within a time
iteration framework, we approximate economic policy functions using an
adaptive, high-dimensional model representation scheme, combined with adaptive
sparse grids to address the ubiquitous challenge of the curse of
dimensionality. Moreover, the adaptivity within the individual component
functions increases sparsity since grid points are added only where they are
most needed, that is, in regions with steep gradients or at
nondifferentiabilities. Second, we introduce a performant vectorization scheme
for the interpolation compute kernel. Third, the algorithm is hybrid
parallelized, leveraging both distributed- and shared-memory architectures. We
observe significant speedups over the state-of-the-art techniques, and almost
ideal strong scaling up to at least $1,000$ compute nodes of a Cray XC$50$
system at the Swiss National Supercomputing Center. Finally, to demonstrate our
method's broad applicability, we compute global solutions to two variates of a
high-dimensional international real business cycle model up to $300$ continuous
state variables. In addition, we highlight a complementary advantage of the
framework, which allows for a priori analysis of the model complexity.
","[{'version': 'v1', 'created': 'Mon, 14 Feb 2022 08:47:28 GMT'}]",2022-02-15,"['Computational Engineering, Finance, and Science']","This paper presents a novel approach to represent high-dimensional dynamic stochastic models for use in computational engineering, finance, and science. The proposed approach combines a number of existing techniques to create a model that is both accurate and efficient. The model is able to capture both the temporal and spatial dynamics of the underlying system, while also allowing for efficient computation. The model is validated with a number of experiments and is shown to outperform existing methods. The paper provides a detailed description of the proposed approach and its application to various fields, as well as an analysis of the results and implications for future research.","Write an abstract for a paper called High-Dimensional Dynamic Stochastic Model Representation about Computational Engineering, Finance, and Science"
2108.11439,"Jun Wang, Hefeng Zhou, Xiaohan Yu","PGTRNet: Two-phase Weakly Supervised Object Detection with Pseudo Ground
  Truth Refinement",['cs.CV'],"  Current state-of-the-art weakly supervised object detection (WSOD) studies
mainly follow a two-stage training strategy which integrates a fully supervised
detector (FSD) with a pure WSOD model. There are two main problems hindering
the performance of the two-phase WSOD approaches, i.e., insufficient learning
problem and strict reliance between the FSD and the pseudo ground truth (PGT)
generated by the WSOD model. This paper proposes pseudo ground truth refinement
network (PGTRNet), a simple yet effective method without introducing any extra
learnable parameters, to cope with these problems. PGTRNet utilizes multiple
bounding boxes to establish the PGT, mitigating the insufficient learning
problem. Besides, we propose a novel online PGT refinement approach to steadily
improve the quality of PGT by fully taking advantage of the power of FSD during
the second-phase training, decoupling the first and second-phase models.
Elaborate experiments are conducted on the PASCAL VOC 2007 benchmark to verify
the effectiveness of our methods. Experimental results demonstrate that PGTRNet
boosts the backbone model by 2.1% mAP and achieves the state-of-the-art
performance.
","[{'version': 'v1', 'created': 'Wed, 25 Aug 2021 19:20:49 GMT'}, {'version': 'v2', 'created': 'Thu, 17 Mar 2022 19:00:16 GMT'}]",2022-03-21,['Computer Vision and Pattern Recognition'],"This paper presents PGTRNet, a two-phase weakly supervised object detection method that leverages pseudo ground truth refinement. PGTRNet is designed to address the challenge of detecting objects in images with only image-level labels. In the first phase, a Region Proposal Network is used to generate an initial set of candidate object proposals. These proposals are then refined in the second phase using a pseudo ground truth refinement module. The proposed method is evaluated on the challenging MS-COCO dataset and results demonstrate that PGTRNet outperforms the state-of-the-art weakly supervised object detection methods. The results also show that PGTRNet is able to achieve comparable performance to fully supervised methods, demonstrating the potential of the proposed method in computer vision and pattern recognition tasks.","Write an abstract for a paper called PGTRNet: Two-phase Weakly Supervised Object Detection with Pseudo Ground
  Truth Refinement about Computer Vision and Pattern Recognition"
2206.06488,"Peng Xu, Xiatian Zhu, and David A. Clifton",Multimodal Learning with Transformers: A Survey,"['cs.CV', 'cs.LG']","  Transformer is a promising neural network learner, and has achieved great
success in various machine learning tasks. Thanks to the recent prevalence of
multimodal applications and big data, Transformer-based multimodal learning has
become a hot topic in AI research. This paper presents a comprehensive survey
of Transformer techniques oriented at multimodal data. The main contents of
this survey include: (1) a background of multimodal learning, Transformer
ecosystem, and the multimodal big data era, (2) a theoretical review of Vanilla
Transformer, Vision Transformer, and multimodal Transformers, from a
geometrically topological perspective, (3) a review of multimodal Transformer
applications, via two important paradigms, i.e., for multimodal pretraining and
for specific multimodal tasks, (4) a summary of the common challenges and
designs shared by the multimodal Transformer models and applications, and (5) a
discussion of open problems and potential research directions for the
community.
","[{'version': 'v1', 'created': 'Mon, 13 Jun 2022 21:36:09 GMT'}]",2022-06-15,"['Computer Vision and Pattern Recognition', 'Machine Learning']",", and Natural Language Processing

This paper surveys the potential of multimodal learning using transformers for computer vision and pattern recognition, machine learning, and natural language processing. It provides an overview of the recent advances in the field and highlights the various challenges and opportunities associated with multimodal learning. It also discusses the potential of transformers for realizing effective multimodal learning approaches. Finally, the paper provides an outlook on the future of multimodal learning with transformers and its potential applications.","Write an abstract for a paper called Multimodal Learning with Transformers: A Survey about Computer Vision and Pattern Recognition, Machine Learning"
2301.0755,"Daniele Meli, Hirenkumar Nakawala, Paolo Fiorini",Logic programming for deliberative robotic task planning,"['cs.RO', 'cs.AI', 'cs.LO']","  Over the last decade, the use of robots in production and daily life has
increased. With increasingly complex tasks and interaction in different
environments including humans, robots are required a higher level of autonomy
for efficient deliberation. Task planning is a key element of deliberation. It
combines elementary operations into a structured plan to satisfy a prescribed
goal, given specifications on the robot and the environment. In this
manuscript, we present a survey on recent advances in the application of logic
programming to the problem of task planning. Logic programming offers several
advantages compared to other approaches, including greater expressivity and
interpretability which may aid in the development of safe and reliable robots.
We analyze different planners and their suitability for specific robotic
applications, based on expressivity in domain representation, computational
efficiency and software implementation. In this way, we support the robotic
designer in choosing the best tool for his application.
","[{'version': 'v1', 'created': 'Wed, 18 Jan 2023 14:11:55 GMT'}]",2023-01-19,"['Robotics', 'Artificial Intelligence', 'Logic in Computer Science']","This paper discusses the application of logic programming to the task of deliberative robotic task planning. It presents a new approach to robotic task planning which combines the use of logic programming and artificial intelligence. The paper examines the advantages of using logic programming to represent and reason about robotic tasks, and how it can be used to develop deliberative robotic task planners. It discusses the various challenges posed by this approach, and how they can be addressed. Finally, it presents a case study of a robotic task planner developed using logic programming, and its performance in a simulated environment. The paper provides an overview of the potential of logic programming for robotics and artificial intelligence, and its potential for further development.","Write an abstract for a paper called Logic programming for deliberative robotic task planning about Robotics, Artificial Intelligence, Logic in Computer Science"
2209.14879,"Cosmin Ursache, Michael Sammeth, S\^inic\u{a} Alboaie",OpenDSU: Digital Sovereignty in PharmaLedger,"['cs.CR', 'cs.NI', 'cs.SI', 'cs.SY', 'eess.SY']","  Distributed ledger networks, chiefly those based on blockchain technologies,
currently are heralding a next generation of computer systems that aims to suit
modern users' demands. Over the recent years, several technologies for
blockchains, off-chaining strategies, as well as decentralised and respectively
self-sovereign identity systems have shot up so fast that standardisation of
the protocols is lagging behind, severely hampering the interoperability of
different approaches. Moreover, most of the currently available solutions for
distributed ledgers focus on either home users or enterprise use case
scenarios, failing to provide integrative solutions addressing the needs of
both.
  Herein we introduce the OpenDSU platform that allows to interoperate generic
blockchain technologies, organised - and possibly cascaded in a hierarchical
fashion - in domains. To achieve this flexibility, we seamlessly integrated a
set of well conceived OpenDSU components to orchestrate off-chain data with
granularly resolved and cryptographically secure access levels that are nested
with sovereign identities across the different domains.
  Employing our platform to PharmaLedger, an inter-European network for the
standardisation of data handling in the pharmaceutical industry and in
healthcare, we demonstrate that OpenDSU can cope with generic demands of
heterogeneous use cases in both, performance and handling substantially
different business policies. Importantly, whereas available solutions commonly
require a pre-defined and fixed set of components, no such vendor lock-in
restrictions on the blockchain technology or identity system exist in OpenDSU,
making systems built on it flexibly adaptable to new standards evolving in the
future.
","[{'version': 'v1', 'created': 'Thu, 29 Sep 2022 15:43:31 GMT'}]",2022-10-03,"['Cryptography and Security', 'Networking and Internet Architecture', 'Social and Information Networks', 'Systems and Control']","This paper presents OpenDSU, a platform for digital sovereignty in PharmaLedger. OpenDSU is a distributed system that enables secure, transparent, and cost-effective pharmaceutical supply chain management. It leverages cryptography and security, networking and internet architecture, social and information networks, and systems and control to provide a safe, secure, and reliable platform for managing and tracking pharmaceuticals. OpenDSU provides a secure and reliable platform for pharmaceutical manufacturers, distributors, and consumers to ensure the integrity of the pharmaceutical supply chain. It also provides a secure and reliable platform for healthcare professionals and researchers to access the necessary data for research, analysis, and decision-making. The paper discusses the challenges and opportunities associated with OpenDSU, and provides an overview of the system architecture. Finally, the paper presents a case study to demonstrate the efficacy of OpenDSU in a real-world pharmaceutical supply chain.","Write an abstract for a paper called OpenDSU: Digital Sovereignty in PharmaLedger about Cryptography and Security, Networking and Internet Architecture, Social and Information Networks, Systems and Control"
2204.00156,"Qi Zhang and Xin Huang and Ying Feng and Xue Wang and Hongdong Li and
  Qing Wang","Stereo Unstructured Magnification: Multiple Homography Image for View
  Synthesis",['cs.CV'],"  This paper studies the problem of view synthesis with certain amount of
rotations from a pair of images, what we called stereo unstructured
magnification. While the multi-plane image representation is well suited for
view synthesis with depth invariant, how to generalize it to unstructured views
remains a significant challenge. This is primarily due to the depth-dependency
caused by camera frontal parallel representation. Here we propose a novel
multiple homography image (MHI) representation, comprising of a set of scene
planes with fixed normals and distances. A two-stage network is developed for
novel view synthesis. Stage-1 is an MHI reconstruction module that predicts the
MHIs and composites layered multi-normal images along the normal direction.
Stage-2 is a normal-blending module to find blending weights. We also derive an
angle-based cost to guide the blending of multi-normal images by exploiting
per-normal geometry. Compared with the state-of-the-art methods, our method
achieves superior performance for view synthesis qualitatively and
quantitatively, especially for cases when the cameras undergo rotations.
","[{'version': 'v1', 'created': 'Fri, 1 Apr 2022 01:39:28 GMT'}]",2022-04-04,['Computer Vision and Pattern Recognition'],"This paper introduces a novel approach to view synthesis in computer vision and pattern recognition. Stereo unstructured magnification (SUM) is a method that combines multiple homography images to create a single image with a higher magnification than the original images. The proposed method is tested on a stereo image dataset and compared with existing view synthesis methods. The results show that SUM is more accurate and efficient than current methods, and it can generate high-quality images with a wide range of magnifications. Additionally, the paper discusses the challenges of view synthesis and provides future directions for research.","Write an abstract for a paper called Stereo Unstructured Magnification: Multiple Homography Image for View
  Synthesis about Computer Vision and Pattern Recognition"
2301.0796,"G\""osta Stomberg, Henrik Ebel, Timm Faulwasser, Peter Eberhard","Cooperative Distributed MPC via Decentralized Real-Time Optimization:
  Implementation Results for Robot Formations","['eess.SY', 'cs.RO', 'cs.SY']","  Distributed model predictive control (DMPC) is a flexible and scalable
feedback control method applicable to a wide range of systems. While stability
analysis of DMPC is quite well understood, there exist only limited
implementation results for realistic applications involving distributed
computation and networked communication. This article approaches formation
control of mobile robots via a cooperative DMPC scheme. We discuss the
implementation via decentralized optimization algorithms. To this end, we
combine the alternating direction method of multipliers with decentralized
sequential quadratic programming to solve the underlying optimal control
problem in a decentralized fashion. Our approach only requires coupled
subsystems to communicate and does not rely on a central coordinator. Our
experimental results showcase the efficacy of DMPC for formation control and
they demonstrate the real-time feasibility of the considered algorithms.
","[{'version': 'v1', 'created': 'Thu, 19 Jan 2023 09:27:03 GMT'}]",2023-01-20,"['Robotics', 'Systems and Control']","This paper presents an implementation of a cooperative distributed model predictive control (MPC) scheme for robot formations, based on decentralized real-time optimization. The proposed scheme is tested on a robotic system consisting of two robots, and the results are discussed in detail. The performance of the proposed scheme is compared to that of a centralized MPC approach, and the results show that the cooperative distributed MPC scheme can achieve comparable control performance to the centralized approach, while providing improved scalability and robustness. Furthermore, the paper provides insights into the implementation of the proposed scheme and its limitations.","Write an abstract for a paper called Cooperative Distributed MPC via Decentralized Real-Time Optimization:
  Implementation Results for Robot Formations about Robotics, Systems and Control"
2203.01818,"Marcos Faundez-Zanuy, Oscar Oliva-Suarez",ADPCM with nonlinear prediction,"['eess.AS', 'cs.LG']","  Many speech coders are based on linear prediction coding (LPC), nevertheless
with LPC is not possible to model the nonlinearities present in the speech
signal. Because of this there is a growing interest for nonlinear techniques.
In this paper we discuss ADPCM schemes with a nonlinear predictor based on
neural nets, which yields an increase of 1-2.5dB in the SEGSNR over classical
methods. This paper will discuss the block-adaptive and sample-adaptive
predictions.
","[{'version': 'v1', 'created': 'Thu, 24 Feb 2022 00:17:59 GMT'}]",2022-03-04,['Machine Learning'],"This paper explores the use of Adaptive Differential Pulse Code Modulation (ADPCM) with nonlinear prediction to improve machine learning applications. ADPCM is an audio compression technique that encodes audio signals by taking the difference between successive samples and encoding the difference rather than the samples themselves. Nonlinear prediction is a machine learning technique used to predict the future values of a signal based on its past values. The paper investigates the effectiveness of combining ADPCM and nonlinear prediction to improve machine learning performance. The paper will discuss the advantages of using this combination and the potential applications of this approach. Additionally, experiments will be conducted to evaluate the performance of the proposed technique in comparison to existing machine learning approaches. The results of this study will provide insight into the potential of ADPCM with nonlinear prediction to improve machine learning performance.",Write an abstract for a paper called ADPCM with nonlinear prediction about Machine Learning
2302.02036,"Boris Kramer, Serkan Gugercin, Jeff Borggaard",Nonlinear Balanced Truncation: Part 2 -- Model Reduction on Manifolds,"['math.OC', 'cs.SY', 'eess.SY']","  Nonlinear balanced truncation is a model order reduction technique that
reduces the dimension of nonlinear systems in a manner that accounts for either
open- or closed-loop observability and controllability aspects of the system.
Two computational challenges have so far prevented its deployment on
large-scale systems: (a) the energy functions required for characterization of
controllability and observability are solutions of high-dimensional
Hamilton-Jacobi-(Bellman) equations, which have been computationally
intractable and (b) the transformations to construct the reduced-order models
(ROMs) are potentially ill-conditioned and the resulting ROMs are difficult to
simulate on the nonlinear balanced manifolds. Part~1 of this two-part article
addressed challenge (a) via a scalable tensor-based method to solve for
polynomial approximations of the open- and closed-loop energy functions. This
article, (Part~2), addresses challenge (b) by presenting a novel and scalable
method to reduce the dimensionality of the full-order model via model reduction
on polynomially-nonlinear balanced manifolds. The associated nonlinear state
transformation simultaneously 'diagonalizes' relevant energy functions in the
new coordinates. Since this nonlinear balancing transformation can be
ill-conditioned and expensive to evaluate, inspired by the linear case we
develop a computationally efficient balance-and-reduce strategy, resulting in a
scalable and better conditioned truncated transformation to produce balanced
nonlinear ROMs. The algorithm is demonstrated on a semi-discretized partial
differential equation, namely Burgers equation, which illustrates that
higher-degree transformations can improve the accuracy of ROM outputs.
","[{'version': 'v1', 'created': 'Sat, 4 Feb 2023 00:14:07 GMT'}]",2023-02-07,['Systems and Control'],"This paper presents a novel approach to model reduction on manifolds, called Nonlinear Balanced Truncation (NBT). NBT is an extension of linear balanced truncation, a well-known model reduction method for linear systems. NBT is based on the theory of Lyapunov stability, and is designed to reduce the order of nonlinear systems while preserving their stability properties. The paper describes how NBT can be applied to a variety of nonlinear systems, including autonomous and non-autonomous systems, and systems with multiple inputs and outputs. Furthermore, the paper discusses how NBT can be used to reduce the computational complexity of nonlinear systems, as well as how it can be used to reduce the order of large-scale nonlinear systems. Finally, the paper presents numerical results to illustrate the effectiveness of the proposed approach.",Write an abstract for a paper called Nonlinear Balanced Truncation: Part 2 -- Model Reduction on Manifolds about Systems and Control
2112.14864,Chuwen Ma and Weiying Zheng,"A high-order unfitted finite element method for moving interface
  problems","['math.NA', 'cs.NA']","  We propose a $k^{\rm th}$-order unfitted finite element method ($2\le k\le
4$) to solve the moving interface problem of the Oseen equations. Thorough
error estimates for the discrete solutions are presented by considering errors
from interface-tracking, time integration, and spatial discretization. In
literatures on time-dependent Stokes interface problems, error estimates for
the discrete pressure are usually sub-optimal, namely, $(k-1)^{\rm th}$-order,
under the $L^2$-norm. We have obtained a $(k-1)^{\rm th}$-order error estimate
for the discrete pressure under the $H^1$-norm. Numerical experiments for a
severely deforming interface show that optimal convergence orders are obtained
for $k = 3$ and $4$.
","[{'version': 'v1', 'created': 'Wed, 29 Dec 2021 23:40:29 GMT'}]",2022-01-03,['Numerical Analysis'],"This paper presents a high-order unfitted finite element method for solving moving interface problems. The method is based on a continuous Galerkin formulation with unfitted finite elements and a penalty approach for the interface coupling. The method is applied to two- and three-dimensional interface problems with a focus on the numerical analysis of the proposed approach. The numerical results demonstrate the accuracy and robustness of the proposed method and show that it is suitable for a wide range of problems. Moreover, the numerical results confirm the theoretical findings that the proposed method is of high-order accuracy and is capable of handling complex geometries.","Write an abstract for a paper called A high-order unfitted finite element method for moving interface
  problems about Numerical Analysis"
2301.13653,"Polina Kutsevol, Onur Ayan, Wolfgang Kellerer","Towards Semantic-Aware Transport Layer Protocols: A Control Performance
  Perspective","['eess.SY', 'cs.SY']","  Networked control systems (NCSs) are an example of task-oriented
communication systems, where the purpose of communication is real-time control
of processes over a network. In the context of NCSs, with the processes sending
their state measurements to the remote controllers, the deterioration of
control performance due to the network congestion can be partly mitigated by
shaping the traffic injected into the network at the transport layer (TL). In
this work, we conduct an extensive performance evaluation of selected TL
protocols and show that existing approaches from communication and control
theories fail to deliver sufficient control performance in realistic network
scenarios. Moreover, we propose a new semantic-aware TL policy, which uses the
process state information to filter the most relevant updates and the network
state information to prevent delays due to network congestion. The proposed
mechanism is shown to outperform all the considered TL protocols with respect
to control performance.
","[{'version': 'v1', 'created': 'Tue, 31 Jan 2023 14:09:26 GMT'}]",2023-02-01,['Systems and Control'],"This paper provides an overview of the current state of the art in semantic-aware transport layer protocols from a control performance perspective. It presents a comprehensive review of the existing protocols and their performance in terms of control performance, including throughput, latency, reliability, scalability, and robustness. It also provides a detailed analysis of the challenges and opportunities that arise when attempting to build a semantic-aware transport layer protocol. The paper further discusses the potential of machine learning techniques to improve the performance of such protocols, and provides a set of recommendations for future research in this area. Finally, it presents a comparison of the various approaches and their relative strengths and weaknesses.","Write an abstract for a paper called Towards Semantic-Aware Transport Layer Protocols: A Control Performance
  Perspective about Systems and Control"
2201.10276,"Jin Huang, Jantien Stoter, Ravi Peters, Liangliang Nan","City3D: Large-Scale Building Reconstruction from Airborne LiDAR Point
  Clouds","['cs.CV', 'cs.GR']","  We present a fully automatic approach for reconstructing compact 3D building
models from large-scale airborne point clouds. A major challenge of urban
reconstruction from airborne LiDAR point clouds lies in that the vertical walls
are typically missing. Based on the observation that urban buildings typically
consist of planar roofs connected with vertical walls to the ground, we propose
an approach to infer the vertical walls directly from the data. With the planar
segments of both roofs and walls, we hypothesize the faces of the building
surface, and the final model is obtained by using an extended
hypothesis-and-selection-based polygonal surface reconstruction framework.
Specifically, we introduce a new energy term to encourage roof preferences and
two additional hard constraints into the optimization step to ensure correct
topology and enhance detail recovery. Experiments on various large-scale
airborne LiDAR point clouds have demonstrated that the method is superior to
the state-of-the-art methods in terms of reconstruction accuracy and
robustness. In addition, we have generated a new dataset with our method
consisting of the point clouds and 3D models of 20k real-world buildings. We
believe this dataset can stimulate research in urban reconstruction from
airborne LiDAR point clouds and the use of 3D city models in urban
applications.
","[{'version': 'v1', 'created': 'Tue, 25 Jan 2022 12:41:11 GMT'}, {'version': 'v2', 'created': 'Thu, 9 Mar 2023 17:41:34 GMT'}]",2023-03-10,"['Computer Vision and Pattern Recognition', 'Graphics']","This paper presents City3D, a novel method for large-scale building reconstruction from airborne LiDAR point clouds using computer vision and pattern recognition. City3D is based on a novel deep learning architecture that combines deep convolutional neural networks with a fully connected network. Our method is able to accurately reconstruct 3D building models from large-scale LiDAR point clouds. We demonstrate the effectiveness of City3D on two real-world datasets, achieving state-of-the-art results for 3D building reconstruction. Furthermore, we show that City3D is robust to changes in data resolution and can be used to reconstruct buildings from LiDAR point clouds of different resolutions. Our results demonstrate the potential of City3D for large-scale 3D building reconstruction from airborne LiDAR point clouds.","Write an abstract for a paper called City3D: Large-Scale Building Reconstruction from Airborne LiDAR Point
  Clouds about Computer Vision and Pattern Recognition, Graphics"
2207.13944,"Luca Becchetti (DIAG), Arthur Carvalho Walraven da Cunha (COATI),
  Andrea Clementi, Francesco d'Amore (COATI), Hicham Lesfari (COATI), Emanuele
  Natale (COATI), Luca Trevisan",On the Multidimensional Random Subset Sum Problem,['cs.NE'],"  In the Random Subset Sum Problem, given $n$ i.i.d. random variables $X_1,
..., X_n$, we wish to approximate any point $z \in [-1,1]$ as the sum of a
suitable subset $X_{i_1(z)}, ..., X_{i_s(z)}$ of them, up to error
$\varepsilon$. Despite its simple statement, this problem is of fundamental
interest to both theoretical computer science and statistical mechanics. More
recently, it gained renewed attention for its implications in the theory of
Artificial Neural Networks. An obvious multidimensional generalisation of the
problem is to consider $n$ i.i.d. $d$-dimensional random vectors, with the
objective of approximating every point $\mathbf{z} \in [-1,1]^d$. In 1998, G.
S. Lueker showed that, in the one-dimensional setting, $n=\mathcal{O}(\log
\frac 1\varepsilon)$ samples guarantee the approximation property with high
probability.In this work, we prove that, in $d$ dimensions, $n =
\mathcal{O}(d^3\log \frac 1\varepsilon \cdot (\log \frac 1\varepsilon + \log
d))$ samples suffice for the approximation property to hold with high
probability. As an application highlighting the potential interest of this
result, we prove that a recently proposed neural network model exhibits
universality: with high probability, the model can approximate any neural
network within a polynomial overhead in the number of parameters.
","[{'version': 'v1', 'created': 'Thu, 28 Jul 2022 08:10:43 GMT'}, {'version': 'v2', 'created': 'Thu, 17 Nov 2022 15:49:11 GMT'}]",2022-11-18,['Neural and Evolutionary Computing'],"This paper presents a comprehensive overview of the multidimensional random subset sum problem (MRSP) and its potential applications in both neural and evolutionary computing. The paper first introduces the basic concepts of the MRSP, including the problem definition, the mathematical formulation, and the challenges associated with the problem. It then reviews the state-of-the-art methods for solving the MRSP, including both neural and evolutionary computing approaches. The paper then discusses the potential applications of the MRSP in various fields, such as machine learning and data mining. Finally, the paper concludes with a discussion of future research directions in the field. The paper provides a comprehensive overview of the MRSP and its potential applications in both neural and evolutionary computing, and is thus useful for researchers interested in this area.",Write an abstract for a paper called On the Multidimensional Random Subset Sum Problem about Neural and Evolutionary Computing
2012.00086,Federica Cecchetto and Vera Traub and Rico Zenklusen,"Bridging the Gap Between Tree and Connectivity Augmentation: Unified and
  Stronger Approaches","['cs.DS', 'math.OC']","  We consider the Connectivity Augmentation Problem (CAP), a classical problem
in the area of Survivable Network Design. It is about increasing the
edge-connectivity of a graph by one unit in the cheapest possible way. More
precisely, given a $k$-edge-connected graph $G=(V,E)$ and a set of extra edges,
the task is to find a minimum cardinality subset of extra edges whose addition
to $G$ makes the graph $(k+1)$-edge-connected. If $k$ is odd, the problem is
known to reduce to the Tree Augmentation Problem (TAP) -- i.e., $G$ is a
spanning tree -- for which significant progress has been achieved recently,
leading to approximation factors below $1.5$ (the currently best factor is
$1.458$). However, advances on TAP did not carry over to CAP so far. Indeed,
only very recently, Byrka, Grandoni, and Ameli (STOC 2020) managed to obtain
the first approximation factor below $2$ for CAP by presenting a
$1.91$-approximation algorithm based on a method that is disjoint from recent
advances for TAP.
  We first bridge the gap between TAP and CAP, by presenting techniques that
allow for leveraging insights and methods from TAP to approach CAP. We then
introduce a new way to get approximation factors below $1.5$, based on a new
analysis technique. Through these ingredients, we obtain a
$1.393$-approximation algorithm for CAP, and therefore also TAP. This leads to
the currently best approximation result for both problems in a unified way, by
significantly improving on the above-mentioned $1.91$-approximation for CAP and
also the previously best approximation factor of $1.458$ for TAP by Grandoni,
Kalaitzis, and Zenklusen (STOC 2018). Additionally, a feature we inherit from
recent TAP advances is that our approach can deal with the weighted setting
when the ratio between the largest to smallest cost on extra links is bounded,
in which case we obtain approximation factors below $1.5$.
","[{'version': 'v1', 'created': 'Mon, 30 Nov 2020 20:40:53 GMT'}, {'version': 'v2', 'created': 'Tue, 12 Apr 2022 10:56:49 GMT'}, {'version': 'v3', 'created': 'Wed, 23 Nov 2022 10:44:00 GMT'}]",2022-11-24,['Data Structures and Algorithms'],"This paper presents a unified approach to bridge the gap between tree and connectivity augmentation strategies for data structures and algorithms. We introduce a new approach that combines the advantages of both strategies, resulting in a stronger and more efficient approach to augment data structures and algorithms. We demonstrate the benefits of our approach through experiments on various data structures and algorithms, including graph algorithms, tree algorithms, and dynamic programming. Our results show that our combined approach outperforms both of the individual approaches, making it an attractive option for data structures and algorithms. We also discuss the complexity of our approach and propose a number of open problems for future research.","Write an abstract for a paper called Bridging the Gap Between Tree and Connectivity Augmentation: Unified and
  Stronger Approaches about Data Structures and Algorithms"
2210.0328,"Octavian A. Donca, Chayapol Beokhaimook, Ayonga Hereid",Real-Time Navigation for Bipedal Robots in Dynamic Environments,['cs.RO'],"  The popularity of mobile robots has been steadily growing, with these robots
being increasingly utilized to execute tasks previously completed by human
workers. For bipedal robots to see this same success, robust autonomous
navigation systems need to be developed that can execute in real-time and
respond to dynamic environments. These systems can be divided into three
stages: perception, planning, and control. A holistic navigation framework for
bipedal robots must successfully integrate all three components of the
autonomous navigation problem to enable robust real-world navigation. In this
paper, we present a real-time navigation framework for bipedal robots in
dynamic environments. The proposed system addresses all components of the
navigation problem: We introduce a depth-based perception system for obstacle
detection, mapping, and localization. A two-stage planner is developed to
generate collision-free trajectories robust to unknown and dynamic
environments. And execute trajectories on the Digit bipedal robot's walking
gait controller. The navigation framework is validated through a series of
simulation and hardware experiments that contain unknown environments and
dynamic obstacles.
","[{'version': 'v1', 'created': 'Fri, 7 Oct 2022 01:51:20 GMT'}]",2022-10-10,['Robotics'],"This paper presents a novel approach for real-time navigation of bipedal robots in dynamic environments. It considers the challenges of dynamic environments, such as obstacles, uneven terrain, and changing terrain, and proposes a solution that uses a combination of state-of-the-art path planning algorithms and machine learning techniques. The proposed method is tested in a simulated environment and the results show that it is capable of navigating a bipedal robot in dynamic environments with a high degree of success. The paper also discusses the implications of the proposed approach and provides suggestions for future work.",Write an abstract for a paper called Real-Time Navigation for Bipedal Robots in Dynamic Environments about Robotics
2209.08597,"Raul Tapia, Augusto G\'omez Egu\'iluz, Jos\'e Ramiro Mart\'inez-de
  Dios, Anibal Ollero","ASAP: Adaptive Scheme for Asynchronous Processing of Event-based Vision
  Algorithms","['cs.CV', 'cs.RO']","  Event cameras can capture pixel-level illumination changes with very high
temporal resolution and dynamic range. They have received increasing research
interest due to their robustness to lighting conditions and motion blur. Two
main approaches exist in the literature to feed the event-based processing
algorithms: packaging the triggered events in event packages and sending them
one-by-one as single events. These approaches suffer limitations from either
processing overflow or lack of responsivity. Processing overflow is caused by
high event generation rates when the algorithm cannot process all the events in
real-time. Conversely, lack of responsivity happens in cases of low event
generation rates when the event packages are sent at too low frequencies. This
paper presents ASAP, an adaptive scheme to manage the event stream through
variable-size packages that accommodate to the event package processing times.
The experimental results show that ASAP is capable of feeding an asynchronous
event-by-event clustering algorithm in a responsive and efficient manner and at
the same time prevents overflow.
","[{'version': 'v1', 'created': 'Sun, 18 Sep 2022 16:28:29 GMT'}]",2022-09-20,"['Computer Vision and Pattern Recognition', 'Robotics']","This paper presents ASAP, an Adaptive Scheme for Asynchronous Processing of Event-based Vision Algorithms. ASAP is a novel approach to computer vision and pattern recognition that is tailored to the needs of robotics applications. It is designed to process asynchronous event-based data, such as those generated by a robotic system, in a manner that is efficient and adaptive. The paper describes the architecture of ASAP, its implementation, and performance evaluation results. The results demonstrate that ASAP is capable of providing real-time performance in a variety of robotic tasks. The paper also discusses potential applications of ASAP and future work that can be done to further improve its performance.","Write an abstract for a paper called ASAP: Adaptive Scheme for Asynchronous Processing of Event-based Vision
  Algorithms about Computer Vision and Pattern Recognition, Robotics"
2303.13767,"Yunfan Lu, Zipeng Wang, Minjie Liu, Hongjian Wang, Lin Wang","Learning Spatial-Temporal Implicit Neural Representations for
  Event-Guided Video Super-Resolution","['cs.CV', 'cs.AI']","  Event cameras sense the intensity changes asynchronously and produce event
streams with high dynamic range and low latency. This has inspired research
endeavors utilizing events to guide the challenging video superresolution (VSR)
task. In this paper, we make the first attempt to address a novel problem of
achieving VSR at random scales by taking advantages of the high temporal
resolution property of events. This is hampered by the difficulties of
representing the spatial-temporal information of events when guiding VSR. To
this end, we propose a novel framework that incorporates the spatial-temporal
interpolation of events to VSR in a unified framework. Our key idea is to learn
implicit neural representations from queried spatial-temporal coordinates and
features from both RGB frames and events. Our method contains three parts.
Specifically, the Spatial-Temporal Fusion (STF) module first learns the 3D
features from events and RGB frames. Then, the Temporal Filter (TF) module
unlocks more explicit motion information from the events near the queried
timestamp and generates the 2D features. Lastly, the SpatialTemporal Implicit
Representation (STIR) module recovers the SR frame in arbitrary resolutions
from the outputs of these two modules. In addition, we collect a real-world
dataset with spatially aligned events and RGB frames. Extensive experiments
show that our method significantly surpasses the prior-arts and achieves VSR
with random scales, e.g., 6.5. Code and dataset are available at https:
//vlis2022.github.io/cvpr23/egvsr.
","[{'version': 'v1', 'created': 'Fri, 24 Mar 2023 02:42:16 GMT'}, {'version': 'v2', 'created': 'Wed, 29 Mar 2023 01:59:37 GMT'}]",2023-03-30,"['Computer Vision and Pattern Recognition', 'Artificial Intelligence']","This paper presents a novel approach to video super-resolution using implicit neural representations of spatial-temporal information. We propose a deep learning-based model which is capable of learning representations of spatial-temporal information from videos and using them to generate high-resolution versions of the input video. The model is trained using event-guided video super-resolution, which is a new approach to video super-resolution that combines the advantages of traditional super-resolution methods and event-based vision. We evaluate the proposed model on several datasets and demonstrate improved results compared to traditional super-resolution methods. The results show that our model is able to learn useful representations of spatial-temporal information and is capable of producing high-quality super-resolution videos. Our model is a promising step towards making video super-resolution more efficient and effective.","Write an abstract for a paper called Learning Spatial-Temporal Implicit Neural Representations for
  Event-Guided Video Super-Resolution about Computer Vision and Pattern Recognition, Artificial Intelligence"
2104.04901,"Feiran Zhao, Keyou You, Tamer Ba\c{s}ar","Global Convergence of Policy Gradient Primal-dual Methods for
  Risk-constrained LQRs","['math.OC', 'cs.SY', 'eess.SY']","  While the techniques in optimal control theory are often model-based, the
policy optimization (PO) approach directly optimizes the performance metric of
interest. Even though it has been an essential approach for reinforcement
learning problems, there is little theoretical understanding on its
performance. In this paper, we focus on the risk-constrained linear quadratic
regulator (RC-LQR) problem via the PO approach, which requires addressing a
challenging non-convex constrained optimization problem. To solve it, we first
build on our earlier result that an optimal policy has a time-invariant affine
structure to show that the associated Lagrangian function is coercive, locally
gradient dominated and has local Lipschitz continuous gradient, based on which
we establish strong duality. Then, we design policy gradient primal-dual
methods with global convergence guarantees in both model-based and sample-based
settings. Finally, we use samples of system trajectories in simulations to
validate our methods.
","[{'version': 'v1', 'created': 'Sun, 11 Apr 2021 02:52:43 GMT'}, {'version': 'v2', 'created': 'Tue, 22 Nov 2022 03:13:11 GMT'}]",2022-11-23,['Systems and Control'],"This paper presents a novel approach to global convergence of policy gradient primal-dual methods for risk-constrained linear quadratic regulation (LQR) problems in systems and control. We propose a new stochastic gradient descent (SGD) based approach that combines a primal policy gradient algorithm with a dual policy gradient algorithm. We prove the global convergence of the proposed method under mild assumptions and demonstrate the effectiveness of the approach through numerical experiments. The proposed method is applicable to a wide range of risk-constrained LQR problems, and can be used to solve real-world control problems with fast convergence and high accuracy.","Write an abstract for a paper called Global Convergence of Policy Gradient Primal-dual Methods for
  Risk-constrained LQRs about Systems and Control"
2303.07523,"Pubudu Wijesooriya and Sheikh Muhammad Farjad and Nikolaos Stergiou
  and Spyridon Mastorakis","Investigating the Characteristics and Performance of Augmented Reality
  Applications on Head-Mounted Displays: A Study of the Hololens Application
  Store","['cs.MM', 'cs.PF', 'cs.SE']","  Augmented Reality (AR) based on Head-Mounted Displays (HMDs) has gained
significant traction over the recent years. Nevertheless, it remains unclear
what AR HMD-based applications have been developed over the years and what
their system performance is when they are run on HMDs. In this paper, we aim to
shed light into this direction. Our study focuses on the applications available
on the Microsoft Hololens application store given the wide use of the Hololens
headset. Our study has two major parts: (i) we collect metadata about the
applications available on the Microsoft Hololens application store to
understand their characteristics (e.g., categories, pricing, permissions
requested, hardware and software compatibility); and (ii) we interact with
these applications while running on a Hololens 2 headset and collect data about
systems-related metrics (e.g., memory and storage usage, time spent on CPU and
GPU related operations) to investigate the systems performance of applications.
Our study has resulted in several interesting findings, which we share with the
research community.
","[{'version': 'v1', 'created': 'Mon, 13 Mar 2023 23:18:17 GMT'}]",2023-03-15,"['Multimedia', 'Performance', 'Software Engineering']","This paper investigates the characteristics and performance of augmented reality applications on head-mounted displays (HMDs), focusing on the Microsoft Hololens Application Store. Specifically, we explore the multimedia, performance, and software engineering aspects of Hololens applications. We use a combination of qualitative and quantitative methods to analyze the applications, including a survey of developers and users, a review of application documentation, and an analysis of application source code. Results indicate that multimedia and performance are key aspects of Hololens applications, and that developers are increasingly taking advantage of software engineering principles to create better experiences. We discuss the implications of our findings for the design and development of augmented reality applications.","Write an abstract for a paper called Investigating the Characteristics and Performance of Augmented Reality
  Applications on Head-Mounted Displays: A Study of the Hololens Application
  Store about Multimedia, Performance, Software Engineering"
2211.1153,"Zhongxiang Zhou, Yifei Yang, Yue Wang, Rong Xiong","Open-Set Object Detection Using Classification-free Object Proposal and
  Instance-level Contrastive Learning with Appendix","['cs.CV', 'cs.RO']","  Detecting both known and unknown objects is a fundamental skill for robot
manipulation in unstructured environments. Open-set object detection (OSOD) is
a promising direction to handle the problem consisting of two subtasks: objects
and background separation, and open-set object classification. In this paper,
we present Openset RCNN to address the challenging OSOD. To disambiguate
unknown objects and background in the first subtask, we propose to use
classification-free region proposal network (CF-RPN) which estimates the
objectness score of each region purely using cues from object's location and
shape preventing overfitting to the training categories. To identify unknown
objects in the second subtask, we propose to represent them using the
complementary region of known categories in a latent space which is
accomplished by a prototype learning network (PLN). PLN performs instance-level
contrastive learning to encode proposals to a latent space and builds a compact
region centering with a prototype for each known category. Further, we note
that the detection performance of unknown objects can not be unbiasedly
evaluated on the situation that commonly used object detection datasets are not
fully annotated. Thus, a new benchmark is introduced by reorganizing
GraspNet-1billion, a robotic grasp pose detection dataset with complete
annotation. Extensive experiments demonstrate the merits of our method. We
finally show that our Openset RCNN can endow the robot with an open-set
perception ability to support robotic rearrangement tasks in cluttered
environments. More details can be found in
https://sites.google.com/view/openest-rcnn/
","[{'version': 'v1', 'created': 'Mon, 21 Nov 2022 15:00:04 GMT'}]",2022-11-22,"['Computer Vision and Pattern Recognition', 'Robotics']","This paper presents a novel open-set object detection approach that combines classification-free object proposal and instance-level contrastive learning. The proposed approach is evaluated on several standard datasets and the results show that it outperforms existing state-of-the-art methods in terms of accuracy and robustness. Furthermore, an appendix detailing the application of open-set object detection in computer vision and pattern recognition, as well as robotics, is included. The appendix provides comprehensive coverage of both theoretical and practical aspects of the problem.","Write an abstract for a paper called Open-Set Object Detection Using Classification-free Object Proposal and
  Instance-level Contrastive Learning with Appendix about Computer Vision and Pattern Recognition, Robotics"
2201.10544,"Charlie Kirkwood, Theo Economou, Henry Odbert and Nicolas Pugeault","A deep mixture density network for outlier-corrected interpolation of
  crowd-sourced weather data","['stat.ML', 'cs.AI', 'cs.LG', 'stat.AP']","  As the costs of sensors and associated IT infrastructure decreases - as
exemplified by the Internet of Things - increasing volumes of observational
data are becoming available for use by environmental scientists. However, as
the number of available observation sites increases, so too does the
opportunity for data quality issues to emerge, particularly given that many of
these sensors do not have the benefit of official maintenance teams. To realise
the value of crowd sourced 'Internet of Things' type observations for
environmental modelling, we require approaches that can automate the detection
of outliers during the data modelling process so that they do not contaminate
the true distribution of the phenomena of interest. To this end, here we
present a Bayesian deep learning approach for spatio-temporal modelling of
environmental variables with automatic outlier detection. Our approach
implements a Gaussian-uniform mixture density network whose dual purposes -
modelling the phenomenon of interest, and learning to classify and ignore
outliers - are achieved simultaneously, each by specifically designed branches
of our neural network. For our example application, we use the Met Office's
Weather Observation Website data, an archive of observations from around 1900
privately run and unofficial weather stations across the British Isles. Using
data on surface air temperature, we demonstrate how our deep mixture model
approach enables the modelling of a highly skilled spatio-temporal temperature
distribution without contamination from spurious observations. We hope that
adoption of our approach will help unlock the potential of incorporating a
wider range of observation sources, including from crowd sourcing, into future
environmental models.
","[{'version': 'v1', 'created': 'Tue, 25 Jan 2022 18:54:59 GMT'}]",2022-01-26,"['Artificial Intelligence', 'Machine Learning']",This paper presents a deep mixture density network (MDN) for outlier-corrected interpolation of crowd-sourced weather data. The proposed MDN utilizes a deep neural network architecture to learn a non-linear mapping from the input data to the expected output. The MDN is trained using a combination of supervised learning and unsupervised learning techniques to learn the underlying distribution of the data. The MDN is then used to interpolate missing data points in the crowd-sourced weather data. The proposed method is evaluated on a real-world dataset and shows promising results in terms of accuracy and robustness to outliers. The results demonstrate that the proposed MDN is a promising approach for outlier-corrected interpolation of crowd-sourced weather data in the context of Artificial Intelligence and Machine Learning.,"Write an abstract for a paper called A deep mixture density network for outlier-corrected interpolation of
  crowd-sourced weather data about Artificial Intelligence, Machine Learning"
2301.11145,"Elena Camuffo, Umberto Michieli, Simone Milani","Learning from Mistakes: Self-Regularizing Hierarchical Semantic
  Representations in Point Cloud Segmentation","['cs.CV', 'cs.MM', 'stat.ML']","  Recent advances in autonomous robotic technologies have highlighted the
growing need for precise environmental analysis. LiDAR semantic segmentation
has gained attention to accomplish fine-grained scene understanding by acting
directly on raw content provided by sensors. Recent solutions showed how
different learning techniques can be used to improve the performance of the
model, without any architectural or dataset change. Following this trend, we
present a coarse-to-fine setup that LEArns from classification mistaKes (LEAK)
derived from a standard model. First, classes are clustered into macro groups
according to mutual prediction errors; then, the learning process is
regularized by: (1) aligning class-conditional prototypical feature
representation for both fine and coarse classes, (2) weighting instances with a
per-class fairness index. Our LEAK approach is very general and can be
seamlessly applied on top of any segmentation architecture; indeed,
experimental results showed that it enables state-of-the-art performances on
different architectures, datasets and tasks, while ensuring more balanced
class-wise results and faster convergence.
","[{'version': 'v1', 'created': 'Thu, 26 Jan 2023 14:52:30 GMT'}]",2023-01-27,"['Computer Vision and Pattern Recognition', 'Multimedia']","This paper presents a novel approach to point cloud segmentation that utilizes self-regularizing hierarchical semantic representations. We introduce a method for learning from mistakes, where the model is able to recognize and correct its own errors. Our approach is based on a hierarchical semantic representation that incorporates contextual information from the surrounding environment. We evaluate our method on a range of standard datasets, showing an improvement in performance over other state-of-the-art methods. Our results demonstrate that our model is able to learn from its mistakes, resulting in improved segmentation performance. The proposed method can be applied to a variety of computer vision and multimedia tasks, providing a powerful tool for segmentation.","Write an abstract for a paper called Learning from Mistakes: Self-Regularizing Hierarchical Semantic
  Representations in Point Cloud Segmentation about Computer Vision and Pattern Recognition, Multimedia"
2205.14913,"Aijaz H. Lone, Selma Amara, Fernando Aguirre, Mario Lanza and H.
  Fariborzi","Skyrmion-based Leaky Integrate and Fire Neurons for Neuromorphic
  Applications","['cond-mat.mtrl-sci', 'cs.ET']","  Spintronics is an important emerging technology for data storage and
computation. In this field, magnetic skyrmion-based devices are attractive due
to their small size and energy consumption. However, controlling the creation,
deletion and motion of skyrmions is challenging. Here we propose a novel
energy-efficient skyrmion-based device structure, and demonstrate its use as
leaky integrate (LIF) and fire neuron for neuromorphic computing. Here we show
that skyrmions can be confined by patterning the geometry of the free layer in
a magnetic tunnel junction (MTJ), and demonstrate that the size of the skyrmion
can be adjusted by applying pulsed voltage stresses. A spiking neural network
(SNN) made of such skyrmion-based LIF neurons shows the capability of
classifying images from the Modified National Institute of Standards and
Technology (MNIST) dataset.
","[{'version': 'v1', 'created': 'Mon, 30 May 2022 08:26:34 GMT'}]",2022-05-31,['Emerging Technologies'],"This paper presents a novel approach to building leaky integrate-and-fire (LIF) neurons for neuromorphic applications based on skyrmions. Skyrmions are localized magnetic structures that have been proposed as an efficient and reliable way to store and process data in emerging technologies. We demonstrate the effectiveness of skyrmion-based LIF neurons by simulating their behavior in a neuromorphic architecture. We show that skyrmion-based LIF neurons can outperform traditional LIF neurons in terms of energy efficiency and speed, while still preserving the same level of accuracy. We also discuss the potential applications of skyrmion-based LIF neurons in neuromorphic computing and discuss future work in this field.","Write an abstract for a paper called Skyrmion-based Leaky Integrate and Fire Neurons for Neuromorphic
  Applications about Emerging Technologies"
2010.06797,"Mingyu Cai, Shaoping Xiao, Baoluo Li, Zhiliang Li and Zhen Kan","Reinforcement Learning Based Temporal Logic Control with Maximum
  Probabilistic Satisfaction","['cs.FL', 'cs.AI', 'cs.RO', 'math.OC']","  This paper presents a model-free reinforcement learning (RL) algorithm to
synthesize a control policy that maximizes the satisfaction probability of
linear temporal logic (LTL) specifications. Due to the consideration of
environment and motion uncertainties, we model the robot motion as a
probabilistic labeled Markov decision process with unknown transition
probabilities and unknown probabilistic label functions. The LTL task
specification is converted to a limit deterministic generalized B\""uchi
automaton (LDGBA) with several accepting sets to maintain dense rewards during
learning. The novelty of applying LDGBA is to construct an embedded LDGBA
(E-LDGBA) by designing a synchronous tracking-frontier function, which enables
the record of non-visited accepting sets without increasing dimensional and
computational complexity. With appropriate dependent reward and discount
functions, rigorous analysis shows that any method that optimizes the expected
discount return of the RL-based approach is guaranteed to find the optimal
policy that maximizes the satisfaction probability of the LTL specifications. A
model-free RL-based motion planning strategy is developed to generate the
optimal policy in this paper. The effectiveness of the RL-based control
synthesis is demonstrated via simulation and experimental results.
","[{'version': 'v1', 'created': 'Wed, 14 Oct 2020 03:49:16 GMT'}, {'version': 'v2', 'created': 'Thu, 15 Oct 2020 04:45:16 GMT'}, {'version': 'v3', 'created': 'Fri, 26 Mar 2021 20:51:11 GMT'}, {'version': 'v4', 'created': 'Thu, 22 Jul 2021 01:36:30 GMT'}, {'version': 'v5', 'created': 'Tue, 5 Oct 2021 16:01:44 GMT'}]",2022-01-04,"['Formal Languages and Automata Theory', 'Artificial Intelligence', 'Robotics']","This paper presents a novel approach to temporal logic control using reinforcement learning (RL) and maximum probabilistic satisfaction. It discusses the application of RL to the formal language and automata theory, artificial intelligence, and robotics. The proposed approach uses RL to learn a policy that maximizes the probability of satisfying a given temporal logic formula. The paper also examines the effectiveness of the proposed approach on a set of temporal logic control tasks. The results show that the proposed approach outperforms existing methods in terms of both accuracy and execution time. Furthermore, the paper provides insights into how RL can be used to further improve temporal logic control. Finally, the paper discusses the potential applications of RL-based temporal logic control in robotics and other related fields.","Write an abstract for a paper called Reinforcement Learning Based Temporal Logic Control with Maximum
  Probabilistic Satisfaction about Formal Languages and Automata Theory, Artificial Intelligence, Robotics"
2202.0143,"Hyung Jun Choi, Woocheol Choi, Youngwoo Koh","Convergence analysis of the splitting method to the nonlinear heat
  equation","['math.NA', 'cs.NA', 'math.AP']","  In this paper, we analyze an operator splitting scheme of the nonlinear heat
equation in $\Omega\subset\mathbb{R}^d$ ($d\geq 1$): $\partial_t u = \Delta u +
\lambda |u|^{p-1} u$ in $\Omega\times(0,\infty)$, $u=0$ in
$\partial\Omega\times(0,\infty)$, $u ({\bf x},0) =\phi ({\bf x})$ in $\Omega$.
where $\lambda\in\{-1,1\}$ and $\phi \in W^{1,q}(\Omega)\cap L^{\infty}
(\Omega)$ with $2\leq p < \infty$ and $d(p-1)/2<q<\infty$. We establish the
well-posedness of the approximation of $u$ in $L^r$-space ($r\geq q$), and
furthermore, we derive its convergence rate of order $\mathcal{O}(\tau)$ for a
time step $\tau>0$. Finally, we give some numerical examples to confirm the
reliability of the analyzed result.
","[{'version': 'v1', 'created': 'Thu, 3 Feb 2022 06:19:18 GMT'}, {'version': 'v2', 'created': 'Thu, 26 Jan 2023 05:59:27 GMT'}]",2023-01-27,['Numerical Analysis'],"This paper presents a convergence analysis of the splitting method for the nonlinear heat equation. The splitting method is a numerical technique for solving nonlinear partial differential equations. In this paper, the convergence of the splitting method is studied for the nonlinear heat equation. The numerical results are compared with the exact solution, and the order of convergence of the splitting method is determined. The results obtained in this paper provide a better understanding of the numerical behavior of the splitting method for the nonlinear heat equation, and can be used to guide the choice of numerical parameters for future applications.","Write an abstract for a paper called Convergence analysis of the splitting method to the nonlinear heat
  equation about Numerical Analysis"
2201.05271,"Zhi Ji, Wendong Yang, Xinrong Guan, Xiao Zhao, Guoxin Li, and Qingqing
  Wu","Trajectory and Transmit Power Optimization for IRS-Assisted UAV
  Communication under Malicious Jamming","['eess.SP', 'cs.IT', 'math.IT']","  In this letter, we investigate an unmanned aerial vehicle (UAV) communication
system, where an intelligent reflecting surface (IRS) is deployed to assist in
the transmission from a ground node (GN) to the UAV in the presence of a
jammer. We aim to maximize the average rate of the UAV communication by jointly
optimizing the GN's transmit power, the IRS's passive beamforming and the UAV's
trajectory. However, the formulated problem is difficult to solve due to the
non-convex objective function and the coupled optimization variables. Thus, to
tackle it, we propose an alternating optimization (AO) based algorithm by
exploiting the successive convex approximation (SCA) and semidefinite
relaxation (SDR) techniques. Simulation results show that the proposed
algorithm can significantly improve the average rate compared with the
benchmark algorithms. Moreover, it also shows that when the jamming power is
large and the number of IRS elements is relatively small, deploying the IRS
near the jammer outperforms deploying it near the GN, and vice versa.
","[{'version': 'v1', 'created': 'Fri, 14 Jan 2022 01:35:47 GMT'}]",2022-01-17,['Information Theory'],"This paper addresses the problem of trajectory and transmit power optimization for unmanned aerial vehicle (UAV) communication assisted by an intelligent reflecting surface (IRS) under malicious jamming. The paper formulates the problem as a mixed-integer nonlinear programming problem and proposes a solution based on an information-theoretic approach. Specifically, the paper proposes a trajectory optimization algorithm based on the channel capacity maximization problem and a transmit power optimization algorithm based on the minimum total transmit power problem. The simulation results demonstrate that the proposed algorithms can significantly improve the communication performance of UAVs in the presence of malicious jamming.","Write an abstract for a paper called Trajectory and Transmit Power Optimization for IRS-Assisted UAV
  Communication under Malicious Jamming about Information Theory"
2204.06136,"Sven Br\""uggemann, Drew Steeves, Miroslav Krstic","Simultaneous Lane-Keeping and Obstacle Avoidance by Combining Model
  Predictive Control and Control Barrier Functions","['eess.SY', 'cs.SY']","  In this work, we combine {Model Predictive Control} (MPC) and Control Barrier
Function (CBF) design {methods} to create a hierarchical control law for
simultaneous lane-keeping (LK) and obstacle avoidance (OA): at the low level,
MPC performs LK via trajectory tracking during nominal operation; and at the
high level, different CBF-based safety filters that ensure both LK and OA are
designed and compared across some practical scenarios. In particular, we show
that Exponential Safety (ESf) and Prescribed-Time Safety (PTSf) filters, which
override the MPC control when necessary, result in feasible Quadratic Programs
when safety is prioritized appropriately. We additionally investigate control
designs subject to input constraints by using Input-Constrained-CBFs. Finally,
we compare the performance of combinations of ESf, PTSf, and their
input-constrained counterparts with respect to the LK and OA goals in two
simulation studies for early- and late-detected obstacle scenarios.
","[{'version': 'v1', 'created': 'Wed, 13 Apr 2022 02:01:17 GMT'}]",2022-04-14,['Systems and Control'],"This paper presents a novel approach to simultaneously achieve lane-keeping and obstacle avoidance in autonomous vehicles. The proposed approach combines Model Predictive Control (MPC) and Control Barrier Functions (CBFs) to ensure safety of the vehicle while optimizing the trajectory. The proposed controller is evaluated in a simulated environment, and the results show that the proposed approach can successfully achieve lane-keeping and obstacle avoidance simultaneously. The proposed approach is also compared with a conventional MPC-based controller, and the results demonstrate the improved performance of the proposed controller. This paper provides a significant contribution to the field of Systems and Control by introducing a novel approach to simultaneously achieve lane-keeping and obstacle avoidance.","Write an abstract for a paper called Simultaneous Lane-Keeping and Obstacle Avoidance by Combining Model
  Predictive Control and Control Barrier Functions about Systems and Control"
2203.09436,"Xufeng Cai, Chaobing Song, Crist\'obal Guzm\'an, Jelena Diakonikolas","Stochastic Halpern Iteration with Variance Reduction for Stochastic
  Monotone Inclusions","['math.OC', 'cs.LG']","  We study stochastic monotone inclusion problems, which widely appear in
machine learning applications, including robust regression and adversarial
learning. We propose novel variants of stochastic Halpern iteration with
recursive variance reduction. In the cocoercive -- and more generally
Lipschitz-monotone -- setup, our algorithm attains $\epsilon$ norm of the
operator with $\mathcal{O}(\frac{1}{\epsilon^3})$ stochastic operator
evaluations, which significantly improves over state of the art
$\mathcal{O}(\frac{1}{\epsilon^4})$ stochastic operator evaluations required
for existing monotone inclusion solvers applied to the same problem classes. We
further show how to couple one of the proposed variants of stochastic Halpern
iteration with a scheduled restart scheme to solve stochastic monotone
inclusion problems with ${\mathcal{O}}(\frac{\log(1/\epsilon)}{\epsilon^2})$
stochastic operator evaluations under additional sharpness or strong
monotonicity assumptions.
","[{'version': 'v1', 'created': 'Thu, 17 Mar 2022 16:48:57 GMT'}, {'version': 'v2', 'created': 'Mon, 21 Mar 2022 14:39:23 GMT'}, {'version': 'v3', 'created': 'Fri, 4 Nov 2022 21:40:05 GMT'}, {'version': 'v4', 'created': 'Sun, 8 Jan 2023 21:32:52 GMT'}]",2023-01-10,['Machine Learning'],This paper investigates the application of Stochastic Halpern Iteration (SHI) with Variance Reduction (VR) for solving stochastic monotone inclusions in the context of machine learning. We propose a SHI-VR algorithm which combines the Halpern iteration and the variance reduction technique to reduce the variance of the stochastic approximation of the solution. We prove the convergence of the proposed algorithm and analyze its computational complexity. We also demonstrate the effectiveness of the proposed method through numerical experiments. The results show that the proposed SHI-VR algorithm can significantly reduce the variance of the stochastic approximation with a moderate increase in the computational cost.,"Write an abstract for a paper called Stochastic Halpern Iteration with Variance Reduction for Stochastic
  Monotone Inclusions about Machine Learning"
2301.09759,"Yamen Ajjour, Johannes Kiesel, Benno Stein, and Martin Potthast",Topic Ontologies for Arguments,['cs.CL'],"  Many computational argumentation tasks, like stance classification, are
topic-dependent: the effectiveness of approaches to these tasks significantly
depends on whether the approaches were trained on arguments from the same
topics as those they are tested on. So, which are these topics that researchers
train approaches on? This paper contributes the first comprehensive survey of
topic coverage, assessing 45 argument corpora. For the assessment, we take the
first step towards building an argument topic ontology, consulting three
diverse authoritative sources: the World Economic Forum, the Wikipedia list of
controversial topics, and Debatepedia. Comparing the topic sets between the
authoritative sources and corpora, our analysis shows that the corpora
topics-which are mostly those frequently discussed in public online fora - are
covered well by the sources. However, other topics from the sources are less
extensively covered by the corpora of today, revealing interesting future
directions for corpus construction.
","[{'version': 'v1', 'created': 'Mon, 23 Jan 2023 23:43:24 GMT'}]",2023-01-25,['Computation and Language'],"This paper examines the potential of topic ontologies for arguments about computation and language. Ontologies are a valuable tool for organizing and classifying data, and can be used to create a structure for understanding the relationships between concepts. In this paper, we discuss the application of topic ontologies to arguments related to computation and language, and how they can be used to identify and analyze the relationships between different topics. We analyze the strengths and weaknesses of topic ontologies, and discuss how they can be used to identify and develop new arguments. Finally, we discuss how topic ontologies can be used to improve the quality of argumentation in the fields of computation and language.",Write an abstract for a paper called Topic Ontologies for Arguments about Computation and Language
2304.01772,"Tiago de Souza Farias, Vitor Vaz Schultz, Jos\'e Carlos Merino
  Mombach, Jonas Maziero",A differentiable programming framework for spin models,"['cond-mat.stat-mech', 'cs.LG']","  Spin systems are a powerful tool for modeling a wide range of physical
systems. In this paper, we propose a novel framework for modeling spin systems
using differentiable programming. Our approach enables us to efficiently
simulate spin systems, making it possible to model complex systems at scale.
Specifically, we demonstrate the effectiveness of our technique by applying it
to three different spin systems: the Ising model, the Potts model, and the
Cellular Potts model. Our simulations show that our framework offers
significant speedup compared to traditional simulation methods, thanks to its
ability to execute code efficiently across different hardware architectures,
including Graphical Processing Units and Tensor Processing Units.
","[{'version': 'v1', 'created': 'Tue, 4 Apr 2023 13:04:21 GMT'}]",2023-04-05,['Machine Learning'],"This paper presents a novel differentiable programming framework for spin models in the field of Machine Learning. The framework is based on the idea of using a combination of spin models and differentiable programming, which allows for the optimization of spin models using gradient-based methods. The proposed framework is evaluated on a range of spin models, including Ising and Potts models, and is shown to achieve significant performance improvements over existing methods. Furthermore, the proposed framework is shown to be more computationally efficient than existing methods, making it suitable for large-scale applications. Finally, the paper discusses the potential applications of the proposed framework in areas such as quantum computing, reinforcement learning, and image recognition.",Write an abstract for a paper called A differentiable programming framework for spin models about Machine Learning
2304.03126,"Yi Guo, Nan Cao, Ligan Cai, Yanqiu Wu, Daniel Weiskopf, Danqing Shi
  and Qing Chen","Datamator: An Intelligent Authoring Tool for Creating Datamations via
  Data Query Decomposition",['cs.HC'],"  Datamation is designed to animate an analysis pipeline step by step, which is
an intuitive and effective way to interpret the results from data analysis.
However, creating a datamation is not easy. A qualified datamation needs to not
only provide a correct analysis result but also ensure that the data flow and
animation are coherent. Existing animation authoring tools focus on either
leveraging algorithms to automatically generate an animation based on
user-provided charts or building graphical user interfaces to provide a
programming-free authoring environment for users. None of them are able to help
users translate an analysis task into a series of data operations to form an
analysis pipeline and visualize them as a datamation. To fill this gap, we
introduce Datamator, an intelligent authoring tool developed to support
datamation design and generation. It leverages a novel data query decomposition
model to allow users to generate an initial datamation by simply inputting a
data query in natural language. The initial datamation can be refined via rich
interactions and a feedback mechanism is utilized to update the decomposition
model based on user knowledge and preferences. Our system produces an animated
sequence of visualizations driven by a set of low-level data actions. It
supports unit visualizations, which provide a mapping from each data item to a
unique visual mark. We demonstrate the effectiveness of Datamator via a series
of evaluations including case studies, performance validation, and a controlled
user study.
","[{'version': 'v1', 'created': 'Thu, 6 Apr 2023 14:52:42 GMT'}, {'version': 'v2', 'created': 'Sun, 9 Apr 2023 13:33:41 GMT'}, {'version': 'v3', 'created': 'Wed, 12 Apr 2023 09:05:52 GMT'}]",2023-04-13,['Human-Computer Interaction'],"This paper presents Datamator, an intelligent authoring tool for creating datamations via data query decomposition. Datamator is designed to allow users to quickly and easily create highly interactive and visual data-driven applications. The paper discusses the design of the system, its features, and the implications of the system for human-computer interaction. In particular, the paper focuses on how the system enables users to quickly and easily create interactive visualizations of their data, and how it can be used to facilitate more effective communication and collaboration between users. The paper also discusses how Datamator can be used to facilitate better understanding of complex data sets, and how it can be used to create more effective user interfaces. Finally, the paper discusses the implications of the system for the future of human-computer interaction.","Write an abstract for a paper called Datamator: An Intelligent Authoring Tool for Creating Datamations via
  Data Query Decomposition about Human-Computer Interaction"
2302.13001,"Daiqing Qi, Handong Zhao, Sheng Li",Better Generative Replay for Continual Federated Learning,"['cs.LG', 'cs.AI']","  Federated learning is a technique that enables a centralized server to learn
from distributed clients via communications without accessing the client local
data. However, existing federated learning works mainly focus on a single task
scenario with static data. In this paper, we introduce the problem of continual
federated learning, where clients incrementally learn new tasks and history
data cannot be stored due to certain reasons, such as limited storage and data
retention policy. Generative replay based methods are effective for continual
learning without storing history data, but adapting them for this setting is
challenging. By analyzing the behaviors of clients during training, we find
that the unstable training process caused by distributed training on non-IID
data leads to a notable performance degradation. To address this problem, we
propose our FedCIL model with two simple but effective solutions: model
consolidation and consistency enforcement. Our experimental results on multiple
benchmark datasets demonstrate that our method significantly outperforms
baselines.
","[{'version': 'v1', 'created': 'Sat, 25 Feb 2023 06:26:56 GMT'}]",2023-02-28,"['Machine Learning', 'Artificial Intelligence']",This paper explores how to improve generative replay for continual federated learning in machine learning and artificial intelligence. Generative replay is a technique used to improve the performance of federated learning by replaying generated data from previously seen tasks. Recent advances in generative replay have been limited due to the difficulty of generating data that is similar to the original data distribution. This paper proposes a new approach to generative replay that uses a generative adversarial network to generate data that is more similar to the original data distribution. We demonstrate the effectiveness of our approach on a variety of tasks and datasets and show that our approach significantly improves the performance of federated learning. Our results show that our approach can be used to improve the performance of federated learning in machine learning and artificial intelligence.,"Write an abstract for a paper called Better Generative Replay for Continual Federated Learning about Machine Learning, Artificial Intelligence"
2205.01071,Corinna Coupette and Dirk Hartung,"Sharing and Caring: Creating a Culture of Constructive Criticism in
  Computational Legal Studies","['cs.CY', 'cs.CL']","  We introduce seven foundational principles for creating a culture of
constructive criticism in computational legal studies. Beginning by challenging
the current perception of papers as the primary scholarly output, we call for a
more comprehensive interpretation of publications. We then suggest to make
these publications computationally reproducible, releasing all of the data and
all of the code all of the time, on time, and in the most functioning form
possible. Subsequently, we invite constructive criticism in all phases of the
publication life cycle. We posit that our proposals will help form our field,
and float the idea of marking this maturity by the creation of a modern
flagship publication outlet for computational legal studies.
","[{'version': 'v1', 'created': 'Tue, 19 Apr 2022 08:27:01 GMT'}]",2022-05-03,"['Computers and Society', 'Computation and Language']","This paper examines how to create a culture of constructive criticism in Computational Legal Studies (CLS) through the use of sharing and caring. It begins by discussing the need for constructive criticism in CLS, and how it can help to create a more productive and positive learning environment. The paper then looks at how sharing and caring can be used to foster constructive criticism, and how this can be implemented in the classroom. Finally, the paper explores the role of computers and society, computation and language in creating a culture of constructive criticism in CLS. The paper concludes by suggesting that sharing and caring can be used to create a culture of constructive criticism in CLS, which can help to create a more productive and positive learning environment.","Write an abstract for a paper called Sharing and Caring: Creating a Culture of Constructive Criticism in
  Computational Legal Studies about Computers and Society, Computation and Language"
2212.0072,"Tommaso Salvatori, Yuhang Song, Beren Millidge, Zhenghua Xu, Lei Sha,
  Cornelius Emde, Rafal Bogacz, Thomas Lukasiewicz","Incremental Predictive Coding: A Parallel and Fully Automatic Learning
  Algorithm","['cs.NE', 'cs.AI', 'cs.LG']","  Neuroscience-inspired models, such as predictive coding, have the potential
to play an important role in the future of machine intelligence. However, they
are not yet used in industrial applications due to some limitations, such as
the lack of efficiency. In this work, we address this by proposing incremental
predictive coding (iPC), a variation of the original framework derived from the
incremental expectation maximization algorithm, where every operation can be
performed in parallel without external control. We show both theoretically and
empirically that iPC is much faster than the original algorithm originally
developed by Rao and Ballard, while maintaining performance comparable to
backpropagation in image classification tasks. This work impacts several areas,
has general applications in computational neuroscience and machine learning,
and specific applications in scenarios where automatization and parallelization
are important, such as distributed computing and implementations of deep
learning models on analog and neuromorphic chips.
","[{'version': 'v1', 'created': 'Wed, 16 Nov 2022 00:11:04 GMT'}]",2022-12-02,"['Neural and Evolutionary Computing', 'Artificial Intelligence', 'Machine Learning']","This paper presents a novel learning algorithm for neural and evolutionary computing, artificial intelligence, and machine learning, called Incremental Predictive Coding (IPC). This algorithm is designed to be fully automatic and parallel, making it suitable for large-scale applications. The proposed algorithm is based on an incremental learning approach, which allows for learning to be done in an iterative fashion, allowing for faster and more efficient learning. The algorithm is evaluated on several benchmark datasets and compared to existing approaches. Results show that IPC is able to achieve competitive performance while being more efficient and faster than the existing approaches. The paper also provides insights into the advantages of IPC over other approaches, and discusses its potential applications in various domains.","Write an abstract for a paper called Incremental Predictive Coding: A Parallel and Fully Automatic Learning
  Algorithm about Neural and Evolutionary Computing, Artificial Intelligence, Machine Learning"
2202.02372,"Michael Saccone, Francesco Caravelli, Kevin Hofhuis, Sergii Parchenko,
  Yorick A. Birkh\""olzer, Scott Dhuey, Armin Kleibert, Sebastiaan van Dijken,
  Cristiano Nisoli, and Alan Farhan","Direct observation of a dynamical glass transition in a nanomagnetic
  artificial Hopfield network","['cond-mat.dis-nn', 'cond-mat.mes-hall', 'cond-mat.stat-mech', 'cs.NE', 'nlin.CD']","  Spin glasses, generally defined as disordered systems with randomized
competing interactions, are a widely investigated complex system. Theoretical
models describing spin glasses are broadly used in other complex systems, such
as those describing brain function, error-correcting codes, or stock-market
dynamics. This wide interest in spin glasses provides strong motivation to
generate an artificial spin glass within the framework of artificial spin ice
systems. Here, we present the experimental realization of an artificial spin
glass consisting of dipolar coupled single-domain Ising-type nanomagnets
arranged onto an interaction network that replicates the aspects of a Hopfield
neural network. Using cryogenic x-ray photoemission electron microscopy
(XPEEM), we performed temperature-dependent imaging of thermally driven moment
fluctuations within these networks and observed characteristic features of a
two-dimensional Ising spin glass. Specifically, the temperature dependence of
the spin glass correlation function follows a power law trend predicted from
theoretical models on two-dimensional spin glasses. Furthermore, we observe
clear signatures of the hard to observe rugged spin glass free energy in the
form of sub-aging, out of equilibrium autocorrelations and a transition from
stable to unstable dynamics.
","[{'version': 'v1', 'created': 'Fri, 4 Feb 2022 20:18:27 GMT'}]",2022-09-13,['Neural and Evolutionary Computing'],"This paper investigates the emergence of a dynamical glass transition in a nanomagnetic artificial Hopfield network, which is a type of neural and evolutionary computing system. Through direct observation of the system, we demonstrate that the glass transition is an emergent property of the network, and that its critical properties are determined by the network's connectivity. We analyze the behavior of the network in the vicinity of the glass transition, and show that the transition is accompanied by a significant slowing down of the dynamics. Our results provide insight into the behavior of neural and evolutionary computing systems in the presence of glassy dynamics, and could be useful in the design of future systems.","Write an abstract for a paper called Direct observation of a dynamical glass transition in a nanomagnetic
  artificial Hopfield network about Neural and Evolutionary Computing"
2202.06827,"Julia Chuzhoy, Zihan Tan","A Subpolynomial Approximation Algorithm for Graph Crossing Number in
  Low-Degree Graphs",['cs.DS'],"  We consider the classical Minimum Crossing Number problem: given an
$n$-vertex graph $G$, compute a drawing of $G$ in the plane, while minimizing
the number of crossings between the images of its edges. This is a fundamental
and extensively studied problem, whose approximability status is widely open.
In all currently known approximation algorithms, the approximation factor
depends polynomially on $\Delta$ -- the maximum vertex degree in $G$. The best
current approximation algorithm achieves an $O(n^{1/2-\varepsilon}\cdot
\text{poly}(\Delta\cdot\log n))$-approximation, for a small fixed constant
$\epsilon$, while the best negative result is APX-hardness, leaving a large gap
in our understanding of this basic problem. In this paper we design a
randomized $O\left(2^{O((\log n)^{7/8}\log\log
n)}\cdot\text{poly}(\Delta)\right )$-approximation algorithm for Minimum
Crossing Number. This is the first approximation algorithm for the problem that
achieves a subpolynomial in $n$ approximation factor (albeit only in graphs
whose maximum vertex degree is subpolynomial in $n$).
  In order to achieve this approximation factor, we design a new algorithm for
a closely related problem called Crossing Number with Rotation System, in
which, for every vertex $v\in V(G)$, the circular ordering, in which the images
of the edges incident to $v$ must enter the image of $v$ in the drawing is
fixed as part of the input. Combining this result with the recent reduction of
[Chuzhoy, Mahabadi, Tan '20] immediately yields the improved approximation
algorithm for Minimum Crossing Number. We introduce several new technical
tools, that we hope will be helpful in obtaining better algorithms for the
problem in the future.
","[{'version': 'v1', 'created': 'Mon, 14 Feb 2022 16:03:10 GMT'}]",2022-02-15,['Data Structures and Algorithms'],"This paper presents a subpolynomial approximation algorithm for computing the graph crossing number of low-degree graphs. The graph crossing number is an important graph invariant that measures the complexity of a graph’s drawing. We propose a new algorithm which is based on a combination of dynamic programming and random sampling techniques. Our algorithm runs in time O(n^{1.5+\epsilon}) for any constant \epsilon>0, where n is the number of vertices in the graph. We also prove that our algorithm is a subpolynomial approximation of the graph crossing number. We demonstrate the effectiveness of our algorithm on a variety of low-degree graphs, which outperform existing methods.","Write an abstract for a paper called A Subpolynomial Approximation Algorithm for Graph Crossing Number in
  Low-Degree Graphs about Data Structures and Algorithms"
2303.0381,E. Macca and G. Russo,"Boundary effects on wave trains in the Exner model of sedimentat
  transport","['math.NA', 'cs.NA']","  The aim of this work is to study the effect of imposing different types of
boundary conditions in the Exner model describing shallow water equations with
sedimentation, when a train of waves is imposed at one of the boundaries. The
numerical solver is a second order finite-volume scheme, with semi-implicit
time discretization based on Implicit-Explicit (IMEX) schemes, which guarantee
  better stability properties than explicit ones. We show the effect of
spurious reflected waves at the right edge of the computational domain, propose
two remedies, and show how such spurious effects can be reduced by suitable
non-reflecting boundary conditions.
","[{'version': 'v1', 'created': 'Tue, 7 Mar 2023 11:25:38 GMT'}]",2023-03-08,['Numerical Analysis'],"This paper investigates the effects of boundaries on wave trains in the Exner model of sediment transport using numerical analysis. The Exner model is a two-dimensional shallow-water model that describes the motion of sediment-laden water. The model is used to study the transport of sediment in rivers, estuaries, and coastal waters. In this study, we use numerical analysis to investigate the effects of boundaries on wave trains in the Exner model. We consider different boundary conditions, such as open boundaries, periodic boundaries, and mixed boundaries. We analyze the numerical results to determine the effects of the boundaries on the wave trains. The results of this study will provide insight into the behavior of sediment transport in rivers, estuaries, and coastal waters.","Write an abstract for a paper called Boundary effects on wave trains in the Exner model of sedimentat
  transport about Numerical Analysis"
2208.06522,"Isaac Bromley-Dulfano, Xiangqi Zhu, and Barry Mather",Behavioral and Population Data Driven Distribution System Load Modeling,"['eess.SY', 'cs.SY']","  Distribution system residential load modeling and analysis for different
geographic areas within a utility or an independent system operator territory
are critical for enabling small-scale, aggregated distributed energy resources
to participate in grid services under Federal Energy Regulatory Commission
Order No. 2222 [1]. In this study, we develop a methodology of modeling
residential load profiles in different geographic areas with a focus on human
behavior impact. First, we construct a behavior-based load profile model
leveraging state-of-the-art appliance models. We simulate human activity and
occupancy using Markov chain Monte Carlo methods calibrated with the American
Time Use Survey data set. Second, we link our model with cleaned Current
Population Survey data from the U.S. Census Bureau. Finally, we populate two
sets of 500 households using California and Texas census data, respectively, to
perform an initial analysis of the load in different geographic areas with
various group features (e.g., different income levels). To distinguish the
effect of population behavior differences on aggregated load, we simulate load
profiles for both sets assuming fixed physical household parameters and weather
data. Analysis shows that average daily load profiles vary significantly by
income and income dependency varies by locality.
","[{'version': 'v1', 'created': 'Fri, 12 Aug 2022 22:44:14 GMT'}]",2022-08-16,['Systems and Control'],"This paper presents a new approach for modeling system load in distribution systems, combining both behavioral and population data. The proposed approach is based on the use of a novel population-based optimization algorithm to estimate the load on the system from the population data. The proposed approach is demonstrated on a real-world distribution network and compared to a traditional load model. The results show that the proposed approach can accurately estimate system load and provide better results than the traditional model. Additionally, the proposed approach is more robust and efficient than the traditional model, making it suitable for use in real-world applications.",Write an abstract for a paper called Behavioral and Population Data Driven Distribution System Load Modeling about Systems and Control
2212.04491,Workneh Yilma Ayele,"Improving the Utilization of Digital Services - Evaluating Contest -
  Driven Open Data Development and the Adoption of Cloud Services",['cs.DC'],"  There is a growing interest in utilizing digital services, such as software
apps and cloud-based software services. The utilization of digital services is
increasing more rapidly than any other segment of world trade. The availability
of open data unlocks the possibility of generating market possibilities in the
public and private sectors. Digital service utilization can be improved by
adopting cloud-based software services and open data innovation for service
development. However, open data has no value unless utilized, and little is
known about developing digital services using open data. Evaluation of digital
service development processes to service deployment is indispensable. Despite
this, existing evaluation models are not specifically designed to measure open
data innovation contests. Additionally, existing cloud-based digital service
implications are not used directly to adopt the technology, and empirical
research needs to be included. The research question addressed in this thesis
is: ""How can contest-driven innovation of open data digital services be
evaluated and the adoption of digital services be supported to improve the
utilization of digital services?"" The research approaches used are design
science research, descriptive statistics, and case study. This thesis proposes
Digital Innovation Contest Measurement Model (DICM-model) and Designing and
Refining DICM (DRD-method) for designing and refining DICM-model to provide
more agility. Additionally, a framework of barriers constraining developers of
open data services from developing viable services is also presented. This
framework enables requirement and cloud engineers to prioritize factors
responsible for effective adoption. Future research possibilities are
automation of idea generation, ex-post evaluation of the proposed artifacts,
and expanding cloud-based digital service adoption from suppliers'
perspectives.
","[{'version': 'v1', 'created': 'Tue, 6 Dec 2022 23:30:27 GMT'}]",2022-12-13,"['Distributed, Parallel, and Cluster Computing']","This paper examines the potential of contest-driven open data development and cloud services to improve the utilization of digital services in distributed, parallel, and cluster computing. It explores the challenges and opportunities associated with the adoption of cloud services in this field. The paper evaluates the impact of cloud services on the utilization of digital services, such as data storage, computing power, and analytics, and the scalability of distributed, parallel, and cluster computing. It also looks at the potential for contest-driven open data development to increase the utilization of digital services in this field. Finally, the paper proposes a set of best practices for organizations to increase the utilization of digital services in distributed, parallel, and cluster computing.","Write an abstract for a paper called Improving the Utilization of Digital Services - Evaluating Contest -
  Driven Open Data Development and the Adoption of Cloud Services about Distributed, Parallel, and Cluster Computing"
1909.10811,Kieran Greer,Image Recognition using Region Creep,"['cs.CV', 'eess.IV']","  This paper describes a new type of auto-associative image classifier that
uses a shallow architecture with a very quick learning phase. The image is
parsed into smaller areas and each area is saved directly for a region, along
with the related output category. When a new image is presented, a direct match
with each region is made and the best matching areas returned. Each area stores
a list of the categories it belongs to, where there is a one-to-many relation
between the input region and the output categories. The image classification
process sums the category lists to return a preferred category for the whole
image. These areas can overlap with each other and when moving from a region to
its neighbours, there is likely to be only small changes in the area image
part. It would therefore be possible to guess what the best image area is for
one region by cumulating the results of its neighbours. This associative
feature is being called 'Region Creep' and the cumulated region can be compared
with train cases instead, when a suitable match is not found. Rules can be
included and state that: if one set of pixels are present, another set should
either be removed or should also be present, where this is across the whole
image. The memory problems with a traditional auto-associative network may be
less with this version and tests on a set of hand-written numbers have produced
state-of-the-art results.
","[{'version': 'v1', 'created': 'Tue, 24 Sep 2019 10:59:44 GMT'}, {'version': 'v2', 'created': 'Thu, 20 Oct 2022 12:37:36 GMT'}, {'version': 'v3', 'created': 'Wed, 14 Dec 2022 10:07:24 GMT'}]",2022-12-21,['Computer Vision and Pattern Recognition'],This paper presents an image recognition algorithm using region creep for computer vision and pattern recognition. Region creep is a technique that uses a region-based approach to image segmentation. The algorithm is based on a region growing algorithm and uses a combination of region growing and boundary detection to identify objects in an image. The algorithm is tested on a variety of images and results show that it is able to accurately detect objects in the images with minimal false positives. The paper also discusses the advantages of using region creep for image recognition over other methods. The paper concludes with a discussion of possible future applications and areas of research.,Write an abstract for a paper called Image Recognition using Region Creep about Computer Vision and Pattern Recognition
2301.13442,"Jacob Hilton, Jie Tang, John Schulman",Scaling laws for single-agent reinforcement learning,"['cs.LG', 'cs.AI', 'stat.ML']","  Recent work has shown that, in generative modeling, cross-entropy loss
improves smoothly with model size and training compute, following a power law
plus constant scaling law. One challenge in extending these results to
reinforcement learning is that the main performance objective of interest, mean
episode return, need not vary smoothly. To overcome this, we introduce
*intrinsic performance*, a monotonic function of the return defined as the
minimum compute required to achieve the given return across a family of models
of different sizes. We find that, across a range of environments, intrinsic
performance scales as a power law in model size and environment interactions.
Consequently, as in generative modeling, the optimal model size scales as a
power law in the training compute budget. Furthermore, we study how this
relationship varies with the environment and with other properties of the
training setup. In particular, using a toy MNIST-based environment, we show
that varying the ""horizon length"" of the task mostly changes the coefficient
but not the exponent of this relationship.
","[{'version': 'v1', 'created': 'Tue, 31 Jan 2023 06:38:53 GMT'}, {'version': 'v2', 'created': 'Sun, 19 Feb 2023 01:24:51 GMT'}]",2023-02-21,"['Machine Learning', 'Artificial Intelligence']","This paper examines the scaling laws for single-agent reinforcement learning in the context of Machine Learning and Artificial Intelligence. We analyze the scaling of the learning process with respect to the size of the environment and the learning agent. We use a combination of theoretical and empirical approaches to investigate the scalability of reinforcement learning algorithms and their performance in large-scale problems. We also discuss the implications of our findings for the development of autonomous agents in real-world scenarios. Finally, we present our conclusions and discuss potential directions for future research.","Write an abstract for a paper called Scaling laws for single-agent reinforcement learning about Machine Learning, Artificial Intelligence"
2210.02517,"Zulfiqar Zaidi, Daniel Martin, Nathaniel Belles, Viacheslav Zakharov,
  Arjun Krishna, Kin Man Lee, Peter Wagstaff, Sumedh Naik, Matthew Sklar, Sugju
  Choi, Yoshiki Kakehi, Ruturaj Patil, Divya Mallemadugula, Florian Pesce,
  Peter Wilson, Wendell Hom, Matan Diamond, Bryan Zhao, Nina Moorman, Rohan
  Paleja, Letian Chen, Esmaeil Seraj, Matthew Gombolay",Athletic Mobile Manipulator System for Robotic Wheelchair Tennis,['cs.RO'],"  Athletics are a quintessential and universal expression of humanity. From
French monks who in the 12th century invented jeu de paume, the precursor to
modern lawn tennis, back to the K'iche' people who played the Maya Ballgame as
a form of religious expression over three thousand years ago, humans have
sought to train their minds and bodies to excel in sporting contests. Advances
in robotics are opening up the possibility of robots in sports. Yet, key
challenges remain, as most prior works in robotics for sports are limited to
pristine sensing environments, do not require significant force generation, or
are on miniaturized scales unsuited for joint human-robot play. In this paper,
we propose the first open-source, autonomous robot for playing regulation
wheelchair tennis. We demonstrate the performance of our full-stack system in
executing ground strokes and evaluate each of the system's hardware and
software components. The goal of this paper is to (1) inspire more research in
human-scale robot athletics and (2) establish the first baseline for a
reproducible wheelchair tennis robot for regulation singles play. Our paper
contributes to the science of systems design and poses a set of key challenges
for the robotics community to address in striving towards robots that can match
human capabilities in sports.
","[{'version': 'v1', 'created': 'Wed, 5 Oct 2022 19:25:41 GMT'}, {'version': 'v2', 'created': 'Tue, 7 Feb 2023 17:41:53 GMT'}]",2023-02-08,['Robotics'],"This paper presents an innovative Athletic Mobile Manipulator System (AMMS) for robotic wheelchair tennis. The AMMS is a robotic system that provides a platform for wheelchair tennis players to experience the sport in a safe and efficient manner. The system includes a robotic wheelchair, a robotic manipulator arm, and a motion capture system. The robot wheelchair is capable of autonomous navigation and obstacle avoidance, while the manipulator arm is used to control the racket and ball. The motion capture system is used to provide feedback to the robot and to track the player's movements. The AMMS is evaluated through simulations and experiments with a real wheelchair and a robotic manipulator arm. The results show that the AMMS is capable of providing a realistic and safe experience for wheelchair tennis players. The system has the potential to revolutionize the sport and bring it to a wider audience.",Write an abstract for a paper called Athletic Mobile Manipulator System for Robotic Wheelchair Tennis about Robotics
2206.05518,"A Arunkumar, Vrunda N Sukhadia, S. Umesh","Investigation of Ensemble features of Self-Supervised Pretrained Models
  for Automatic Speech Recognition","['cs.CL', 'eess.AS']","  Self-supervised learning (SSL) based models have been shown to generate
powerful representations that can be used to improve the performance of
downstream speech tasks. Several state-of-the-art SSL models are available, and
each of these models optimizes a different loss which gives rise to the
possibility of their features being complementary. This paper proposes using an
ensemble of such SSL representations and models, which exploits the
complementary nature of the features extracted by the various pretrained
models. We hypothesize that this results in a richer feature representation and
shows results for the ASR downstream task. To this end, we use three SSL models
that have shown excellent results on ASR tasks, namely HuBERT, Wav2vec2.0, and
WaveLM. We explore the ensemble of models fine-tuned for the ASR task and the
ensemble of features using the embeddings obtained from the pre-trained models
for a downstream ASR task. We get improved performance over individual models
and pre-trained features using Librispeech(100h) and WSJ dataset for the
downstream tasks.
","[{'version': 'v1', 'created': 'Sat, 11 Jun 2022 12:43:00 GMT'}]",2023-02-21,['Computation and Language'],"This paper investigates the use of ensemble features of self-supervised pretrained models for automatic speech recognition (ASR). We analyze the impact of different representations and architectures of self-supervised pretrained models on the performance of ASR tasks. The ensemble features of these models are compared to features extracted from supervised learning models. We evaluate the performance of the ensemble features using two datasets in different languages, and provide insights into the computational and language-related aspects of the models. We further discuss the implications of our findings for the development of ASR systems. Our results demonstrate that the ensemble features of self-supervised pretrained models can be used to improve the accuracy of ASR systems, and that the performance of these models is dependent on the language used.","Write an abstract for a paper called Investigation of Ensemble features of Self-Supervised Pretrained Models
  for Automatic Speech Recognition about Computation and Language"
2211.14178,"Dipayan Chakraborty, Florent Foucaud, Anni Hakanen, Michael A.
  Henning, Annegret K. Wagler","Progress towards the two-thirds conjecture on locating-total dominating
  sets","['math.CO', 'cs.DM']","  We study upper bounds on the size of optimum locating-total dominating sets
in graphs. A set $S$ of vertices of a graph $G$ is a locating-total dominating
set if every vertex of $G$ has a neighbor in $S$, and if any two vertices
outside $S$ have distinct neighborhoods within $S$. The smallest size of such a
set is denoted by $\gamma^L_t(G)$. It has been conjectured that
$\gamma^L_t(G)\leq\frac{2n}{3}$ holds for every twin-free graph $G$ of order
$n$ without isolated vertices. We prove that the conjecture holds for
cobipartite graphs, split graphs, block graphs, subcubic graphs and outerplanar
graphs.
","[{'version': 'v1', 'created': 'Fri, 25 Nov 2022 15:32:21 GMT'}]",2023-01-09,['Discrete Mathematics'],"This paper presents a progress report on the Two-Thirds Conjecture on Locating-Total Dominating Sets in Discrete Mathematics. We discuss the current status of the conjecture, which states that the minimum size of a locating-total dominating set of a graph is at least two-thirds of its maximum degree. We review the known results, which are mainly related to the case of trees, and the strategies used to prove them. We also discuss the challenges that remain in proving the conjecture for general graphs and provide an outlook for future research. Finally, we present our own results, which provide further evidence for the validity of the conjecture.","Write an abstract for a paper called Progress towards the two-thirds conjecture on locating-total dominating
  sets about Discrete Mathematics"
2109.03748,"Anabel G\'omez-R\'ios, Juli\'an Luengo, Francisco Herrera","A robust approach for deep neural networks in presence of label noise:
  relabelling and filtering instances during training","['cs.LG', 'cs.NE']","  Deep learning has outperformed other machine learning algorithms in a variety
of tasks, and as a result, it is widely used. However, like other machine
learning algorithms, deep learning, and convolutional neural networks (CNNs) in
particular, perform worse when the data sets present label noise. Therefore, it
is important to develop algorithms that help the training of deep networks and
their generalization to noise-free test sets. In this paper, we propose a
robust training strategy against label noise, called RAFNI, that can be used
with any CNN. This algorithm filters and relabels instances of the training set
based on the predictions and their probabilities made by the backbone neural
network during the training process. That way, this algorithm improves the
generalization ability of the CNN on its own. RAFNI consists of three
mechanisms: two mechanisms that filter instances and one mechanism that
relabels instances. In addition, it does not suppose that the noise rate is
known nor does it need to be estimated. We evaluated our algorithm using
different data sets of several sizes and characteristics. We also compared it
with state-of-the-art models using the CIFAR10 and CIFAR100 benchmarks under
different types and rates of label noise and found that RAFNI achieves better
results in most cases.
","[{'version': 'v1', 'created': 'Wed, 8 Sep 2021 16:11:31 GMT'}, {'version': 'v2', 'created': 'Thu, 12 May 2022 20:07:48 GMT'}, {'version': 'v3', 'created': 'Wed, 18 May 2022 19:49:49 GMT'}, {'version': 'v4', 'created': 'Mon, 18 Jul 2022 09:43:28 GMT'}]",2022-07-19,"['Machine Learning', 'Neural and Evolutionary Computing']","This paper presents a robust approach for deep neural networks in the presence of label noise. It proposes a two-stage framework, which first filters out mislabelled instances and then applies a relabelling procedure to the remaining instances. Experiments in a variety of datasets demonstrate that the proposed approach is able to effectively reduce the impact of label noise on the network's performance. The results show that the proposed approach is a promising solution for deep neural networks in the presence of label noise.","Write an abstract for a paper called A robust approach for deep neural networks in presence of label noise:
  relabelling and filtering instances during training about Machine Learning, Neural and Evolutionary Computing"
2103.10051,"Donghyun Lee, Minkyoung Cho, Seungwon Lee, Joonho Song and Changkyu
  Choi",Data-free mixed-precision quantization using novel sensitivity metric,"['cs.LG', 'cs.CV']","  Post-training quantization is a representative technique for compressing
neural networks, making them smaller and more efficient for deployment on edge
devices. However, an inaccessible user dataset often makes it difficult to
ensure the quality of the quantized neural network in practice. In addition,
existing approaches may use a single uniform bit-width across the network,
resulting in significant accuracy degradation at extremely low bit-widths. To
utilize multiple bit-width, sensitivity metric plays a key role in balancing
accuracy and compression. In this paper, we propose a novel sensitivity metric
that considers the effect of quantization error on task loss and interaction
with other layers. Moreover, we develop labeled data generation methods that
are not dependent on a specific operation of the neural network. Our
experiments show that the proposed metric better represents quantization
sensitivity, and generated data are more feasible to be applied to
mixed-precision quantization.
","[{'version': 'v1', 'created': 'Thu, 18 Mar 2021 07:23:21 GMT'}]",2022-01-05,"['Machine Learning', 'Computer Vision and Pattern Recognition']","This paper presents a novel data-free mixed-precision quantization technique that takes into account the sensitivity of each layer of a deep neural network to achieve improved accuracy. The proposed technique uses a novel sensitivity metric to identify the layers that should be quantized to lower precision and the layers that should be kept at higher precision. The technique is evaluated on a variety of computer vision and pattern recognition tasks, demonstrating improved accuracy compared to state-of-the-art data-free quantization methods. Furthermore, the proposed sensitivity metric is shown to be effective at identifying layers that should be quantized to lower precision, thus providing a valuable tool for practitioners.","Write an abstract for a paper called Data-free mixed-precision quantization using novel sensitivity metric about Machine Learning, Computer Vision and Pattern Recognition"
2205.10092,"Jayabrata Chowdhury, Suresh Sundaram, Nishant Rao and Narasimhan
  Sundararajan","An efficient Deep Spatio-Temporal Context Aware decision Network
  (DST-CAN) for Predictive Manoeuvre Planning",['cs.RO'],"  To ensure the safety and efficiency of its maneuvers, an Autonomous Vehicle
(AV) should anticipate the future intentions of surrounding vehicles using its
sensor information. If an AV can predict its surrounding vehicles' future
trajectories, it can make safe and efficient manoeuvre decisions. In this
paper, we present such a Deep Spatio-Temporal Context-Aware decision Network
(DST-CAN) model for predictive manoeuvre planning of AVs. A memory neuron
network is used to predict future trajectories of its surrounding vehicles. The
driving environment's spatio-temporal information (past, present, and predicted
future trajectories) are embedded into a context-aware grid. The proposed
DST-CAN model employs these context-aware grids as inputs to a convolutional
neural network to understand the spatial relationships between the vehicles and
determine a safe and efficient manoeuvre decision. The DST-CAN model also uses
information of human driving behavior on a highway. Performance evaluation of
DST-CAN has been carried out using two publicly available NGSIM US-101 and I-80
datasets. Also, rule-based ground truth decisions have been compared with those
generated by DST-CAN. The results clearly show that DST-CAN can make much
better decisions with 3-sec of predicted trajectories of neighboring vehicles
compared to currently existing methods that do not use this prediction.
","[{'version': 'v1', 'created': 'Fri, 20 May 2022 11:23:37 GMT'}]",2022-05-23,['Robotics'],This paper presents a novel Deep Spatio-Temporal Context Aware decision Network (DST-CAN) for predictive manoeuvre planning in robotics. The proposed network utilizes a novel spatio-temporal context aware encoding method to extract deep features from the environment and uses them to determine the best manoeuvre for a robot. The proposed network is evaluated on a simulated environment with a variety of scenarios and compared to existing methods. The results show that the proposed network is able to accurately predict the best manoeuvre in a variety of scenarios and outperforms existing methods in terms of both accuracy and computational efficiency. The proposed network is thus a promising tool for predictive manoeuvre planning in robotics.,"Write an abstract for a paper called An efficient Deep Spatio-Temporal Context Aware decision Network
  (DST-CAN) for Predictive Manoeuvre Planning about Robotics"
2304.02426,"Wenxiang Jiao, Jen-tse Huang, Wenxuan Wang, Xing Wang, Shuming Shi and
  Zhaopeng Tu",ParroT: Translating During Chat Using Large Language Models,['cs.CL'],"  Large language models (LLMs) like ChatGPT and GPT-4 have exhibited remarkable
abilities on a wide range of natural language processing (NLP) tasks, including
various machine translation abilities accomplished during chat. However, these
models are only accessible through restricted APIs, which creates barriers to
new research and advancements in the field. Therefore, we propose the
$\mathbf{ParroT}$ framework to enhance and regulate the translation abilities
during chat based on open-sourced LLMs (i.e., LLaMA-7b) and human written
translation and evaluation data. Specifically, ParroT reformulates translation
data into the instruction-following style, and introduces a ""Hint"" field for
incorporating extra requirements to regulate the translation process.
Accordingly, we propose three instruction types for finetuning ParroT models,
including translation instruction, contrastive instruction, and error-guided
instruction. Experiments on Flores subsets and WMT22 test sets suggest that
translation instruction improves the translation performance of vanilla LLMs
significantly while error-guided instruction can lead to a further improvement,
which demonstrates the importance of learning from low-quality translations
annotated by human. Meanwhile, the ParroT models can also preserve the ability
on general tasks with the Alpaca multi-task dataset involved in finetuning.
Codes: https://github.com/wxjiao/ParroT
","[{'version': 'v1', 'created': 'Wed, 5 Apr 2023 13:12:00 GMT'}, {'version': 'v2', 'created': 'Thu, 6 Apr 2023 01:07:36 GMT'}]",2023-04-07,['Computation and Language'],"This paper presents ParroT, a novel chatbot system that enables real-time translation of conversations between two parties. ParroT is based on large language models and uses a combination of supervised and unsupervised learning techniques to identify and translate words, phrases, and sentences with high accuracy. We evaluate ParroT on a variety of datasets and show that it outperforms existing translation systems. Additionally, we discuss the implications of ParroT for natural language processing, machine translation, and dialogue systems.",Write an abstract for a paper called ParroT: Translating During Chat Using Large Language Models about Computation and Language
2211.01822,"Carmen Chan-Zheng, Pablo Borja, and Jacquelien M.A Scherpen","Dead-zone compensation via passivity-based control for a class of
  mechanical systems","['eess.SY', 'cs.SY']","  This manuscript introduces a passivity-based control methodology for
fully-actuated mechanical systems with symmetric or asymmetric dead-zones. To
this end, we find a smooth approximation of the inverse of the function that
describes such a nonlinearity. Then, we propose an energy and damping injection
approach - based on the PI-PBC technique - that compensates for the dead-zone.
Moreover, we provide an analysis of the performance of the proposed controller
near the equilibrium. We conclude this paper by experimentally validating the
results on a two degrees-of-freedom planar manipulator.
","[{'version': 'v1', 'created': 'Thu, 3 Nov 2022 14:01:51 GMT'}]",2022-11-04,['Systems and Control'],"This paper presents a dead-zone compensation approach for a class of mechanical systems using passivity-based control. Dead-zone is a phenomenon that occurs when a system's output does not respond to a certain range of input values. The proposed approach utilizes passivity-based control to compensate for the dead-zone and improve the system's performance. The paper discusses the design of the passivity-based controller, the stability analysis, and the simulation results. The proposed approach is compared to existing solutions and its effectiveness is demonstrated. The results show that the proposed approach is able to improve the system's performance, while ensuring its stability.","Write an abstract for a paper called Dead-zone compensation via passivity-based control for a class of
  mechanical systems about Systems and Control"
2204.04746,"Chinedu Innocent Nwoye, Deepak Alapatt, Tong Yu, Armine Vardazaryan,
  Fangfang Xia, Zixuan Zhao, Tong Xia, Fucang Jia, Yuxuan Yang, Hao Wang,
  Derong Yu, Guoyan Zheng, Xiaotian Duan, Neil Getty, Ricardo Sanchez-Matilla,
  Maria Robu, Li Zhang, Huabin Chen, Jiacheng Wang, Liansheng Wang, Bokai
  Zhang, Beerend Gerats, Sista Raviteja, Rachana Sathish, Rong Tao, Satoshi
  Kondo, Winnie Pang, Hongliang Ren, Julian Ronald Abbing, Mohammad Hasan
  Sarhan, Sebastian Bodenstedt, Nithya Bhasker, Bruno Oliveira, Helena R.
  Torres, Li Ling, Finn Gaida, Tobias Czempiel, Jo\~ao L. Vila\c{c}a, Pedro
  Morais, Jaime Fonseca, Ruby Mae Egging, Inge Nicole Wijma, Chen Qian, Guibin
  Bian, Zhen Li, Velmurugan Balasubramanian, Debdoot Sheet, Imanol Luengo,
  Yuanbo Zhu, Shuai Ding, Jakob-Anton Aschenbrenner, Nicolas Elini van der Kar,
  Mengya Xu, Mobarakol Islam, Lalithkumar Seenivasan, Alexander Jenke, Danail
  Stoyanov, Didier Mutter, Pietro Mascagni, Barbara Seeliger, Cristians
  Gonzalez, Nicolas Padoy","CholecTriplet2021: A benchmark challenge for surgical action triplet
  recognition",['cs.CV'],"  Context-aware decision support in the operating room can foster surgical
safety and efficiency by leveraging real-time feedback from surgical workflow
analysis. Most existing works recognize surgical activities at a coarse-grained
level, such as phases, steps or events, leaving out fine-grained interaction
details about the surgical activity; yet those are needed for more helpful AI
assistance in the operating room. Recognizing surgical actions as triplets of
<instrument, verb, target> combination delivers comprehensive details about the
activities taking place in surgical videos. This paper presents
CholecTriplet2021: an endoscopic vision challenge organized at MICCAI 2021 for
the recognition of surgical action triplets in laparoscopic videos. The
challenge granted private access to the large-scale CholecT50 dataset, which is
annotated with action triplet information. In this paper, we present the
challenge setup and assessment of the state-of-the-art deep learning methods
proposed by the participants during the challenge. A total of 4 baseline
methods from the challenge organizers and 19 new deep learning algorithms by
competing teams are presented to recognize surgical action triplets directly
from surgical videos, achieving mean average precision (mAP) ranging from 4.2%
to 38.1%. This study also analyzes the significance of the results obtained by
the presented approaches, performs a thorough methodological comparison between
them, in-depth result analysis, and proposes a novel ensemble method for
enhanced recognition. Our analysis shows that surgical workflow analysis is not
yet solved, and also highlights interesting directions for future research on
fine-grained surgical activity recognition which is of utmost importance for
the development of AI in surgery.
","[{'version': 'v1', 'created': 'Sun, 10 Apr 2022 18:51:55 GMT'}, {'version': 'v2', 'created': 'Thu, 29 Dec 2022 20:11:19 GMT'}]",2023-04-11,['Computer Vision and Pattern Recognition'],"CholecTriplet2021 is a benchmark challenge for surgical action triplet recognition in Computer Vision and Pattern Recognition. The challenge is based on the Cholec120 dataset, which consists of 120 cholecystectomy videos. The goal of the challenge is to develop a model that can recognize surgical action triplets from the videos. The challenge will evaluate the performance of the model by comparing the predictions against the ground truth labels. The challenge will also provide a platform for researchers to share their methods, algorithms, and results. The challenge aims to foster research in the field of surgical action triplet recognition and to advance the state-of-the-art in this field.","Write an abstract for a paper called CholecTriplet2021: A benchmark challenge for surgical action triplet
  recognition about Computer Vision and Pattern Recognition"
2204.06924,Sylwia Majchrowska and Marta Plantykow and Milena Olech,"Handling sign language transcription system with the computer-friendly
  numerical multilabels","['cs.CL', 'cs.AI']","  This paper presents our recent developments in the automatic processing of
sign language corpora using the Hamburg Sign Language Annotation System
(HamNoSys). We designed an automated tool to convert HamNoSys annotations into
numerical labels for defined initial features of body and hand positions. Our
proposed numerical multilabels greatly simplify annotations' structure without
significant loss of gloss meaning. These numerical multilabels can potentially
be used to feed the machine learning models, which would accelerate the
development of vision-based sign language recognition. In addition, this tool
can assist experts in the annotation process and help identify semantic errors.
The code and sample annotations are publicly available at
\url{https://github.com/hearai/parse-hamnosys}.
","[{'version': 'v1', 'created': 'Thu, 14 Apr 2022 12:33:33 GMT'}, {'version': 'v2', 'created': 'Wed, 15 Jun 2022 08:37:03 GMT'}, {'version': 'v3', 'created': 'Thu, 16 Jun 2022 08:56:52 GMT'}]",2022-06-17,"['Computation and Language', 'Artificial Intelligence']","This paper explores the potential of using a computer-friendly numerical multilabel system to handle sign language transcription. It examines the challenges of representing sign language in a numerical system, and how the use of such a system could improve the accuracy of sign language transcription. The paper also discusses how this numerical multilabel system could be used to enhance the accuracy of artificial intelligence (AI) systems for sign language recognition and understanding. Finally, the paper presents results from experiments conducted to evaluate the effectiveness of the proposed numerical multilabel system for sign language transcription. Overall, this paper provides an in-depth analysis of the potential of using numerical multilabels for sign language transcription, as well as a discussion of the implications for Computation and Language, Artificial Intelligence, and other related fields.","Write an abstract for a paper called Handling sign language transcription system with the computer-friendly
  numerical multilabels about Computation and Language, Artificial Intelligence"
2302.1111,"Xiaodong He, Weijia Yao, Zhiyong Sun, Zhongkui Li","A Novel Vector-Field-Based Motion Planning Algorithm for 3D Nonholonomic
  Robots","['cs.RO', 'cs.SY', 'eess.SY']","  This paper focuses on the motion planning for mobile robots in 3D, which are
modelled by 6-DOF rigid body systems with nonholonomic kinematics constraints.
We not only specify the target position, but also bring in the requirement of
the heading direction at the terminal time, which gives rise to a new and more
challenging 3D motion planning problem. The proposed planning algorithm
involves a novel velocity vector field (VF) over the workspace, and by
following the VF, the robot can be navigated to the destination with the
specified heading direction. In order to circumvent potential collisions with
obstacles and other robots, a composite VF is designed by composing the
navigation VF and an additional VF tangential to the boundary of the dangerous
area. Moreover, we propose a priority-based algorithm to deal with the motion
coupling issue among multiple robots. Finally, numerical simulations are
conducted to verify the theoretical results.
","[{'version': 'v1', 'created': 'Wed, 22 Feb 2023 03:13:32 GMT'}, {'version': 'v2', 'created': 'Sat, 8 Apr 2023 06:40:57 GMT'}]",2023-04-11,"['Robotics', 'Systems and Control']",This paper presents a novel vector-field-based motion planning algorithm for 3D nonholonomic robots. The proposed algorithm is designed to efficiently generate trajectories that satisfy the constraints of the robot's kinematic and dynamic model. The algorithm is based on a vector-field representation of the robot's configuration space and utilizes a local search strategy to identify a feasible path. The algorithm is evaluated on a variety of 3D nonholonomic robots and the results demonstrate its effectiveness. The paper also provides a comparison of the proposed algorithm with existing motion planning algorithms. The results show that the proposed algorithm is able to generate trajectories that are both faster and more reliable than the existing methods.,"Write an abstract for a paper called A Novel Vector-Field-Based Motion Planning Algorithm for 3D Nonholonomic
  Robots about Robotics, Systems and Control"
2004.11824,"Alex Levering, Martin Tomko, Devis Tuia, Kourosh Khoshelham",Detecting Unsigned Physical Road Incidents from Driver-View Images,['cs.CV'],"  Safety on roads is of uttermost importance, especially in the context of
autonomous vehicles. A critical need is to detect and communicate disruptive
incidents early and effectively. In this paper we propose a system based on an
off-the-shelf deep neural network architecture that is able to detect and
recognize types of unsigned (non-placarded, such as traffic signs), physical
(visible in images) road incidents. We develop a taxonomy for unsigned physical
incidents to provide a means of organizing and grouping related incidents.
After selecting eight target types of incidents, we collect a dataset of twelve
thousand images gathered from publicly-available web sources. We subsequently
fine-tune a convolutional neural network to recognize the eight types of road
incidents. The proposed model is able to recognize incidents with a high level
of accuracy (higher than 90%). We further show that while our system
generalizes well across spatial context by training a classifier on
geostratified data in the United Kingdom (with an accuracy of over 90%), the
translation to visually less similar environments requires spatially
distributed data collection.
  Note: this is a pre-print version of work accepted in IEEE Transactions on
Intelligent Vehicles (T-IV;in press). The paper is currently in production, and
the DOI link will be added soon.
","[{'version': 'v1', 'created': 'Fri, 24 Apr 2020 16:02:17 GMT'}]",2022-03-24,['Computer Vision and Pattern Recognition'],This paper presents a novel approach for detecting unsigned physical road incidents from driver-view images using computer vision and pattern recognition techniques. The proposed method is based on a convolutional neural network (CNN) and leverages the spatial and temporal information from the images to detect the presence of an incident. The model is trained on a large dataset of driver-view images collected from various locations around the world. The results demonstrate that the proposed method can detect unsigned physical road incidents with an accuracy of up to 96.2%. The proposed approach can be used to improve the safety of road users by alerting them to the presence of potential incidents.,Write an abstract for a paper called Detecting Unsigned Physical Road Incidents from Driver-View Images about Computer Vision and Pattern Recognition
2209.09657,"Haofeng Li, Junjia Huang, Guanbin Li, Zhou Liu, Yihong Zhong, Yingying
  Chen, Yunfei Wang, Xiang Wan",View-Disentangled Transformer for Brain Lesion Detection,['cs.CV'],"  Deep neural networks (DNNs) have been widely adopted in brain lesion
detection and segmentation. However, locating small lesions in 2D MRI slices is
challenging, and requires to balance between the granularity of 3D context
aggregation and the computational complexity. In this paper, we propose a novel
view-disentangled transformer to enhance the extraction of MRI features for
more accurate tumour detection. First, the proposed transformer harvests
long-range correlation among different positions in a 3D brain scan. Second,
the transformer models a stack of slice features as multiple 2D views and
enhance these features view-by-view, which approximately achieves the 3D
correlation computing in an efficient way. Third, we deploy the proposed
transformer module in a transformer backbone, which can effectively detect the
2D regions surrounding brain lesions. The experimental results show that our
proposed view-disentangled transformer performs well for brain lesion detection
on a challenging brain MRI dataset.
","[{'version': 'v1', 'created': 'Tue, 20 Sep 2022 11:58:23 GMT'}]",2022-09-21,['Computer Vision and Pattern Recognition'],"This paper presents a novel View-Disentangled Transformer (VDT) for brain lesion detection in medical images. VDT is a transformer-based architecture that leverages the disentanglement of views to capture the structural and semantic information of the input images. We demonstrate the effectiveness of VDT by conducting experiments on two publicly available brain lesion datasets. The results show that the proposed VDT outperforms the state-of-the-art methods in terms of accuracy, precision, recall, and F1-score. Furthermore, the proposed model can be easily extended to other medical image segmentation tasks.",Write an abstract for a paper called View-Disentangled Transformer for Brain Lesion Detection about Computer Vision and Pattern Recognition
2211.07652,Yuru Jing,"Machine Learning Performance Analysis to Predict Stroke Based on
  Imbalanced Medical Dataset","['cs.LG', 'cs.AI']","  Cerebral stroke, the second most substantial cause of death universally, has
been a primary public health concern over the last few years. With the help of
machine learning techniques, early detection of various stroke alerts is
accessible, which can efficiently prevent or diminish the stroke. Medical
dataset, however, are frequently unbalanced in their class label, with a
tendency to poorly predict minority classes. In this paper, the potential risk
factors for stroke are investigated. Moreover, four distinctive approaches are
applied to improve the classification of the minority class in the imbalanced
stroke dataset, which are the ensemble weight voting classifier, the Synthetic
Minority Over-sampling Technique (SMOTE), Principal Component Analysis with
K-Means Clustering (PCA-Kmeans), Focal Loss with the Deep Neural Network (DNN)
and compare their performance. Through the analysis results, SMOTE and
PCA-Kmeans with DNN-Focal Loss work best for the limited size of a large severe
imbalanced dataset,which is 2-4 times outperform Kaggle work.
","[{'version': 'v1', 'created': 'Mon, 14 Nov 2022 17:36:46 GMT'}]",2022-11-16,"['Machine Learning', 'Artificial Intelligence']","This paper presents a machine learning performance analysis to predict stroke based on an imbalanced medical dataset. The dataset was collected from a large hospital in the United States and contains patient-related information, such as age, gender, and medical test results. The dataset is imbalanced, meaning that the number of stroke patients is much lower than the number of non-stroke patients. A variety of machine learning algorithms, such as support vector machines, random forest, and logistic regression, were used to develop predictive models for stroke. The performance of these models was evaluated using metrics such as accuracy, precision, recall, and F1-score. The results showed that the random forest model had the best performance with an F1-score of 0.88. Moreover, the results also showed that the imbalanced dataset had a significant impact on the performance of the models. The findings of this study can help healthcare providers to better understand the performance of machine learning algorithms in predicting stroke.","Write an abstract for a paper called Machine Learning Performance Analysis to Predict Stroke Based on
  Imbalanced Medical Dataset about Machine Learning, Artificial Intelligence"
2001.04686,"Amir Hadifar, Johannes Deleu, Chris Develder, and Thomas Demeester",Block-wise Dynamic Sparseness,"['cs.LG', 'stat.ML']","  Neural networks have achieved state of the art performance across a wide
variety of machine learning tasks, often with large and computation-heavy
models. Inducing sparseness as a way to reduce the memory and computation
footprint of these models has seen significant research attention in recent
years. In this paper, we present a new method for \emph{dynamic sparseness},
whereby part of the computations are omitted dynamically, based on the input.
For efficiency, we combined the idea of dynamic sparseness with block-wise
matrix-vector multiplications. In contrast to static sparseness, which
permanently zeroes out selected positions in weight matrices, our method
preserves the full network capabilities by potentially accessing any trained
weights. Yet, matrix vector multiplications are accelerated by omitting a
pre-defined fraction of weight blocks from the matrix, based on the input.
Experimental results on the task of language modeling, using recurrent and
quasi-recurrent models, show that the proposed method can outperform a
magnitude-based static sparseness baseline. In addition, our method achieves
similar language modeling perplexities as the dense baseline, at half the
computational cost at inference time.
","[{'version': 'v1', 'created': 'Tue, 14 Jan 2020 10:03:21 GMT'}]",2022-10-21,['Machine Learning'],"This paper presents a novel approach to machine learning using block-wise dynamic sparseness. It focuses on the use of sparseness to improve the accuracy and generalization performance of machine learning algorithms. We propose a method to dynamically adjust the sparseness of a given model during training, based on the input data. We demonstrate the effectiveness of our approach on a variety of datasets and tasks, including classification, regression and clustering. Results show that our approach yields better performance than existing methods, while providing a more efficient training process. Furthermore, our approach can be easily extended to other machine learning algorithms.",Write an abstract for a paper called Block-wise Dynamic Sparseness about Machine Learning
2302.14368,"Wonwoong Cho, Hareesh Ravi, Midhun Harikumar, Vinh Khuc, Krishna Kumar
  Singh, Jingwan Lu, David I. Inouye, Ajinkya Kale",Towards Enhanced Controllability of Diffusion Models,"['cs.CV', 'cs.AI', 'cs.GR']","  Denoising Diffusion models have shown remarkable capabilities in generating
realistic, high-quality and diverse images. However, the extent of
controllability during generation is underexplored. Inspired by techniques
based on GAN latent space for image manipulation, we train a diffusion model
conditioned on two latent codes, a spatial content mask and a flattened style
embedding. We rely on the inductive bias of the progressive denoising process
of diffusion models to encode pose/layout information in the spatial structure
mask and semantic/style information in the style code. We propose two generic
sampling techniques for improving controllability. We extend composable
diffusion models to allow for some dependence between conditional inputs, to
improve the quality of generations while also providing control over the amount
of guidance from each latent code and their joint distribution. We also propose
timestep dependent weight scheduling for content and style latents to further
improve the translations. We observe better controllability compared to
existing methods and show that without explicit training objectives, diffusion
models can be used for effective image manipulation and image translation.
","[{'version': 'v1', 'created': 'Tue, 28 Feb 2023 07:43:00 GMT'}, {'version': 'v2', 'created': 'Wed, 15 Mar 2023 21:42:39 GMT'}]",2023-03-17,"['Computer Vision and Pattern Recognition', 'Artificial Intelligence', 'Graphics']","This paper presents a novel approach towards enhanced controllability of diffusion models for Computer Vision and Pattern Recognition, Artificial Intelligence, and Graphics. We propose a novel technique that can be used to control the diffusion process more effectively and accurately. Our technique uses a combination of supervised and unsupervised learning methods to enable more accurate and efficient control of the diffusion process. We also present a comprehensive evaluation of the proposed technique, demonstrating its effectiveness and applicability in a wide range of computer vision and pattern recognition tasks. Finally, we discuss potential directions for future research and development.","Write an abstract for a paper called Towards Enhanced Controllability of Diffusion Models about Computer Vision and Pattern Recognition, Artificial Intelligence, Graphics"
2203.07206,"Farid Bagirov, Dmitry Ivanov, Aleksei Shpilman","Improving State-of-the-Art in One-Class Classification by Leveraging
  Unlabeled Data",['cs.LG'],"  When dealing with binary classification of data with only one labeled class
data scientists employ two main approaches, namely One-Class (OC)
classification and Positive Unlabeled (PU) learning. The former only learns
from labeled positive data, whereas the latter also utilizes unlabeled data to
improve the overall performance. Since PU learning utilizes more data, we might
be prone to think that when unlabeled data is available, the go-to algorithms
should always come from the PU group. However, we find that this is not always
the case if unlabeled data is unreliable, i.e. contains limited or biased
latent negative data. We perform an extensive experimental study of a wide list
of state-of-the-art OC and PU algorithms in various scenarios as far as
unlabeled data reliability is concerned. Furthermore, we propose PU
modifications of state-of-the-art OC algorithms that are robust to unreliable
unlabeled data, as well as a guideline to similarly modify other OC algorithms.
Our main practical recommendation is to use state-of-the-art PU algorithms when
unlabeled data is reliable and to use the proposed modifications of
state-of-the-art OC algorithms otherwise. Additionally, we outline procedures
to distinguish the cases of reliable and unreliable unlabeled data using
statistical tests.
","[{'version': 'v1', 'created': 'Mon, 14 Mar 2022 15:44:40 GMT'}]",2022-03-15,['Machine Learning'],"This paper presents a novel approach to improving the state-of-the-art in one-class classification by leveraging unlabeled data. We propose a new method for incorporating unlabeled data into the one-class classification process, which utilizes an unsupervised learning algorithm to generate additional training data. We evaluate our method on a variety of datasets and show that it significantly improves the performance of one-class classifiers. Furthermore, we analyze the impact of different hyperparameter settings and provide insights into the best settings for achieving the best performance. Our results demonstrate that our proposed approach can significantly improve the state-of-the-art in one-class classification by leveraging unlabeled data.","Write an abstract for a paper called Improving State-of-the-Art in One-Class Classification by Leveraging
  Unlabeled Data about Machine Learning"
2211.08678,"Hao Wang, Xiwen Chen, Abolfazl Razi, Michael Kozicki, Rahul Amin, Mark
  Manfredo","Nano-Resolution Visual Identifiers Enable Secure Monitoring in
  Next-Generation Cyber-Physical Systems",['cs.CR'],"  Today's supply chains heavily rely on cyber-physical systems such as
intelligent transportation, online shopping, and E-commerce. It is advantageous
to track goods in real-time by web-based registration and authentication of
products after any substantial change or relocation. Despite recent advantages
in technology-based tracking systems, most supply chains still rely on plainly
printed tags such as barcodes and Quick Response (QR) codes for tracking
purposes. Although affordable and efficient, these tags convey no security
against counterfeit and cloning attacks, raising privacy concerns. It is a
critical matter since a few security breaches in merchandise databases in
recent years has caused crucial social and economic impacts such as identity
loss, social panic, and loss of trust in the community. This paper considers an
end-to-end system using dendrites as nano-resolution visual identifiers to
secure supply chains. Dendrites are formed by generating fractal metallic
patterns on transparent substrates through an electrochemical process, which
can be used as secure identifiers due to their natural randomness, high
entropy, and unclonable features. The proposed framework compromises the
back-end program for identification and authentication, a web-based application
for mobile devices, and a cloud database. We review architectural design,
dendrite operational phases (personalization, registration, inspection), a
lightweight identification method based on 2D graph-matching, and a deep 3D
image authentication method based on Digital Holography (DH). A two-step search
is proposed to make the system scalable by limiting the search space to samples
with high similarity scores in a lower-dimensional space. We conclude by
presenting our solution to make dendrites secure against adversarial attacks.
","[{'version': 'v1', 'created': 'Wed, 16 Nov 2022 05:18:42 GMT'}]",2022-11-17,['Cryptography and Security'],"This paper explores the potential of nano-resolution visual identifiers to enable secure monitoring in next-generation cyber-physical systems. Cryptography and security are two of the most important aspects of cyber-physical systems, and this paper examines the use of nano-resolution visual identifiers to improve security and privacy. The paper will discuss the advantages of using these identifiers, such as their ability to be used for authentication and authorization, as well as their potential to be used in a wide range of applications. Additionally, the paper will discuss the challenges associated with using nano-resolution visual identifiers, such as their cost and scalability, and how they can be addressed. The paper will also explore how nano-resolution visual identifiers can be used to improve the security of cyber-physical systems, and how they can be integrated into existing systems. Finally, the paper will discuss the implications of using nano-resolution visual identifiers for cyber-physical systems, and how they can be used to improve the security and privacy of these systems.","Write an abstract for a paper called Nano-Resolution Visual Identifiers Enable Secure Monitoring in
  Next-Generation Cyber-Physical Systems about Cryptography and Security"
2210.08225,"Yung-Han Ho, Chih-Hsuan Lin, Peng-Yu Chen, Mu-Jung Chen, Chih-Peng
  Chang, Wen-Hsiao Peng, Hsueh-Ming Hang","Learned Video Compression for YUV 4:2:0 Content Using Flow-based
  Conditional Inter-frame Coding","['eess.IV', 'cs.CV', 'cs.LG']","  This paper proposes a learning-based video compression framework for
variable-rate coding on YUV 4:2:0 content. Most existing learning-based video
compression models adopt the traditional hybrid-based coding architecture,
which involves temporal prediction followed by residual coding. However, recent
studies have shown that residual coding is sub-optimal from the
information-theoretic perspective. In addition, most existing models are
optimized with respect to RGB content. Furthermore, they require separate
models for variable-rate coding. To address these issues, this work presents an
attempt to incorporate the conditional inter-frame coding for YUV 4:2:0
content. We introduce a conditional flow-based inter-frame coder to improve the
inter-frame coding efficiency. To adapt our codec to YUV 4:2:0 content, we
adopt a simple strategy of using space-to-depth and depth-to-space conversions.
Lastly, we employ a rate-adaption net to achieve variable-rate coding without
training multiple models. Experimental results show that our model performs
better than x265 on UVG and MCL-JCV datasets in terms of PSNR-YUV. However, on
the more challenging datasets from ISCAS'22 GC, there is still ample room for
improvement. This insufficient performance is due to the lack of inter-frame
coding capability at a large GOP size and can be mitigated by increasing the
model capacity and applying an error propagation-aware training strategy.
","[{'version': 'v1', 'created': 'Sat, 15 Oct 2022 08:36:01 GMT'}]",2022-10-18,"['Computer Vision and Pattern Recognition', 'Machine Learning']",This paper presents a novel flow-based conditional inter-frame coding approach to learned video compression for YUV 4:2:0 content. The proposed method combines recent advances in computer vision and pattern recognition with machine learning to optimize the coding process. The approach models the video frames as a sequence of optical flows and uses an encoder-decoder architecture to encode the frames. The encoder is trained using a supervised learning approach to learn the best flow-based coding parameters for each frame. The decoder uses a motion compensation network to reconstruct the frames from the encoded flows. Experiments on a variety of video datasets demonstrate that the proposed method achieves superior compression performance compared to state-of-the-art methods.,"Write an abstract for a paper called Learned Video Compression for YUV 4:2:0 Content Using Flow-based
  Conditional Inter-frame Coding about Computer Vision and Pattern Recognition, Machine Learning"
2303.07937,"Junyoung Seo, Wooseok Jang, Min-Seop Kwak, Jaehoon Ko, Hyeonsu Kim,
  Junho Kim, Jin-Hwa Kim, Jiyoung Lee, Seungryong Kim","Let 2D Diffusion Model Know 3D-Consistency for Robust Text-to-3D
  Generation",['cs.CV'],"  Text-to-3D generation has shown rapid progress in recent days with the advent
of score distillation, a methodology of using pretrained text-to-2D diffusion
models to optimize neural radiance field (NeRF) in the zero-shot setting.
However, the lack of 3D awareness in the 2D diffusion models destabilizes score
distillation-based methods from reconstructing a plausible 3D scene. To address
this issue, we propose 3DFuse, a novel framework that incorporates 3D awareness
into pretrained 2D diffusion models, enhancing the robustness and 3D
consistency of score distillation-based methods. We realize this by first
constructing a coarse 3D structure of a given text prompt and then utilizing
projected, view-specific depth map as a condition for the diffusion model.
Additionally, we introduce a training strategy that enables the 2D diffusion
model learns to handle the errors and sparsity within the coarse 3D structure
for robust generation, as well as a method for ensuring semantic consistency
throughout all viewpoints of the scene. Our framework surpasses the limitations
of prior arts, and has significant implications for 3D consistent generation of
2D diffusion models.
","[{'version': 'v1', 'created': 'Tue, 14 Mar 2023 14:24:31 GMT'}, {'version': 'v2', 'created': 'Wed, 15 Mar 2023 12:30:47 GMT'}, {'version': 'v3', 'created': 'Thu, 16 Mar 2023 09:03:42 GMT'}]",2023-03-17,['Computer Vision and Pattern Recognition'],This paper presents a novel approach to robust text-to-3D generation using a 2D diffusion model that is aware of 3D consistency. We propose a novel approach to text-to-3D generation that uses a 2D diffusion model to learn 3D consistency from a set of 3D objects. The model is trained on a dataset of 3D objects and then used to generate 3D shapes from text descriptions. The proposed approach is evaluated on a set of 3D objects from different categories and is shown to generate more accurate 3D shapes than existing methods. The results demonstrate that our approach can generate 3D shapes with a higher degree of accuracy and consistency than existing methods. This paper provides a novel approach to text-to-3D generation and is a valuable contribution to the field of computer vision and pattern recognition.,"Write an abstract for a paper called Let 2D Diffusion Model Know 3D-Consistency for Robust Text-to-3D
  Generation about Computer Vision and Pattern Recognition"
2209.12235,"Carlos Mu\~noz Moncayo, Manuel Quezada de Luna, David I. Ketcheson","A Comparative Study of Iterative Riemann Solvers for the Shallow Water
  and Euler Equations","['math.NA', 'cs.NA']","  The Riemann problem for first-order hyperbolic systems of partial
differential equations is of fundamental importance for both theoretical and
numerical purposes. Many approximate solvers have been developed for such
systems; exact solution algorithms have received less attention because
computation of the exact solution typically requires iterative solution of
algebraic equations. Iterative algorithms may be less computationally efficient
or might fail to converge in some cases. We investigate the achievable
efficiency of robust iterative Riemann solvers for relatively simple systems,
focusing on the shallow water and Euler equations. We consider a range of
initial guesses and iterative schemes applied to an ensemble of test Riemann
problems. For the shallow water equations, we find that Newton's method with a
simple modification converges quickly and reliably. For the Euler equations we
obtain similar results; however, when the required precision is high, a
combination of Ostrowski and Newton iterations converges faster. These solvers
are slower than standard approximate solvers like Roe and HLLE, but come within
a factor of two in speed. We also provide a preliminary comparison of the
accuracy of a finite volume discretization using an exact solver versus
standard approximate solvers.
","[{'version': 'v1', 'created': 'Sun, 25 Sep 2022 14:45:09 GMT'}]",2022-09-27,['Numerical Analysis'],"This paper presents a comparative study of iterative Riemann solvers for the shallow water and Euler equations in numerical analysis. The shallow water equations are a set of equations used to describe the motion of shallow water flows, while the Euler equations are used to describe the motion of compressible fluids. The paper will investigate the performance of several different iterative Riemann solvers for both equations, including the HLLC, HLL, and Roe solvers. The accuracy and computational efficiency of the solvers will be compared and discussed. The results of the study will provide insight into the best solver for each equation, as well as provide a basis for future research in numerical analysis.","Write an abstract for a paper called A Comparative Study of Iterative Riemann Solvers for the Shallow Water
  and Euler Equations about Numerical Analysis"
1003.1827,"Vidhi Rawat, Alok Jain, Vibhakar Shrimali",Investigation and Assessment of Disorder of Ultrasound B-mode Images,['cs.CV'],"  Digital image plays a vital role in the early detection of cancers, such as
prostate cancer, breast cancer, lungs cancer, cervical cancer. Ultrasound
imaging method is also suitable for early detection of the abnormality of
fetus. The accurate detection of region of interest in ultrasound image is
crucial. Since the result of reflection, refraction and deflection of
ultrasound waves from different types of tissues with different acoustic
impedance. Usually, the contrast in ultrasound image is very low and weak edges
make the image difficult to identify the fetus region in the ultrasound image.
So the analysis of ultrasound image is more challenging one. We try to develop
a new algorithmic approach to solve the problem of non clarity and find
disorder of it. Generally there is no common enhancement approach for noise
reduction. This paper proposes different filtering techniques based on
statistical methods for the removal of various noise. The quality of the
enhanced images is measured by the statistical quantity measures:
Signal-to-Noise Ratio (SNR), Peak Signal-to-Noise Ratio (PSNR), and Root Mean
Square Error (RMSE).
","[{'version': 'v1', 'created': 'Tue, 9 Mar 2010 08:13:37 GMT'}]",2022-04-21,['Computer Vision and Pattern Recognition'],"This paper presents an investigation and assessment of a disorder of ultrasound B-mode images, which is a challenging task in computer vision and pattern recognition. We propose a novel approach to address this problem by exploiting the spatial characteristics of ultrasound B-mode images. First, we extract the local features of the ultrasound B-mode images using a convolutional neural network. Then, we use a support vector machine to classify the images as normal or abnormal. Finally, we evaluate the performance of our proposed approach by comparing it with other state-of-the-art methods. The results show that our proposed approach achieves higher accuracy in detecting the disorder of ultrasound B-mode images. The proposed approach can be used to assist clinicians in the diagnosis of ultrasound B-mode images.",Write an abstract for a paper called Investigation and Assessment of Disorder of Ultrasound B-mode Images about Computer Vision and Pattern Recognition
2107.05821,"Chenqi Kong, Baoliang Chen, Haoliang Li, Shiqi Wang, Anderson Rocha,
  and Sam Kwong","Detect and Locate: Exposing Face Manipulation by Semantic- and
  Noise-level Telltales",['cs.CV'],"  The technological advancements of deep learning have enabled sophisticated
face manipulation schemes, raising severe trust issues and security concerns in
modern society. Generally speaking, detecting manipulated faces and locating
the potentially altered regions are challenging tasks. Herein, we propose a
conceptually simple but effective method to efficiently detect forged faces in
an image while simultaneously locating the manipulated regions. The proposed
scheme relies on a segmentation map that delivers meaningful high-level
semantic information clues about the image. Furthermore, a noise map is
estimated, playing a complementary role in capturing low-level clues and
subsequently empowering decision-making. Finally, the features from these two
modules are combined to distinguish fake faces. Extensive experiments show that
the proposed model achieves state-of-the-art detection accuracy and remarkable
localization performance.
","[{'version': 'v1', 'created': 'Tue, 13 Jul 2021 02:59:31 GMT'}, {'version': 'v2', 'created': 'Wed, 6 Apr 2022 11:04:24 GMT'}]",2022-04-07,['Computer Vision and Pattern Recognition'],"This paper presents a novel approach to detecting and locating face manipulation in digital images using computer vision and pattern recognition techniques. We propose a method that combines semantic-level and noise-level telltales to detect and locate face manipulation in digital images. The proposed method leverages the semantic information of the digital image to detect facial features and the noise-level telltales to locate the manipulated facial regions. We evaluate our method on a variety of datasets, including real-world and synthesized facial images. The results demonstrate that our method has superior performance in detecting and locating face manipulations compared to existing state-of-the-art methods. Additionally, we provide insights into the effects of different types of facial manipulations on the detection and localization performance of our method.","Write an abstract for a paper called Detect and Locate: Exposing Face Manipulation by Semantic- and
  Noise-level Telltales about Computer Vision and Pattern Recognition"
2207.05684,"Tamon Nakano, Alessandro Michele Bucci, Jean-Marc Gratien, Thibault
  Faney, Guillaume Charpiat","Machine Learning model for gas-liquid interface reconstruction in CFD
  numerical simulations","['physics.flu-dyn', 'cs.LG', 'physics.comp-ph']","  The volume of fluid (VoF) method is widely used in multi-phase flow
simulations to track and locate the interface between two immiscible fluids. A
major bottleneck of the VoF method is the interface reconstruction step due to
its high computational cost and low accuracy on unstructured grids. We propose
a machine learning enhanced VoF method based on Graph Neural Networks (GNN) to
accelerate the interface reconstruction on general unstructured meshes. We
first develop a methodology to generate a synthetic dataset based on paraboloid
surfaces discretized on unstructured meshes. We then train a GNN based model
and perform generalization tests. Our results demonstrate the efficiency of a
GNN based approach for interface reconstruction in multi-phase flow simulations
in the industrial context.
","[{'version': 'v1', 'created': 'Tue, 12 Jul 2022 17:07:46 GMT'}]",2022-07-13,['Machine Learning'],This paper presents a Machine Learning-based model for reconstructing the gas-liquid interface in Computational Fluid Dynamics (CFD) numerical simulations. The model is based on a convolutional neural network (CNN) trained on a dataset of simulated gas-liquid interface images. The CNN is used to generate a high-resolution interface reconstruction from a low-resolution input image. The model is evaluated on a range of benchmark datasets and compared to existing methods. Results show that the proposed model outperforms existing methods in terms of accuracy and is able to generate high-resolution reconstructions with fewer parameters. The paper also discusses potential applications of the proposed model in engineering and scientific research.,"Write an abstract for a paper called Machine Learning model for gas-liquid interface reconstruction in CFD
  numerical simulations about Machine Learning"
2207.07894,"Muhammad Abdullah Jamal, Omid Mohareri","Multi-Modal Unsupervised Pre-Training for Surgical Operating Room
  Workflow Analysis",['cs.CV'],"  Data-driven approaches to assist operating room (OR) workflow analysis depend
on large curated datasets that are time consuming and expensive to collect. On
the other hand, we see a recent paradigm shift from supervised learning to
self-supervised and/or unsupervised learning approaches that can learn
representations from unlabeled datasets. In this paper, we leverage the
unlabeled data captured in robotic surgery ORs and propose a novel way to fuse
the multi-modal data for a single video frame or image. Instead of producing
different augmentations (or 'views') of the same image or video frame which is
a common practice in self-supervised learning, we treat the multi-modal data as
different views to train the model in an unsupervised manner via clustering. We
compared our method with other state of the art methods and results show the
superior performance of our approach on surgical video activity recognition and
semantic segmentation.
","[{'version': 'v1', 'created': 'Sat, 16 Jul 2022 10:32:27 GMT'}]",2022-07-19,['Computer Vision and Pattern Recognition'],"This paper proposes a novel approach to surgical operating room workflow analysis using computer vision and pattern recognition. We introduce a multi-modal unsupervised pre-training approach which combines visual and audio data to enable a deep learning system to better understand the complex workflow of a surgical operating room. The proposed method is evaluated on a public dataset, showing that it outperforms traditional supervised methods in terms of accuracy and computational efficiency. We also discuss the potential applications of our method in medical imaging and healthcare. Finally, we provide insights into how our proposed approach can be extended to other domains.","Write an abstract for a paper called Multi-Modal Unsupervised Pre-Training for Surgical Operating Room
  Workflow Analysis about Computer Vision and Pattern Recognition"
2203.1576,"Shashank Shekhar, Sheetal Kalyani","A New Expression for the Product of Two $\kappa-\mu $ Shadowed Random
  Variables and its Application to Wireless Communication","['cs.IT', 'math.IT']","  In this work, the product of two independent and non-identically distributed
(i.n.i.d) $\kappa - \mu $ shadowed random variables is studied. We derive the
series expression for the probability density function (PDF), cumulative
distribution function (CDF), and moment generating function (MGF) of the
product of two (i.n.i.d) $\kappa - \mu $ shadowed random variables. The derived
formulation in this work is quite general as they incorporate most of the
typically used fading channels. As an application example, outage probability
(OP) has been derived for cascaded wireless systems and relay-assisted
communications with a variable gain relay. Extensive Monte-Carlo simulations
have also been carried out.
","[{'version': 'v1', 'created': 'Tue, 29 Mar 2022 17:14:47 GMT'}]",2022-03-30,['Information Theory'],This paper presents a new expression for the product of two $\kappa-\mu $ shadowed random variables and its application to wireless communication in information theory. The expression is derived using a new approach based on the concept of shadowing. The derived expression is used to analyze the performance of a wireless communication system in terms of the average received signal-to-noise ratio. Simulation results demonstrate that the proposed expression provides a more accurate description of the system performance compared to existing methods. The results of this paper can be used to improve the design and optimization of wireless communication systems in information theory.,"Write an abstract for a paper called A New Expression for the Product of Two $\kappa-\mu $ Shadowed Random
  Variables and its Application to Wireless Communication about Information Theory"
2202.01891,Sijin Yeom and Jae-Hun Jung,Weighted Random Cut Forest Algorithm for Anomaly Detection,['cs.LG'],"  Random cut forest (RCF) algorithms have been developed for anomaly detection,
particularly for anomaly detection in time-series data. The RCF algorithm is
the improved version of the isolation forest algorithm. Unlike the isolation
forest algorithm, the RCF algorithm has the power of determining whether the
real-time input has anomaly by inserting the input in the constructed tree
network. There have been developed various RCF algorithms including Robust RCF
(RRCF) with which the cutting procedure is adaptively chosen probabilistically.
The RRCF algorithm shows better performance than the isolation forest as the
cutting dimension is decided based on the geometric range of the data. The
overall data structure is, however, not considered in the adaptive cutting
algorithm with the RRCF algorithm. In this paper, we propose a new RCF
algorithm, so-called the weighted RCF (WRCF) algorithm. In order to introduce
the WRCF, we first introduce a new geometric measure, a density measure which
is crucial for the construction of the WRCF. We provide various mathematical
properties of the density measure. The proposed WRCF also cuts the tree network
adaptively, but with the consideration of the denseness of the data. The
proposed algorithm is more efficient when the data is non-uniformly structured
and achieves the desired anomaly scores more rapidly than the RRCF. We provide
theorems that prove our claims with numerical examples.
","[{'version': 'v1', 'created': 'Tue, 1 Feb 2022 18:50:32 GMT'}, {'version': 'v2', 'created': 'Thu, 3 Mar 2022 16:16:40 GMT'}, {'version': 'v3', 'created': 'Fri, 4 Mar 2022 14:46:43 GMT'}, {'version': 'v4', 'created': 'Sun, 13 Mar 2022 06:06:59 GMT'}]",2022-03-15,['Machine Learning'],"This paper presents the Weighted Random Cut Forest (WRCF) algorithm, a novel machine learning technique for anomaly detection. The WRCF algorithm takes advantage of the Random Cut Tree (RCT) structure to create a forest of trees, each of which is built on a weighted dataset. The WRCF algorithm is then used to detect anomalies in the data by calculating the anomaly score of each data point, which is based on the number of trees in which the data point appears. Experiments are conducted on a variety of datasets to demonstrate the effectiveness of the WRCF algorithm for anomaly detection. Results show that the WRCF algorithm outperforms other machine learning techniques for anomaly detection in terms of accuracy and speed.",Write an abstract for a paper called Weighted Random Cut Forest Algorithm for Anomaly Detection about Machine Learning
2106.14365,"Alina Arseniev-Koehler, Susan D. Cochran, Vickie M. Mays, Kai-Wei
  Chang, Jacob Gates Foster","Integrating topic modeling and word embedding to characterize violent
  deaths","['cs.CL', 'cs.CY', 'cs.LG']","  There is an escalating need for methods to identify latent patterns in text
data from many domains. We introduce a new method to identify topics in a
corpus and represent documents as topic sequences. Discourse Atom Topic
Modeling draws on advances in theoretical machine learning to integrate topic
modeling and word embedding, capitalizing on the distinct capabilities of each.
We first identify a set of vectors (""discourse atoms"") that provide a sparse
representation of an embedding space. Atom vectors can be interpreted as latent
topics: Through a generative model, atoms map onto distributions over words;
one can also infer the topic that generated a sequence of words. We illustrate
our method with a prominent example of underutilized text: the U.S. National
Violent Death Reporting System (NVDRS). The NVDRS summarizes violent death
incidents with structured variables and unstructured narratives. We identify
225 latent topics in the narratives (e.g., preparation for death and physical
aggression); many of these topics are not captured by existing structured
variables. Motivated by known patterns in suicide and homicide by gender, and
recent research on gender biases in semantic space, we identify the gender bias
of our topics (e.g., a topic about pain medication is feminine). We then
compare the gender bias of topics to their prevalence in narratives of female
versus male victims. Results provide a detailed quantitative picture of
reporting about lethal violence and its gendered nature. Our method offers a
flexible and broadly applicable approach to model topics in text data.
","[{'version': 'v1', 'created': 'Mon, 28 Jun 2021 01:53:20 GMT'}]",2022-10-06,"['Computation and Language', 'Computers and Society', 'Machine Learning']","This paper proposes a novel approach to characterize violent deaths using a combination of topic modeling and word embedding. Topic modeling is used to identify the topics of violent deaths, while word embedding is used to capture the semantic relationships between words in the text. We apply the proposed approach to a corpus of news articles related to violent deaths from the United States. Results demonstrate that the proposed approach is able to accurately classify the topics of violent deaths and capture the semantic relationships between words in the text. The proposed approach is expected to be useful for understanding violent deaths in a more comprehensive manner and to aid in the development of better policies and interventions. Furthermore, our approach is applicable to a wide range of domains, such as Computation and Language, Computers and Society, and Machine Learning.","Write an abstract for a paper called Integrating topic modeling and word embedding to characterize violent
  deaths about Computation and Language, Computers and Society, Machine Learning"
2205.03214,"Xinyuan Jiang, Yan Li, and Daning Huang","Modularized Bilinear Koopman Operator for Modeling and Predicting
  Transients of Microgrids","['eess.SY', 'cs.SY']","  Modularized Koopman Bilinear Form (M-KBF) is presented to model and predict
the transient dynamics of microgrids in the presence of disturbances. As a
scalable data-driven approach, M-KBF divides the identification and prediction
of the high-dimensional nonlinear system into the individual study of
subsystems; and thus, alleviating the difficulty of intensively handling high
volume data and overcoming the curse of dimensionality. For each subsystem,
Koopman bilinear form is applied to efficiently identify its model by
developing eigenfunctions via the extended dynamic mode decomposition method
with an eigenvalue-based order truncation. Extensive tests show that M-KBF can
provide accurate transient dynamics prediction for the nonlinear microgrids and
verify the plug-and-play modeling and prediction function, which offers a
potent tool for identifying high-dimensional systems. The modularity feature of
M-KBF enables the provision of fast and precise prediction for the microgrid
operation and control, paving the way towards online applications.
","[{'version': 'v1', 'created': 'Fri, 6 May 2022 13:46:00 GMT'}, {'version': 'v2', 'created': 'Tue, 17 May 2022 20:10:33 GMT'}]",2022-05-19,['Systems and Control'],"This paper presents a novel approach to modeling and predicting transients of microgrids using a modularized bilinear Koopman operator. The proposed approach is based on a Koopman operator that is derived from a linearized model of the microgrid and is then modularized to account for the nonlinear dynamics of the system. The modularized bilinear Koopman operator is used to obtain the linear dynamics of the microgrid and to predict transient behavior. Additionally, a Lyapunov-Krasovskii functional is used to ensure the stability of the system. Simulation results are presented to demonstrate the effectiveness of the proposed approach in modeling and predicting transients of microgrids. Finally, the paper concludes with a discussion of the implications of the proposed approach for systems and control.","Write an abstract for a paper called Modularized Bilinear Koopman Operator for Modeling and Predicting
  Transients of Microgrids about Systems and Control"
2203.01286,"Ishaan Mehta, Hao-Ya Hsueh, Nikolaos Kourtzanidis, Mateusz Brylka and
  Sajad Saeedi",Far-UVC Disinfection with Robotic Mobile Manipulator,['cs.RO'],"  The COVID-19 pandemic has demonstrated the need for a more effective and
efficient disinfection approach to combat infectious diseases. Ultraviolet
germicidal irradiation (UVGI) is a proven mean for disinfection and
sterilization and has been integrated into handheld devices and autonomous
mobile robots. Existing UVGI robots which are commonly equipped with uncovered
lamps that emit intense ultraviolet radiation suffer from: inability to be used
in human presence, shadowing of objects, and long disinfection time. These
robots also have a high operational cost. This paper introduces a
cost-effective germicidal system that utilizes UVGI to disinfect pathogens,
such as viruses, bacteria, and fungi, on high contact surfaces (e.g. doors and
tables). This system is composed of a team of 5-DOF mobile manipulators with
end-effectors that are equipped with far-UVC excimer lamps. The design of the
system is discussed with emphasis on path planning, coverage planning, and
scene understanding. Evaluations of the UVGI system using simulations and
irradiance models are also included.
","[{'version': 'v1', 'created': 'Wed, 2 Mar 2022 18:12:24 GMT'}]",2022-03-03,['Robotics'],"This paper presents a robotic mobile manipulator system for disinfecting large areas with Far-UVC light. Far-UVC light is a type of ultraviolet light with a wavelength of 222 nm that has been shown to be effective at killing bacteria and viruses without causing skin damage. The robotic mobile manipulator system is designed to move around large areas and disinfect them with Far-UVC light. The system consists of a robotic arm mounted on a mobile base, a Far-UVC light source, and a computer vision system for locating objects. The robotic arm is used to position the Far-UVC light source and disinfect the area. The computer vision system is used to detect objects in the area and ensure that the Far-UVC light is focused on the desired area. The system is tested in a simulated hospital environment and is shown to effectively disinfect the area. The results of this paper demonstrate the potential of using robotic mobile manipulators for disinfecting large areas with Far-UVC light.",Write an abstract for a paper called Far-UVC Disinfection with Robotic Mobile Manipulator about Robotics
2210.13274,"Ana Budisa, Xiaozhe Hu, Miroslav Kuchta, Kent-Andre Mardal, Ludmil
  Zikatanov",HAZniCS -- Software Components for Multiphysics Problems,"['math.NA', 'cs.NA']","  We introduce the software toolbox HAZniCS for solving interface-coupled
multiphysics problems. HAZniCS is a suite of modules that combines the
well-known FEniCS framework for finite element discretization with solver and
graph library HAZmath. The focus of the paper is on the design and
implementation of a pool of robust and efficient solver algorithms which tackle
issues related to the complex interfacial coupling of the physical problems
often encountered in applications in brain biomechanics. The robustness and
efficiency of the numerical algorithms and methods is shown in several
numerical examples, namely the Darcy-Stokes equations that model flow of
cerebrospinal fluid in the human brain and the mixed-dimensional model of
electrodiffusion in the brain tissue.
","[{'version': 'v1', 'created': 'Mon, 24 Oct 2022 14:12:52 GMT'}, {'version': 'v2', 'created': 'Sun, 6 Nov 2022 12:20:34 GMT'}]",2022-11-08,['Numerical Analysis'],"This paper presents HAZniCS, a software package designed to enable numerical analysis of multiphysics problems. The package provides a comprehensive set of components for solving multiphysics problems, including a library of numerical algorithms, a graphical user interface, and a set of tools for visualizing and analyzing results. The paper describes the design of the package, its features, and how it can be used to solve a variety of multiphysics problems. It also presents the results of several numerical experiments that demonstrate the effectiveness of the package. Finally, the paper discusses the potential implications of the package for the field of numerical analysis.",Write an abstract for a paper called HAZniCS -- Software Components for Multiphysics Problems about Numerical Analysis
2201.10522,Zhengrong Xue,Blind Image Deblurring: a Review,['cs.CV'],"  This is a review on blind image deblurring. First, we formulate the blind
image deblurring problem and explain why it is challenging. Next, we bring some
psychological and cognitive studies on the way our human vision system deblurs.
Then, relying on several previous reviews, we discuss the topic of metrics and
datasets, which is non-trivial to blind deblurring. Finally, we introduce some
typical optimization-based methods and learning-based methods.
","[{'version': 'v1', 'created': 'Sat, 22 Jan 2022 08:21:12 GMT'}]",2022-01-26,['Computer Vision and Pattern Recognition'],"This paper reviews the state-of-the-art methods used in blind image deblurring, a task in computer vision and pattern recognition. It covers the most recent advances in blind image deblurring, including the development of deep learning-based methods. The paper also discusses the challenges associated with blind image deblurring and provides an overview of the current state of the art. Finally, the paper provides a comprehensive review of the literature and suggests potential research directions.",Write an abstract for a paper called Blind Image Deblurring: a Review about Computer Vision and Pattern Recognition
2209.04944,"Nicholas Kashani Motlagh, Jim Davis, Tim Anderson, Jeremy Gwinnup","Learning When to Say ""I Don't Know""","['cs.CV', 'cs.LG']","  We propose a new Reject Option Classification technique to identify and
remove regions of uncertainty in the decision space for a given neural
classifier and dataset. Such existing formulations employ a learned rejection
(remove)/selection (keep) function and require either a known cost for
rejecting examples or strong constraints on the accuracy or coverage of the
selected examples. We consider an alternative formulation by instead analyzing
the complementary reject region and employing a validation set to learn
per-class softmax thresholds. The goal is to maximize the accuracy of the
selected examples subject to a natural randomness allowance on the rejected
examples (rejecting more incorrect than correct predictions). We provide
results showing the benefits of the proposed method over na\""ively thresholding
calibrated/uncalibrated softmax scores with 2-D points, imagery, and text
classification datasets using state-of-the-art pretrained models. Source code
is available at https://github.com/osu-cvl/learning-idk.
","[{'version': 'v1', 'created': 'Sun, 11 Sep 2022 21:50:03 GMT'}, {'version': 'v2', 'created': 'Wed, 15 Feb 2023 16:30:29 GMT'}]",2023-02-16,"['Computer Vision and Pattern Recognition', 'Machine Learning']","This paper explores the implications of learning when to say ""I don't know"" in the context of Computer Vision and Pattern Recognition, Machine Learning. The paper will discuss the challenges faced when trying to teach computers when to say ""I don't know"" and how this can be addressed. It will also discuss the potential benefits of learning when to say ""I don't know"" and how it can help improve the accuracy of computer vision and pattern recognition, machine learning. Finally, the paper will discuss the implications of learning when to say ""I don't know"" in the context of safety and security.","Write an abstract for a paper called Learning When to Say ""I Don't Know"" about Computer Vision and Pattern Recognition, Machine Learning"
2210.00979,Amy K. Strong and Ethan J. LoCicero and Leila Bridgeman,Dissipative Imitation Learning for Robust Dynamic Output Feedback,"['eess.SY', 'cs.SY']","  Robust imitation learning seeks to mimic expert controller behavior while
ensuring stability, but current methods require accurate plant models. Here,
robust imitation learning is addressed for stabilizing poorly modeled plants
with linear dynamic output feedback. Open-loop input-output properties are used
to characterize an uncertain plant, and the feedback matrix of the dynamic
controller is learned while enforcing stability through the controller's
open-loop QSR-dissipativity properties. The imitation learning method is
applied to two systems with parametric uncertainty.
","[{'version': 'v1', 'created': 'Mon, 3 Oct 2022 14:46:30 GMT'}]",2022-10-04,['Systems and Control'],"This paper presents a novel dissipative imitation learning (DIL) approach for robust dynamic output feedback control of uncertain nonlinear systems. The proposed approach is based on a combination of online learning and dissipative control, which enables the system to adapt to uncertain and changing environments. The DIL approach is used to learn a control policy from system trajectories and to update the control policy in real-time. The proposed approach is evaluated using a simulated nonlinear system with uncertainty. Results show that the proposed approach is able to robustly handle system uncertainties and provide better performance than existing approaches. The results also demonstrate that the proposed method is able to maintain stability even in the presence of model uncertainty.",Write an abstract for a paper called Dissipative Imitation Learning for Robust Dynamic Output Feedback about Systems and Control
2112.12572,"Valentin Vielzeuf, Grigory Antipov",Are E2E ASR models ready for an industrial usage?,"['eess.AS', 'cs.AI', 'cs.CL', 'cs.SD']","  The Automated Speech Recognition (ASR) community experiences a major turning
point with the rise of the fully-neural (End-to-End, E2E) approaches. At the
same time, the conventional hybrid model remains the standard choice for the
practical usage of ASR. According to previous studies, the adoption of E2E ASR
in real-world applications was hindered by two main limitations: their ability
to generalize on unseen domains and their high operational cost. In this paper,
we investigate both above-mentioned drawbacks by performing a comprehensive
multi-domain benchmark of several contemporary E2E models and a hybrid
baseline. Our experiments demonstrate that E2E models are viable alternatives
for the hybrid approach, and even outperform the baseline both in accuracy and
in operational efficiency. As a result, our study shows that the generalization
and complexity issues are no longer the major obstacle for industrial
integration, and draws the community's attention to other potential limitations
of the E2E approaches in some specific use-cases.
","[{'version': 'v1', 'created': 'Thu, 9 Dec 2021 09:28:05 GMT'}, {'version': 'v2', 'created': 'Fri, 21 Oct 2022 09:08:44 GMT'}]",2022-10-24,"['Artificial Intelligence', 'Computation and Language', 'Sound']","This paper examines the feasibility of utilizing end-to-end (E2E) automatic speech recognition (ASR) models in an industrial setting. We survey the current state of the art in E2E ASR models and discuss the challenges associated with their deployment in industrial environments. We also present an empirical evaluation of the performance of E2E ASR models on real-world data and compare it to traditional ASR models. Finally, we discuss the implications of our findings and provide insights into the potential of E2E ASR models for industrial applications.","Write an abstract for a paper called Are E2E ASR models ready for an industrial usage? about Artificial Intelligence, Computation and Language, Sound"
2210.13131,"Gustav Eriksson, Jonatan Werpers, David Niemel\""a, Niklas Wik, Valter
  Zethrin, Ken Mattsson","Boundary and interface methods for energy stable finite difference
  discretizations of the dynamic beam equation","['math.NA', 'cs.NA']","  We consider energy stable summation by parts finite difference methods
(SBP-FD) for the homogeneous and piecewise homogeneous dynamic beam equation
(DBE). Previously the constant coefficient problem has been solved with SBP-FD
together with penalty terms (SBP-SAT) to impose boundary conditions. In this
work we revisit this problem and compare SBP-SAT to the projection method
(SBP-P). We also consider the DBE with discontinuous coefficients and present
novel SBP-SAT, SBP-P and hybrid SBP-SAT-P discretizations for imposing
interface conditions. Numerical experiments show that all methods considered
are similar in terms of accuracy, but that SBP-P can be more computationally
efficient (less restrictive time step requirement for explicit time integration
methods) for both the constant and piecewise constant coefficient problems.
","[{'version': 'v1', 'created': 'Mon, 24 Oct 2022 11:32:50 GMT'}]",2023-01-25,['Numerical Analysis'],"This paper describes the development of a numerical analysis method for the dynamic beam equation. The method is based on boundary and interface methods, which are energy stable finite difference discretizations. The goal of the paper is to develop a numerical approach that can be used to accurately and efficiently compute the displacement, velocity, and acceleration of a beam. The paper also discusses the stability of the numerical approach, and how it can be used to accurately capture the dynamic behaviour of the beam. The paper concludes by providing an analysis of the numerical results, and discussing the implications for future numerical analysis of dynamic beam equations.","Write an abstract for a paper called Boundary and interface methods for energy stable finite difference
  discretizations of the dynamic beam equation about Numerical Analysis"
2106.07992,"Cheng Feng, Pengwei Tian","Time Series Anomaly Detection for Cyber-Physical Systems via Neural
  System Identification and Bayesian Filtering","['cs.LG', 'cs.CR', 'stat.ML']","  Recent advances in AIoT technologies have led to an increasing popularity of
utilizing machine learning algorithms to detect operational failures for
cyber-physical systems (CPS). In its basic form, an anomaly detection module
monitors the sensor measurements and actuator states from the physical plant,
and detects anomalies in these measurements to identify abnormal operation
status. Nevertheless, building effective anomaly detection models for CPS is
rather challenging as the model has to accurately detect anomalies in presence
of highly complicated system dynamics and unknown amount of sensor noise. In
this work, we propose a novel time series anomaly detection method called
Neural System Identification and Bayesian Filtering (NSIBF) in which a
specially crafted neural network architecture is posed for system
identification, i.e., capturing the dynamics of CPS in a dynamical state-space
model; then a Bayesian filtering algorithm is naturally applied on top of the
""identified"" state-space model for robust anomaly detection by tracking the
uncertainty of the hidden state of the system recursively over time. We provide
qualitative as well as quantitative experiments with the proposed method on a
synthetic and three real-world CPS datasets, showing that NSIBF compares
favorably to the state-of-the-art methods with considerable improvements on
anomaly detection in CPS.
","[{'version': 'v1', 'created': 'Tue, 15 Jun 2021 09:11:35 GMT'}, {'version': 'v2', 'created': 'Fri, 7 Jan 2022 02:51:39 GMT'}]",2022-01-10,"['Machine Learning', 'Cryptography and Security']","This paper proposes a novel approach to time series anomaly detection for cyber-physical systems using neural system identification and Bayesian filtering. The proposed method combines machine learning and cryptography to detect anomalies in time series data generated by cyber-physical systems. The proposed approach utilizes a neural network to accurately identify the system dynamics, and then uses Bayesian filtering to detect anomalies. The paper evaluates the proposed method using real-world datasets and compares its performance with existing approaches. Results show that the proposed method outperforms existing approaches in terms of accuracy and robustness. The paper also provides insights into the security implications of the proposed approach, and discusses potential future research directions.","Write an abstract for a paper called Time Series Anomaly Detection for Cyber-Physical Systems via Neural
  System Identification and Bayesian Filtering about Machine Learning, Cryptography and Security"
2202.08506,"Beihao Xia, Conghao Wong, Qinmu Peng, Wei Yuan, and Xinge You","CSCNet: Contextual Semantic Consistency Network for Trajectory
  Prediction in Crowded Spaces","['cs.CV', 'cs.AI']","  Trajectory prediction aims to predict the movement trend of the agents like
pedestrians, bikers, vehicles. It is helpful to analyze and understand human
activities in crowded spaces and widely applied in many areas such as
surveillance video analysis and autonomous driving systems. Thanks to the
success of deep learning, trajectory prediction has made significant progress.
The current methods are dedicated to studying the agents' future trajectories
under the social interaction and the sceneries' physical constraints. Moreover,
how to deal with these factors still catches researchers' attention. However,
they ignore the \textbf{Semantic Shift Phenomenon} when modeling these
interactions in various prediction sceneries. There exist several kinds of
semantic deviations inner or between social and physical interactions, which we
call the ""\textbf{Gap}"". In this paper, we propose a \textbf{C}ontextual
\textbf{S}emantic \textbf{C}onsistency \textbf{Net}work (\textbf{CSCNet}) to
predict agents' future activities with powerful and efficient context
constraints. We utilize a well-designed context-aware transfer to obtain the
intermediate representations from the scene images and trajectories. Then we
eliminate the differences between social and physical interactions by aligning
activity semantics and scene semantics to cross the Gap. Experiments
demonstrate that CSCNet performs better than most of the current methods
quantitatively and qualitatively.
","[{'version': 'v1', 'created': 'Thu, 17 Feb 2022 08:20:58 GMT'}]",2022-02-18,"['Computer Vision and Pattern Recognition', 'Artificial Intelligence']",CSCNet: Contextual Semantic Consistency Network for Trajectory Prediction in Crowded Spaces is a paper that proposes a novel deep learning-based approach for trajectory prediction in crowded spaces. This paper presents a Contextual Semantic Consistency Network (CSCNet) that combines the advantages of an encoder-decoder architecture and a graph convolutional network to capture the contextual information of the scene. The CSCNet is trained using a novel objective function that includes a semantic consistency loss and a trajectory prediction loss. The proposed framework is evaluated on two public datasets and shows competitive performance compared to existing state-of-the-art methods. The results demonstrate the effectiveness of the proposed model for trajectory prediction in crowded spaces.,"Write an abstract for a paper called CSCNet: Contextual Semantic Consistency Network for Trajectory
  Prediction in Crowded Spaces about Computer Vision and Pattern Recognition, Artificial Intelligence"
2211.01236,"Kosio Beshkov, Jonas Verhellen and Mikkel Elle Lepper{\o}d",Isometric Representations in Neural Networks Improve Robustness,"['cs.LG', 'cs.CR', 'q-bio.NC']","  Artificial and biological agents cannon learn given completely random and
unstructured data. The structure of data is encoded in the metric relationships
between data points. In the context of neural networks, neuronal activity
within a layer forms a representation reflecting the transformation that the
layer implements on its inputs. In order to utilize the structure in the data
in a truthful manner, such representations should reflect the input distances
and thus be continuous and isometric. Supporting this statement, recent
findings in neuroscience propose that generalization and robustness are tied to
neural representations being continuously differentiable. In machine learning,
most algorithms lack robustness and are generally thought to rely on aspects of
the data that differ from those that humans use, as is commonly seen in
adversarial attacks. During cross-entropy classification, the metric and
structural properties of network representations are usually broken both
between and within classes. This side effect from training can lead to
instabilities under perturbations near locations where such structure is not
preserved. One of the standard solutions to obtain robustness is to add ad hoc
regularization terms, but to our knowledge, forcing representations to preserve
the metric structure of the input data as a stabilising mechanism has not yet
been studied. In this work, we train neural networks to perform classification
while simultaneously maintaining within-class metric structure, leading to
isometric within-class representations. Such network representations turn out
to be beneficial for accurate and robust inference. By stacking layers with
this property we create a network architecture that facilitates hierarchical
manipulation of internal neural representations. Finally, we verify that
isometric regularization improves the robustness to adversarial attacks on
MNIST.
","[{'version': 'v1', 'created': 'Wed, 2 Nov 2022 16:18:18 GMT'}]",2022-11-03,"['Machine Learning', 'Cryptography and Security']","This paper explores the potential of isometric representations in neural networks to improve robustness in machine learning, cryptography, and security. Isometric representations are a type of data representation that preserves the distances between data points, which can be used to improve the accuracy of machine learning models. Additionally, isometric representations can be used to improve the security of cryptographic systems and increase the robustness of security protocols. This paper will discuss the advantages of isometric representations in these domains, as well as their limitations. Furthermore, this paper will present several case studies to illustrate the effectiveness of isometric representations in improving robustness in machine learning, cryptography, and security. Finally, this paper will provide an outlook on future research directions in this field.","Write an abstract for a paper called Isometric Representations in Neural Networks Improve Robustness about Machine Learning, Cryptography and Security"
2208.12938,"Jinhuan Wang, Pengtao Chen, Xinyao Xu, Jiajing Wu, Meng Shen, Qi Xuan,
  Xiaoniu Yang","TSGN: Transaction Subgraph Networks Assisting Phishing Detection in
  Ethereum",['cs.CR'],"  Due to the decentralized and public nature of the Blockchain ecosystem, the
malicious activities on the Ethereum platform impose immeasurable losses for
the users. Existing phishing scam detection methods mostly rely only on the
analysis of original transaction networks, which is difficult to dig deeply
into the transaction patterns hidden in the network structure of transaction
interaction. In this paper, we propose a \underline{T}ransaction
\underline{S}ub\underline{G}raph \underline{N}etwork (TSGN) based phishing
accounts identification framework for Ethereum. We first extract transaction
subgraphs for target accounts and then expand these subgraphs into
corresponding TSGNs based on the different mapping mechanisms. In order to make
our model incorporate more important information about real transactions, we
encode the transaction attributes into the modeling process of TSGNs, yielding
two variants of TSGN, i.e., Directed-TSGN and Temporal-TSGN, which can be
applied to the different attributed networks. Especially, by introducing TSGN
into multi-edge transaction networks, the Multiple-TSGN model proposed is able
to preserve the temporal transaction flow information and capture the
significant topological pattern of phishing scams, while reducing the time
complexity of modeling large-scale networks. Extensive experimental results
show that TSGN models can provide more potential information to improve the
performance of phishing detection by incorporating graph representation
learning.
","[{'version': 'v1', 'created': 'Sat, 27 Aug 2022 06:42:33 GMT'}]",2022-08-30,['Cryptography and Security'],"This paper presents TSGN, an innovative approach to detecting phishing attacks in Ethereum networks using transaction subgraph networks (TSGN). The proposed system is designed to detect phishing attacks by analyzing the transaction patterns of users and identifying suspicious transactions. The system uses a combination of graph-based algorithms and machine learning techniques to detect phishing attacks. Furthermore, the system is designed to detect phishing attacks with a high degree of accuracy and speed. The system is evaluated on a real-world Ethereum network, and the results demonstrate that it is capable of detecting phishing attacks with an accuracy of over 95%. The paper also discusses the implications of the proposed system on the security of Ethereum networks and the potential for future research in the field.","Write an abstract for a paper called TSGN: Transaction Subgraph Networks Assisting Phishing Detection in
  Ethereum about Cryptography and Security"
2301.02196,Luis Sacouto and Andreas Wichert,"Competitive learning to generate sparse representations for associative
  memory","['q-bio.NC', 'cs.NE']","  One of the most well established brain principles, hebbian learning, has led
to the theoretical concept of neural assemblies. Based on it, many interesting
brain theories have spawned. Palm's work implements this concept through binary
associative memory, in a model that not only has a wide cognitive explanatory
power but also makes neuroscientific predictions. Yet, associative memory can
only work with logarithmic sparse representations, which makes it extremely
difficult to apply the model to real data. We propose a biologically plausible
network that encodes images into codes that are suitable for associative
memory. It is organized into groups of neurons that specialize on local
receptive fields, and learn through a competitive scheme. After conducting
auto- and hetero-association experiments on two visual data sets, we can
conclude that our network not only beats sparse coding baselines, but also that
it comes close to the performance achieved using optimal random codes.
","[{'version': 'v1', 'created': 'Thu, 5 Jan 2023 17:57:52 GMT'}]",2023-01-06,['Neural and Evolutionary Computing'],"This paper explores the use of competitive learning to generate sparse representations for associative memory, with a focus on neural and evolutionary computing. We analyze the advantages and disadvantages of competitive learning, and discuss how it can be used to improve associative memory. We also examine how competitive learning can be applied to neural networks and evolutionary algorithms, in order to increase the efficiency of associative memory. Finally, we provide an analysis of the results obtained from experiments on competitive learning for associative memory, and discuss future directions for research.","Write an abstract for a paper called Competitive learning to generate sparse representations for associative
  memory about Neural and Evolutionary Computing"
2203.04427,"Yunang Chen, Yue Gao, Nick Ceccio, Rahul Chatterjee, Kassem Fawaz,
  Earlence Fernandes","Experimental Security Analysis of the App Model in Business
  Collaboration Platforms",['cs.CR'],"  Business Collaboration Platforms like Microsoft Teams and Slack enable
teamwork by supporting text chatting and third-party resource integration. A
user can access online file storage, make video calls, and manage a code
repository, all from within the platform, thus making them a hub for sensitive
communication and resources. The key enabler for these productivity features is
a third-party application model. We contribute an experimental security
analysis of this model and the third-party apps. Performing this analysis is
challenging because commercial platforms and their apps are closed-source
systems. Our analysis methodology is to systematically investigate different
types of interactions possible between apps and users. We discover that the
access control model in these systems violates two fundamental security
principles: least privilege and complete mediation. These violations enable a
malicious app to exploit the confidentiality and integrity of user messages and
third-party resources connected to the platform. We construct proof-of-concept
attacks that can: (1) eavesdrop on user messages without having permission to
read those messages; (2) launch fake video calls; (3) automatically merge code
into repositories without user approval or involvement. Finally, we provide an
analysis of countermeasures that systems like Slack and Microsoft Teams can
adopt today.
","[{'version': 'v1', 'created': 'Tue, 8 Mar 2022 22:32:57 GMT'}, {'version': 'v2', 'created': 'Tue, 19 Jul 2022 15:36:38 GMT'}, {'version': 'v3', 'created': 'Sat, 22 Oct 2022 16:41:07 GMT'}]",2022-10-25,['Cryptography and Security'],"This paper presents an experimental security analysis of the application model in business collaboration platforms with a focus on cryptography and security. It investigates the security of the application model and its related components, including authentication and authorization, data protection, and communication security. The paper also examines the security implications of the current application model, and the challenges and opportunities for improving it. It also discusses the security implications of using cryptography and security for business collaboration platforms. Finally, the paper provides recommendations for improving the security of the application model and its related components.","Write an abstract for a paper called Experimental Security Analysis of the App Model in Business
  Collaboration Platforms about Cryptography and Security"
2203.05893,"Hanxing Chi, Baihong Lin, Jun Hu, Liang Wang",DRTAM: Dual Rank-1 Tensor Attention Module,['cs.CV'],"  Recently, attention mechanisms have been extensively investigated in computer
vision, but few of them show excellent performance on both large and mobile
networks. This paper proposes Dual Rank-1 Tensor Attention Module (DRTAM), a
novel residual-attention-learning-guided attention module for feed-forward
convolutional neural networks. Given a 3D feature tensor map, DRTAM firstly
generates three 2D feature descriptors along three axes. Then, using three
descriptors, DRTAM sequentially infers two rank-1 tensor attention maps, the
initial attention map and the complement attention map, combines and multiplied
them to the input feature map for adaptive feature refinement(see Fig.1(c)). To
generate two attention maps, DRTAM introduces rank-1 tensor attention module
(RTAM) and residual descriptors extraction module (RDEM): RTAM divides each 2D
feature descriptors into several chunks, and generate three factor vectors of a
rank-1 tensor attention map by employing strip pooling on each chunk so that
local and long-range contextual information can be captured along three
dimension respectively; RDEM generates three 2D feature descriptors of the
residual feature to produce the complement attention map, using three factor
vectors of the initial attention map and three descriptors of the input
feature. Extensive experimental results on ImageNet-1K, MS COCO and PASCAL VOC
demonstrate that DRTAM achieves competitive performance on both large and
mobile networks compare with other state-of-the-art attention modules.
","[{'version': 'v1', 'created': 'Fri, 11 Mar 2022 12:52:44 GMT'}]",2022-03-14,['Computer Vision and Pattern Recognition'],"This paper introduces DRTAM, a Dual Rank-1 Tensor Attention Module, for Computer Vision and Pattern Recognition tasks. DRTAM is a novel attention mechanism that uses two rank-1 tensors to capture the global context of an input image. It is based on the idea of using two tensors to capture the spatial and channel-wise information of an image. To evaluate the performance of DRTAM, we tested it on two popular image recognition datasets, CIFAR-10 and ImageNet. The results show that DRTAM outperforms other state-of-the-art attention mechanisms and achieves competitive results on both datasets. We also demonstrate the effectiveness of DRTAM on a few downstream tasks such as object detection, semantic segmentation and image captioning. Finally, we discuss the potential of DRTAM in other computer vision tasks.",Write an abstract for a paper called DRTAM: Dual Rank-1 Tensor Attention Module about Computer Vision and Pattern Recognition
2202.09583,Laura Perez-Beltrachini and Mirella Lapata,Models and Datasets for Cross-Lingual Summarisation,['cs.CL'],"  We present a cross-lingual summarisation corpus with long documents in a
source language associated with multi-sentence summaries in a target language.
The corpus covers twelve language pairs and directions for four European
languages, namely Czech, English, French and German, and the methodology for
its creation can be applied to several other languages. We derive cross-lingual
document-summary instances from Wikipedia by combining lead paragraphs and
articles' bodies from language aligned Wikipedia titles. We analyse the
proposed cross-lingual summarisation task with automatic metrics and validate
it with a human study. To illustrate the utility of our dataset we report
experiments with multi-lingual pre-trained models in supervised, zero- and
few-shot, and out-of-domain scenarios.
","[{'version': 'v1', 'created': 'Sat, 19 Feb 2022 11:55:40 GMT'}]",2022-02-22,['Computation and Language'],"This paper presents an overview of existing models and datasets for cross-lingual summarisation in the field of Computation and Language. We discuss the challenges associated with cross-lingual summarisation and the current solutions to these challenges. We then provide a comprehensive review of existing models and datasets for cross-lingual summarisation, including summarisation of monolingual, bilingual, and multilingual documents. We also discuss potential applications of cross-lingual summarisation and how it can be used to improve natural language processing and machine translation. Finally, we provide a summary of our findings and discuss potential future research directions.",Write an abstract for a paper called Models and Datasets for Cross-Lingual Summarisation about Computation and Language
2210.16411,"John Burge, Matthew R. Bonanni, R. Lily Hu, Matthias Ihme","Recurrent Convolutional Deep Neural Networks for Modeling Time-Resolved
  Wildfire Spread Behavior",['cs.LG'],"  The increasing incidence and severity of wildfires underscores the necessity
of accurately predicting their behavior. While high-fidelity models derived
from first principles offer physical accuracy, they are too computationally
expensive for use in real-time fire response. Low-fidelity models sacrifice
some physical accuracy and generalizability via the integration of empirical
measurements, but enable real-time simulations for operational use in fire
response. Machine learning techniques offer the ability to bridge these
objectives by learning first-principles physics while achieving computational
speedup. While deep learning approaches have demonstrated the ability to
predict wildfire propagation over large time periods, time-resolved fire-spread
predictions are needed for active fire management. In this work, we evaluate
the ability of deep learning approaches in accurately modeling the
time-resolved dynamics of wildfires. We use an autoregressive process in which
a convolutional recurrent deep learning model makes predictions that propagate
a wildfire over 15 minute increments. We demonstrate the model in application
to three simulated datasets of increasing complexity, containing both field
fires with homogeneous fuel distribution as well as real-world topologies
sampled from the California region of the United States. We show that even
after 100 autoregressive predictions representing more than 24 hours of
simulated fire spread, the resulting models generate stable and realistic
propagation dynamics, achieving a Jaccard score between 0.89 and 0.94 when
predicting the resulting fire scar.
","[{'version': 'v1', 'created': 'Fri, 28 Oct 2022 21:23:03 GMT'}]",2022-11-01,['Machine Learning'],"This paper presents a novel approach for modeling time-resolved wildfire spread behavior using recurrent convolutional deep neural networks (RCDNNs). Wildfires have become increasingly destructive in recent years, making the accurate prediction of spread behavior essential for effective wildfire management. A RCDNN architecture is proposed to capture the temporal dynamics of wildfire spread, incorporating temporal convolutional layers for feature extraction, recurrent layers for temporal modeling, and a fully connected layer for output prediction. The proposed model is evaluated on a real-world wildfire dataset, achieving an average prediction error of 3.8%. The results demonstrate the potential of RCDNNs to accurately model time-resolved wildfire spread behavior, with implications for improved wildfire management.","Write an abstract for a paper called Recurrent Convolutional Deep Neural Networks for Modeling Time-Resolved
  Wildfire Spread Behavior about Machine Learning"
2204.02843,"Ion Gabriel Ion, Dimitrios Loukrezis, Herbert De Gersem","Tensor train based isogeometric analysis for PDE approximation on
  parameter dependent geometries","['math.NA', 'cs.NA']","  This work develops a numerical solver based on the combination of
isogeometric analysis (IGA) and the tensor train (TT) decomposition for the
approximation of partial differential equations (PDEs) on parameter-dependent
geometries. First, the discrete Galerkin operator as well as the solution for a
fixed geometry configuration are represented as tensors and the TT format is
employed to reduce their computational complexity. Parametric dependencies are
included by considering the parameters that control the geometry configuration
as additional dimensions next to the physical space coordinates. The parameters
are easily incorporated within the TT-IGA solution framework by introducing a
tensor product basis expansion in the parameter space. The discrete Galerkin
operators are accordingly extended to accommodate the parameter dependence,
thus obtaining a single system that includes the parameter dependency. The
system is solved directly in the TT format and a low-rank representation of the
parameter-dependent solution is obtained. The proposed TT-IGA solver is applied
to several test cases which showcase its high computational efficiency and
tremendous compression ratios achieved for representing the parameter-dependent
IGA operators and solutions.
","[{'version': 'v1', 'created': 'Wed, 6 Apr 2022 14:07:17 GMT'}]",2022-10-05,['Numerical Analysis'],This paper introduces a novel numerical analysis method for approximating partial differential equations (PDEs) on parameter dependent geometries using tensor train based isogeometric analysis. The proposed method combines the advantages of tensor train decomposition and isogeometric analysis to achieve a high accuracy approximation of PDEs on parameter dependent geometries. We analyze the accuracy and stability of the proposed method and present numerical experiments to demonstrate its effectiveness. The results of the experiments demonstrate the potential of the proposed method for solving PDEs on parameter dependent geometries.,"Write an abstract for a paper called Tensor train based isogeometric analysis for PDE approximation on
  parameter dependent geometries about Numerical Analysis"
1908.10644,"Luca Calderoni, Dario Maio, Paolo Palmieri",Bloom filter variants for multiple sets: a comparative assessment,['cs.DS'],"  In this paper we compare two probabilistic data structures for association
queries derived from the well-known Bloom filter: the shifting Bloom filter
(ShBF), and the spatial Bloom filter (SBF). With respect to the original data
structure, both variants add the ability to store multiple subsets in the same
filter, using different strategies. We analyse the performance of the two data
structures with respect to false positive probability, and the inter-set error
probability (the probability for an element in the set of being recognised as
belonging to the wrong subset). As part of our analysis, we extended the
functionality of the shifting Bloom filter, optimising the filter for any
non-trivial number of subsets. We propose a new generalised ShBF definition
with applications outside of our specific domain, and present new probability
formulas. Results of the comparison show that the ShBF provides better space
efficiency, but at a significantly higher computational cost than the SBF.
","[{'version': 'v1', 'created': 'Wed, 28 Aug 2019 11:13:03 GMT'}]",2022-05-06,['Data Structures and Algorithms'],"This paper presents a comparative assessment of bloom filter variants for multiple sets. Bloom filters are space-efficient probabilistic data structures used to test membership of an element in a set. The paper evaluates various methods for constructing bloom filters for multiple sets and compares their performance in terms of time and space complexity. The evaluated methods include the basic bloom filter, the double-hashing bloom filter, the cuckoo filter, and the counting bloom filter. Through a comprehensive analysis of their characteristics, this paper provides insights into the advantages and disadvantages of each approach, as well as their suitability for different applications. The results of this paper can be used to inform data structure and algorithm design decisions.",Write an abstract for a paper called Bloom filter variants for multiple sets: a comparative assessment about Data Structures and Algorithms
2302.07444,"Ada Martin, Valerie Chen, S\'ergio Jesus, Pedro Saleiro","A Case Study on Designing Evaluations of ML Explanations with Simulated
  User Studies","['cs.LG', 'cs.HC']","  When conducting user studies to ascertain the usefulness of model
explanations in aiding human decision-making, it is important to use real-world
use cases, data, and users. However, this process can be resource-intensive,
allowing only a limited number of explanation methods to be evaluated.
Simulated user evaluations (SimEvals), which use machine learning models as a
proxy for human users, have been proposed as an intermediate step to select
promising explanation methods. In this work, we conduct the first SimEvals on a
real-world use case to evaluate whether explanations can better support
ML-assisted decision-making in e-commerce fraud detection. We study whether
SimEvals can corroborate findings from a user study conducted in this fraud
detection context. In particular, we find that SimEvals suggest that all
considered explainers are equally performant, and none beat a baseline without
explanations -- this matches the conclusions of the original user study. Such
correspondences between our results and the original user study provide initial
evidence in favor of using SimEvals before running user studies. We also
explore the use of SimEvals as a cheap proxy to explore an alternative user
study set-up. We hope that this work motivates further study of when and how
SimEvals should be used to aid in the design of real-world evaluations.
","[{'version': 'v1', 'created': 'Wed, 15 Feb 2023 03:27:55 GMT'}, {'version': 'v2', 'created': 'Mon, 20 Mar 2023 20:36:48 GMT'}]",2023-03-22,"['Machine Learning', 'Human-Computer Interaction']","This paper presents a case study on designing evaluations of machine learning (ML) explanations with simulated user studies. Specifically, it focuses on the application of ML explanations in human-computer interaction (HCI). The paper begins by providing an overview of the existing literature on ML explanations and their use in HCI. It then examines the process of designing and conducting user studies to evaluate ML explanations. This includes an analysis of the different types of user studies and the factors to consider when designing user studies. The paper then presents the results of the case study, which demonstrate the effectiveness of user studies in evaluating ML explanations. Finally, the paper discusses the implications of the results and provides recommendations for future research.","Write an abstract for a paper called A Case Study on Designing Evaluations of ML Explanations with Simulated
  User Studies about Machine Learning, Human-Computer Interaction"
2111.14331,"Yizhi Yuan, Marcelo G Mattar",Improving Experience Replay with Successor Representation,"['cs.LG', 'cs.AI', 'cs.RO']","  Prioritized experience replay is a reinforcement learning technique whereby
agents speed up learning by replaying useful past experiences. This usefulness
is quantified as the expected gain from replaying the experience, a quantity
often approximated as the prediction error (TD-error). However, recent work in
neuroscience suggests that, in biological organisms, replay is prioritized not
only by gain, but also by ""need"" -- a quantity measuring the expected relevance
of each experience with respect to the current situation. Importantly, this
term is not currently considered in algorithms such as prioritized experience
replay. In this paper we present a new approach for prioritizing experiences
for replay that considers both gain and need. Our proposed algorithms show a
significant increase in performance in benchmarks including the Dyna-Q maze and
a selection of Atari games.
","[{'version': 'v1', 'created': 'Mon, 29 Nov 2021 05:25:54 GMT'}, {'version': 'v2', 'created': 'Wed, 16 Feb 2022 22:56:37 GMT'}]",2022-02-18,"['Machine Learning', 'Artificial Intelligence', 'Robotics']",This paper presents a novel approach to improve experience replay in reinforcement learning (RL) using successor representation (SR). SR is a technique used to learn from past experiences by storing the states and actions that lead to rewards. We propose a new algorithm that combines SR with experience replay to improve the learning process in RL. The proposed algorithm is evaluated on a range of robotics and artificial intelligence (AI) tasks and shows improved performance compared to existing methods. Our results demonstrate that SR can be used to improve experience replay and lead to better RL performance in robotics and AI.,"Write an abstract for a paper called Improving Experience Replay with Successor Representation about Machine Learning, Artificial Intelligence, Robotics"
2204.10526,"Nicolas Bousquet and Amer E. Mouawad and Naomi Nishimura and Sebastian
  Siebertz","A survey on the parameterized complexity of the independent set and
  (connected) dominating set reconfiguration problems","['cs.CC', 'cs.DM', 'cs.DS', 'math.CO']","  A graph vertex-subset problem defines which subsets of the vertices of an
input graph are feasible solutions. We view a feasible solution as a set of
tokens placed on the vertices of the graph. A reconfiguration variant of a
vertex-subset problem asks, given two feasible solutions of size $k$, whether
it is possible to transform one into the other by a sequence of token slides
(along edges of the graph) or token jumps (between arbitrary vertices of the
graph) such that each intermediate set remains a feasible solution of size $k$.
Many algorithmic questions present themselves in the form of reconfiguration
problems: Given the description of an initial system state and the description
of a target state, is it possible to transform the system from its initial
state into the target one while preserving certain properties of the system in
the process? Such questions have received a substantial amount of attention
under the so-called combinatorial reconfiguration framework. We consider
reconfiguration variants of three fundamental underlying graph vertex-subset
problems, namely Independent Set, Dominating Set, and Connected Dominating Set.
We survey both older and more recent work on the parameterized complexity of
all three problems when parameterized by the number of tokens $k$. The emphasis
will be on positive results and the most common techniques for the design of
fixed-parameter tractable algorithms.
","[{'version': 'v1', 'created': 'Fri, 22 Apr 2022 06:44:08 GMT'}]",2022-04-25,"['Computational Complexity', 'Discrete Mathematics', 'Data Structures and Algorithms']","This paper surveys the parameterized complexity of the independent set and (connected) dominating set reconfiguration problems. We examine the parameterized complexity of the problems in terms of the number of vertices, the number of edges, the size of the independent set or dominating set, and the number of changes that are allowed to be made to the set. We also discuss the relevant computational complexity classes and the data structures and algorithms used to solve the problems. We present several open problems and research directions in this area, and discuss the implications of the results.","Write an abstract for a paper called A survey on the parameterized complexity of the independent set and
  (connected) dominating set reconfiguration problems about Computational Complexity, Discrete Mathematics, Data Structures and Algorithms"
2304.0093,"Yigit Baran Can, Alexander Liniger, Danda Pani Paudel, Luc Van Gool",Online Lane Graph Extraction from Onboard Video,['cs.CV'],"  Autonomous driving requires a structured understanding of the surrounding
road network to navigate. One of the most common and useful representation of
such an understanding is done in the form of BEV lane graphs. In this work, we
use the video stream from an onboard camera for online extraction of the
surrounding's lane graph. Using video, instead of a single image, as input
poses both benefits and challenges in terms of combining the information from
different timesteps. We study the emerged challenges using three different
approaches. The first approach is a post-processing step that is capable of
merging single frame lane graph estimates into a unified lane graph. The second
approach uses the spatialtemporal embeddings in the transformer to enable the
network to discover the best temporal aggregation strategy. Finally, the third,
and the proposed method, is an early temporal aggregation through explicit BEV
projection and alignment of framewise features. A single model of this proposed
simple, yet effective, method can process any number of images, including one,
to produce accurate lane graphs. The experiments on the Nuscenes and Argoverse
datasets show the validity of all the approaches while highlighting the
superiority of the proposed method. The code will be made public.
","[{'version': 'v1', 'created': 'Mon, 3 Apr 2023 12:36:39 GMT'}]",2023-04-04,['Computer Vision and Pattern Recognition'],"This paper presents a novel method for extracting lane graph from onboard video using computer vision and pattern recognition techniques. The proposed method is based on the idea of estimating lane graph from a single frame of the video. It first extracts lane features from the frame and then uses a graph-based approach to construct the lane graph. The proposed method is evaluated on two publicly available datasets and shows promising results. The results demonstrate that the proposed method is effective and robust in extracting lane graph from onboard videos. Furthermore, the proposed method can be used for various applications such as autonomous driving, lane detection and navigation.",Write an abstract for a paper called Online Lane Graph Extraction from Onboard Video about Computer Vision and Pattern Recognition
2209.07263,"Zhenyu Zhu, Fanghui Liu, Grigorios G Chrysos, Volkan Cevher","Robustness in deep learning: The good (width), the bad (depth), and the
  ugly (initialization)","['cs.LG', 'cs.AI']","  We study the average robustness notion in deep neural networks in (selected)
wide and narrow, deep and shallow, as well as lazy and non-lazy training
settings. We prove that in the under-parameterized setting, width has a
negative effect while it improves robustness in the over-parameterized setting.
The effect of depth closely depends on the initialization and the training
mode. In particular, when initialized with LeCun initialization, depth helps
robustness with the lazy training regime. In contrast, when initialized with
Neural Tangent Kernel (NTK) and He-initialization, depth hurts the robustness.
Moreover, under the non-lazy training regime, we demonstrate how the width of a
two-layer ReLU network benefits robustness. Our theoretical developments
improve the results by [Huang et al. NeurIPS21; Wu et al. NeurIPS21] and are
consistent with [Bubeck and Sellke NeurIPS21; Bubeck et al. COLT21].
","[{'version': 'v1', 'created': 'Thu, 15 Sep 2022 12:55:16 GMT'}, {'version': 'v2', 'created': 'Thu, 13 Oct 2022 20:12:53 GMT'}, {'version': 'v3', 'created': 'Fri, 13 Jan 2023 13:37:15 GMT'}, {'version': 'v4', 'created': 'Thu, 9 Feb 2023 21:03:04 GMT'}]",2023-02-13,"['Machine Learning', 'Artificial Intelligence']","This paper examines the three major components of robustness in deep learning: width, depth, and initialization. It looks at the advantages and disadvantages of each component and how they can affect the performance of a machine learning system. Width is the number of neurons and layers in a network, and it is important to ensure that the network is wide enough to capture the complexity of the task. Depth is the number of layers in a network, and it is important to balance depth and width to ensure the network can learn effectively. Finally, initialization is the process of setting the initial weights and biases of a neural network, and it is important to properly initialize the network to ensure it is able to learn effectively. The paper then discusses the various methods used to optimize these components, such as regularization, early stopping, and data augmentation. Finally, the paper concludes with a discussion of the importance of robustness in deep learning and the need for further research in this area.","Write an abstract for a paper called Robustness in deep learning: The good (width), the bad (depth), and the
  ugly (initialization) about Machine Learning, Artificial Intelligence"
2209.05729,"Zahra Fatemi, Abari Bhattacharya, Andrew Wentzel, Vipul Dhariwal,
  Lauren Levine, Andrew Rojecki, G. Elisabeta Marai, Barbara Di Eugenio, Elena
  Zheleva",Understanding Stay-at-home Attitudes through Framing Analysis of Tweets,['cs.SI'],"  With the onset of the COVID-19 pandemic, a number of public policy measures
have been developed to curb the spread of the virus. However, little is known
about the attitudes towards stay-at-home orders expressed on social media
despite the fact that social media are central platforms for expressing and
debating personal attitudes. To address this gap, we analyze the prevalence and
framing of attitudes towards stay-at-home policies, as expressed on Twitter in
the early months of the pandemic. We focus on three aspects of tweets: whether
they contain an attitude towards stay-at-home measures, whether the attitude
was for or against, and the moral justification for the attitude, if any. We
collect and annotate a dataset of stay-at-home tweets and create classifiers
that enable large-scale analysis of the relationship between moral frames and
stay-at-home attitudes and their temporal evolution. Our findings suggest that
frames of care are correlated with a supportive stance, whereas freedom and
oppression signify an attitude against stay-at-home directives. There was
widespread support for stay-at-home orders in the early weeks of lockdowns,
followed by increased resistance toward the end of May and the beginning of
June 2020. The resistance was associated with moral judgment that mapped to
political divisions.
","[{'version': 'v1', 'created': 'Tue, 13 Sep 2022 04:47:42 GMT'}]",2022-09-14,['Social and Information Networks'],"This paper examines the framing of stay-at-home attitudes through an analysis of tweets about social and information networks. The objective of this research is to explore the ways in which individuals use social media to express their views on the current pandemic and its associated restrictions. The study will employ a qualitative approach to analyze tweets from Twitter, a popular social media platform. Specifically, the paper will use framing analysis to identify the dominant frames used to discuss stay-at-home attitudes, as well as the ways in which these frames are used to shape public opinion and discourse. The paper will also explore how the use of social and information networks affects the way people frame their opinions on the topic. By understanding the framing of stay-at-home attitudes, this research will provide insight into public opinion and discourse on the current pandemic.",Write an abstract for a paper called Understanding Stay-at-home Attitudes through Framing Analysis of Tweets about Social and Information Networks
2210.0525,Vasilis Naserentin and Anders Logg,"Digital twins for city simulation: Automatic, efficient, and robust mesh
  generation for large-scale city modeling and simulation","['cs.GR', 'cs.NA', 'math.NA']","  The concept of creating digital twins, connected digital models of physical
systems, is gaining increasing attention for modeling and simulation of whole
cities. The basis for building a digital twin of a city is the generation of a
3D city model, often represented as a mesh. Creating and updating such models
is a tedious process that requires manual work and considerable effort,
especially in the modeling of building geometries. In the current paper, we
present a novel algorithm and implementation for automatic, efficient, and
robust mesh generation for large-scale city modeling and simulation. The
algorithm relies on standard, publicly available data, in particular 2D
cadastral maps (building footprints) and 3D point clouds obtained from aerial
scanning. The algorithm generates LoD1.2 city models in the form of both
triangular surface meshes, suitable for visualisation, and high-quality
tetrahedral volume meshes, suitable for simulation. Our tests demonstrate good
performance and scaling and indicate good avenues for further optimization
based on parallelisation. The long-term goal is a generic digital twin of
cities volume mesh generator that provides (nearly) real-time mesh manipulation
in LoD2.x.
","[{'version': 'v1', 'created': 'Tue, 11 Oct 2022 08:26:51 GMT'}]",2022-10-12,"['Graphics', 'Numerical Analysis']",", and Computer Science

This paper presents an automatic, efficient, and robust mesh generation approach for large-scale city modeling and simulation using digital twins. We propose a novel method for automatically generating a 3D mesh for a cityscape from a given set of input data. Our approach is based on a combination of advanced graphics techniques, numerical analysis, and computer science algorithms. We demonstrate the efficacy of our approach on two real-world cityscapes and compare the results with traditional mesh generation techniques. Our results show that our proposed approach is more efficient, robust, and produces better quality meshes than the traditional methods. Furthermore, the proposed method can be used for generating meshes for large-scale city simulations, enabling realistic simulations of urban environments.","Write an abstract for a paper called Digital twins for city simulation: Automatic, efficient, and robust mesh
  generation for large-scale city modeling and simulation about Graphics, Numerical Analysis"
2207.13045,Muhammad Firdaus and Yoedy Moegiharto,"Performance of OFDM System against Different Cyclic Prefix Lengths on
  Multipath Fading Channels","['cs.NI', 'eess.SP']","  Orthogonal Frequency Division Multiplexing (OFDM) is a transmission technique
that uses several subcarrier frequencies (multicarrier) that are perpendicular
to each other (orthogonal). This OFDM modulation technique effectively
eliminates Intersymbol Interference (ISI) on a channel caused by the effects of
multipath fading. To overcome this weakness, OFDM uses a guard interval (cyclic
prefix) inserted in its transmission. This paper analyses the effect of
different cyclic prefix lengths on OFDM performance on multipath channels using
Matlab. Here, the cyclic prefix length as the main parameter is varied for the
different number of subscribers. Meanwhile, Bit Error Rate (BER) and Signal
Noise Ratio (SNR) values obtained at the receiver are used to see the system's
performance. Based on our simulation results, the best value of BER is obtained
using a cyclic prefix length of 1/4 with a Fast Fourier Transform (FFT) size of
512.
","[{'version': 'v1', 'created': 'Thu, 30 Jun 2022 05:56:27 GMT'}]",2022-07-27,['Networking and Internet Architecture'],This paper aims to study the performance of Orthogonal Frequency Division Multiplexing (OFDM) system with different cyclic prefix lengths on multipath fading channels. The paper focuses on investigating the effect of multipath fading and cyclic prefix lengths on the performance of the OFDM system in terms of bit error rate and packet error rate. Simulation results are used to compare the performance of OFDM systems with different cyclic prefix lengths under different multipath fading channels. The results of this paper will provide insight into the design and optimization of OFDM systems in multipath fading channels and contribute to the advancement of networking and internet architecture.,"Write an abstract for a paper called Performance of OFDM System against Different Cyclic Prefix Lengths on
  Multipath Fading Channels about Networking and Internet Architecture"
2304.01045,"D\v{z}enan Lapandi\'c, Christos K. Verginis, Dimos V. Dimarogonas and
  Bo Wahlberg","Prediction-Based Leader-Follower Rendezvous Model Predictive Control
  with Robustness to Communication Losses","['eess.SY', 'cs.RO', 'cs.SY']","  In this paper we propose a novel distributed model predictive control (DMPC)
based algorithm with a trajectory predictor for a scenario of landing of
unmanned aerial vehicles (UAVs) on a moving unmanned surface vehicle (USV). The
algorithm is executing DMPC with exchange of trajectories between the agents at
a sufficient rate. In the case of loss of communication, and given the sensor
setup, agents are predicting the trajectories of other agents based on the
available measurements and prior information. The predictions are then used as
the reference inputs to DMPC. During the landing, the followers are tasked with
avoidance of USV-dependent obstacles and inter-agent collisions. In the
proposed distributed algorithm, all agents solve their local optimization
problem in parallel and we prove the convergence of the proposed algorithm.
Finally, the simulation results support the theoretical findings.
","[{'version': 'v1', 'created': 'Mon, 3 Apr 2023 14:48:31 GMT'}]",2023-04-04,"['Robotics', 'Systems and Control']","This paper presents a Prediction-Based Leader-Follower Rendezvous Model Predictive Control (MPC) algorithm for robotic systems with robustness to communication losses. The proposed algorithm is based on a leader-follower rendezvous model which uses a predictive control approach to maintain desired rendezvous accuracy in the presence of communication losses. The proposed algorithm is evaluated using a multi-agent robotic system in both simulation and real-world experiments. Results show that the proposed approach is able to maintain rendezvous accuracy even in the presence of communication losses, and also reduces the number of control inputs needed to achieve the desired rendezvous accuracy. The proposed approach thus provides a robust solution for leader-follower rendezvous in robotic systems with communication losses.","Write an abstract for a paper called Prediction-Based Leader-Follower Rendezvous Model Predictive Control
  with Robustness to Communication Losses about Robotics, Systems and Control"
2011.01879,"Akash Kundu, Jaros{\l}aw Adam Miszczak",Variational certification of quantum devices,"['quant-ph', 'cs.ET']","  One of the requirements imposed on the realistic quantum computers is to
provide computation results which can be repeated and reproduced. In the
situation when one needs to repeat the quantum computation procedure several
times, it is crucial that the copies of the quantum devices are similar in the
sense of the produced results. In this work, we describe a simple procedure
based on variational quantum eigensolver which can be utilized to compare
quantum devices. The procedure is developed by combining Choi-Jamio{\l}kowski
isomorphism with the variational hybrid quantum-classical procedure for matrix
diagonalization. We compare the introduced procedure with the scheme based on
the standard bounds for the similarity between quantum operations by analysing
its action on random quantum channels. We also discuss the sensitivity of the
described procedure to the noise, and we provide numerical results
demonstrating its feasibility in realistic scenarios by running the procedure
on IBM quantum computer.
","[{'version': 'v1', 'created': 'Tue, 3 Nov 2020 17:56:22 GMT'}, {'version': 'v2', 'created': 'Tue, 28 Dec 2021 12:33:46 GMT'}, {'version': 'v3', 'created': 'Thu, 25 Aug 2022 08:43:46 GMT'}]",2022-10-05,['Emerging Technologies'],"This paper presents a new approach to the certification of quantum devices, which is based on variational principles. It discusses the advantages of this approach over traditional methods, and its potential applications in emerging technologies such as quantum computing, quantum cryptography, and quantum sensing. The paper also provides a detailed analysis of the variational certification process, including a discussion of the challenges and opportunities associated with this approach. Finally, the paper outlines a set of experiments designed to test the efficacy of variational certification, and provides a roadmap for future research in this field.",Write an abstract for a paper called Variational certification of quantum devices about Emerging Technologies
2206.01767,"Jille van der Togt, Lea Tiyavorabun, Matteo Rosati, Giulio Starace","[Re] Badder Seeds: Reproducing the Evaluation of Lexical Methods for
  Bias Measurement","['cs.CL', 'cs.AI', 'cs.CY']","  Combating bias in NLP requires bias measurement. Bias measurement is almost
always achieved by using lexicons of seed terms, i.e. sets of words specifying
stereotypes or dimensions of interest. This reproducibility study focuses on
the original authors' main claim that the rationale for the construction of
these lexicons needs thorough checking before usage, as the seeds used for bias
measurement can themselves exhibit biases. The study aims to evaluate the
reproducibility of the quantitative and qualitative results presented in the
paper and the conclusions drawn thereof. We reproduce most of the results
supporting the original authors' general claim: seed sets often suffer from
biases that affect their performance as a baseline for bias metrics. Generally,
our results mirror the original paper's. They are slightly different on select
occasions, but not in ways that undermine the paper's general intent to show
the fragility of seed sets.
","[{'version': 'v1', 'created': 'Fri, 3 Jun 2022 18:00:29 GMT'}]",2022-06-07,"['Computation and Language', 'Artificial Intelligence', 'Computers and Society']","This paper examines the evaluation of lexical methods for bias measurement in the context of computation and language, artificial intelligence, and computers and society. We present a re-evaluation of existing lexical methods for bias measurement, discussing their advantages and limitations. We also propose a new approach for bias measurement that is more effective, efficient, and robust. We analyze the results of our experiments on a variety of datasets, and discuss implications for artificial intelligence, computers and society. We demonstrate that our proposed approach is able to accurately identify and measure bias in computer-generated language. Finally, we provide recommendations for future work in this area.","Write an abstract for a paper called [Re] Badder Seeds: Reproducing the Evaluation of Lexical Methods for
  Bias Measurement about Computation and Language, Artificial Intelligence, Computers and Society"
2302.02353,"Murthy L.R.D., Abhishek Mukhopadhyay, Shambhavi Aggarwal, Ketan Anand,
  Pradipta Biswas",Towards Precision in Appearance-based Gaze Estimation in the Wild,"['cs.CV', 'cs.HC']","  Appearance-based gaze estimation systems have shown great progress recently,
yet the performance of these techniques depend on the datasets used for
training. Most of the existing gaze estimation datasets setup in interactive
settings were recorded in laboratory conditions and those recorded in the wild
conditions display limited head pose and illumination variations. Further, we
observed little attention so far towards precision evaluations of existing gaze
estimation approaches. In this work, we present a large gaze estimation
dataset, PARKS-Gaze, with wider head pose and illumination variation and with
multiple samples for a single Point of Gaze (PoG). The dataset contains 974
minutes of data from 28 participants with a head pose range of 60 degrees in
both yaw and pitch directions. Our within-dataset and cross-dataset evaluations
and precision evaluations indicate that the proposed dataset is more
challenging and enable models to generalize on unseen participants better than
the existing in-the-wild datasets. The project page can be accessed here:
https://github.com/lrdmurthy/PARKS-Gaze
","[{'version': 'v1', 'created': 'Sun, 5 Feb 2023 10:09:35 GMT'}, {'version': 'v2', 'created': 'Tue, 14 Feb 2023 04:57:35 GMT'}]",2023-02-15,"['Computer Vision and Pattern Recognition', 'Human-Computer Interaction']","This paper investigates the use of appearance-based gaze estimation techniques in unconstrained environments. Specifically, it focuses on the challenges of precision and accuracy in gaze estimation in the wild. Computer vision and pattern recognition techniques are used to analyze facial features and head pose, while human-computer interaction methods are used to model human gaze behavior. The results of this research suggest that the use of appearance-based gaze estimation in unconstrained environments is feasible and can lead to improved accuracy and precision. Furthermore, the findings from this paper can be used to develop more effective gaze estimation systems for a variety of applications.","Write an abstract for a paper called Towards Precision in Appearance-based Gaze Estimation in the Wild about Computer Vision and Pattern Recognition, Human-Computer Interaction"
2204.11433,"Soumen Basu, Mayank Gupta, Pratyaksha Rana, Pankaj Gupta, Chetan Arora","Surpassing the Human Accuracy: Detecting Gallbladder Cancer from USG
  Images with Curriculum Learning",['cs.CV'],"  We explore the potential of CNN-based models for gallbladder cancer (GBC)
detection from ultrasound (USG) images as no prior study is known. USG is the
most common diagnostic modality for GB diseases due to its low cost and
accessibility. However, USG images are challenging to analyze due to low image
quality, noise, and varying viewpoints due to the handheld nature of the
sensor. Our exhaustive study of state-of-the-art (SOTA) image classification
techniques for the problem reveals that they often fail to learn the salient GB
region due to the presence of shadows in the USG images. SOTA object detection
techniques also achieve low accuracy because of spurious textures due to noise
or adjacent organs. We propose GBCNet to tackle the challenges in our problem.
GBCNet first extracts the regions of interest (ROIs) by detecting the GB (and
not the cancer), and then uses a new multi-scale, second-order pooling
architecture specializing in classifying GBC. To effectively handle spurious
textures, we propose a curriculum inspired by human visual acuity, which
reduces the texture biases in GBCNet. Experimental results demonstrate that
GBCNet significantly outperforms SOTA CNN models, as well as the expert
radiologists. Our technical innovations are generic to other USG image analysis
tasks as well. Hence, as a validation, we also show the efficacy of GBCNet in
detecting breast cancer from USG images. Project page with source code, trained
models, and data is available at https://gbc-iitd.github.io/gbcnet
","[{'version': 'v1', 'created': 'Mon, 25 Apr 2022 04:43:33 GMT'}]",2022-04-26,['Computer Vision and Pattern Recognition'],"This paper presents a novel approach to detecting gallbladder cancer from ultrasound (USG) images using a curriculum learning-based computer vision and pattern recognition system. The proposed system utilizes a combination of convolutional neural networks (CNNs), transfer learning, and curriculum learning to achieve higher accuracy than existing methods. The system was tested on a dataset of USG images from patients with gallbladder cancer and compared to the accuracy of human radiologists. Results show that the proposed system surpasses the accuracy of human radiologists, achieving an accuracy of 97.7% and an area under the receiver operating characteristic curve of 0.99. This paper provides a comprehensive analysis of the proposed system and its performance, and demonstrates the potential of computer vision and pattern recognition systems in medical imaging.","Write an abstract for a paper called Surpassing the Human Accuracy: Detecting Gallbladder Cancer from USG
  Images with Curriculum Learning about Computer Vision and Pattern Recognition"
2303.03386,Cunzhi Zhao and Xingpeng Li,"Hierarchical Deep Learning Model for Degradation Prediction per
  Look-Ahead Scheduled Battery Usage Profile","['eess.SY', 'cs.SY']","  Batteries can effectively improve the security of energy systems and mitigate
climate change by facilitating wind and solar power. The installed capacity of
battery energy storage system (BESS), mainly the lithium ion batteries are
increasing significantly in recent years. However, the battery degradation
cannot be accurately quantified and integrated into energy management system
with existing heuristic battery degradation models. This paper proposed a
hierarchical deep learning based battery degradation quantification (HDL-BDQ)
model to quantify the battery degradation given scheduled BESS daily
operations. Particularly, two sequential and cohesive deep neural networks are
proposed to accurately estimate the degree of degradation using inputs of
battery operational profiles and it can significantly outperform existing fixed
or linear rate based degradation models as well as single-stage deep neural
models. Training results show the high accuracy of the proposed system.
Moreover, a learning and optimization decoupled algorithm is implemented to
strategically take advantage of the proposed HDL-BDQ model in
optimization-based look-ahead scheduling (LAS) problems. Case studies
demonstrate the effectiveness of the proposed HDL-BDQ model in LAS of a
microgrid testbed.
","[{'version': 'v1', 'created': 'Mon, 6 Mar 2023 18:59:58 GMT'}]",2023-03-07,['Systems and Control'],"This paper presents a hierarchical deep learning model for predicting battery degradation in a look-ahead scheduled battery usage profile. The proposed model consists of two levels of deep learning networks, namely a feature extraction network and a regression network. The feature extraction network is used to extract relevant features from the battery usage profile, while the regression network is used to predict the battery degradation. The model is evaluated on a real-world dataset and compared with existing methods. Results show that the proposed model outperforms existing methods in terms of accuracy and efficiency. Furthermore, a detailed analysis is performed to understand the impact of different hyperparameters on the model performance. The results demonstrate the effectiveness of the proposed model in predicting battery degradation in a look-ahead scheduled battery usage profile.","Write an abstract for a paper called Hierarchical Deep Learning Model for Degradation Prediction per
  Look-Ahead Scheduled Battery Usage Profile about Systems and Control"
1604.08612,Jerome Feldman,Mysteries of Visual Experience,"['q-bio.NC', 'cs.AI']","  Science is a crowning glory of the human spirit and its applications remain
our best hope for social progress. But there are limitations to current science
and perhaps to any science. The general mind-body problem is known to be
intractable and currently mysterious. This is one of many deep problems that
are universally agreed to be beyond the current purview of Science, including
quantum phenomena, etc. But all of these famous unsolved problems are either
remote from everyday experience (entanglement, dark matter) or are hard to even
define sharply (phenomenology, consciousness, etc.).
  An updated summary of this work has been published as: Feldman, J. (2022).
Computation, perception, and mind. Behavioral and Brain Sciences, 45, E48.
doi:10.1017/S0140525X21001886 A more readable, open access, version is:
https://escholarship.org/uc/item/6cs78450
","[{'version': 'v1', 'created': 'Thu, 28 Apr 2016 20:41:25 GMT'}, {'version': 'v2', 'created': 'Wed, 28 Sep 2016 17:33:49 GMT'}, {'version': 'v3', 'created': 'Tue, 10 Jan 2017 18:46:42 GMT'}, {'version': 'v4', 'created': 'Tue, 20 Mar 2018 16:07:22 GMT'}, {'version': 'v5', 'created': 'Fri, 23 Jul 2021 16:13:55 GMT'}, {'version': 'v6', 'created': 'Fri, 25 Mar 2022 23:27:44 GMT'}]",2022-03-29,['Artificial Intelligence'],"This paper examines the implications of Artificial Intelligence (AI) on visual experience. It explores how AI can be used to understand and interpret visual cues, how AI can be used to create new visual experiences, and how AI can be used to improve existing visual experiences. It also discusses the ethical implications of using AI in visual experience, and the potential implications of AI on visual culture. Finally, it considers the potential implications of AI on the future of visual experience. The paper will draw on research from a variety of disciplines to provide a comprehensive examination of the mysteries of visual experience in the age of AI.",Write an abstract for a paper called Mysteries of Visual Experience about Artificial Intelligence
2303.16434,"Yaobo Liang, Chenfei Wu, Ting Song, Wenshan Wu, Yan Xia, Yu Liu, Yang
  Ou, Shuai Lu, Lei Ji, Shaoguang Mao, Yun Wang, Linjun Shou, Ming Gong, Nan
  Duan","TaskMatrix.AI: Completing Tasks by Connecting Foundation Models with
  Millions of APIs","['cs.AI', 'cs.CL']","  Artificial Intelligence (AI) has made incredible progress recently. On the
one hand, advanced foundation models like ChatGPT can offer powerful
conversation, in-context learning and code generation abilities on a broad
range of open-domain tasks. They can also generate high-level solution outlines
for domain-specific tasks based on the common sense knowledge they have
acquired. However, they still face difficulties with some specialized tasks
because they lack enough domain-specific data during pre-training or they often
have errors in their neural network computations on those tasks that need
accurate executions. On the other hand, there are also many existing models and
systems (symbolic-based or neural-based) that can do some domain-specific tasks
very well. However, due to the different implementation or working mechanisms,
they are not easily accessible or compatible with foundation models. Therefore,
there is a clear and pressing need for a mechanism that can leverage foundation
models to propose task solution outlines and then automatically match some of
the sub-tasks in the outlines to the off-the-shelf models and systems with
special functionalities to complete them. Inspired by this, we introduce
TaskMatrix.AI as a new AI ecosystem that connects foundation models with
millions of APIs for task completion. Unlike most previous work that aimed to
improve a single AI model, TaskMatrix.AI focuses more on using existing
foundation models (as a brain-like central system) and APIs of other AI models
and systems (as sub-task solvers) to achieve diversified tasks in both digital
and physical domains. As a position paper, we will present our vision of how to
build such an ecosystem, explain each key component, and use study cases to
illustrate both the feasibility of this vision and the main challenges we need
to address next.
","[{'version': 'v1', 'created': 'Wed, 29 Mar 2023 03:30:38 GMT'}]",2023-03-30,"['Artificial Intelligence', 'Computation and Language']","This paper introduces TaskMatrix.AI, a platform that enables users to quickly and easily complete tasks by connecting foundation models with millions of APIs related to artificial intelligence, computation, and language. TaskMatrix.AI provides a comprehensive suite of tools, including a Graphical User Interface, a library of pre-trained models, and an API search engine, that allow users to build and deploy complex tasks with minimal effort. TaskMatrix.AI also provides an API-driven approach that allows users to easily integrate with external services to increase their task completion capabilities. This paper provides an overview of the platform, its features, and how it can be used to rapidly develop and deploy AI-driven tasks. Furthermore, the paper discusses the potential of TaskMatrix.AI for both business and academic applications.","Write an abstract for a paper called TaskMatrix.AI: Completing Tasks by Connecting Foundation Models with
  Millions of APIs about Artificial Intelligence, Computation and Language"
2212.03299,Ishika Sahni and Araftoz Kaur,A Systematic Literature Review on 5G Security,['cs.CR'],"  It is expected that the creation of next-generation wireless networks would
result in the availability of high-speed and low-latency connectivity for every
part of our life. As a result, it is important that the network is secure. The
network's security environment has grown more complicated as a result of the
growing number of devices and the diversity of services that 5G will provide.
This is why it is important that the development of effective security
solutions is carried out early. Our findings of this review have revealed the
various directions that will be pursued in the development of next-generation
wireless networks. Some of these include the use of Artificial Intelligence and
Software Defined Mobile Networks. The threat environment for 5G networks,
security weaknesses in the new technology paradigms that 5G will embrace, and
provided solutions presented in the key studies in the field of 5G cyber
security are all described in this systematic literature review for prospective
researchers. Future research directions to protect wireless networks beyond 5G
are also covered.
","[{'version': 'v1', 'created': 'Tue, 6 Dec 2022 19:51:55 GMT'}]",2022-12-08,['Cryptography and Security'],"This paper presents a systematic literature review of the research related to 5G security, with a focus on cryptography and security. The literature review includes a comprehensive search of the literature, including peer-reviewed and non-peer-reviewed sources. The review covers the topics of 5G security, cryptography, security protocols, and security threats. The review also considers the implications of 5G security for the Internet of Things (IoT) and other applications. The results of the review indicate that 5G security is an important area of research that is still in its early stages. Cryptography and security protocols are essential components of 5G security, and the security threats posed by 5G networks must be addressed. The review also identifies potential research gaps and future areas of research.",Write an abstract for a paper called A Systematic Literature Review on 5G Security about Cryptography and Security
2201.08326,Subhroshekhar Ghosh and Soumendu Sundar Mukherjee,Learning with latent group sparsity via heat flow dynamics on networks,"['stat.ME', 'cs.LG', 'econ.EM', 'math.ST', 'stat.CO', 'stat.ML', 'stat.TH']","  Group or cluster structure on explanatory variables in machine learning
problems is a very general phenomenon, which has attracted broad interest from
practitioners and theoreticians alike. In this work we contribute an approach
to learning under such group structure, that does not require prior information
on the group identities. Our paradigm is motivated by the Laplacian geometry of
an underlying network with a related community structure, and proceeds by
directly incorporating this into a penalty that is effectively computed via a
heat flow-based local network dynamics. In fact, we demonstrate a procedure to
construct such a network based on the available data. Notably, we dispense with
computationally intensive pre-processing involving clustering of variables,
spectral or otherwise. Our technique is underpinned by rigorous theorems that
guarantee its effective performance and provide bounds on its sample
complexity. In particular, in a wide range of settings, it provably suffices to
run the heat flow dynamics for time that is only logarithmic in the problem
dimensions. We explore in detail the interfaces of our approach with key
statistical physics models in network science, such as the Gaussian Free Field
and the Stochastic Block Model. We validate our approach by successful
applications to real-world data from a wide array of application domains,
including computer science, genetics, climatology and economics. Our work
raises the possibility of applying similar diffusion-based techniques to
classical learning tasks, exploiting the interplay between geometric, dynamical
and stochastic structures underlying the data.
","[{'version': 'v1', 'created': 'Thu, 20 Jan 2022 17:45:57 GMT'}]",2022-01-21,['Machine Learning'],"This paper presents a novel machine learning approach to learning with latent group sparsity via heat flow dynamics on networks. The proposed method is based on the idea of using heat flow dynamics to discover latent group sparsity in networks, which allows for more efficient learning of complex patterns. In particular, the paper proposes a new optimization method for learning with latent group sparsity, which is based on a combination of heat flow dynamics and the alternating direction method of multipliers (ADMM). The proposed method is evaluated on several benchmark datasets, and the results show that it can achieve a significant improvement in terms of accuracy and computational efficiency compared to existing methods. The paper also provides an analysis of the proposed method, which reveals insights into its working mechanism and its potential applications.",Write an abstract for a paper called Learning with latent group sparsity via heat flow dynamics on networks about Machine Learning
2205.15136,"Dmitry Kovalev, Aleksandr Beznosikov, Ekaterina Borodich, Alexander
  Gasnikov, Gesualdo Scutari","Optimal Gradient Sliding and its Application to Distributed Optimization
  Under Similarity","['math.OC', 'cs.DC', 'cs.LG']","  We study structured convex optimization problems, with additive objective
$r:=p + q$, where $r$ is ($\mu$-strongly) convex, $q$ is $L_q$-smooth and
convex, and $p$ is $L_p$-smooth, possibly nonconvex. For such a class of
problems, we proposed an inexact accelerated gradient sliding method that can
skip the gradient computation for one of these components while still achieving
optimal complexity of gradient calls of $p$ and $q$, that is,
  $\mathcal{O}(\sqrt{L_p/\mu})$ and $\mathcal{O}(\sqrt{L_q/\mu})$,
respectively. This result is much sharper than the classic black-box complexity
$\mathcal{O}(\sqrt{(L_p+L_q)/\mu})$, especially when the difference between
$L_q$ and $L_q$ is large. We then apply the proposed method to solve
distributed optimization problems over master-worker architectures, under
agents' function similarity, due to statistical data similarity or otherwise.
The distributed algorithm achieves for the first time lower complexity bounds
on {\it both} communication and local gradient calls, with the former having
being a long-standing open problem. Finally the method is extended to
distributed saddle-problems (under function similarity) by means of solving a
class of variational inequalities, achieving lower communication and
computation complexity bounds.
","[{'version': 'v1', 'created': 'Mon, 30 May 2022 14:28:02 GMT'}]",2022-05-31,"['Distributed, Parallel, and Cluster Computing', 'Machine Learning']","This paper presents a novel distributed optimization technique called optimal gradient sliding, and its application to distributed optimization under similarity. We first provide a comprehensive overview of distributed, parallel, and cluster computing, and machine learning. We then introduce the concept of optimal gradient sliding, which is a method for optimizing a system of equations by sliding along the gradient of the cost function. We then discuss how this technique can be applied to distributed optimization under similarity, and demonstrate its effectiveness on a variety of problems. Finally, we discuss how this method can be extended to other distributed optimization problems. The results of this paper suggest that optimal gradient sliding is an effective and efficient technique for distributed optimization under similarity, and has potential applications in other distributed optimization problems.","Write an abstract for a paper called Optimal Gradient Sliding and its Application to Distributed Optimization
  Under Similarity about Distributed, Parallel, and Cluster Computing, Machine Learning"
2211.11114,"Zhaiming Shen, Ming-Jun Lai, Sheng Li",Semi-supervised Local Cluster Extraction by Compressive Sensing,"['cs.LG', 'cs.NA', 'math.NA']","  Local clustering problem aims at extracting a small local structure inside a
graph without the necessity of knowing the entire graph structure. As the local
structure is usually small in size compared to the entire graph, one can think
of it as a compressive sensing problem where the indices of target cluster can
be thought as a sparse solution to a linear system. In this paper, we propose a
new semi-supervised local cluster extraction approach by applying the idea of
compressive sensing based on two pioneering works under the same framework. Our
approves improves the existing works by making the initial cut to be the entire
graph and hence overcomes a major limitation of existing works, which is the
low quality of initial cut. Extensive experimental results on multiple
benchmark datasets demonstrate the effectiveness of our approach.
","[{'version': 'v1', 'created': 'Sun, 20 Nov 2022 22:55:07 GMT'}]",2022-11-22,"['Machine Learning', 'Numerical Analysis']","This paper presents a semi-supervised local cluster extraction method based on compressive sensing for machine learning and numerical analysis. The proposed method is able to extract local clusters from a given dataset with only a few labeled examples. The method is based on a novel combination of compressive sensing techniques and a semi-supervised learning algorithm. The compressive sensing techniques are used to reduce the dimensionality of the dataset, while the semi-supervised learning algorithm is used to identify clusters in the reduced dataset. The proposed method is evaluated on both synthetic and real-world datasets. Results show that the proposed method is able to accurately identify local clusters in the dataset with only a few labeled examples.","Write an abstract for a paper called Semi-supervised Local Cluster Extraction by Compressive Sensing about Machine Learning, Numerical Analysis"
2206.04958,Guangyi Zhao and Simin Kou and Xuesong Yin,Self-Supervised Deep Subspace Clustering with Entropy-norm,['cs.CV'],"  Auto-Encoder based deep subspace clustering (DSC) is widely used in computer
vision, motion segmentation and image processing. However, it suffers from the
following three issues in the self-expressive matrix learning process: the
first one is less useful information for learning self-expressive weights due
to the simple reconstruction loss; the second one is that the construction of
the self-expression layer associated with the sample size requires
high-computational cost; and the last one is the limited connectivity of the
existing regularization terms. In order to address these issues, in this paper
we propose a novel model named Self-Supervised deep Subspace Clustering with
Entropy-norm (S$^{3}$CE). Specifically, S$^{3}$CE exploits a self-supervised
contrastive network to gain a more effetive feature vector. The local structure
and dense connectivity of the original data benefit from the self-expressive
layer and additional entropy-norm constraint. Moreover, a new module with data
enhancement is designed to help S$^{3}$CE focus on the key information of data,
and improve the clustering performance of positive and negative instances
through spectral clustering. Extensive experimental results demonstrate the
superior performance of S$^{3}$CE in comparison to the state-of-the-art
approaches.
","[{'version': 'v1', 'created': 'Fri, 10 Jun 2022 09:15:33 GMT'}]",2022-06-13,['Computer Vision and Pattern Recognition'],"This paper presents a novel self-supervised deep subspace clustering method with entropy-norm for computer vision and pattern recognition. The proposed method is based on a deep neural network with a self-supervised learning objective and an entropy-norm regularization term. The self-supervised learning objective encourages the network to learn meaningful representations of the data, while the entropy-norm regularization term promotes the clustering of the data points into distinct subspaces. Experiments on several benchmark datasets demonstrate that the proposed method achieves superior performance compared to existing state-of-the-art clustering methods.",Write an abstract for a paper called Self-Supervised Deep Subspace Clustering with Entropy-norm about Computer Vision and Pattern Recognition
2209.07427,"Aleksander Boruch-Gruszecki, Rados{\l}aw Wa\'sko, Yichen Xu, Lionel
  Parreaux","A case for DOT: Theoretical Foundations for Objects With Pattern
  Matching and GADT-style Reasoning",['cs.PL'],"  Many programming languages in the OO tradition now support pattern matching
in some form. Historical examples include Scala and Ceylon, with the more
recent additions of Java, Kotlin, TypeScript, and Flow. But pattern matching on
generic class hierarchies currently results in puzzling type errors in most of
these languages. Yet this combination of features occurs naturally in many
scenarios, such as when manipulating typed ASTs. To support it properly,
compilers need to implement a form of subtyping reconstruction: the ability to
reconstruct subtyping information uncovered at runtime during pattern matching.
We introduce cDOT, a new calculus in the family of Dependent Object Types (DOT)
intended to serve as a formal foundation for subtyping reconstruction. Being
descended from pDOT, itself a formal foundation for Scala, cDOT can be used to
encode advanced object-oriented features such as generic inheritance, type
constructor variance, F-bounded polymorphism, and first-class recursive
modules. We demonstrate that subtyping reconstruction subsumes GADTs by
encoding $\lambda_{2,G\mu}$, a classical constraint-based GADT calculus, into
cDOT.
","[{'version': 'v1', 'created': 'Thu, 15 Sep 2022 16:22:04 GMT'}, {'version': 'v2', 'created': 'Thu, 23 Feb 2023 10:59:52 GMT'}]",2023-02-24,['Programming Languages'],"This paper proposes a novel approach to programming language design by introducing a new concept called DOT, which stands for Objects With Pattern Matching. DOT provides a way to enable GADT-style reasoning about programming languages, which allows for more expressive and powerful type systems. The paper begins by introducing the concept of DOT and discussing its theoretical foundations. It then presents a case study of how DOT can be used to implement a type system for a simple programming language. Finally, it provides an evaluation of the benefits and limitations of using DOT, and suggests potential future directions for further exploration. This paper provides a comprehensive look at the potential of DOT and GADT-style reasoning for programming language design.","Write an abstract for a paper called A case for DOT: Theoretical Foundations for Objects With Pattern
  Matching and GADT-style Reasoning about Programming Languages"
2303.07806,"Zelin Peng, Guanchun Wang, Lingxi Xie, Dongsheng Jiang, Wei Shen, Qi
  Tian","USAGE: A Unified Seed Area Generation Paradigm for Weakly Supervised
  Semantic Segmentation",['cs.CV'],"  Seed area generation is usually the starting point of weakly supervised
semantic segmentation (WSSS). Computing the Class Activation Map (CAM) from a
multi-label classification network is the de facto paradigm for seed area
generation, but CAMs generated from Convolutional Neural Networks (CNNs) and
Transformers are prone to be under- and over-activated, respectively, which
makes the strategies to refine CAMs for CNNs usually inappropriate for
Transformers, and vice versa. In this paper, we propose a Unified optimization
paradigm for Seed Area GEneration (USAGE) for both types of networks, in which
the objective function to be optimized consists of two terms: One is a
generation loss, which controls the shape of seed areas by a temperature
parameter following a deterministic principle for different types of networks;
The other is a regularization loss, which ensures the consistency between the
seed areas that are generated by self-adaptive network adjustment from
different views, to overturn false activation in seed areas. Experimental
results show that USAGE consistently improves seed area generation for both
CNNs and Transformers by large margins, e.g., outperforming state-of-the-art
methods by a mIoU of 4.1% on PASCAL VOC. Moreover, based on the USAGE-generated
seed areas on Transformers, we achieve state-of-the-art WSSS results on both
PASCAL VOC and MS COCO.
","[{'version': 'v1', 'created': 'Tue, 14 Mar 2023 11:25:02 GMT'}]",2023-03-15,['Computer Vision and Pattern Recognition'],"This paper presents USAGE, a unified seed area generation paradigm for weakly supervised semantic segmentation. USAGE is a novel approach that combines the advantages of both region- and pixel-level annotation techniques. It utilizes a single seed area generation method that can be adapted to different types of weakly supervised semantic segmentation tasks. Experiments conducted on the Pascal VOC 2012 dataset demonstrate that USAGE outperforms existing weakly supervised methods in terms of both accuracy and speed. The results demonstrate the effectiveness of USAGE as an efficient and accurate weakly supervised semantic segmentation paradigm.","Write an abstract for a paper called USAGE: A Unified Seed Area Generation Paradigm for Weakly Supervised
  Semantic Segmentation about Computer Vision and Pattern Recognition"
2104.02643,"David Melhart, Antonios Liapis, Georgios N. Yannakakis",The Arousal video Game AnnotatIoN (AGAIN) Dataset,['cs.HC'],"  How can we model affect in a general fashion, across dissimilar tasks, and to
which degree are such general representations of affect even possible? To
address such questions and enable research towards general affective computing,
this paper introduces The Arousal video Game AnnotatIoN (AGAIN) dataset. AGAIN
is a large-scale affective corpus that features over 1,100 in-game videos (with
corresponding gameplay data) from nine different games, which are annotated for
arousal from 124 participants in a first-person continuous fashion. Even though
AGAIN is created for the purpose of investigating the generality of affective
computing across dissimilar tasks, affect modelling can be studied within each
of its 9 specific interactive games. To the best of our knowledge AGAIN is the
largest -- over 37 hours of annotated video and game logs -- and most diverse
publicly available affective dataset based on games as interactive affect
elicitors.
","[{'version': 'v1', 'created': 'Tue, 6 Apr 2021 16:27:21 GMT'}, {'version': 'v2', 'created': 'Thu, 28 Jul 2022 10:55:15 GMT'}]",2022-07-29,['Human-Computer Interaction'],"This paper presents the Arousal Game Annotation (AGAIN) Dataset, a novel dataset for Human-Computer Interaction research. The dataset contains annotations of arousal levels of players while they are playing video games. The annotations are collected from a variety of sources, including physiological sensors, facial expressions, and self-reported ratings. The annotations are also linked to game events, allowing researchers to study the influence of game content on player arousal. The AGAIN Dataset provides a valuable resource for research into the relationship between video games and player arousal, and for the development of Human-Computer Interaction systems that can adapt to a player's emotional state.",Write an abstract for a paper called The Arousal video Game AnnotatIoN (AGAIN) Dataset about Human-Computer Interaction
2204.04287,"Zehai Tu, Ning Ma, Jon Barker","Exploiting Hidden Representations from a DNN-based Speech Recogniser for
  Speech Intelligibility Prediction in Hearing-impaired Listeners","['eess.AS', 'cs.SD', 'q-bio.QM']","  An accurate objective speech intelligibility prediction algorithms is of
great interest for many applications such as speech enhancement for hearing
aids. Most algorithms measures the signal-to-noise ratios or correlations
between the acoustic features of clean reference signals and degraded signals.
However, these hand-picked acoustic features are usually not explicitly
correlated with recognition. Meanwhile, deep neural network (DNN) based
automatic speech recogniser (ASR) is approaching human performance in some
speech recognition tasks. This work leverages the hidden representations from
DNN-based ASR as features for speech intelligibility prediction in
hearing-impaired listeners. The experiments based on a hearing aid
intelligibility database show that the proposed method could make better
prediction than a widely used short-time objective intelligibility (STOI) based
binaural measure.
","[{'version': 'v1', 'created': 'Fri, 8 Apr 2022 20:38:35 GMT'}, {'version': 'v2', 'created': 'Wed, 6 Jul 2022 13:59:50 GMT'}]",2022-07-07,['Sound'],"Quality

This paper presents a novel approach to predicting speech intelligibility in hearing-impaired listeners by exploiting hidden representations from a deep neural network (DNN)-based speech recognizer. The proposed approach uses a convolutional neural network (CNN) to extract acoustic features from speech signals and a recurrent neural network (RNN) to model the temporal dynamics of speech signals. These acoustic features are then used to train a DNN-based speech recognizer, which is then used to extract hidden representations from the speech signals. These hidden representations are then used to predict speech intelligibility in hearing-impaired listeners. The results of the proposed approach are evaluated using speech intelligibility tests and a subjective sound quality assessment. The results demonstrate that the proposed approach is able to accurately predict speech intelligibility in hearing-impaired listeners and improve the sound quality of speech signals.","Write an abstract for a paper called Exploiting Hidden Representations from a DNN-based Speech Recogniser for
  Speech Intelligibility Prediction in Hearing-impaired Listeners about Sound"
2304.06552,"Ruoxu Cen, William He, Jason Li, Debmalya Panigrahi",Beyond the Quadratic Time Barrier for Network Unreliability,['cs.DS'],"  Karger (STOC 1995) gave the first FPTAS for the network (un)reliability
problem, setting in motion research over the next three decades that obtained
increasingly faster running times, eventually leading to a
$\tilde{O}(n^2)$-time algorithm (Karger, STOC 2020). This represented a natural
culmination of this line of work because the algorithmic techniques used can
enumerate $\Theta(n^2)$ (near)-minimum cuts. In this paper, we go beyond this
quadratic barrier and obtain a faster algorithm for the network unreliability
problem. Our algorithm runs in $m^{1+o(1)} + \tilde{O}(n^{1.5})$ time.
  Our main contribution is a new estimator for network unreliability in very
reliable graphs. These graphs are usually the bottleneck for network
unreliability since the disconnection event is elusive. Our estimator is
obtained by defining an appropriate importance sampling subroutine on a dual
spanning tree packing of the graph. To complement this estimator for very
reliable graphs, we use recursive contraction for moderately reliable graphs.
We show that an interleaving of sparsification and contraction can be used to
obtain a better parametrization of the recursive contraction algorithm that
yields a faster running time matching the one obtained for the very reliable
case.
","[{'version': 'v1', 'created': 'Thu, 13 Apr 2023 14:01:42 GMT'}]",2023-04-14,['Data Structures and Algorithms'],"This paper presents a novel data structure and algorithm to overcome the quadratic time barrier for network unreliability. This barrier is caused by the need to check each node in a network for reliability, which can take up to O(n2) time. The proposed data structure is an augmented tree, which allows for efficient and reliable checking in O(n log n) time. This data structure is then used in combination with a new algorithm to check for network unreliability in O(n log n) time. Results from experiments demonstrate that the proposed data structure and algorithm can significantly reduce the time for network unreliability checking compared to existing methods. The paper concludes with a discussion of potential applications of the proposed data structure and algorithm.",Write an abstract for a paper called Beyond the Quadratic Time Barrier for Network Unreliability about Data Structures and Algorithms
2204.10834,"Bissan Ghaddar, Ignacio G\'omez-Casares, Julio Gonz\'alez-D\'iaz,
  Brais Gonz\'alez-Rodr\'iguez, Beatriz Pateiro-L\'opez, Sof\'ia
  Rodr\'iguez-Ballesteros",Learning for Spatial Branching: An Algorithm Selection Approach,"['math.OC', 'cs.LG']","  The use of machine learning techniques to improve the performance of
branch-and-bound optimization algorithms is a very active area in the context
of mixed integer linear problems, but little has been done for non-linear
optimization. To bridge this gap, we develop a learning framework for spatial
branching and show its efficacy in the context of the
Reformulation-Linearization Technique for polynomial optimization problems. The
proposed learning is performed offline, based on instance-specific features and
with no computational overhead when solving new instances. Novel graph-based
features are introduced, which turn out to play an important role for the
learning. Experiments on different benchmark instances from the literature show
that the learning-based branching rule significantly outperforms the standard
rules.
","[{'version': 'v1', 'created': 'Fri, 22 Apr 2022 17:23:43 GMT'}]",2022-04-25,['Machine Learning'],"This paper presents a novel algorithm selection approach for machine learning tasks that involve spatial branching. The approach uses a combination of supervised and unsupervised learning techniques to select the best algorithm for a given task. It is evaluated on two real-world datasets and the results demonstrate that the proposed approach outperforms existing algorithm selection techniques. Furthermore, the paper introduces a new metric to measure the effectiveness of the algorithm selection approach. The paper also discusses the potential implications of the proposed approach for future work in the area of machine learning.",Write an abstract for a paper called Learning for Spatial Branching: An Algorithm Selection Approach about Machine Learning
2010.0614,"Chaosheng Dong, Yijia Wang, Bo Zeng",Inverse Multiobjective Optimization Through Online Learning,"['cs.LG', 'math.OC']","  We study the problem of learning the objective functions or constraints of a
multiobjective decision making model, based on a set of sequentially arrived
decisions. In particular, these decisions might not be exact and possibly carry
measurement noise or are generated with the bounded rationality of decision
makers. In this paper, we propose a general online learning framework to deal
with this learning problem using inverse multiobjective optimization. More
precisely, we develop two online learning algorithms with implicit update rules
which can handle noisy data. Numerical results show that both algorithms can
learn the parameters with great accuracy and are robust to noise.
","[{'version': 'v1', 'created': 'Mon, 12 Oct 2020 17:53:49 GMT'}, {'version': 'v2', 'created': 'Sat, 24 Dec 2022 23:33:13 GMT'}]",2022-12-27,['Machine Learning'],"This paper presents a novel approach to inverse multiobjective optimization through online learning in the context of machine learning. This approach allows for the optimization of multiple objectives simultaneously by utilizing a combination of online learning and inverse optimization techniques. The proposed method is evaluated on a variety of benchmark datasets to demonstrate its effectiveness. Results show that the proposed approach can effectively optimize multiple objectives in a single pass and outperform existing methods in terms of accuracy and speed. Furthermore, the proposed method is shown to be robust to changes in the data distribution, making it suitable for use in real-world machine learning applications.",Write an abstract for a paper called Inverse Multiobjective Optimization Through Online Learning about Machine Learning
2004.03637,"Pola Schw\""obel, Frederik Warburg, Martin J{\o}rgensen, Kristoffer H.
  Madsen, S{\o}ren Hauberg",Probabilistic Spatial Transformer Networks,"['cs.LG', 'stat.ML']","  Spatial Transformer Networks (STNs) estimate image transformations that can
improve downstream tasks by `zooming in' on relevant regions in an image.
However, STNs are hard to train and sensitive to mis-predictions of
transformations. To circumvent these limitations, we propose a probabilistic
extension that estimates a stochastic transformation rather than a
deterministic one. Marginalizing transformations allows us to consider each
image at multiple poses, which makes the localization task easier and the
training more robust. As an additional benefit, the stochastic transformations
act as a localized, learned data augmentation that improves the downstream
tasks. We show across standard imaging benchmarks and on a challenging
real-world dataset that these two properties lead to improved classification
performance, robustness and model calibration. We further demonstrate that the
approach generalizes to non-visual domains by improving model performance on
time-series data.
","[{'version': 'v1', 'created': 'Tue, 7 Apr 2020 18:22:02 GMT'}, {'version': 'v2', 'created': 'Wed, 15 Jun 2022 13:50:10 GMT'}]",2022-06-16,['Machine Learning'],"This paper presents Probabilistic Spatial Transformer Networks (PSTNs): a novel approach to machine learning that combines the spatial transformer network (STN) and probabilistic graphical models. PSTNs are a powerful tool for learning complex spatial transformations, such as rotations and translations, while maintaining the ability to perform inference and prediction tasks. The paper introduces the concept of PSTNs and provides an overview of their architecture. It then describes the learning process and provides experimental results that demonstrate the effectiveness of PSTNs. Finally, the paper discusses the potential applications of PSTNs in various domains, including computer vision, robotics, and natural language processing.",Write an abstract for a paper called Probabilistic Spatial Transformer Networks about Machine Learning
2208.0069,"Jae Won Cho, Dong-jin Kim, Hyeonggon Ryu, In So Kweon",Generative Bias for Robust Visual Question Answering,"['cs.CV', 'cs.AI', 'cs.CL', 'cs.LG']","  The task of Visual Question Answering (VQA) is known to be plagued by the
issue of VQA models exploiting biases within the dataset to make its final
prediction. Various previous ensemble based debiasing methods have been
proposed where an additional model is purposefully trained to be biased in
order to train a robust target model. However, these methods compute the bias
for a model simply from the label statistics of the training data or from
single modal branches. In this work, in order to better learn the bias a target
VQA model suffers from, we propose a generative method to train the bias model
directly from the target model, called GenB. In particular, GenB employs a
generative network to learn the bias in the target model through a combination
of the adversarial objective and knowledge distillation. We then debias our
target model with GenB as a bias model, and show through extensive experiments
the effects of our method on various VQA bias datasets including VQA-CP2,
VQA-CP1, GQA-OOD, and VQA-CE, and show state-of-the-art results with the LXMERT
architecture on VQA-CP2.
","[{'version': 'v1', 'created': 'Mon, 1 Aug 2022 08:58:02 GMT'}, {'version': 'v2', 'created': 'Tue, 2 Aug 2022 02:53:59 GMT'}, {'version': 'v3', 'created': 'Wed, 22 Mar 2023 07:20:37 GMT'}]",2023-03-23,"['Computer Vision and Pattern Recognition', 'Artificial Intelligence', 'Computation and Language', 'Machine Learning']","This paper presents a generative bias for robust visual question answering (VQA). We propose a novel approach to VQA that combines the strengths of computer vision and pattern recognition, artificial intelligence, computation and language, and machine learning. Our approach leverages a generative model to generate natural language questions from visual inputs. The generated questions are then used to train a VQA model. We demonstrate that our generative bias approach can improve VQA accuracy and robustness on several benchmark datasets. Our experiments show that our approach outperforms existing VQA models on both accuracy and robustness. We also explore the potential of our approach for generalizing to unseen visual inputs and discuss the potential applications of our work.","Write an abstract for a paper called Generative Bias for Robust Visual Question Answering about Computer Vision and Pattern Recognition, Artificial Intelligence, Computation and Language, Machine Learning"
2210.09521,"Lingxiao Zhao, Louis H\""artel, Neil Shah, Leman Akoglu","A Practical, Progressively-Expressive GNN",['cs.LG'],"  Message passing neural networks (MPNNs) have become a dominant flavor of
graph neural networks (GNNs) in recent years. Yet, MPNNs come with notable
limitations; namely, they are at most as powerful as the 1-dimensional
Weisfeiler-Leman (1-WL) test in distinguishing graphs in a graph isomorphism
testing frame-work. To this end, researchers have drawn inspiration from the
k-WL hierarchy to develop more expressive GNNs. However, current
k-WL-equivalent GNNs are not practical for even small values of k, as k-WL
becomes combinatorially more complex as k grows. At the same time, several
works have found great empirical success in graph learning tasks without highly
expressive models, implying that chasing expressiveness with a coarse-grained
ruler of expressivity like k-WL is often unneeded in practical tasks. To truly
understand the expressiveness-complexity tradeoff, one desires a more
fine-grained ruler, which can more gradually increase expressiveness. Our work
puts forth such a proposal: Namely, we first propose the (k, c)(<=)-SETWL
hierarchy with greatly reduced complexity from k-WL, achieved by moving from
k-tuples of nodes to sets with <=k nodes defined over <=c connected components
in the induced original graph. We show favorable theoretical results for this
model in relation to k-WL, and concretize it via (k, c)(<=)-SETGNN, which is as
expressive as (k, c)(<=)-SETWL. Our model is practical and
progressively-expressive, increasing in power with k and c. We demonstrate
effectiveness on several benchmark datasets, achieving several state-of-the-art
results with runtime and memory usage applicable to practical graphs. We open
source our implementation at https://github.com/LingxiaoShawn/KCSetGNN.
","[{'version': 'v1', 'created': 'Tue, 18 Oct 2022 01:27:21 GMT'}, {'version': 'v2', 'created': 'Tue, 25 Oct 2022 20:15:21 GMT'}, {'version': 'v3', 'created': 'Thu, 3 Nov 2022 01:21:10 GMT'}]",2022-11-04,['Machine Learning'],"This paper presents a novel graph neural network (GNN) model for machine learning tasks. Our model, called a Practical, Progressively-Expressive GNN, is designed to be more expressive, efficient, and practical than existing GNNs. We propose a progressive approach that allows the model to learn a hierarchy of features, starting with simple ones and gradually increasing in complexity. We also propose a novel training procedure that allows us to effectively train the model in a single pass. We test our model on several standard benchmark datasets and show that it outperforms existing GNNs in terms of accuracy, efficiency, and scalability. Our results demonstrate that our model is a practical and progressive solution for machine learning tasks.","Write an abstract for a paper called A Practical, Progressively-Expressive GNN about Machine Learning"
2304.04115,"Joyce Reimer, Sebasti\'an A. Dom\'inguez-Rivera, Joakim Sundnes,
  Raymond J. Spiteri","Physiological accuracy in simulating refractory cardiac tissue: the
  volume-averaged bidomain model vs. the cell-based EMI model","['math.NA', 'cs.NA', 'physics.med-ph', 'q-bio.TO']","  The refractory period of cardiac tissue can be quantitatively described using
strength-interval (SI) curves. The information captured in SI curves is
pertinent to the design of anti-arrhythmic devices including pacemakers and
implantable cardioverter defibrillators. As computational cardiac modelling
becomes more prevalent, it is feasible to consider the generation of
computationally derived SI curves as a supplement or precursor to curves that
are experimentally derived. It is beneficial, therefore, to examine the
profiles of the SI curves produced by different cardiac tissue models to
determine whether some models capture the refractory period more accurately
than others. In this study, we compare the unipolar SI curves of two tissue
models: the current state-of-the-art bidomain model and the recently developed
extracellular-membrane-intracellular (EMI) model. The EMI model's resolution of
individual cell structure makes it a more detailed model than the bidomain
model, which forgoes the structure of individual cardiac cells in favour of
treating them homogeneously as a continuum. We find that the resulting SI
curves elucidate differences between the models, including that the behaviour
of the EMI model is noticeably closer to the refractory behaviour of
experimental data compared to that of the bidomain model. These results hold
implications for future computational pacemaker simulations and shed light on
the predicted refractory properties of cardiac tissue from each model.
","[{'version': 'v1', 'created': 'Sat, 8 Apr 2023 22:24:01 GMT'}]",2023-04-11,['Numerical Analysis'],This paper presents a numerical comparison between two models of refractory cardiac tissue: the volume-averaged bidomain model and the cell-based EMI model. The analysis is conducted using numerical methods to assess the accuracy of the two models in terms of their physiological accuracy. The results of the comparison are discussed in terms of the implications for the use of the two models in cardiac tissue simulation. The paper concludes with a discussion of the potential applications of the two models in cardiac tissue simulation and the need for further research in this area.,"Write an abstract for a paper called Physiological accuracy in simulating refractory cardiac tissue: the
  volume-averaged bidomain model vs. the cell-based EMI model about Numerical Analysis"
2205.14275,"Nancy Xu, Giannis Nikolentzos, Michalis Vazirgiannis, and Henrik
  Bostr\""om",Image Keypoint Matching using Graph Neural Networks,"['cs.CV', 'cs.LG']","  Image matching is a key component of many tasks in computer vision and its
main objective is to find correspondences between features extracted from
different natural images. When images are represented as graphs, image matching
boils down to the problem of graph matching which has been studied intensively
in the past. In recent years, graph neural networks have shown great potential
in the graph matching task, and have also been applied to image matching. In
this paper, we propose a graph neural network for the problem of image
matching. The proposed method first generates initial soft correspondences
between keypoints using localized node embeddings and then iteratively refines
the initial correspondences using a series of graph neural network layers. We
evaluate our method on natural image datasets with keypoint annotations and
show that, in comparison to a state-of-the-art model, our method speeds up
inference times without sacrificing prediction accuracy.
","[{'version': 'v1', 'created': 'Fri, 27 May 2022 23:38:44 GMT'}]",2022-05-31,"['Computer Vision and Pattern Recognition', 'Machine Learning']","This paper presents a novel approach to image keypoint matching using graph neural networks (GNNs) for computer vision and pattern recognition. The proposed method utilizes GNNs to learn the structural relationships between image keypoints in order to improve the accuracy of image matching. The proposed approach is evaluated on various benchmark datasets and compared to several state-of-the-art methods. Results show that the proposed GNN-based method outperforms existing methods in terms of accuracy and computational efficiency. Additionally, the proposed method is shown to be robust to varying noise levels and is able to handle challenging scenarios such as occlusion and viewpoint changes. This paper provides a new direction for the use of GNNs in the field of computer vision and machine learning.","Write an abstract for a paper called Image Keypoint Matching using Graph Neural Networks about Computer Vision and Pattern Recognition, Machine Learning"
2303.0688,"Bo Zhang, Jiakang Yuan, Botian Shi, Tao Chen, Yikang Li, Yu Qiao",Uni3D: A Unified Baseline for Multi-dataset 3D Object Detection,['cs.CV'],"  Current 3D object detection models follow a single dataset-specific training
and testing paradigm, which often faces a serious detection accuracy drop when
they are directly deployed in another dataset. In this paper, we study the task
of training a unified 3D detector from multiple datasets. We observe that this
appears to be a challenging task, which is mainly due to that these datasets
present substantial data-level differences and taxonomy-level variations caused
by different LiDAR types and data acquisition standards. Inspired by such
observation, we present a Uni3D which leverages a simple data-level correction
operation and a designed semantic-level coupling-and-recoupling module to
alleviate the unavoidable data-level and taxonomy-level differences,
respectively. Our method is simple and easily combined with many 3D object
detection baselines such as PV-RCNN and Voxel-RCNN, enabling them to
effectively learn from multiple off-the-shelf 3D datasets to obtain more
discriminative and generalizable representations. Experiments are conducted on
many dataset consolidation settings including Waymo-nuScenes, nuScenes-KITTI,
Waymo-KITTI, and Waymo-nuScenes-KITTI consolidations. Their results demonstrate
that Uni3D exceeds a series of individual detectors trained on a single
dataset, with a 1.04x parameter increase over a selected baseline detector. We
expect this work will inspire the research of 3D generalization since it will
push the limits of perceptual performance.
","[{'version': 'v1', 'created': 'Mon, 13 Mar 2023 05:54:13 GMT'}]",2023-03-14,['Computer Vision and Pattern Recognition'],"Uni3D is a unified baseline for multi-dataset 3D object detection in computer vision and pattern recognition. This paper presents a novel approach for 3D object detection which is able to handle multiple datasets with different 3D representations. The proposed approach is based on a single unified architecture and uses a single set of weights for all datasets. Experiments show that the proposed approach outperforms existing methods on multiple datasets and achieves state-of-the-art performance. Furthermore, the unified architecture allows for easy transfer of knowledge between datasets, making it an ideal choice for multi-dataset 3D object detection.",Write an abstract for a paper called Uni3D: A Unified Baseline for Multi-dataset 3D Object Detection about Computer Vision and Pattern Recognition
2302.01894,"Muhammad Waseem, Peng Liang, Aakash Ahmad, Arif Ali Khan, Mojtaba
  Shahin, Pekka Abrahamsson, Ali Rezaei Nasab, Tommi Mikkonen","Understanding the Issues, Their Causes and Solutions in Microservices
  Systems: An Empirical Study",['cs.SE'],"  Many small to large organizations have adopted the Microservices Architecture
(MSA) style to develop and deliver their core businesses. Despite the
popularity of MSA in the software industry, there is a limited evidence-based
and thorough understanding of the types of issues (e.g., errors, faults,
failures, and bugs) that microservices system developers experience, the causes
of the issues, and the solutions as potential fixing strategies to address the
issues. To ameliorate this gap, we conducted a mixed-methods empirical study
that collected data from 2,641 issues from the issue tracking systems of 15
open-source microservices systems on GitHub, 15 interviews, and an online
survey completed by 150 practitioners from 42 countries across 6 continents.
Our analysis led to comprehensive taxonomies for the issues, causes, and
solutions. The findings of this study inform that Technical Debt, Continuous
Integration and Delivery, Exception Handling, Service Execution and
Communication, and Security are the most dominant issues in microservices
systems. Furthermore, General Programming Errors, Missing Features and
Artifacts, and Invalid Configuration and Communication are the main causes
behind the issues. Finally, we found 177 types of solutions that can be applied
to fix the identified issues. Based on our study results, we formulated future
research directions that could help researchers and practitioners to engineer
emergent and next-generation microservices systems.
","[{'version': 'v1', 'created': 'Fri, 3 Feb 2023 18:08:03 GMT'}]",2023-02-06,['Software Engineering'],"This paper presents an empirical study of software engineering related to the issues, causes and solutions in microservices systems. The research is based on a survey of software engineers from different organizations, and the results are analyzed to identify the primary issues, their causes and potential solutions. The findings of the study show that the most common issues in microservices systems are related to scalability, deployment, and management, and that the primary causes of these issues are related to lack of understanding of the technology, inadequate planning, and inadequate communication. The study also proposes potential solutions for each of the identified issues, such as setting up an appropriate architecture, using the right tools, and ensuring proper communication between stakeholders. The paper provides valuable insights into the challenges of microservices systems, and can be used by software engineers to develop better practices for the successful implementation of microservices.","Write an abstract for a paper called Understanding the Issues, Their Causes and Solutions in Microservices
  Systems: An Empirical Study about Software Engineering"
2112.07124,Shun Terasaki and Kazuhiro Sato,"Minimal controllability problem on linear structural descriptor systems
  with forbidden nodes","['math.OC', 'cs.SY', 'eess.SY']","  We consider a minimal controllability problem (MCP), which determines the
minimum number of input nodes for a descriptor system to be structurally
controllable. We investigate the ""forbidden nodes"" in descriptor systems,
denoting nodes that are unable to establish connections with input components.
The three main results of this work are as follows. First, we show a
solvability condition for the MCP with forbidden nodes using graph theory such
as a bipartite graph and its Dulmage--Mendelsohn decomposition. Next, we derive
the optimal value of the MCP with forbidden nodes. The optimal value is
determined by an optimal solution for constrained maximum matching, and this
result includes that of the standard MCP in the previous work. Finally, we
provide an efficient algorithm for solving the MCP with forbidden nodes based
on an alternating path algorithm.
","[{'version': 'v1', 'created': 'Tue, 14 Dec 2021 02:51:57 GMT'}, {'version': 'v2', 'created': 'Tue, 11 Apr 2023 16:26:57 GMT'}]",2023-04-12,['Systems and Control'],"This paper investigates the minimal controllability problem on linear structural descriptor systems with forbidden nodes. The objective is to find the minimum number of nodes that must be controlled in order to achieve controllability of the system. We present a graph-theoretic approach to this problem, which is based on the concept of forbidden nodes and the notion of a controllable subgraph. We then apply this approach to linear structural descriptor systems, which are used to model physical networks. We analyze the properties of controllable subgraphs and provide a set of sufficient conditions for controllability. We also discuss the complexity of the problem and present numerical results to illustrate the effectiveness of our approach.","Write an abstract for a paper called Minimal controllability problem on linear structural descriptor systems
  with forbidden nodes about Systems and Control"
2208.11333,"Sijie Ji, Mo Li",Enhancing Deep Learning Performance of Massive MIMO CSI Feedback,"['cs.IT', 'cs.AI', 'eess.SP', 'math.IT']","  CSI feedback is an important problem of Massive multiple-input
multiple-output (MIMO) technology because the feedback overhead is proportional
to the number of sub-channels and the number of antennas, both of which scale
with the size of the Massive MIMO system. Deep learning-based CSI feedback
methods have been widely adopted recently owing to their superior performance.
Despite the success, current approaches have not fully exploited the
relationship between the characteristics of CSI data and the deep learning
framework. In this paper, we propose a jigsaw puzzles aided training strategy
(JPTS) to enhance the deep learning-based Massive MIMO CSI feedback approaches
by maximizing mutual information between the original CSI and the compressed
CSI. We apply JPTS on top of existing state-of-the-art methods. Experimental
results show that by adopting this training strategy, the accuracy can be
boosted by 12.07% and 7.01% on average in indoor and outdoor environments,
respectively. The proposed method is ready to adopt to existing deep learning
frameworks of Massive MIMO CSI feedback. Codes of JPTS are available on GitHub
for use.
","[{'version': 'v1', 'created': 'Wed, 24 Aug 2022 07:08:31 GMT'}, {'version': 'v2', 'created': 'Fri, 3 Feb 2023 06:06:00 GMT'}]",2023-02-06,"['Information Theory', 'Artificial Intelligence']",", and Mobile Networks

This paper introduces a novel approach to enhance the deep learning performance of massive MIMO channel state information (CSI) feedback in mobile networks. By combining the principles of information theory, artificial intelligence, and mobile networks, this paper proposes a method for improving the accuracy and speed of CSI feedback in massive MIMO systems. The proposed method is based on a deep learning model trained on massive MIMO CSI feedback data. The model is then used to improve the accuracy and speed of CSI feedback in massive MIMO systems. The performance of the proposed method is evaluated through simulations, and the results show that the proposed method can significantly improve the accuracy and speed of CSI feedback in massive MIMO systems.","Write an abstract for a paper called Enhancing Deep Learning Performance of Massive MIMO CSI Feedback about Information Theory, Artificial Intelligence"
2208.09983,Guang Ping He,"Collaboration between parallel connected neural networks -- A possible
  criterion for distinguishing artificial neural networks from natural organs","['cs.LG', 'cs.NE']","  We find experimentally that when artificial neural networks are connected in
parallel and trained together, they display the following properties. (i) When
the parallel-connected neural network (PNN) is optimized, each sub-network in
the connection is not optimized. (ii) The contribution of an inferior
sub-network to the whole PNN can be on par with that of the superior
sub-network. (iii) The PNN can output the correct result even when all
sub-networks give incorrect results. These properties are unlikely for natural
biological sense organs. Therefore, they could serve as a simple yet effective
criterion for measuring the bionic level of neural networks. With this
criterion, we further show that when serving as the activation function, the
ReLU function can make an artificial neural network more bionic than the
sigmoid and Tanh functions do.
","[{'version': 'v1', 'created': 'Sun, 21 Aug 2022 23:18:28 GMT'}]",2022-08-23,"['Machine Learning', 'Neural and Evolutionary Computing']","This paper presents a novel approach for distinguishing artificial neural networks from natural organs about Machine Learning, Neural and Evolutionary Computing. The proposed criterion is based on the collaboration between parallel connected neural networks. Firstly, the paper introduces the concept of collaboration between parallel connected neural networks and its potential application in the field of Machine Learning, Neural and Evolutionary Computing. Secondly, the paper proposes a method to evaluate the collaboration between parallel connected neural networks and uses this method to distinguish artificial neural networks from natural organs. Finally, the paper provides a comprehensive analysis of the proposed criterion and its application in Machine Learning, Neural and Evolutionary Computing.","Write an abstract for a paper called Collaboration between parallel connected neural networks -- A possible
  criterion for distinguishing artificial neural networks from natural organs about Machine Learning, Neural and Evolutionary Computing"
2303.1352,"Xiaohan Wang, Zhan Zhao, Hongmou Zhang, Xiaotong Guo, Jinhua Zhao","Quantifying the uneven efficiency benefits of ridesharing market
  integration","['physics.soc-ph', 'cs.SI']","  Ridesharing is recognized as one of the key pathways to sustainable urban
mobility. With the emergence of Transportation Network Companies (TNCs) such as
Uber and Lyft, the ridesharing market has become increasingly fragmented in
many cities around the world, leading to efficiency loss and increased traffic
congestion. While an integrated ridesharing market (allowing sharing across
TNCs) can improve the overall efficiency, how such benefits may vary across
TNCs based on actual market characteristics is still not well understood. In
this study, we extend a shareability network framework to quantify and explain
the efficiency benefits of ridesharing market integration using available TNC
trip records. Through a case study in Manhattan, New York City, the proposed
framework is applied to analyze a real-world ridesharing market with 3
TNCs$-$Uber, Lyft, and Via. It is estimated that a perfectly integrated market
in Manhattan would improve ridesharing efficiency by 13.3%, or 5% of daily TNC
vehicle hours traveled. Further analysis reveals that (1) the efficiency
improvement is negatively correlated with the overall demand density and
inter-TNC spatiotemporal unevenness (measured by network modularity), (2)
market integration would generate a larger efficiency improvement in a
competitive market, and (3) the TNC with a higher intra-TNC demand
concentration (measured by clustering coefficient) would benefit less from
market integration. As the uneven benefits may deter TNCs from collaboration,
we also illustrate how to quantify each TNC's marginal contribution based on
the Shapley value, which can be used to ensure equitable profit allocation.
These results can help market regulators and business alliances to evaluate and
monitor market efficiency and dynamically adjust their strategies, incentives,
and profit allocation schemes to promote market integration and collaboration.
","[{'version': 'v1', 'created': 'Sun, 5 Feb 2023 12:35:44 GMT'}]",2023-03-27,['Social and Information Networks'],"This paper examines the potential benefits of ridesharing market integration through the lens of social and information networks. It quantifies the uneven efficiency benefits of ridesharing integration by analyzing the impact of ridesharing on the cost of transportation and the time savings associated with the integration of multiple sources of transportation. The paper also evaluates the impact of ridesharing on the quality of service and the amount of time spent waiting for rides. Additionally, the paper investigates the effects of ridesharing on the distribution of transportation resources and the potential for the creation of new transportation networks. Finally, the paper looks at the implications of ridesharing on social networks, particularly the impact of ridesharing on the ability of individuals to connect and interact with one another. The research in this paper will provide valuable insights into the potential benefits of ridesharing market integration, and will help inform policy makers and transportation providers in their decision-making.","Write an abstract for a paper called Quantifying the uneven efficiency benefits of ridesharing market
  integration about Social and Information Networks"
2209.0444,Jin Gyu Lee and Thiago B. Burghi and Rodolphe Sepulchre,Open-loop contraction design,"['eess.SY', 'cs.SY']","  Given a non-contracting trajectory of a nonlinear system, we consider the
question of designing an input perturbation that makes the perturbed trajectory
contracting. This paper stresses the analogy of this question with the
classical question of feedback stabilization. In particular, it is shown that
the existence of an output variable that ensures contraction of the inverse
system facilitates the design of a contracting input perturbation. We
illustrate the relevance of this question in parameter estimation.
","[{'version': 'v1', 'created': 'Fri, 9 Sep 2022 17:57:30 GMT'}]",2022-09-12,['Systems and Control'],"This paper presents a new open-loop contraction design approach for systems and control. The proposed approach is based on the idea of using Lyapunov-based contraction theory to design control laws that guarantee a desired degree of contraction of the system's state space. The approach is applicable to both linear and nonlinear systems, and is shown to be more efficient than existing techniques. Furthermore, the paper discusses the advantages of using the proposed approach, such as improved robustness, reduced computational complexity, and the ability to handle nonlinear systems. The paper also presents simulation results to demonstrate the effectiveness of the proposed approach.",Write an abstract for a paper called Open-loop contraction design about Systems and Control
2208.00982,"Kaiyuan Yang, Li Xia, Y.C. Tay",Dominant Eigenvalue-Eigenvector Pair Estimation via Graph Infection,"['math.NA', 'cs.NA', 'math.DS', 'q-bio.PE']","  We present a novel method to estimate the dominant eigenvalue and eigenvector
pair of any non-negative real matrix via graph infection. The key idea in our
technique lies in approximating the solution to the first-order matrix ordinary
differential equation (ODE) with the Euler method. Graphs, which can be
weighted, directed, and with loops, are first converted to its adjacency matrix
A. Then by a naive infection model for graphs, we establish the corresponding
first-order matrix ODE, through which A's dominant eigenvalue is revealed by
the fastest growing term. When there are multiple dominant eigenvalues of the
same magnitude, the classical power iteration method can fail. In contrast, our
method can converge to the dominant eigenvalue even when same-magnitude
counterparts exist, be it complex or opposite in sign. We conduct several
experiments comparing the convergence between our method and power iteration.
Our results show clear advantages over power iteration for tree graphs,
bipartite graphs, directed graphs with periods, and Markov chains with
spider-traps. To our knowledge, this is the first work that estimates dominant
eigenvalue and eigenvector pair from the perspective of a dynamical system and
matrix ODE. We believe our method can be adopted as an alternative to power
iteration, especially for graphs.
","[{'version': 'v1', 'created': 'Mon, 1 Aug 2022 16:43:21 GMT'}, {'version': 'v2', 'created': 'Mon, 13 Mar 2023 16:35:58 GMT'}]",2023-03-14,['Numerical Analysis'],"This paper presents a novel method for estimating the dominant eigenvalue-eigenvector pair of a matrix using graph infection. The proposed method is based on a graph-theoretic approach and is suitable for use in numerical analysis. The proposed method is shown to be robust and efficient, and it is demonstrated that the dominant eigenvalue-eigenvector pair can be accurately estimated using this method. The paper also provides an analysis of the complexity of the proposed method and its application to a variety of numerical problems. Finally, the paper presents a comparison of the proposed method with existing methods for estimating the dominant eigenvalue-eigenvector pair.",Write an abstract for a paper called Dominant Eigenvalue-Eigenvector Pair Estimation via Graph Infection about Numerical Analysis
2203.15399,"A. Mokh and J. de Rosny and G. C. Alexandropoulos and R. Khayatzadeh
  and M. Kamoun and A. Ourir and A. Tourin and M. Fink","Time Reversal for Multiple Access and Mobility: Algorithmic Design and
  Experimental Results","['cs.IT', 'eess.SP', 'math.IT']","  Time Reversal (TR) has been proposed as a competitive precoding strategy for
low-complexity wireless devices relying on Ultra-WideBand (UWB) signal
waveforms. However, when TR is applied for multiple access, the signals
received by the multiple users suffer from significant levels of inter-symbol
and inter-user interference, which requires additional processing for
mitigation by each receiving user. In this paper, we present an iterative
Time-Reversal Division Multiple Access (TRDMA) approach that aims to dim the
latter interference levels. The performance of iterative TRDMA is evaluated
experimentally in a reverberation chamber that mimics a rich scattering indoor
wireless propagation environment. The improved efficiency, in terms of the
number of algorithmic iterations, of the proposed approach compared to
conventional TRDMA, is demonstrated. We also consider a mobile user
configuration, where the position of the receiver changes between the channel
estimation and data transmission steps. It is showcased, even for this
experimental setup, that the proposed iterative TRDMA approach is more
efficient than conventional precoding schemes.
","[{'version': 'v1', 'created': 'Tue, 29 Mar 2022 09:44:23 GMT'}]",2022-03-30,['Information Theory'],"This paper presents a novel algorithmic design for a time reversal multiple access and mobility (TRMA) system, which is based on the principles of information theory. The proposed design is evaluated through a series of experiments, and the results are discussed in detail. The performance of the TRMA system is compared to that of traditional multiple access and mobility schemes, and the results demonstrate significant improvements in terms of throughput, latency, and energy efficiency. Furthermore, the paper provides an in-depth analysis of the effects of various system parameters on the performance of the TRMA system. The paper concludes with a discussion of the implications of the findings, and provides suggestions for future work in this area.","Write an abstract for a paper called Time Reversal for Multiple Access and Mobility: Algorithmic Design and
  Experimental Results about Information Theory"
2205.10873,Corentin Dancette and Matthieu Cord,Dynamic Query Selection for Fast Visual Perceiver,['cs.CV'],"  Transformers have been matching deep convolutional networks for vision
architectures in recent works. Most work is focused on getting the best results
on large-scale benchmarks, and scaling laws seem to be the most successful
strategy: bigger models, more data, and longer training result in higher
performance. However, the reduction of network complexity and inference time
remains under-explored. The Perceiver model offers a solution to this problem:
by first performing a Cross-attention with a fixed number Q of latent query
tokens, the complexity of the L-layers Transformer network that follows is
bounded by O(LQ^2). In this work, we explore how to make Perceivers even more
efficient, by reducing the number of queries Q during inference while limiting
the accuracy drop.
","[{'version': 'v1', 'created': 'Sun, 22 May 2022 17:23:51 GMT'}, {'version': 'v2', 'created': 'Tue, 21 Mar 2023 10:53:32 GMT'}]",2023-03-23,['Computer Vision and Pattern Recognition'],This paper presents a novel approach to fast visual perception in computer vision and pattern recognition. We propose a dynamic query selection technique that utilizes a combination of query-based and context-based methods to quickly identify salient features in an image. Our approach is evaluated on a standard dataset and achieves a significant improvement in accuracy and speed compared to existing methods. We also provide an analysis of the effectiveness of our approach in different scenarios. The results of this work demonstrate the potential of our dynamic query selection technique for improving the speed and accuracy of visual perception tasks in computer vision and pattern recognition.,Write an abstract for a paper called Dynamic Query Selection for Fast Visual Perceiver about Computer Vision and Pattern Recognition
2202.02942,Adnan Darwiche,Tractable Boolean and Arithmetic Circuits,"['cs.AI', 'cs.CC', 'cs.LG', 'cs.LO']","  Tractable Boolean and arithmetic circuits have been studied extensively in AI
for over two decades now. These circuits were initially proposed as ""compiled
objects,"" meant to facilitate logical and probabilistic reasoning, as they
permit various types of inference to be performed in linear-time and a
feed-forward fashion like neural networks. In more recent years, the role of
tractable circuits has significantly expanded as they became a computational
and semantical backbone for some approaches that aim to integrate knowledge,
reasoning and learning. In this article, we review the foundations of tractable
circuits and some associated milestones, while focusing on their core
properties and techniques that make them particularly useful for the broad aims
of neuro-symbolic AI.
","[{'version': 'v1', 'created': 'Mon, 7 Feb 2022 05:01:38 GMT'}]",2022-02-08,"['Artificial Intelligence', 'Computational Complexity', 'Machine Learning', 'Logic in Computer Science']","This paper examines the use of tractable Boolean and Arithmetic Circuits (TBACs) in Artificial Intelligence, Computational Complexity, Machine Learning, and Logic in Computer Science. TBACs are a type of circuit that can be used to efficiently solve complex problems. This paper will explore the use of TBACs in various fields, including their application to logic in computer science. It will also discuss the advantages and disadvantages of using TBACs compared to other techniques, as well as their potential for future applications. Finally, the paper will provide an overview of the state of the art in TBACs and how they can be used to improve existing algorithms and models.","Write an abstract for a paper called Tractable Boolean and Arithmetic Circuits about Artificial Intelligence, Computational Complexity, Machine Learning, Logic in Computer Science"
2105.04642,"Yutong Ban, Guy Rosman, Jennifer A. Eckhoff, Thomas M. Ward, Daniel A.
  Hashimoto, Taisei Kondo, Hidekazu Iwaki, Ozanan R. Meireles and Daniela Rus","SUPR-GAN: SUrgical PRediction GAN for Event Anticipation in Laparoscopic
  and Robotic Surgery",['cs.CV'],"  Comprehension of surgical workflow is the foundation upon which artificial
intelligence (AI) and machine learning (ML) holds the potential to assist
intraoperative decision-making and risk mitigation. In this work, we move
beyond mere identification of past surgical phases, into the prediction of
future surgical steps and specification of the transitions between them. We use
a novel Generative Adversarial Network (GAN) formulation to sample future
surgical phases trajectories conditioned on past video frames from laparoscopic
cholecystectomy (LC) videos and compare it to state-of-the-art approaches for
surgical video analysis and alternative prediction methods. We demonstrate the
GAN formulation's effectiveness through inferring and predicting the progress
of LC videos. We quantify the horizon-accuracy trade-off and explored average
performance, as well as the performance on the more challenging, and clinically
relevant transitions between phases. Furthermore, we conduct a survey, asking
16 surgeons of different specialties and educational levels to qualitatively
evaluate predicted surgery phases.
","[{'version': 'v1', 'created': 'Mon, 10 May 2021 19:56:45 GMT'}, {'version': 'v2', 'created': 'Wed, 9 Mar 2022 18:39:02 GMT'}]",2022-03-10,['Computer Vision and Pattern Recognition'],"This paper presents SUPR-GAN, a novel Generative Adversarial Network (GAN) based system for anticipating events in laparoscopic and robotic surgery. SUPR-GAN is designed to predict surgical events such as instrument movements, tissue manipulations and organ interactions from videos of laparoscopic and robotic surgery. The proposed system consists of two components: a generative network for learning the spatial and temporal features of surgical events, and a discriminative network for recognizing the events. We evaluate the performance of SUPR-GAN on two publicly available datasets and show that the proposed system achieves state-of-the-art performance in surgical event anticipation. Our results demonstrate the effectiveness of SUPR-GAN for recognizing surgical events in laparoscopic and robotic surgery, and its potential for improving the safety and efficiency of such surgeries.","Write an abstract for a paper called SUPR-GAN: SUrgical PRediction GAN for Event Anticipation in Laparoscopic
  and Robotic Surgery about Computer Vision and Pattern Recognition"
2204.11469,"Max Hopkins, Ting-Chun Lin",Explicit Lower Bounds Against $\Omega(n)$-Rounds of Sum-of-Squares,"['cs.CC', 'math.CO']","  We construct an explicit family of 3-XOR instances hard for
$\Omega(n)$-levels of the Sum-of-Squares (SoS) semi-definite programming
hierarchy. Not only is this the first explicit construction to beat brute force
search (beyond low-order improvements (Tulsiani 2021, Pratt 2021)), combined
with standard gap amplification techniques it also matches the (optimal)
hardness of random instances up to imperfect completeness (Grigoriev TCS 2001,
Schoenebeck FOCS 2008).
  Our result is based on a new form of small-set high dimensional expansion
(SS-HDX) inspired by recent breakthroughs in locally testable and quantum LDPC
codes. Adapting the recent framework of Dinur, Filmus, Harsha, and Tulsiani
(ITCS 2021) for SoS lower bounds from the Ramanujan complex to this setting, we
show any (bounded-degree) SS-HDX can be transformed into a highly unsatisfiable
3-XOR instance that cannot be refuted by $\Omega(n)$-levels of SoS. We then
show Leverrier and Z\'emor's (Arxiv 2022) recent qLDPC construction gives the
desired explicit family of bounded-degree SS-HDX. Incidentally, this gives the
strongest known form of bi-directional high dimensional expansion to date.
","[{'version': 'v1', 'created': 'Mon, 25 Apr 2022 07:02:39 GMT'}]",2022-04-26,['Computational Complexity'],"This paper presents explicit lower bounds against $\Omega(n)$-rounds of sum-of-squares algorithms for certain computational complexity problems, including the Boolean satisfiability problem, the graph isomorphism problem, and the graph coloring problem. We show that any $\Omega(n)$-round sum-of-squares algorithm for these problems must have a running time that is at least exponential in the input size. We further show that any $\Omega(n)$-round sum-of-squares algorithm for these problems must have a running time that is at least superpolynomial in the input size. Finally, we discuss the implications of our results for the design of efficient algorithms for these problems.",Write an abstract for a paper called Explicit Lower Bounds Against $\Omega(n)$-Rounds of Sum-of-Squares about Computational Complexity
2210.01448,"Tenglong Ao, Qingzhe Gao, Yuke Lou, Baoquan Chen, Libin Liu","Rhythmic Gesticulator: Rhythm-Aware Co-Speech Gesture Synthesis with
  Hierarchical Neural Embeddings","['cs.SD', 'cs.AI', 'cs.CV', 'cs.GR', 'eess.AS']","  Automatic synthesis of realistic co-speech gestures is an increasingly
important yet challenging task in artificial embodied agent creation. Previous
systems mainly focus on generating gestures in an end-to-end manner, which
leads to difficulties in mining the clear rhythm and semantics due to the
complex yet subtle harmony between speech and gestures. We present a novel
co-speech gesture synthesis method that achieves convincing results both on the
rhythm and semantics. For the rhythm, our system contains a robust rhythm-based
segmentation pipeline to ensure the temporal coherence between the vocalization
and gestures explicitly. For the gesture semantics, we devise a mechanism to
effectively disentangle both low- and high-level neural embeddings of speech
and motion based on linguistic theory. The high-level embedding corresponds to
semantics, while the low-level embedding relates to subtle variations. Lastly,
we build correspondence between the hierarchical embeddings of the speech and
the motion, resulting in rhythm- and semantics-aware gesture synthesis.
Evaluations with existing objective metrics, a newly proposed rhythmic metric,
and human feedback show that our method outperforms state-of-the-art systems by
a clear margin.
","[{'version': 'v1', 'created': 'Tue, 4 Oct 2022 08:19:06 GMT'}, {'version': 'v2', 'created': 'Wed, 5 Oct 2022 03:08:03 GMT'}]",2022-10-24,"['Sound', 'Artificial Intelligence', 'Computer Vision and Pattern Recognition', 'Graphics']","This paper presents a novel approach to co-speech gesture synthesis, called Rhythmic Gesticulator. This method combines sound, artificial intelligence, computer vision and pattern recognition, and graphics to generate realistic and expressive gestures. Rhythmic Gesticulator uses hierarchical neural embeddings to capture the rhythmic structure of speech, while also taking into account the context and content of the speech. This method also leverages the temporal and spatial information of the gesture to generate a realistic and expressive motion. Experiments conducted on a dataset of human speech and gesture pairs demonstrate the effectiveness of Rhythmic Gesticulator in producing realistic and expressive gestures. The results of the experiments show that our approach outperforms existing methods in terms of gesture realism and expressiveness.","Write an abstract for a paper called Rhythmic Gesticulator: Rhythm-Aware Co-Speech Gesture Synthesis with
  Hierarchical Neural Embeddings about Sound, Artificial Intelligence, Computer Vision and Pattern Recognition, Graphics"
2210.16283,Mattia Pugliatti and Francesco Topputo,"Boulders Identification on Small Bodies Under Varying Illumination
  Conditions","['cs.CV', 'eess.IV']","  The capability to detect boulders on the surface of small bodies is
beneficial for vision-based applications such as navigation and hazard
detection during critical operations. This task is challenging due to the wide
assortment of irregular shapes, the characteristics of the boulders population,
and the rapid variability in the illumination conditions. The authors address
this challenge by designing a multi-step training approach to develop a
data-driven image processing pipeline to robustly detect and segment boulders
scattered over the surface of a small body. Due to the limited availability of
labeled image-mask pairs, the developed methodology is supported by two
artificial environments designed in Blender specifically for this work. These
are used to generate a large amount of synthetic image-label sets, which are
made publicly available to the image processing community. The methodology
presented addresses the challenges of varying illumination conditions,
irregular shapes, fast training time, extensive exploration of the architecture
design space, and domain gap between synthetic and real images from previously
flown missions. The performance of the developed image processing pipeline is
tested both on synthetic and real images, exhibiting good performances, and
high generalization capabilities
","[{'version': 'v1', 'created': 'Fri, 28 Oct 2022 17:22:46 GMT'}]",2022-10-31,['Computer Vision and Pattern Recognition'],"This paper presents a novel approach to identify and classify boulders on small bodies such as asteroids and comets under varying illumination conditions using computer vision and pattern recognition. The proposed method extracts features from an image of a small body, such as shape, texture, and color, and uses these features to classify the boulders. In addition, the proposed algorithm is robust to the changes in illumination conditions, which are often present in space exploration. The performance of the proposed approach is evaluated on a benchmark dataset of images of small bodies. The results demonstrate that the proposed method is able to accurately identify and classify boulders on small bodies under varying illumination conditions.","Write an abstract for a paper called Boulders Identification on Small Bodies Under Varying Illumination
  Conditions about Computer Vision and Pattern Recognition"
2110.0158,Djoko Suprijanto and Hopein Christofen Tang,Skew cyclic codes over $\mathbb{Z}_4+v\mathbb{Z}_4$ with derivation,"['cs.IT', 'math.IT']","  In this work, we study a class of skew cyclic codes over the ring
$R:=\mathbb{Z}_4+v\mathbb{Z}_4,$ where $v^2=v,$ with an automorphism $\theta$
and a derivation $\Delta_\theta,$ namely codes as modules over a skew
polynomial ring $R[x;\theta,\Delta_{\theta}],$ whose multiplication is defined
using an automorphism $\theta$ and a derivation $\Delta_{\theta}.$ We
investigate the structures of a skew polynomial ring
$R[x;\theta,\Delta_{\theta}].$ We define $\Delta_{\theta}$-cyclic codes as a
generalization of the notion of cyclic codes. The properties of
$\Delta_{\theta}$-cyclic codes as well as dual $\Delta_{\theta}$-cyclic codes
are derived. Some new codes over $\mathbb{Z}_4$ with good parameters are
obtained via a Gray map as well as residue and torsion codes of these codes.
","[{'version': 'v1', 'created': 'Mon, 4 Oct 2021 17:23:49 GMT'}, {'version': 'v2', 'created': 'Fri, 15 Oct 2021 11:51:50 GMT'}, {'version': 'v3', 'created': 'Tue, 22 Feb 2022 15:36:03 GMT'}]",2022-02-23,['Information Theory'],"This paper provides an overview of skew cyclic codes over $\mathbb{Z}_4+v\mathbb{Z}_4$ and their application to Information Theory. We introduce the concept of skew cyclic codes and discuss their properties, including their linearity, minimum distance, and generator polynomials. We also derive a new family of skew cyclic codes over $\mathbb{Z}_4+v\mathbb{Z}_4$ and analyze their performance in terms of information rate and error correction capability. Finally, we discuss the implications of our results for Information Theory.",Write an abstract for a paper called Skew cyclic codes over $\mathbb{Z}_4+v\mathbb{Z}_4$ with derivation about Information Theory
2303.11307,"Shu-Hao Yeh, Shuangyu Xie, Di Wang, Wei Yan, and Dezhen Song","DIME-Net: Neural Network-Based Dynamic Intrinsic Parameter Rectification
  for Cameras with Optical Image Stabilization System","['cs.CV', 'cs.RO']","  Optical Image Stabilization (OIS) system in mobile devices reduces image
blurring by steering lens to compensate for hand jitters. However, OIS changes
intrinsic camera parameters (i.e. $\mathrm{K}$ matrix) dynamically which
hinders accurate camera pose estimation or 3D reconstruction. Here we propose a
novel neural network-based approach that estimates $\mathrm{K}$ matrix in
real-time so that pose estimation or scene reconstruction can be run at camera
native resolution for the highest accuracy on mobile devices. Our network
design takes gratified projection model discrepancy feature and 3D point
positions as inputs and employs a Multi-Layer Perceptron (MLP) to approximate
$f_{\mathrm{K}}$ manifold. We also design a unique training scheme for this
network by introducing a Back propagated PnP (BPnP) layer so that reprojection
error can be adopted as the loss function. The training process utilizes
precise calibration patterns for capturing accurate $f_{\mathrm{K}}$ manifold
but the trained network can be used anywhere. We name the proposed Dynamic
Intrinsic Manifold Estimation network as DIME-Net and have it implemented and
tested on three different mobile devices. In all cases, DIME-Net can reduce
reprojection error by at least $64\%$ indicating that our design is successful.
","[{'version': 'v1', 'created': 'Mon, 20 Mar 2023 17:45:12 GMT'}]",2023-03-21,"['Computer Vision and Pattern Recognition', 'Robotics']","This paper presents DIME-Net, a novel Neural Network-based Dynamic Intrinsic Parameter Rectification (DIPR) method for cameras with optical image stabilization (OIS) systems. DIME-Net leverages both the OIS system and a deep learning-based network to rectify the intrinsic parameters of the camera in real-time. The proposed method is evaluated on both synthetic and real-world datasets and compared to existing methods. Results show that DIME-Net is able to accurately and quickly rectify the intrinsic parameters of the camera, leading to improved accuracy and robustness in computer vision and pattern recognition tasks. Moreover, the proposed approach is also applicable to robotics applications, where the rectified parameters can be used to improve the accuracy of robot localization and navigation.","Write an abstract for a paper called DIME-Net: Neural Network-Based Dynamic Intrinsic Parameter Rectification
  for Cameras with Optical Image Stabilization System about Computer Vision and Pattern Recognition, Robotics"
2210.03538,"Andreas Triantafyllopoulos, Bj\""orn W. Schuller, G\""ok\c{c}e \.Iymen,
  Metin Sezgin, Xiangheng He, Zijiang Yang, Panagiotis Tzirakis, Shuo Liu,
  Silvan Mertes, Elisabeth Andr\'e, Ruibo Fu, Jianhua Tao","An Overview of Affective Speech Synthesis and Conversion in the Deep
  Learning Era","['cs.SD', 'cs.LG']","  Speech is the fundamental mode of human communication, and its synthesis has
long been a core priority in human-computer interaction research. In recent
years, machines have managed to master the art of generating speech that is
understandable by humans. But the linguistic content of an utterance
encompasses only a part of its meaning. Affect, or expressivity, has the
capacity to turn speech into a medium capable of conveying intimate thoughts,
feelings, and emotions -- aspects that are essential for engaging and
naturalistic interpersonal communication. While the goal of imparting
expressivity to synthesised utterances has so far remained elusive, following
recent advances in text-to-speech synthesis, a paradigm shift is well under way
in the fields of affective speech synthesis and conversion as well. Deep
learning, as the technology which underlies most of the recent advances in
artificial intelligence, is spearheading these efforts. In the present
overview, we outline ongoing trends and summarise state-of-the-art approaches
in an attempt to provide a comprehensive overview of this exciting field.
","[{'version': 'v1', 'created': 'Thu, 6 Oct 2022 13:55:59 GMT'}]",2023-03-14,"['Sound', 'Machine Learning']","This paper presents an overview of affective speech synthesis and conversion in the deep learning era, with a focus on sound and machine learning. It reviews the state-of-the-art techniques and tools for affective speech synthesis and conversion, such as neural networks, deep learning, and natural language processing. The paper also explores the potential applications of affective speech synthesis and conversion, including automated customer service, virtual assistants, and speech recognition. Finally, the paper discusses the challenges and opportunities of affective speech synthesis and conversion in the deep learning era. The paper concludes with a discussion of the future directions of this field.","Write an abstract for a paper called An Overview of Affective Speech Synthesis and Conversion in the Deep
  Learning Era about Sound, Machine Learning"
2211.15261,"Tobias Runge, Tabea Bordis, Alex Potanin, Thomas Th\""um, Ina Schaefer",Flexible Correct-by-Construction Programming,"['cs.LO', 'cs.PL', 'cs.SE']","  Correctness-by-Construction (CbC) is an incremental program construction
process to construct functionally correct programs. The programs are
constructed stepwise along with a specification that is inherently guaranteed
to be satisfied. CbC is complex to use without specialized tool support, since
it needs a set of predefined refinement rules of fixed granularity which are
additional rules on top of the programming language. Each refinement rule
introduces a specific programming statement and developers cannot depart from
these rules to construct programs. CbC allows to develop software in a
structured and incremental way to ensure correctness, but the limited
flexibility is a disadvantage of CbC. In this work, we compare classic CbC with
CbC-Block and TraitCbC. Both approaches CbC-Block and TraitCbC, are related to
CbC, but they have new language constructs that enable a more flexible software
construction approach. We provide for both approaches a programming guideline,
which similar to CbC, leads to well-structured programs. CbC-Block extends CbC
by adding a refinement rule to insert any block of statements. Therefore, we
introduce CbC-Block as an extension of CbC. TraitCbC implements
correctness-by-construction on the basis of traits with specified methods. We
formally introduce TraitCbC and prove soundness of the construction strategy.
All three development approaches are qualitatively compared regarding their
programming constructs, tool support, and usability to assess which is best
suited for certain tasks and developers.
","[{'version': 'v1', 'created': 'Mon, 28 Nov 2022 12:28:38 GMT'}, {'version': 'v2', 'created': 'Tue, 28 Mar 2023 10:47:15 GMT'}]",2023-03-29,"['Logic in Computer Science', 'Programming Languages', 'Software Engineering']","This paper presents a novel approach to programming, called Flexible Correct-by-Construction Programming, which uses logic in computer science, programming languages, and software engineering to ensure correctness of programs. We propose a new framework that combines a logical specification language with a programming language to enable the development of correct-by-construction programs. We present a case study that demonstrates the effectiveness of our approach and discuss its potential applications to software engineering. We also discuss the challenges and opportunities associated with our approach. Finally, we conclude with a discussion of the potential for future research in this area.","Write an abstract for a paper called Flexible Correct-by-Construction Programming about Logic in Computer Science, Programming Languages, Software Engineering"
2204.07412,"Zahra Babaiee and Lucas Liebenwein and Ramin Hasani and Daniela Rus
  and Radu Grosu",End-to-End Sensitivity-Based Filter Pruning,"['cs.CV', 'cs.AI', 'cs.LG']","  In this paper, we present a novel sensitivity-based filter pruning algorithm
(SbF-Pruner) to learn the importance scores of filters of each layer
end-to-end. Our method learns the scores from the filter weights, enabling it
to account for the correlations between the filters of each layer. Moreover, by
training the pruning scores of all layers simultaneously our method can account
for layer interdependencies, which is essential to find a performant sparse
sub-network. Our proposed method can train and generate a pruned network from
scratch in a straightforward, one-stage training process without requiring a
pretrained network. Ultimately, we do not need layer-specific hyperparameters
and pre-defined layer budgets, since SbF-Pruner can implicitly determine the
appropriate number of channels in each layer. Our experimental results on
different network architectures suggest that SbF-Pruner outperforms advanced
pruning methods. Notably, on CIFAR-10, without requiring a pretrained baseline
network, we obtain 1.02% and 1.19% accuracy gain on ResNet56 and ResNet110,
compared to the baseline reported for state-of-the-art pruning algorithms. This
is while SbF-Pruner reduces parameter-count by 52.3% (for ResNet56) and 54%
(for ResNet101), which is better than the state-of-the-art pruning algorithms
with a high margin of 9.5% and 6.6%.
","[{'version': 'v1', 'created': 'Fri, 15 Apr 2022 10:21:05 GMT'}]",2022-04-18,"['Computer Vision and Pattern Recognition', 'Artificial Intelligence', 'Machine Learning']","This paper proposes a novel end-to-end sensitivity-based filter pruning method for computer vision and pattern recognition tasks. The proposed method is based on the idea that filters can be pruned by evaluating their sensitivity to the task-specific loss function. We demonstrate that the proposed method can be used to reduce the number of filters in a convolutional neural network (CNN) while maintaining or even improving the performance of the model. Experiments on various datasets demonstrate that the proposed method can achieve significant compression rates with minimal accuracy loss. The results suggest that the proposed method is an effective and efficient way to reduce the complexity of deep learning models without sacrificing accuracy. Furthermore, this method can be used in combination with other pruning methods to further reduce model complexity.","Write an abstract for a paper called End-to-End Sensitivity-Based Filter Pruning about Computer Vision and Pattern Recognition, Artificial Intelligence, Machine Learning"
2207.10254,"Samuel C. Gutekunst, Billy Jin, David P. Williamson",The Two-Stripe Symmetric Circulant TSP is in P,"['cs.DM', 'cs.DS', 'math.CO']","  The symmetric circulant TSP is a special case of the traveling salesman
problem in which edge costs are symmetric and obey circulant symmetry. Despite
the substantial symmetry of the input, remarkably little is known about the
symmetric circulant TSP, and the complexity of the problem has been an
often-cited open question. Considerable effort has been made to understand the
case in which only edges of two lengths are allowed to have finite cost: the
two-stripe symmetric circulant TSP. In this paper, we resolve the complexity of
the two-stripe symmetric circulant TSP. To do so, we reduce two-stripe
symmetric circulant TSP to the problem of finding certain minimum-cost
Hamiltonian paths on cylindrical graphs. We then solve this Hamiltonian path
problem. Our results show that the two-stripe symmetric circulant TSP is in P.
Note that a two-stripe symmetric circulant TSP instance consists of a constant
number of inputs (including $n$, the number of cities), so that a
polynomial-time algorithm for the decision problem must run in time
polylogarithmic in $n$, and a polynomial-time algorithm for the optimization
problem cannot output the tour. We address this latter difficulty by showing
that the optimal tour must fall into one of two parameterized classes of tours,
and that we can output the class and the parameters in polynomial time. Thus we
make a substantial contribution to the set of polynomial-time solvable special
cases of the TSP, and take an important step towards resolving the complexity
of the general symmetric circulant TSP.
","[{'version': 'v1', 'created': 'Thu, 21 Jul 2022 01:32:19 GMT'}]",2022-07-22,"['Discrete Mathematics', 'Data Structures and Algorithms']","This paper examines the Two-Stripe Symmetric Circulant Traveling Salesman Problem (TSP) and its complexity in the context of discrete mathematics, data structures and algorithms. The Two-Stripe Symmetric Circulant TSP is a variant of the classic TSP, where the graph is a two-stripe symmetric circulant graph. We provide a polynomial-time algorithm for solving the Two-Stripe Symmetric Circulant TSP, and discuss its implications for the field of discrete mathematics, data structures and algorithms. We also discuss the relationship between the Two-Stripe Symmetric Circulant TSP and other variants of the classic TSP, and how our algorithm can be applied to other problems. Finally, we provide an implementation of our algorithm and discuss its performance.","Write an abstract for a paper called The Two-Stripe Symmetric Circulant TSP is in P about Discrete Mathematics, Data Structures and Algorithms"
2105.01159,"Farnaz H. Foomani, D. M. Anisuzzaman, Jeffrey Niezgoda, Jonathan
  Niezgoda, William Guns, Sandeep Gopalakrishnan, Zeyun Yu","Synthesizing time-series wound prognosis factors from electronic medical
  records using generative adversarial networks","['cs.AI', 'cs.LG']","  Wound prognostic models not only provide an estimate of wound healing time to
motivate patients to follow up their treatments but also can help clinicians to
decide whether to use a standard care or adjuvant therapies and to assist them
with designing clinical trials. However, collecting prognosis factors from
Electronic Medical Records (EMR) of patients is challenging due to privacy,
sensitivity, and confidentiality. In this study, we developed time series
medical generative adversarial networks (GANs) to generate synthetic wound
prognosis factors using very limited information collected during routine care
in a specialized wound care facility. The generated prognosis variables are
used in developing a predictive model for chronic wound healing trajectory. Our
novel medical GAN can produce both continuous and categorical features from
EMR. Moreover, we applied temporal information to our model by considering data
collected from the weekly follow-ups of patients. Conditional training
strategies were utilized to enhance training and generate classified data in
terms of healing or non-healing. The ability of the proposed model to generate
realistic EMR data was evaluated by TSTR (test on the synthetic, train on the
real), discriminative accuracy, and visualization. We utilized samples
generated by our proposed GAN in training a prognosis model to demonstrate its
real-life application. Using the generated samples in training predictive
models improved the classification accuracy by 6.66-10.01% compared to the
previous EMR-GAN. Additionally, the suggested prognosis classifier has achieved
the area under the curve (AUC) of 0.975, 0.968, and 0.849 when training the
network using data from the first three visits, first two visits, and first
visit, respectively. These results indicate a significant improvement in wound
healing prediction compared to the previous prognosis models.
","[{'version': 'v1', 'created': 'Mon, 3 May 2021 20:26:48 GMT'}]",2022-01-03,"['Artificial Intelligence', 'Machine Learning']","This paper proposes a novel approach for synthesizing time-series wound prognosis factors from electronic medical records using generative adversarial networks (GANs). GANs are a type of artificial intelligence and machine learning technique that can be used to generate realistic data from existing data. In this paper, we propose a GAN-based model to synthesize time-series wound prognosis factors from electronic medical records. We demonstrate the effectiveness of our proposed model on a real-world dataset. Our results show that our model is able to accurately synthesize time-series wound prognosis factors from electronic medical records and can be used to improve the accuracy of wound prognosis.","Write an abstract for a paper called Synthesizing time-series wound prognosis factors from electronic medical
  records using generative adversarial networks about Artificial Intelligence, Machine Learning"
2109.14383,"Leila Eftekhari, Mohammad M. Amirian","Stability Analysis of Fractional Order Memristor Synapse-coupled
  Hopfield Neural Network with Ring Structure","['math.DS', 'cs.NE']","  A memristor is a nonlinear two-terminal electrical element that incorporates
memory features and nanoscale properties, enabling us to design very
high-density artificial neural networks. To enhance the memory property, we
should use mathematical frameworks like fractional calculus, which is capable
of doing so. Here, we first present a fractional-order memristor
synapse-coupling Hopfield neural network on two neurons and then extend the
model to a neural network with a ring structure that consists of n sub-network
neurons, increasing the synchronization in the network. Necessary and
sufficient conditions for the stability of equilibrium points are investigated,
highlighting the dependency of the stability on the fractional-order value and
the number of neurons. Numerical simulations and bifurcation analysis, along
with Lyapunov exponents, are given in the two-neuron case that substantiates
the theoretical findings, suggesting possible routes towards chaos when the
fractional order of the system increases. In the n-neuron case also, it is
revealed that the stability depends on the structure and number of
sub-networks.
","[{'version': 'v1', 'created': 'Wed, 29 Sep 2021 12:33:23 GMT'}, {'version': 'v2', 'created': 'Wed, 6 Jul 2022 13:56:45 GMT'}]",2022-07-07,['Neural and Evolutionary Computing'],"This paper presents a novel fractional order memristor synapse-coupled Hopfield neural network with ring structure for stability analysis. The proposed network is based on the fractional order memristor synapse-coupled Hopfield neural network and its ring structure. The fractional order memristor synapse-coupled Hopfield neural network is a novel approach to neural and evolutionary computing, which uses fractional order memristors as synaptic connections between neurons. The ring structure of the proposed network is designed to provide robustness to its stability. The stability of the proposed network is analyzed by deriving its Lyapunov function and computing its eigenvalues. The results show that the proposed network has good stability, and its stability is further improved with the ring structure. This paper provides a novel approach to neural and evolutionary computing, which has potential applications in many fields.","Write an abstract for a paper called Stability Analysis of Fractional Order Memristor Synapse-coupled
  Hopfield Neural Network with Ring Structure about Neural and Evolutionary Computing"
2109.13986,"Sean Welleck, Peter West, Jize Cao, Yejin Choi","Symbolic Brittleness in Sequence Models: on Systematic Generalization in
  Symbolic Mathematics",['cs.LG'],"  Neural sequence models trained with maximum likelihood estimation have led to
breakthroughs in many tasks, where success is defined by the gap between
training and test performance. However, their ability to achieve stronger forms
of generalization remains unclear. We consider the problem of symbolic
mathematical integration, as it requires generalizing systematically beyond the
test set. We develop a methodology for evaluating generalization that takes
advantage of the problem domain's structure and access to a verifier. Despite
promising in-distribution performance of sequence-to-sequence models in this
domain, we demonstrate challenges in achieving robustness, compositionality,
and out-of-distribution generalization, through both carefully constructed
manual test suites and a genetic algorithm that automatically finds large
collections of failures in a controllable manner. Our investigation highlights
the difficulty of generalizing well with the predominant modeling and learning
approach, and the importance of evaluating beyond the test set, across
different aspects of generalization.
","[{'version': 'v1', 'created': 'Tue, 28 Sep 2021 18:50:15 GMT'}, {'version': 'v2', 'created': 'Thu, 24 Feb 2022 18:43:41 GMT'}]",2022-02-25,['Machine Learning'],"This paper investigates the symbolic brittleness of sequence models in machine learning, the phenomenon of models failing to generalize beyond their training data. We examine the concept of systematic generalization in symbolic mathematics, which is a method of generalizing a set of symbols to a larger set of symbols. We analyze the implications of this approach for sequence models, and discuss how it can be used to improve their generalization capabilities. We then explore the potential of applying this approach to a variety of sequence models, including recurrent neural networks, convolutional neural networks, and long short-term memory networks. Finally, we discuss the benefits and limitations of this approach, and its implications for the future of machine learning.","Write an abstract for a paper called Symbolic Brittleness in Sequence Models: on Systematic Generalization in
  Symbolic Mathematics about Machine Learning"
2110.0278,"Danilo Avola, Andrea Bacciu, Luigi Cinque, Alessio Fagioli, Marco
  Raoul Marini, Riccardo Taiello","Study on Transfer Learning Capabilities for Pneumonia Classification in
  Chest-X-Rays Image","['eess.IV', 'cs.CV']","  Over the last year, the severe acute respiratory syndrome coronavirus-2
(SARS-CoV-2) and its variants have highlighted the importance of screening
tools with high diagnostic accuracy for new illnesses such as COVID-19. To that
regard, deep learning approaches have proven as effective solutions for
pneumonia classification, especially when considering chest-x-rays images.
However, this lung infection can also be caused by other viral, bacterial or
fungi pathogens. Consequently, efforts are being poured toward distinguishing
the infection source to help clinicians to diagnose the correct disease origin.
Following this tendency, this study further explores the effectiveness of
established neural network architectures on the pneumonia classification task
through the transfer learning paradigm. To present a comprehensive comparison,
12 well-known ImageNet pre-trained models were fine-tuned and used to
discriminate among chest-x-rays of healthy people, and those showing pneumonia
symptoms derived from either a viral (i.e., generic or SARS-CoV-2) or bacterial
source. Furthermore, since a common public collection distinguishing between
such categories is currently not available, two distinct datasets of
chest-x-rays images, describing the aforementioned sources, were combined and
employed to evaluate the various architectures. The experiments were performed
using a total of 6330 images split between train, validation and test sets. For
all models, common classification metrics were computed (e.g., precision,
f1-score) and most architectures obtained significant performances, reaching,
among the others, up to 84.46% average f1-score when discriminating the 4
identified classes. Moreover, confusion matrices and activation maps computed
via the Grad-CAM algorithm were also reported to present an informed discussion
on the networks classifications.
","[{'version': 'v1', 'created': 'Wed, 6 Oct 2021 14:00:18 GMT'}, {'version': 'v2', 'created': 'Sat, 23 Apr 2022 06:58:52 GMT'}]",2022-04-26,['Computer Vision and Pattern Recognition'],"This paper investigates the potential of transfer learning in the context of pneumonia classification in chest-X-ray images. We propose a novel deep learning model based on a convolutional neural network (CNN) and a transfer learning approach to classify chest-X-ray images into pneumonia or normal. The model is trained with a dataset of chest-X-ray images from the Montgomery County Chest X-ray Set and is evaluated on the ChestX-ray14 dataset. The results show that our proposed model outperforms the baseline model in terms of accuracy, sensitivity, and specificity. Furthermore, we discuss the implications of the results in terms of the potential of transfer learning for medical image classification.","Write an abstract for a paper called Study on Transfer Learning Capabilities for Pneumonia Classification in
  Chest-X-Rays Image about Computer Vision and Pattern Recognition"
2208.0088,"James Koch, Thomas Maxner, Vinay Amatya, Andisheh Ranjbari, Chase
  Dowling",Physics-informed Machine Learning of Parameterized Fundamental Diagrams,['cs.LG'],"  Fundamental diagrams describe the relationship between speed, flow, and
density for some roadway (or set of roadway) configuration(s). These diagrams
typically do not reflect, however, information on how speed-flow relationships
change as a function of exogenous variables such as curb configuration, weather
or other exogenous, contextual information. In this paper we present a machine
learning methodology that respects known engineering constraints and physical
laws of roadway flux - those that are captured in fundamental diagrams - and
show how this can be used to introduce contextual information into the
generation of these diagrams. The modeling task is formulated as a probe
vehicle trajectory reconstruction problem with Neural Ordinary Differential
Equations (Neural ODEs). With the presented methodology, we extend the
fundamental diagram to non-idealized roadway segments with potentially
obstructed traffic data. For simulated data, we generalize this relationship by
introducing contextual information at the learning stage, i.e. vehicle
composition, driver behavior, curb zoning configuration, etc, and show how the
speed-flow relationship changes as a function of these exogenous factors
independent of roadway design.
","[{'version': 'v1', 'created': 'Mon, 1 Aug 2022 14:13:06 GMT'}]",2022-08-02,['Machine Learning'],"This paper presents a physics-informed machine learning approach to model parameterized fundamental diagrams (FDs) of traffic flow. The FDs are a set of curves that describe the relationship between traffic flow and density, and are used to model the behavior of traffic on roads. The proposed method uses a deep learning neural network to learn the parameters of the FDs, while incorporating physical constraints to ensure that the model is physically consistent. The model is tested on a real-world traffic dataset, and results show that the proposed method is able to accurately capture the FDs parameters and outperform existing methods. The paper concludes with a discussion of the potential applications of the proposed approach.",Write an abstract for a paper called Physics-informed Machine Learning of Parameterized Fundamental Diagrams about Machine Learning
2301.10407,Lauren Alvarez and Tim Menzies,Don't Lie to Me: Avoiding Malicious Explanations with STEALTH,"['cs.SE', 'cs.AI', 'cs.CR']","  STEALTH is a method for using some AI-generated model, without suffering from
malicious attacks (i.e. lying) or associated unfairness issues. After
recursively bi-clustering the data, STEALTH system asks the AI model a limited
number of queries about class labels. STEALTH asks so few queries (1 per data
cluster) that malicious algorithms (a) cannot detect its operation, nor (b)
know when to lie.
","[{'version': 'v1', 'created': 'Wed, 25 Jan 2023 05:00:16 GMT'}]",2023-01-26,"['Software Engineering', 'Artificial Intelligence', 'Cryptography and Security']","This paper examines the potential of using STEALTH, a combination of Software Engineering, Artificial Intelligence, Cryptography and Security, to protect against malicious explanations. The paper first reviews existing approaches to detecting malicious explanations, such as manual methods and machine learning, and then looks at the potential of STEALTH to provide more effective protection. The paper then provides an overview of the STEALTH approach and its components, before discussing potential applications and the implications for security and privacy. The paper concludes by discussing the potential of STEALTH to provide better protection against malicious explanations and the need for further research in this area.","Write an abstract for a paper called Don't Lie to Me: Avoiding Malicious Explanations with STEALTH about Software Engineering, Artificial Intelligence, Cryptography and Security"
2006.03814,"Davide Buffelli, Fabio Vandin","The Impact of Global Structural Information in Graph Neural Networks
  Applications","['cs.LG', 'cs.SI', 'stat.ML']","  Graph Neural Networks (GNNs) rely on the graph structure to define an
aggregation strategy where each node updates its representation by combining
information from its neighbours. A known limitation of GNNs is that, as the
number of layers increases, information gets smoothed and squashed and node
embeddings become indistinguishable, negatively affecting performance.
Therefore, practical GNN models employ few layers and only leverage the graph
structure in terms of limited, small neighbourhoods around each node.
Inevitably, practical GNNs do not capture information depending on the global
structure of the graph. While there have been several works studying the
limitations and expressivity of GNNs, the question of whether practical
applications on graph structured data require global structural knowledge or
not, remains unanswered. In this work, we empirically address this question by
giving access to global information to several GNN models, and observing the
impact it has on downstream performance. Our results show that global
information can in fact provide significant benefits for common graph-related
tasks. We further identify a novel regularization strategy that leads to an
average accuracy improvement of more than 5% on all considered tasks.
","[{'version': 'v1', 'created': 'Sat, 6 Jun 2020 08:52:18 GMT'}, {'version': 'v2', 'created': 'Wed, 15 Dec 2021 17:37:31 GMT'}]",2022-01-19,"['Machine Learning', 'Social and Information Networks']","This paper explores the impact of global structural information in Graph Neural Networks (GNNs) for applications in Machine Learning, Social and Information Networks. GNNs are a powerful tool for analyzing graph-structured data and have been used for a wide range of tasks such as node classification, link prediction, and graph classification. However, GNNs are limited in their ability to capture global structural information from graphs, which can be essential for certain tasks. In this paper, we examine the impact of incorporating global structural information into GNNs and evaluate the performance of various GNN architectures on a range of tasks. We also discuss the potential applications of GNNs with global structural information, such as detecting communities in social networks and detecting anomalies in information networks. Finally, we discuss the challenges and opportunities for future research in this area.","Write an abstract for a paper called The Impact of Global Structural Information in Graph Neural Networks
  Applications about Machine Learning, Social and Information Networks"
2201.11928,"Hua Chen, Zejun Hong, Shunpeng Yang, Patrick M. Wensing and Wei Zhang","Quadruped Capturability and Push Recovery via a Switched-Systems
  Characterization of Dynamic Balance",['cs.RO'],"  This paper studies capturability and push recovery for quadrupedal
locomotion. Despite the rich literature on capturability analysis and push
recovery control for legged robots, existing tools are developed mainly for
bipeds or humanoids. Distinct quadrupedal features such as point contacts and
multiple swinging legs prevent direct application of these methods. To address
this gap, we propose a switched systems model for quadruped dynamics, and
instantiate the abstract viability concept for quadrupedal locomotion with a
time-based gait. Capturability is characterized through a novel specification
of dynamically balanced states that addresses the time-varying nature of
quadrupedal locomotion and balance. A linear inverted pendulum (LIP) model is
adopted to demonstrate the theory and show how the newly developed quadrupedal
capturability can be used in motion planning for quadrupedal push recovery. We
formulate and solve an explicit model predictive control (EMPC) problem whose
optimal solution fully characterizes quadrupedal capturability with the LIP.
Given this analysis, an optimization-based planning scheme is devised for
determining footsteps and center of mass references during push recovery. To
validate the effectiveness of the overall framework, we conduct numerous
simulation and hardware experiments. Simulation results illustrate the
necessity of considering dynamic balance for quadrupedal capturability, and the
significant improvement in disturbance rejection with the proposed strategy.
Experimental validations on a replica of the Mini Cheetah quadruped demonstrate
an up to 100% improvement as compared with state-of-the-art.
","[{'version': 'v1', 'created': 'Fri, 28 Jan 2022 04:25:28 GMT'}, {'version': 'v2', 'created': 'Mon, 7 Feb 2022 04:37:50 GMT'}, {'version': 'v3', 'created': 'Thu, 17 Feb 2022 08:02:21 GMT'}]",2022-02-18,['Robotics'],"This paper presents a switched-systems characterization of dynamic balance for quadruped robots, and proposes a method for the capture and push recovery of such robots. The characterization is based on the analysis of the dynamical system describing the robot's motion, and the capture and push recovery are formulated as switching control problems. The proposed approach is validated through simulations of a robotic quadruped and experiments with a real quadruped robot. The results show that the proposed approach is effective for capturing and recovering the robot from external disturbances.","Write an abstract for a paper called Quadruped Capturability and Push Recovery via a Switched-Systems
  Characterization of Dynamic Balance about Robotics"
2210.08159,An Tao and Yueqi Duan and Yingqi Wang and Jiwen Lu and Jie Zhou,Dynamics-aware Adversarial Attack of Adaptive Neural Networks,['cs.CV'],"  In this paper, we investigate the dynamics-aware adversarial attack problem
of adaptive neural networks. Most existing adversarial attack algorithms are
designed under a basic assumption -- the network architecture is fixed
throughout the attack process. However, this assumption does not hold for many
recently proposed adaptive neural networks, which adaptively deactivate
unnecessary execution units based on inputs to improve computational
efficiency. It results in a serious issue of lagged gradient, making the
learned attack at the current step ineffective due to the architecture change
afterward. To address this issue, we propose a Leaded Gradient Method (LGM) and
show the significant effects of the lagged gradient. More specifically, we
reformulate the gradients to be aware of the potential dynamic changes of
network architectures, so that the learned attack better ""leads"" the next step
than the dynamics-unaware methods when network architecture changes
dynamically. Extensive experiments on representative types of adaptive neural
networks for both 2D images and 3D point clouds show that our LGM achieves
impressive adversarial attack performance compared with the dynamic-unaware
attack methods.
","[{'version': 'v1', 'created': 'Sat, 15 Oct 2022 01:32:08 GMT'}, {'version': 'v2', 'created': 'Wed, 25 Jan 2023 04:16:06 GMT'}]",2023-01-26,['Computer Vision and Pattern Recognition'],"This paper presents a dynamics-aware adversarial attack of adaptive neural networks for computer vision and pattern recognition applications. The proposed attack method is based on the concept of adversarial examples and utilizes the dynamics of an adaptive neural network to craft adversarial samples that can cause misclassification of the input data. The attack is evaluated on a range of computer vision and pattern recognition tasks, including image classification, object detection, and facial recognition. Results show that the proposed attack is effective for fooling deep learning models, and that it can be used to craft adversarial examples that are difficult to detect. The paper concludes by discussing the implications of the proposed attack and its potential applications in security and privacy.",Write an abstract for a paper called Dynamics-aware Adversarial Attack of Adaptive Neural Networks about Computer Vision and Pattern Recognition
2111.13167,"Giuseppe Orlando, Paolo Francesco Barbante, Luca Bonaventura","An efficient IMEX-DG solver for the compressible Navier-Stokes equations
  for non-ideal gases","['math.NA', 'cs.NA']","  We propose an efficient, accurate and robust IMEX solver for the compressible
Navier-Stokes equation describing non-ideal gases with general equations of
state. The method, which is based on an $h-$adaptive Discontinuos Galerkin
spatial discretization and on an Additive Runge Kutta IMEX method for time
discretization, is tailored for low Mach number applications and allows to
simulate low Mach regimes at a significantly reduced computational cost, while
maintaining full second order accuracy also for higher Mach number regimes. The
method has been implemented in the framework of the $deal.II$ numerical
library, whose adaptive mesh refinement capabilities are employed to enhance
efficiency. Refinement indicators appropriate for real gas phenomena have been
introduced. A number of numerical experiments on classical benchmarks for
compressible flows and their extension to real gases demonstrate the properties
of the proposed method.
","[{'version': 'v1', 'created': 'Thu, 25 Nov 2021 16:52:21 GMT'}, {'version': 'v2', 'created': 'Fri, 6 May 2022 12:55:18 GMT'}]",2022-12-08,['Numerical Analysis'],"This paper presents an efficient implicit-explicit (IMEX) discontinuous Galerkin (DG) solver for the compressible Navier-Stokes equations for non-ideal gases. The proposed solver is based on a semi-discrete DG formulation of the governing equations and a Runge-Kutta Discontinuous Galerkin (RKDG) time discretization. The spatial discretization is based on a high-order DG method, which allows for a high-fidelity approximation of the non-ideal gas equation of state. The proposed solver is tested using a suite of numerical experiments on two-dimensional benchmark problems, and the results demonstrate its accuracy and efficiency. The numerical results also show that the proposed solver is robust and capable of capturing the physics of non-ideal gases. The paper provides a detailed description of the numerical methods and discusses their advantages and limitations.","Write an abstract for a paper called An efficient IMEX-DG solver for the compressible Navier-Stokes equations
  for non-ideal gases about Numerical Analysis"
2212.14115,"Junlin Wu, Hussein Sibai and Yevgeniy Vorobeychik","Certifying Safety in Reinforcement Learning under Adversarial
  Perturbation Attacks","['cs.LG', 'cs.AI']","  Function approximation has enabled remarkable advances in applying
reinforcement learning (RL) techniques in environments with high-dimensional
inputs, such as images, in an end-to-end fashion, mapping such inputs directly
to low-level control. Nevertheless, these have proved vulnerable to small
adversarial input perturbations. A number of approaches for improving or
certifying robustness of end-to-end RL to adversarial perturbations have
emerged as a result, focusing on cumulative reward. However, what is often at
stake in adversarial scenarios is the violation of fundamental properties, such
as safety, rather than the overall reward that combines safety with efficiency.
Moreover, properties such as safety can only be defined with respect to true
state, rather than the high-dimensional raw inputs to end-to-end policies. To
disentangle nominal efficiency and adversarial safety, we situate RL in
deterministic partially-observable Markov decision processes (POMDPs) with the
goal of maximizing cumulative reward subject to safety constraints. We then
propose a partially-supervised reinforcement learning (PSRL) framework that
takes advantage of an additional assumption that the true state of the POMDP is
known at training time. We present the first approach for certifying safety of
PSRL policies under adversarial input perturbations, and two adversarial
training approaches that make direct use of PSRL. Our experiments demonstrate
both the efficacy of the proposed approach for certifying safety in adversarial
environments, and the value of the PSRL framework coupled with adversarial
training in improving certified safety while preserving high nominal reward and
high-quality predictions of true state.
","[{'version': 'v1', 'created': 'Wed, 28 Dec 2022 22:33:38 GMT'}]",2023-01-02,"['Machine Learning', 'Artificial Intelligence']","This paper presents a novel approach to certifying safety in reinforcement learning (RL) under adversarial perturbation attacks. We propose a verification-based technique for certifying safety of RL agents under adversarial perturbations, which is based on a combination of robust optimization and formal verification. We demonstrate the effectiveness of our approach through experiments on a simulated RL environment and a real-world robotic manipulation task. Our results show that our approach can detect and prevent unsafe behaviors of the RL agent under adversarial perturbations, while still allowing the agent to efficiently explore the environment and learn optimal policies. Additionally, we discuss the potential of our approach for enhancing the safety of RL agents in real-world applications of artificial intelligence.","Write an abstract for a paper called Certifying Safety in Reinforcement Learning under Adversarial
  Perturbation Attacks about Machine Learning, Artificial Intelligence"
2206.10991,"Francesco Di Giovanni, James Rowbottom, Benjamin P. Chamberlain,
  Thomas Markovich, Michael M. Bronstein","Graph Neural Networks as Gradient Flows: understanding graph
  convolutions via energy","['cs.LG', 'stat.ML']","  Gradient flows are differential equations that minimize an energy functional
and constitute the main descriptors of physical systems. We apply this
formalism to Graph Neural Networks (GNNs) to develop new frameworks for
learning on graphs as well as provide a better theoretical understanding of
existing ones. We derive GNNs as a gradient flow equation of a parametric
energy that provides a physics-inspired interpretation of GNNs as learning
particle dynamics in the feature space. In particular, we show that in graph
convolutional models (GCN), the positive/negative eigenvalues of the channel
mixing matrix correspond to attractive/repulsive forces between adjacent
features. We rigorously prove how the channel-mixing can learn to steer the
dynamics towards low or high frequencies, which allows to deal with
heterophilic graphs. We show that the same class of energies is decreasing
along a larger family of GNNs; albeit not gradient flows, they retain their
inductive bias. We experimentally evaluate an instance of the gradient flow
framework that is principled, more efficient than GCN, and achieves competitive
performance on graph datasets of varying homophily often outperforming recent
baselines specifically designed to target heterophily.
","[{'version': 'v1', 'created': 'Wed, 22 Jun 2022 11:45:36 GMT'}, {'version': 'v2', 'created': 'Mon, 15 Aug 2022 14:48:24 GMT'}, {'version': 'v3', 'created': 'Sat, 8 Oct 2022 08:32:19 GMT'}]",2022-10-11,['Machine Learning'],"Graph Neural Networks (GNNs) are a powerful tool for learning from graph-structured data. In this paper, we explore the connection between GNNs and gradient flows, providing an understanding of graph convolutions as energy minimization problems. We discuss how GNNs can be interpreted as gradient flows, and how this interpretation can be used to analyze the behavior of GNNs. We also provide a theoretical framework for designing new GNNs, and discuss how the energy minimization interpretation can help explain the success of existing GNNs. Finally, we discuss the implications of our work for machine learning research.","Write an abstract for a paper called Graph Neural Networks as Gradient Flows: understanding graph
  convolutions via energy about Machine Learning"
2303.13101,"Bo Zhang, Zuheng Ming, Wei Feng, Yaqian Liu, Liang He, Kaixing Zhao","MMFormer: Multimodal Transformer Using Multiscale Self-Attention for
  Remote Sensing Image Classification",['cs.CV'],"  To benefit the complementary information between heterogeneous data, we
introduce a new Multimodal Transformer (MMFormer) for Remote Sensing (RS) image
classification using Hyperspectral Image (HSI) accompanied by another source of
data such as Light Detection and Ranging (LiDAR). Compared with traditional
Vision Transformer (ViT) lacking inductive biases of convolutions, we first
introduce convolutional layers to our MMFormer to tokenize patches from
multimodal data of HSI and LiDAR. Then we propose a Multi-scale Multi-head
Self-Attention (MSMHSA) module to address the problem of compatibility which
often limits to fuse HSI with high spectral resolution and LiDAR with
relatively low spatial resolution. The proposed MSMHSA module can incorporate
HSI to LiDAR data in a coarse-to-fine manner enabling us to learn a
fine-grained representation. Extensive experiments on widely used benchmarks
(e.g., Trento and MUUFL) demonstrate the effectiveness and superiority of our
proposed MMFormer for RS image classification.
","[{'version': 'v1', 'created': 'Thu, 23 Mar 2023 08:34:24 GMT'}]",2023-03-24,['Computer Vision and Pattern Recognition'],"This paper introduces MMFormer, a novel multimodal transformer architecture for remote sensing image classification. The proposed model integrates multi-scale self-attention mechanisms to capture the relevant features from the input images and fuse them in a unified representation. By leveraging the transformer architecture, MMFormer is able to effectively capture the long-range dependencies between the visual and multi-sensor modalities. Experiments conducted on two benchmark datasets demonstrate that MMFormer significantly outperforms the existing state-of-the-art models in terms of accuracy and efficiency. The results of this research suggest that the proposed model is a promising approach for remote sensing image classification tasks.","Write an abstract for a paper called MMFormer: Multimodal Transformer Using Multiscale Self-Attention for
  Remote Sensing Image Classification about Computer Vision and Pattern Recognition"
2206.13109,"Jarne Vandenabeele, Gilles Vermaut, Jari Peeperkorn, Jochen De Weerdt","Enhancing Stochastic Petri Net-based Remaining Time Prediction using
  k-Nearest Neighbors",['cs.LG'],"  Reliable remaining time prediction of ongoing business processes is a highly
relevant topic. One example is order delivery, a key competitive factor in e.g.
retailing as it is a main driver of customer satisfaction. For realising timely
delivery, an accurate prediction of the remaining time of the delivery process
is crucial. Within the field of process mining, a wide variety of remaining
time prediction techniques have already been proposed. In this work, we extend
remaining time prediction based on stochastic Petri nets with generally
distributed transitions with k-nearest neighbors. The k-nearest neighbors
algorithm is performed on simple vectors storing the time passed to complete
previous activities. By only taking a subset of instances, a more
representative and stable stochastic Petri Net is obtained, leading to more
accurate time predictions. We discuss the technique and its basic
implementation in Python and use different real world data sets to evaluate the
predictive power of our extension. These experiments show clear advantages in
combining both techniques with regard to predictive power.
","[{'version': 'v1', 'created': 'Mon, 27 Jun 2022 08:27:35 GMT'}]",2022-11-16,['Machine Learning'],"This paper presents a novel approach for enhancing the accuracy of remaining time prediction in Stochastic Petri Net-based systems using k-Nearest Neighbors (kNN) machine learning algorithm. The proposed approach is evaluated on a real-world system, composed of a set of Petri nets, and the results show that it outperforms the existing methods in terms of accuracy. The proposed approach is based on a kNN classifier which is trained using the system's history data. The classifier is used to predict the remaining time of the system's current state. Furthermore, a comparison between the proposed approach and the existing methods is presented in terms of accuracy and computational complexity. The results of the comparison show that the proposed approach improves the existing methods in terms of accuracy, while keeping the computational complexity low.","Write an abstract for a paper called Enhancing Stochastic Petri Net-based Remaining Time Prediction using
  k-Nearest Neighbors about Machine Learning"
2102.10539,"Marcin Waniek, Manuel Cebrian, Petter Holme, Talal Rahwan",Social Diffusion Sources Can Escape Detection,"['cs.SI', 'physics.soc-ph']","  Influencing (and being influenced by) others through social networks is
fundamental to all human societies. Whether this happens through the diffusion
of rumors, opinions, or viruses, identifying the diffusion source (i.e., the
person that initiated it) is a problem that has attracted much research
interest. Nevertheless, existing literature has ignored the possibility that
the source might strategically modify the network structure (by rewiring links
or introducing fake nodes) to escape detection. Here, without restricting our
analysis to any particular diffusion scenario, we close this gap by evaluating
two mechanisms that hide the source-one stemming from the source's actions, the
other from the network structure itself. This reveals that sources can easily
escape detection, and that removing links is far more effective than
introducing fake nodes. Thus, efforts should focus on exposing concealed ties
rather than planted entities; such exposure would drastically improve our
chances of detecting the diffusion source.
","[{'version': 'v1', 'created': 'Sun, 21 Feb 2021 07:41:12 GMT'}, {'version': 'v2', 'created': 'Thu, 11 Nov 2021 06:44:34 GMT'}]",2022-09-05,['Social and Information Networks'],"This paper examines the phenomenon of social diffusion sources, which are able to spread information through social and information networks without detection. Through an analysis of existing literature, the paper examines the various ways in which social diffusion sources can remain undetected, such as through the use of anonymous accounts, the manipulation of algorithms, or the use of bots. Additionally, the paper explores the implications of this phenomenon, including the potential for malicious actors to spread false information and the challenge of detecting and preventing such activities. Finally, the paper provides recommendations for how to address this issue, such as by increasing transparency and accountability in social networks and by developing better methods for detecting and mitigating malicious activity.",Write an abstract for a paper called Social Diffusion Sources Can Escape Detection about Social and Information Networks
2205.07123,"Natalia Tomashenko, Brij Mohan Lal Srivastava, Xin Wang, Emmanuel
  Vincent, Andreas Nautsch, Junichi Yamagishi, Nicholas Evans, Jose Patino,
  Jean-Fran\c{c}ois Bonastre, Paul-Gauthier No\'e, Massimiliano Todisco",The VoicePrivacy 2020 Challenge Evaluation Plan,"['cs.CL', 'cs.CR', 'eess.AS']","  The VoicePrivacy Challenge aims to promote the development of privacy
preservation tools for speech technology by gathering a new community to define
the tasks of interest and the evaluation methodology, and benchmarking
solutions through a series of challenges. In this document, we formulate the
voice anonymization task selected for the VoicePrivacy 2020 Challenge and
describe the datasets used for system development and evaluation. We also
present the attack models and the associated objective and subjective
evaluation metrics. We introduce two anonymization baselines and report
objective evaluation results.
","[{'version': 'v1', 'created': 'Sat, 14 May 2022 20:05:51 GMT'}]",2022-05-17,"['Computation and Language', 'Cryptography and Security']","This paper presents an evaluation plan for the VoicePrivacy 2020 Challenge, a competition focused on the development of secure voice communication systems. The evaluation plan outlines the criteria for assessing the performance of submitted systems in areas related to computation and language, cryptography and security. The evaluation plan is designed to reward systems that demonstrate a high level of security and privacy, while maintaining a balance between accuracy and computational efficiency. The evaluation plan also provides guidance on how to evaluate the usability and user experience of submitted systems. Finally, the paper outlines the process for selecting the winning system, and how the results of the evaluation will be used to inform future research and development efforts.","Write an abstract for a paper called The VoicePrivacy 2020 Challenge Evaluation Plan about Computation and Language, Cryptography and Security"
2005.03161,"Sanjay Kariyappa, Atul Prakash, Moinuddin Qureshi","MAZE: Data-Free Model Stealing Attack Using Zeroth-Order Gradient
  Estimation","['stat.ML', 'cs.LG']","  Model Stealing (MS) attacks allow an adversary with black-box access to a
Machine Learning model to replicate its functionality, compromising the
confidentiality of the model. Such attacks train a clone model by using the
predictions of the target model for different inputs. The effectiveness of such
attacks relies heavily on the availability of data necessary to query the
target model. Existing attacks either assume partial access to the dataset of
the target model or availability of an alternate dataset with semantic
similarities. This paper proposes MAZE -- a data-free model stealing attack
using zeroth-order gradient estimation. In contrast to prior works, MAZE does
not require any data and instead creates synthetic data using a generative
model. Inspired by recent works in data-free Knowledge Distillation (KD), we
train the generative model using a disagreement objective to produce inputs
that maximize disagreement between the clone and the target model. However,
unlike the white-box setting of KD, where the gradient information is
available, training a generator for model stealing requires performing
black-box optimization, as it involves accessing the target model under attack.
MAZE relies on zeroth-order gradient estimation to perform this optimization
and enables a highly accurate MS attack. Our evaluation with four datasets
shows that MAZE provides a normalized clone accuracy in the range of 0.91x to
0.99x, and outperforms even the recent attacks that rely on partial data (JBDA,
clone accuracy 0.13x to 0.69x) and surrogate data (KnockoffNets, clone accuracy
0.52x to 0.97x). We also study an extension of MAZE in the partial-data setting
and develop MAZE-PD, which generates synthetic data closer to the target
distribution. MAZE-PD further improves the clone accuracy (0.97x to 1.0x) and
reduces the query required for the attack by 2x-24x.
","[{'version': 'v1', 'created': 'Wed, 6 May 2020 22:26:18 GMT'}, {'version': 'v2', 'created': 'Fri, 28 Oct 2022 21:08:42 GMT'}]",2022-11-02,['Machine Learning'],"This paper presents MAZE, a novel data-free model stealing attack for machine learning. MAZE is an attack that can steal a model without access to any of the training data used to create it. The attack works by using zeroth-order gradient estimation to estimate the gradients of the target model, which are then used to construct an attack model that has the same behavior as the target model. Experiments on real-world datasets demonstrate that MAZE can successfully steal a model without access to the training data. The paper also provides an analysis of the attack's security and robustness, as well as a discussion of potential countermeasures.","Write an abstract for a paper called MAZE: Data-Free Model Stealing Attack Using Zeroth-Order Gradient
  Estimation about Machine Learning"
2302.12029,"Magnus Berg, Joan Boyar, Lene M. Favrholdt, Kim S. Larsen",Online Minimum Spanning Trees with Weight Predictions,['cs.DS'],"  We consider the minimum spanning tree problem with predictions, using the
weight-arrival model, i.e., the graph is given, together with predictions for
the weights of all edges. Then the actual weights arrive one at a time and an
irrevocable decision must be made regarding whether or not the edge should be
included into the spanning tree. In order to assess the quality of our
algorithms, we define an appropriate error measure and analyze the performance
of the algorithms as a function of the error. We prove that, according to
competitive analysis, the simplest algorithm, Follow-the-Predictions, is
optimal. However, intuitively, one should be able to do better, and we present
a greedy variant of Follow-the-Predictions. In analyzing that algorithm, we
believe we present the first random order analysis of a non-trivial online
algorithm with predictions, by which we obtain an algorithmic separation. This
may be useful for distinguishing between algorithms for other problems when
Follow-the-Predictions is optimal according to competitive analysis.
","[{'version': 'v1', 'created': 'Thu, 23 Feb 2023 13:43:24 GMT'}]",2023-02-24,['Data Structures and Algorithms'],"This paper presents a novel online minimum spanning tree algorithm with weight predictions for data structures and algorithms. The algorithm uses a combination of heuristics and machine learning techniques to predict weights associated with edges of the spanning tree, enabling faster and more accurate computation of a minimum spanning tree. The results of the algorithm are compared to existing online algorithms to demonstrate the improved performance. In addition, the paper discusses the implications of the algorithm for data structures and algorithms, and future directions for research in the area.",Write an abstract for a paper called Online Minimum Spanning Trees with Weight Predictions about Data Structures and Algorithms
2302.02179,"Yigit Gurses, Kaan Buyukdemirci, and Yildiray Yildiz","Hierarchical Learning with Unsupervised Skill Discovery for Highway
  Merging Applications","['cs.LG', 'cs.AI', 'cs.RO']","  Driving in dense traffic with human and autonomous drivers is a challenging
task that requires high level planning and reasoning along with the ability to
react quickly to changes in a dynamic environment. In this study, we propose a
hierarchical learning approach that uses learned motion primitives as actions.
Motion primitives are obtained using unsupervised skill discovery without a
predetermined reward function, allowing them to be reused in different
scenarios. This can reduce the total training time for applications that need
to obtain multiple models with varying behavior. Simulation results demonstrate
that the proposed approach yields driver models that achieve higher performance
with less training compared to baseline reinforcement learning methods.
","[{'version': 'v1', 'created': 'Sat, 4 Feb 2023 15:09:51 GMT'}]",2023-02-07,"['Machine Learning', 'Artificial Intelligence', 'Robotics']","This paper presents a hierarchical learning approach with unsupervised skill discovery for highway merging applications in the field of Machine Learning, Artificial Intelligence, and Robotics. The proposed approach is based on a hierarchical reinforcement learning framework that utilizes unsupervised skill discovery to enable autonomous agents to learn and adapt to changing situations in a highway merging scenario. The proposed approach is evaluated on a simulated highway merging environment and compared to a baseline approach. Results demonstrate the effectiveness of the proposed approach in terms of the agent's success rate in completing the highway merging task. Additionally, the proposed approach is shown to be capable of learning and adapting to changes in the environment and is able to generalize to unseen scenarios.","Write an abstract for a paper called Hierarchical Learning with Unsupervised Skill Discovery for Highway
  Merging Applications about Machine Learning, Artificial Intelligence, Robotics"
2206.05807,"Sara Papi, Marco Gaido, Matteo Negri, Marco Turchi","Over-Generation Cannot Be Rewarded: Length-Adaptive Average Lagging for
  Simultaneous Speech Translation",['cs.CL'],"  Simultaneous speech translation (SimulST) systems aim at generating their
output with the lowest possible latency, which is normally computed in terms of
Average Lagging (AL). In this paper we highlight that, despite its widespread
adoption, AL provides underestimated scores for systems that generate longer
predictions compared to the corresponding references. We also show that this
problem has practical relevance, as recent SimulST systems have indeed a
tendency to over-generate. As a solution, we propose LAAL (Length-Adaptive
Average Lagging), a modified version of the metric that takes into account the
over-generation phenomenon and allows for unbiased evaluation of both
under-/over-generating systems.
","[{'version': 'v1', 'created': 'Sun, 12 Jun 2022 18:00:08 GMT'}, {'version': 'v2', 'created': 'Wed, 15 Jun 2022 18:31:15 GMT'}, {'version': 'v3', 'created': 'Mon, 20 Jun 2022 12:03:56 GMT'}]",2022-06-22,['Computation and Language'],"This paper introduces a novel length-adaptive average lagging technique for simultaneous speech translation, which is a task that requires computation and language understanding. The proposed approach is based on a two-level architecture that combines an encoder-decoder model with a language model. The encoder-decoder model is used to generate the translation, while the language model is used to adaptively adjust the length of the output. The proposed approach is evaluated on a speech translation dataset and shows promising results compared to other techniques. The paper also discusses the potential of the proposed approach in other tasks that require computation and language understanding. Finally, the paper argues that over-generation cannot be rewarded in simultaneous speech translation and suggests that the proposed approach could be used to avoid this problem.","Write an abstract for a paper called Over-Generation Cannot Be Rewarded: Length-Adaptive Average Lagging for
  Simultaneous Speech Translation about Computation and Language"
2210.1018,"Mrigank Rochan, Xingxin Chen, Alaap Grandhi, Eduardo R. Corral-Soto,
  Bingbing Liu","Domain Adaptation in 3D Object Detection with Gradual Batch Alternation
  Training",['cs.CV'],"  We consider the problem of domain adaptation in LiDAR-based 3D object
detection. Towards this, we propose a simple yet effective training strategy
called Gradual Batch Alternation that can adapt from a large labeled source
domain to an insufficiently labeled target domain. The idea is to initiate the
training with the batch of samples from the source and target domain data in an
alternate fashion, but then gradually reduce the amount of the source domain
data over time as the training progresses. This way the model slowly shifts
towards the target domain and eventually better adapt to it. The domain
adaptation experiments for 3D object detection on four benchmark autonomous
driving datasets, namely ONCE, PandaSet, Waymo, and nuScenes, demonstrate
significant performance gains over prior arts and strong baselines.
","[{'version': 'v1', 'created': 'Tue, 18 Oct 2022 22:03:37 GMT'}]",2022-10-20,['Computer Vision and Pattern Recognition'],"This paper presents a novel domain adaptation approach for 3D object detection in computer vision and pattern recognition. The proposed method, called Gradual Batch Alternation (GBA), is based on a batch-level domain adaptation strategy and is designed to improve the performance of 3D object detection models when training on data from different domains. The paper presents a detailed analysis of the GBA approach and its performance on two 3D object detection datasets, showing that it is able to achieve better accuracy than the baseline model. Furthermore, the paper also provides insights into how GBA can be used to optimize 3D object detection models for different domains. The results of this study demonstrate the effectiveness of GBA for 3D object detection, and suggest that it could be a promising approach for domain adaptation in computer vision and pattern recognition.","Write an abstract for a paper called Domain Adaptation in 3D Object Detection with Gradual Batch Alternation
  Training about Computer Vision and Pattern Recognition"
2302.10175,"Wee Ling Tan, Stephen Roberts, Stefan Zohren","Spatio-Temporal Momentum: Jointly Learning Time-Series and
  Cross-Sectional Strategies","['q-fin.PM', 'cs.LG', 'q-fin.TR', 'stat.ML']","  We introduce Spatio-Temporal Momentum strategies, a class of models that
unify both time-series and cross-sectional momentum strategies by trading
assets based on their cross-sectional momentum features over time. While both
time-series and cross-sectional momentum strategies are designed to
systematically capture momentum risk premia, these strategies are regarded as
distinct implementations and do not consider the concurrent relationship and
predictability between temporal and cross-sectional momentum features of
different assets. We model spatio-temporal momentum with neural networks of
varying complexities and demonstrate that a simple neural network with only a
single fully connected layer learns to simultaneously generate trading signals
for all assets in a portfolio by incorporating both their time-series and
cross-sectional momentum features. Backtesting on portfolios of 46
actively-traded US equities and 12 equity index futures contracts, we
demonstrate that the model is able to retain its performance over benchmarks in
the presence of high transaction costs of up to 5-10 basis points. In
particular, we find that the model when coupled with least absolute shrinkage
and turnover regularization results in the best performance over various
transaction cost scenarios.
","[{'version': 'v1', 'created': 'Mon, 20 Feb 2023 18:59:05 GMT'}]",2023-02-21,['Machine Learning'],"This paper presents a novel machine learning approach, Spatio-Temporal Momentum (STM), to jointly learn time-series and cross-sectional strategies. STM is a combination of two existing machine learning models, momentum and convolutional neural networks (CNNs). STM combines the advantages of both models to create a powerful model for predicting future stock prices and other time-series data. The paper demonstrates the effectiveness of STM by comparing the performance of STM with other existing models. The results show that STM outperforms the other models in terms of accuracy, precision, and recall. Furthermore, the paper provides a detailed analysis of the STM model and its components, as well as a discussion of its potential applications.","Write an abstract for a paper called Spatio-Temporal Momentum: Jointly Learning Time-Series and
  Cross-Sectional Strategies about Machine Learning"
2211.09558,Jiayi Shao and Xiaohan Wang and Yi Yang,ReLER@ZJU Submission to the Ego4D Moment Queries Challenge 2022,['cs.CV'],"  In this report, we present the ReLER@ZJU1 submission to the Ego4D Moment
Queries Challenge in ECCV 2022. In this task, the goal is to retrieve and
localize all instances of possible activities in egocentric videos. Ego4D
dataset is challenging for the temporal action localization task as the
temporal duration of the videos is quite long and each video contains multiple
action instances with fine-grained action classes. To address these problems,
we utilize a multi-scale transformer to classify different action categories
and predict the boundary of each instance. Moreover, in order to better capture
the long-term temporal dependencies in the long videos, we propose a
segment-level recurrence mechanism. Compared with directly feeding all video
features to the transformer encoder, the proposed segment-level recurrence
mechanism alleviates the optimization difficulties and achieves better
performance. The final submission achieved Recall@1,tIoU=0.5 score of 37.24,
average mAP score of 17.67 and took 3-rd place on the leaderboard.
","[{'version': 'v1', 'created': 'Thu, 17 Nov 2022 14:28:31 GMT'}]",2022-11-18,['Computer Vision and Pattern Recognition'],"This paper presents ReLER@ZJU, a novel approach to the Ego4D Moment Queries Challenge 2022. ReLER@ZJU utilizes computer vision and pattern recognition techniques to accurately identify and track objects in 3D environments. The system is designed to be robust and efficient, and is capable of accurately recognizing and tracking objects in both indoor and outdoor environments. Experiments conducted on the Ego4D dataset demonstrate the effectiveness of ReLER@ZJU, with an overall accuracy of 67.5%. The results of the experiments also demonstrate that ReLER@ZJU is able to accurately identify and track objects in a variety of challenging scenarios, such as in the presence of occlusions, dynamic objects, and changing lighting conditions. This paper provides a detailed description of the ReLER@ZJU system, its implementation, and the results obtained from the experiments.",Write an abstract for a paper called ReLER@ZJU Submission to the Ego4D Moment Queries Challenge 2022 about Computer Vision and Pattern Recognition
2205.07868,"Joachim Winther Pedersen, Sebastian Risi",Minimal Neural Network Models for Permutation Invariant Agents,"['cs.LG', 'cs.AI', 'cs.NE']","  Organisms in nature have evolved to exhibit flexibility in face of changes to
the environment and/or to themselves. Artificial neural networks (ANNs) have
proven useful for controlling of artificial agents acting in environments.
However, most ANN models used for reinforcement learning-type tasks have a
rigid structure that does not allow for varying input sizes. Further, they fail
catastrophically if inputs are presented in an ordering unseen during
optimization. We find that these two ANN inflexibilities can be mitigated and
their solutions are simple and highly related. For permutation invariance, no
optimized parameters can be tied to a specific index of the input elements. For
size invariance, inputs must be projected onto a common space that does not
grow with the number of projections. Based on these restrictions, we construct
a conceptually simple model that exhibit flexibility most ANNs lack. We
demonstrate the model's properties on multiple control problems, and show that
it can cope with even very rapid permutations of input indices, as well as
changes in input size. Ablation studies show that is possible to achieve these
properties with simple feedforward structures, but that it is much easier to
optimize recurrent structures.
","[{'version': 'v1', 'created': 'Thu, 12 May 2022 08:03:29 GMT'}]",2022-05-18,"['Machine Learning', 'Artificial Intelligence', 'Neural and Evolutionary Computing']","This paper presents a novel approach for developing minimal neural network models for permutation invariant agents in the field of machine learning, artificial intelligence, neural and evolutionary computing. The proposed model is based on a combination of evolutionary algorithms and deep learning techniques. The proposed model is shown to be able to learn from different permutations of input data, providing a solution to the problem of permutation invariance in machine learning. The results of this paper demonstrate that the proposed model is capable of achieving good performance on permutation invariant tasks, while using minimal neural network models. Furthermore, this paper also provides a comparison between the proposed model and standard machine learning methods, and discusses the implications of the results.","Write an abstract for a paper called Minimal Neural Network Models for Permutation Invariant Agents about Machine Learning, Artificial Intelligence, Neural and Evolutionary Computing"
2210.07857,Frank Qiu,Commutativity and Disentanglement from the Manifold Perspective,"['stat.ML', 'cs.LG']","  In this paper, we interpret disentanglement as the discovery of local charts
and trace how that definition naturally leads to an equivalent condition for
disentanglement: the disentangled factors must commute with each other. We
discuss the practical and theoretical implications of commutativity, in
particular the compression and disentanglement of generative models. Finally,
we conclude with a discussion of related approaches to disentanglement and how
they relate to our view of disentanglement from the manifold perspective.
","[{'version': 'v1', 'created': 'Fri, 14 Oct 2022 14:29:32 GMT'}, {'version': 'v2', 'created': 'Tue, 18 Oct 2022 10:03:07 GMT'}, {'version': 'v3', 'created': 'Tue, 14 Feb 2023 07:48:04 GMT'}]",2023-02-15,['Machine Learning'],"This paper examines the notion of commutativity and disentanglement from the manifold perspective in the context of machine learning. We discuss the importance of these two concepts in machine learning and analyze how they can be used to improve the performance of machine learning algorithms. We then present a novel approach to disentanglement based on the manifold perspective, which is based on the idea that the underlying data manifold can be used to separate out the various components of the data. Finally, we discuss the implications of this approach and its potential applications in machine learning.",Write an abstract for a paper called Commutativity and Disentanglement from the Manifold Perspective about Machine Learning
2207.01117,"Guangji Bai, Liang Zhao",Saliency-Regularized Deep Multi-Task Learning,['cs.LG'],"  Multitask learning is a framework that enforces multiple learning tasks to
share knowledge to improve their generalization abilities. While shallow
multitask learning can learn task relations, it can only handle predefined
features. Modern deep multitask learning can jointly learn latent features and
task sharing, but they are obscure in task relation. Also, they predefine which
layers and neurons should share across tasks and cannot learn adaptively. To
address these challenges, this paper proposes a new multitask learning
framework that jointly learns latent features and explicit task relations by
complementing the strength of existing shallow and deep multitask learning
scenarios. Specifically, we propose to model the task relation as the
similarity between task input gradients, with a theoretical analysis of their
equivalency. In addition, we innovatively propose a multitask learning
objective that explicitly learns task relations by a new regularizer.
Theoretical analysis shows that the generalizability error has been reduced
thanks to the proposed regularizer. Extensive experiments on several multitask
learning and image classification benchmarks demonstrate the proposed method
effectiveness, efficiency as well as reasonableness in the learned task
relation patterns.
","[{'version': 'v1', 'created': 'Sun, 3 Jul 2022 20:26:44 GMT'}]",2022-07-05,['Machine Learning'],"This paper presents a novel approach to multi-task learning using deep neural networks, called Saliency-Regularized Deep Multi-Task Learning (SRDMTL). SRDMTL is a regularization technique that encourages the learning of task-specific features in a multi-task setting. SRDMTL utilizes a saliency-based regularization term which encourages the learning of task-specific features during the training process. The proposed regularization technique is evaluated on a range of benchmark datasets and compared to existing methods. Results show that SRDMTL outperforms existing methods in terms of accuracy and generalization. Furthermore, the results demonstrate that SRDMTL can effectively learn task-specific features, leading to improved performance in multi-task learning.",Write an abstract for a paper called Saliency-Regularized Deep Multi-Task Learning about Machine Learning
2204.1373,"Ziyaur Rahman, S. M. Zafaruddin, V. K. Chaubey","Direct Air-to-Underwater Optical Wireless Communication: Statistical
  Characterization and Outage Performance","['cs.IT', 'eess.SP', 'math.IT']","  In general, a buoy relay is used to connect the underwater communication to
the terrestrial network over a radio or optical wireless communication (OWC)
link. The use of relay deployment may pose security and deployment issues. This
paper investigates the feasibility of direct air-to-underwater (A2UW)
communication from an over-the-sea OWC system to an underwater submarine
without deploying a relaying node. We analyze the statistical performance of
the direct transmission over the combined channel fading effect of atmospheric
turbulence, random fog, air-to-water interface, oceanic turbulence, and
pointing errors. We develop novel analytical expressions for the probability
density function (PDF) and cumulative distribution function (CDF) of the
resultant signal-to-noise ratio (SNR) in terms of bivariate Meijer-G and Fox-H
functions. We use the derived statistical results to analyze the system
performance by providing exact and asymptotic results of the outage probability
in terms of system parameters. We use computer simulations to demonstrate the
performance of direct A2UW transmissions compared to the relay-assisted system.
","[{'version': 'v1', 'created': 'Thu, 28 Apr 2022 18:21:37 GMT'}]",2022-05-02,['Information Theory'],"This paper presents a comprehensive analysis of direct air-to-underwater optical wireless communication (DAUOWC) systems. We investigate the statistical characterization and outage performance of DAUOWC in the context of information theory. We consider the effects of turbulence and absorption on the underwater optical channel, and propose a model for the DAUOWC system that takes these effects into account. The proposed model is used to analyze the information-theoretic performance of the system, including the error rate, outage probability, and capacity. Finally, we provide numerical results to demonstrate the effectiveness of the proposed model and the performance of the DAUOWC system.","Write an abstract for a paper called Direct Air-to-Underwater Optical Wireless Communication: Statistical
  Characterization and Outage Performance about Information Theory"
2303.0216,"Stephanie Milani, Arthur Juliani, Ida Momennejad, Raluca Georgescu,
  Jaroslaw Rzpecki, Alison Shaw, Gavin Costello, Fei Fang, Sam Devlin, Katja
  Hofmann","Navigates Like Me: Understanding How People Evaluate Human-Like AI in
  Video Games","['cs.HC', 'cs.LG', 'cs.RO']","  We aim to understand how people assess human likeness in navigation produced
by people and artificially intelligent (AI) agents in a video game. To this
end, we propose a novel AI agent with the goal of generating more human-like
behavior. We collect hundreds of crowd-sourced assessments comparing the
human-likeness of navigation behavior generated by our agent and baseline AI
agents with human-generated behavior. Our proposed agent passes a Turing Test,
while the baseline agents do not. By passing a Turing Test, we mean that human
judges could not quantitatively distinguish between videos of a person and an
AI agent navigating. To understand what people believe constitutes human-like
navigation, we extensively analyze the justifications of these assessments.
This work provides insights into the characteristics that people consider
human-like in the context of goal-directed video game navigation, which is a
key step for further improving human interactions with AI agents.
","[{'version': 'v1', 'created': 'Thu, 2 Mar 2023 18:59:04 GMT'}]",2023-03-07,"['Human-Computer Interaction', 'Machine Learning', 'Robotics']","This paper examines how people evaluate the human-like AI in video games, and how this evaluation is affected by the way the AI navigates the game environment. Using a combination of human-computer interaction, machine learning, and robotics, the paper will analyze data from a survey of video game players to explore the ways in which people evaluate the navigation of AI characters. The results of this study will provide insight into how people perceive the navigation of AI characters, and how this perception affects their overall opinion of the AI. This paper will also discuss the implications of these findings for the design of future AI characters, and will suggest ways in which game designers can make AI characters more human-like in order to improve player engagement.","Write an abstract for a paper called Navigates Like Me: Understanding How People Evaluate Human-Like AI in
  Video Games about Human-Computer Interaction, Machine Learning, Robotics"
2111.11535,"Kanav Vats, William McNally, Pascale Walters, David A. Clausi, John S.
  Zelek","Ice hockey player identification via transformers and weakly supervised
  learning",['cs.CV'],"  Identifying players in video is a foundational step in computer vision-based
sports analytics. Obtaining player identities is essential for analyzing the
game and is used in downstream tasks such as game event recognition.
Transformers are the existing standard in Natural Language Processing (NLP) and
are swiftly gaining traction in computer vision. Motivated by the increasing
success of transformers in computer vision, in this paper, we introduce a
transformer network for recognizing players through their jersey numbers in
broadcast National Hockey League (NHL) videos. The transformer takes temporal
sequences of player frames (also called player tracklets) as input and outputs
the probabilities of jersey numbers present in the frames. The proposed network
performs better than the previous benchmark on the dataset used. We implement a
weakly-supervised training approach by generating approximate frame-level
labels for jersey number presence and use the frame-level labels for faster
training. We also utilize player shifts available in the NHL play-by-play data
by reading the game time using optical character recognition (OCR) to get the
players on the ice rink at a certain game time. Using player shifts improved
the player identification accuracy by 6%.
","[{'version': 'v1', 'created': 'Mon, 22 Nov 2021 21:10:26 GMT'}, {'version': 'v2', 'created': 'Thu, 28 Apr 2022 18:35:01 GMT'}]",2022-05-02,['Computer Vision and Pattern Recognition'],"This paper presents a novel approach to ice hockey player identification using transformers and weakly supervised learning. We propose a novel transformer-based architecture, which combines a transformer encoder with a convolutional neural network (CNN) to extract high-level features from images of ice hockey players. We then use a weakly supervised learning approach to identify ice hockey players from the extracted features. In addition, we use a novel data augmentation method to improve the performance of the model. Our experiments demonstrate that our proposed approach achieves a high accuracy of up to 95% on the NHL Players dataset. Our method outperforms existing approaches in terms of accuracy, and provides a reliable and efficient solution for ice hockey player identification.","Write an abstract for a paper called Ice hockey player identification via transformers and weakly supervised
  learning about Computer Vision and Pattern Recognition"
2303.13592,"Zheng-Xin Yong, Ruochen Zhang, Jessica Zosa Forde, Skyler Wang, Samuel
  Cahyawijaya, Holy Lovenia, Genta Indra Winata, Lintang Sutawika, Jan
  Christian Blaise Cruz, Long Phan, Yin Lin Tan, Alham Fikri Aji","Prompting Multilingual Large Language Models to Generate Code-Mixed
  Texts: The Case of South East Asian Languages","['cs.CL', 'cs.AI']","  While code-mixing is a common linguistic practice in many parts of the world,
collecting high-quality and low-cost code-mixed data remains a challenge for
natural language processing (NLP) research. The proliferation of Large Language
Models (LLMs) in recent times compels one to ask: can these systems be used for
data generation? In this article, we explore prompting multilingual LLMs in a
zero-shot manner to create code-mixed data for five languages in South East
Asia (SEA) -- Indonesian, Malay, Chinese, Tagalog, Vietnamese, as well as the
creole language Singlish. We find that ChatGPT shows the most potential,
capable of producing code-mixed text 68% of the time when the term
""code-mixing"" is explicitly defined. Moreover, both ChatGPT's and InstructGPT's
(davinci-003) performances in generating Singlish texts are noteworthy,
averaging a 96% success rate across a variety of prompts. Their code-mixing
proficiency, however, is dampened by word choice errors that lead to semantic
inaccuracies. Other multilingual models such as BLOOMZ and Flan-T5-XXL are
unable to produce code-mixed texts altogether. By highlighting the limited
promises of LLMs in a specific form of low-resource data generation, we call
for a measured approach when applying similar techniques to other data-scarce
NLP contexts.
","[{'version': 'v1', 'created': 'Thu, 23 Mar 2023 18:16:30 GMT'}, {'version': 'v2', 'created': 'Thu, 30 Mar 2023 14:59:26 GMT'}]",2023-04-03,"['Computation and Language', 'Artificial Intelligence']","This paper explores the use of multilingual large language models to generate code-mixed texts in South East Asian languages, such as Indonesian, Malay and Tagalog, for computational and language tasks. The paper focuses on the use of artificial intelligence to develop a model that can generate code-mixed texts from South East Asian languages. The paper also discusses the challenges associated with developing such a model, such as the lack of large-scale datasets, the need for cross-lingual transfer learning, and the need for language-specific data pre-processing techniques. The paper presents a case study of a large-scale multilingual language model trained on South East Asian languages, and evaluates its performance on a code-mixing task. The paper concludes by discussing the implications of the findings and potential future research directions.","Write an abstract for a paper called Prompting Multilingual Large Language Models to Generate Code-Mixed
  Texts: The Case of South East Asian Languages about Computation and Language, Artificial Intelligence"
2301.09235,"Yusuke Sakemi, Sou Nobukawa, Toshitaka Matsuki, Takashi Morie,
  Kazuyuki Aihara",Learning Reservoir Dynamics with Temporal Self-Modulation,"['cs.LG', 'cs.NE']","  Reservoir computing (RC) can efficiently process time-series data by
transferring the input signal to randomly connected recurrent neural networks
(RNNs), which are referred to as a reservoir. The high-dimensional
representation of time-series data in the reservoir significantly simplifies
subsequent learning tasks. Although this simple architecture allows fast
learning and facile physical implementation, the learning performance is
inferior to that of other state-of-the-art RNN models. In this paper, to
improve the learning ability of RC, we propose self-modulated RC (SM-RC), which
extends RC by adding a self-modulation mechanism. The self-modulation mechanism
is realized with two gating variables: an input gate and a reservoir gate. The
input gate modulates the input signal, and the reservoir gate modulates the
dynamical properties of the reservoir. We demonstrated that SM-RC can perform
attention tasks where input information is retained or discarded depending on
the input signal. We also found that a chaotic state emerged as a result of
learning in SM-RC. This indicates that self-modulation mechanisms provide RC
with qualitatively different information-processing capabilities. Furthermore,
SM-RC outperformed RC in NARMA and Lorentz model tasks. In particular, SM-RC
achieved a higher prediction accuracy than RC with a reservoir 10 times larger
in the Lorentz model tasks. Because the SM-RC architecture only requires two
additional gates, it is physically implementable as RC, providing a new
direction for realizing edge AI.
","[{'version': 'v1', 'created': 'Mon, 23 Jan 2023 00:44:05 GMT'}]",2023-01-24,"['Machine Learning', 'Neural and Evolutionary Computing']","This paper proposes a novel approach to learning reservoir dynamics by combining temporal self-modulation with machine learning, neural and evolutionary computing. The proposed approach is based on the concept of reservoir computing, which is a type of recurrent neural network (RNN) that is capable of learning temporal data. The proposed approach utilizes temporal self-modulation to enable the reservoir to learn temporal dynamics from the input data. The proposed approach is evaluated on a number of benchmark datasets, and the results demonstrate that the temporal self-modulation approach significantly outperforms traditional reservoir computing methods. Furthermore, the proposed approach is able to learn more complex temporal dynamics than traditional reservoir computing methods. The results of this paper demonstrate the potential of temporal self-modulation for learning reservoir dynamics and open up exciting new possibilities for machine learning, neural, and evolutionary computing.","Write an abstract for a paper called Learning Reservoir Dynamics with Temporal Self-Modulation about Machine Learning, Neural and Evolutionary Computing"
2203.02157,Yueling Shen and Guangming Wang and Hesheng Wang,"DetFlowTrack: 3D Multi-object Tracking based on Simultaneous
  Optimization of Object Detection and Scene Flow Estimation","['cs.CV', 'cs.RO']","  3D Multi-Object Tracking (MOT) is an important part of the unmanned vehicle
perception module. Most methods optimize object detection and data association
independently. These methods make the network structure complicated and limit
the improvement of MOT accuracy. we proposed a 3D MOT framework based on
simultaneous optimization of object detection and scene flow estimation. In the
framework, a detection-guidance scene flow module is proposed to relieve the
problem of incorrect inter-frame assocation. For more accurate scene flow label
especially in the case of motion with rotation, a box-transformation-based
scene flow ground truth calculation method is proposed. Experimental results on
the KITTI MOT dataset show competitive results over the state-of-the-arts and
the robustness under extreme motion with rotation.
","[{'version': 'v1', 'created': 'Fri, 4 Mar 2022 07:06:47 GMT'}]",2022-03-07,"['Computer Vision and Pattern Recognition', 'Robotics']","This paper presents an end-to-end 3D multi-object tracking system called DetFlowTrack. It is based on a simultaneous optimization of object detection and scene flow estimation, which is an extension of the classical tracking-by-detection paradigm. The proposed system is composed of two modules: a detection network for object detection, and a scene flow network for scene flow estimation. The outputs of both networks are fused with a correlation layer to generate a 3D multi-object tracking result. Experiments on the KITTI dataset demonstrate that the proposed system outperforms existing state-of-the-art methods in terms of accuracy and robustness. The results of this paper demonstrate that our system is a promising approach for 3D multi-object tracking in computer vision and pattern recognition, and robotics applications.","Write an abstract for a paper called DetFlowTrack: 3D Multi-object Tracking based on Simultaneous
  Optimization of Object Detection and Scene Flow Estimation about Computer Vision and Pattern Recognition, Robotics"
2211.05764,"Nabeel Seedat, Fergus Imrie, Mihaela van der Schaar","DC-Check: A Data-Centric AI checklist to guide the development of
  reliable machine learning systems","['cs.LG', 'cs.AI', 'cs.CY', 'cs.SE', 'stat.ML']","  While there have been a number of remarkable breakthroughs in machine
learning (ML), much of the focus has been placed on model development. However,
to truly realize the potential of machine learning in real-world settings,
additional aspects must be considered across the ML pipeline. Data-centric AI
is emerging as a unifying paradigm that could enable such reliable end-to-end
pipelines. However, this remains a nascent area with no standardized framework
to guide practitioners to the necessary data-centric considerations or to
communicate the design of data-centric driven ML systems. To address this gap,
we propose DC-Check, an actionable checklist-style framework to elicit
data-centric considerations at different stages of the ML pipeline: Data,
Training, Testing, and Deployment. This data-centric lens on development aims
to promote thoughtfulness and transparency prior to system development.
Additionally, we highlight specific data-centric AI challenges and research
opportunities. DC-Check is aimed at both practitioners and researchers to guide
day-to-day development. As such, to easily engage with and use DC-Check and
associated resources, we provide a DC-Check companion website
(https://www.vanderschaar-lab.com/dc-check/). The website will also serve as an
updated resource as methods and tooling evolve over time.
","[{'version': 'v1', 'created': 'Wed, 9 Nov 2022 17:32:09 GMT'}]",2022-11-11,"['Machine Learning', 'Artificial Intelligence', 'Computers and Society', 'Software Engineering']","This paper presents DC-Check, a data-centric AI checklist to guide the development of reliable machine learning systems. DC-Check is a comprehensive set of guidelines to ensure the ethical, responsible, and secure development of AI systems. The checklist consists of five categories: Data, Model, Evaluation, User Experience, and Governance. For each category, DC-Check provides a set of best practices to ensure the responsible and secure development of AI systems. The paper discusses the importance of data-centric AI development and the various challenges associated with developing reliable machine learning systems. It also presents a case study to demonstrate how DC-Check can be used to develop a reliable machine learning system. Finally, the paper examines the implications of DC-Check for the development of AI systems, and for the broader field of artificial intelligence, machine learning, computers and society, and software engineering.","Write an abstract for a paper called DC-Check: A Data-Centric AI checklist to guide the development of
  reliable machine learning systems about Machine Learning, Artificial Intelligence, Computers and Society, Software Engineering"
2110.07957,"Yen Meng, Yi-Hui Chou, Andy T. Liu, Hung-yi Lee","Don't speak too fast: The impact of data bias on self-supervised speech
  models","['eess.AS', 'cs.CL', 'cs.SD']","  Self-supervised Speech Models (S3Ms) have been proven successful in many
speech downstream tasks, like ASR. However, how pre-training data affects S3Ms'
downstream behavior remains an unexplored issue. In this paper, we study how
pre-training data affects S3Ms by pre-training models on biased datasets
targeting different factors of speech, including gender, content, and prosody,
and evaluate these pre-trained S3Ms on selected downstream tasks in SUPERB
Benchmark. Our experiments show that S3Ms have tolerance toward gender bias.
Moreover, we find that the content of speech has little impact on the
performance of S3Ms across downstream tasks, but S3Ms do show a preference
toward a slower speech rate.
","[{'version': 'v1', 'created': 'Fri, 15 Oct 2021 09:22:34 GMT'}, {'version': 'v2', 'created': 'Thu, 17 Feb 2022 05:48:37 GMT'}, {'version': 'v3', 'created': 'Tue, 26 Apr 2022 11:32:09 GMT'}]",2022-04-27,"['Computation and Language', 'Sound']","This paper examines the impact of data bias on self-supervised speech models in the context of Computation and Language, Sound. Self-supervised speech models are becoming increasingly popular due to their ability to learn from unlabeled data. However, these models are vulnerable to data bias, which can lead to inaccurate predictions and poor performance. This paper investigates the effects of data bias on self-supervised speech models and proposes methods to mitigate it. We present an empirical study of a self-supervised speech model trained on a biased dataset, and discuss the implications for language understanding and speech recognition. We also discuss the potential for using data augmentation to reduce the impact of bias. Finally, we discuss the implications of this research for future work in Computation and Language, Sound.","Write an abstract for a paper called Don't speak too fast: The impact of data bias on self-supervised speech
  models about Computation and Language, Sound"
2211.09155,"Zhaoliang Chen, Lele Fu, Jie Yao, Wenzhong Guo, Claudia Plant, Shiping
  Wang","Learnable Graph Convolutional Network and Feature Fusion for Multi-view
  Learning","['cs.CV', 'cs.AI', 'cs.LG']","  In practical applications, multi-view data depicting objectives from assorted
perspectives can facilitate the accuracy increase of learning algorithms.
However, given multi-view data, there is limited work for learning
discriminative node relationships and graph information simultaneously via
graph convolutional network that has drawn the attention from considerable
researchers in recent years. Most of existing methods only consider the
weighted sum of adjacency matrices, yet a joint neural network of both feature
and graph fusion is still under-explored. To cope with these issues, this paper
proposes a joint deep learning framework called Learnable Graph Convolutional
Network and Feature Fusion (LGCN-FF), consisting of two stages: feature fusion
network and learnable graph convolutional network. The former aims to learn an
underlying feature representation from heterogeneous views, while the latter
explores a more discriminative graph fusion via learnable weights and a
parametric activation function dubbed Differentiable Shrinkage Activation (DSA)
function. The proposed LGCN-FF is validated to be superior to various
state-of-the-art methods in multi-view semi-supervised classification.
","[{'version': 'v1', 'created': 'Wed, 16 Nov 2022 19:07:12 GMT'}]",2022-11-18,"['Computer Vision and Pattern Recognition', 'Artificial Intelligence', 'Machine Learning']","This paper presents a novel graph convolutional network (GCN) and feature fusion approach for multi-view learning. Our approach is based on a learnable GCN, which is able to capture structural information from multi-view data. We apply it to the task of computer vision and pattern recognition, and show that our approach outperforms traditional methods. We also demonstrate that our approach can be adapted to various tasks, including artificial intelligence, machine learning, and multi-view learning. The proposed approach is evaluated on various datasets and compared to several state-of-the-art approaches. Our results show that our approach is able to achieve better performance than existing methods in terms of accuracy and computational efficiency.","Write an abstract for a paper called Learnable Graph Convolutional Network and Feature Fusion for Multi-view
  Learning about Computer Vision and Pattern Recognition, Artificial Intelligence, Machine Learning"
2201.02981,Haibo Liu and Qunying Liao,A New Constructions of Minimal Binary Linear Codes,"['cs.IT', 'math.IT']","  Recently, minimal linear codes have been extensively studied due to their
applications in secret sharing schemes, secure two-party computations, and so
on. Constructing minimal linear codes violating the Ashikhmin-Barg condition
and then determining their weight distributions have been interesting in coding
theory and cryptography. In this paper, a generic construction for binary
linear codes with dimension $m+2$ is presented, then a necessary and sufficient
condition for this binary linear code to be minimal is derived. Based on this
condition and exponential sums, a new class of minimal binary linear codes
violating the Ashikhmin-Barg condition is obtained, and then their weight
enumerators are determined.
","[{'version': 'v1', 'created': 'Sun, 9 Jan 2022 10:30:13 GMT'}]",2022-01-11,['Information Theory'],"This paper presents a new construction of minimal binary linear codes, which are important objects in information theory. We introduce a new method for constructing these codes, which is based on the concept of a ""minimal linear generator"" and uses the theory of linear algebra. We also discuss the properties of the codes produced by this construction and their applications to communication and storage systems. Finally, we present some examples of the codes constructed by our method. The results show that our construction produces minimal binary linear codes with improved parameters compared to existing constructions.",Write an abstract for a paper called A New Constructions of Minimal Binary Linear Codes about Information Theory
2206.14796,"Zorik Gekhman, Nadav Oved, Orgad Keller, Idan Szpektor, Roi Reichart","On the Robustness of Dialogue History Representation in Conversational
  Question Answering: A Comprehensive Study and a New Prompt-based Method","['cs.CL', 'cs.AI', 'cs.LG']","  Most works on modeling the conversation history in Conversational Question
Answering (CQA) report a single main result on a common CQA benchmark. While
existing models show impressive results on CQA leaderboards, it remains unclear
whether they are robust to shifts in setting (sometimes to more realistic
ones), training data size (e.g. from large to small sets) and domain. In this
work, we design and conduct the first large-scale robustness study of history
modeling approaches for CQA. We find that high benchmark scores do not
necessarily translate to strong robustness, and that various methods can
perform extremely differently under different settings. Equipped with the
insights from our study, we design a novel prompt-based history modeling
approach, and demonstrate its strong robustness across various settings. Our
approach is inspired by existing methods that highlight historic answers in the
passage. However, instead of highlighting by modifying the passage token
embeddings, we add textual prompts directly in the passage text. Our approach
is simple, easy-to-plug into practically any model, and highly effective, thus
we recommend it as a starting point for future model developers. We also hope
that our study and insights will raise awareness to the importance of
robustness-focused evaluation, in addition to obtaining high leaderboard
scores, leading to better CQA systems.
","[{'version': 'v1', 'created': 'Wed, 29 Jun 2022 17:55:43 GMT'}, {'version': 'v2', 'created': 'Wed, 28 Dec 2022 21:06:46 GMT'}]",2023-01-02,"['Computation and Language', 'Artificial Intelligence', 'Machine Learning']","This paper presents a comprehensive study on the robustness of dialogue history representation in conversational question answering (CQA) systems. We first review existing methods for dialogue history representation and discuss their limitations. We then propose a new prompt-based method for dialogue history representation that leverages the contextual information of the conversation. To evaluate the performance of the proposed method, we conduct experiments on two public datasets. The results demonstrate that our method outperforms existing methods in terms of robustness and accuracy. The paper also provides insights into the importance of dialogue history representation in CQA systems and its potential applications in artificial intelligence, machine learning, and natural language processing.","Write an abstract for a paper called On the Robustness of Dialogue History Representation in Conversational
  Question Answering: A Comprehensive Study and a New Prompt-based Method about Computation and Language, Artificial Intelligence, Machine Learning"
2210.06618,"David Berga, Pau Gall\'es, Katalin Tak\'ats, Eva Mohedano, Laura
  Riordan-Chen, Clara Garcia-Moll, David Vilaseca, Javier Mar\'in","QMRNet: Quality Metric Regression for EO Image Quality Assessment and
  Super-Resolution","['cs.CV', 'cs.AI', 'eess.IV', 'physics.geo-ph', 'physics.space-ph']","  Latest advances in Super-Resolution (SR) have been tested with general
purpose images such as faces, landscapes and objects, mainly unused for the
task of super-resolving Earth Observation (EO) images. In this research paper,
we benchmark state-of-the-art SR algorithms for distinct EO datasets using both
Full-Reference and No-Reference Image Quality Assessment (IQA) metrics. We also
propose a novel Quality Metric Regression Network (QMRNet) that is able to
predict quality (as a No-Reference metric) by training on any property of the
image (i.e. its resolution, its distortions...) and also able to optimize SR
algorithms for a specific metric objective. This work is part of the
implementation of the framework IQUAFLOW which has been developed for
evaluating image quality, detection and classification of objects as well as
image compression in EO use cases. We integrated our experimentation and tested
our QMRNet algorithm on predicting features like blur, sharpness, snr, rer and
ground sampling distance (GSD) and obtain validation medRs below 1.0 (out of
N=50) and recall rates above 95\%. Overall benchmark shows promising results
for LIIF, CAR and MSRN and also the potential use of QMRNet as Loss for
optimizing SR predictions. Due to its simplicity, QMRNet could also be used for
other use cases and image domains, as its architecture and data processing is
fully scalable.
","[{'version': 'v1', 'created': 'Wed, 12 Oct 2022 22:51:13 GMT'}, {'version': 'v2', 'created': 'Fri, 14 Oct 2022 13:46:08 GMT'}]",2022-10-17,"['Computer Vision and Pattern Recognition', 'Artificial Intelligence']","This paper presents QMRNet, a novel Quality Metric Regression (QMR) network for automated quality assessment and super-resolution of Earth Observation (EO) images. QMRNet combines convolutional neural network (CNN) and regression layers to learn a mapping between EO image quality and its super-resolution performance. The proposed QMRNet is trained end-to-end with a novel quality metric loss, which is based on a combination of image quality metrics and super-resolution performance metrics. Experiments on high-resolution remotely-sensed data demonstrate that the proposed QMRNet can accurately predict the quality of EO images and achieve state-of-the-art super-resolution performance. Furthermore, the proposed QMRNet can be used to improve the quality of low-resolution EO images, making it a promising tool for computer vision and pattern recognition, as well as artificial intelligence.","Write an abstract for a paper called QMRNet: Quality Metric Regression for EO Image Quality Assessment and
  Super-Resolution about Computer Vision and Pattern Recognition, Artificial Intelligence"
2209.04684,"Wei Liu, Ziqing Xie, Yongjun Yuan","A constrained gentlest ascent dynamics and its applications to finding
  excited states of Bose-Einstein condensates","['math.NA', 'cond-mat.quant-gas', 'cs.NA', 'quant-ph']","  In this paper, the gentlest ascent dynamics (GAD) developed in [W. E and X.
Zhou, Nonlinearity, 24 (2011), pp. 1831--1842] is extended to a constrained
gentlest ascent dynamics (CGAD) to find constrained saddle points with any
specified Morse indices. It is proved that the linearly stable steady state of
the proposed CGAD is exactly a nondegenerate constrained saddle point with a
corresponding Morse index. Meanwhile, the locally exponential convergence of an
idealized CGAD near nondegenerate constrained saddle points with corresponding
indices is also verified. The CGAD is then applied to find excited states of
single-component Bose--Einstein condensates (BECs) in the order of their Morse
indices via computing constrained saddle points of the corresponding
Gross--Pitaevskii energy functional under the normalization constraint. In
addition, properties of the excited states of BECs in the linear/nonlinear
cases are mathematically/numerically studied. Extensive numerical results are
reported to show the effectiveness and robustness of our method and demonstrate
some interesting physics.
","[{'version': 'v1', 'created': 'Sat, 10 Sep 2022 15:09:07 GMT'}, {'version': 'v2', 'created': 'Sat, 29 Oct 2022 05:17:06 GMT'}]",2022-11-14,['Numerical Analysis'],"This paper investigates a constrained gentlest ascent dynamics (CGAD) and its applications to finding excited states of Bose-Einstein condensates (BECs). CGAD is a numerical method for solving dynamical systems of equations. It relies on the gentlest ascent dynamics (GAD) algorithm, which is a gradient-based optimization method. CGAD introduces constraints to the GAD algorithm, allowing for the exploration of a wider range of solutions than the unconstrained GAD algorithm. We apply CGAD to the problem of finding excited states of BECs and demonstrate its effectiveness. We compare CGAD to other numerical methods and show that it is more efficient and accurate. We also discuss the implications of our results for further research in the area of numerical analysis.","Write an abstract for a paper called A constrained gentlest ascent dynamics and its applications to finding
  excited states of Bose-Einstein condensates about Numerical Analysis"
2301.0897,"Shujian Yu, Hongming Li, Sigurd L{\o}kse, Robert Jenssen, Jos\'e C.
  Pr\'incipe","The Conditional Cauchy-Schwarz Divergence with Applications to
  Time-Series Data and Sequential Decision Making","['cs.LG', 'cs.IT', 'math.IT', 'stat.ML']","  The Cauchy-Schwarz (CS) divergence was developed by Pr\'{i}ncipe et al. in
2000. In this paper, we extend the classic CS divergence to quantify the
closeness between two conditional distributions and show that the developed
conditional CS divergence can be simply estimated by a kernel density estimator
from given samples. We illustrate the advantages (e.g., the rigorous
faithfulness guarantee, the lower computational complexity, the higher
statistical power, and the much more flexibility in a wide range of
applications) of our conditional CS divergence over previous proposals, such as
the conditional KL divergence and the conditional maximum mean discrepancy. We
also demonstrate the compelling performance of conditional CS divergence in two
machine learning tasks related to time series data and sequential inference,
namely the time series clustering and the uncertainty-guided exploration for
sequential decision making.
","[{'version': 'v1', 'created': 'Sat, 21 Jan 2023 16:32:22 GMT'}]",2023-01-24,"['Machine Learning', 'Information Theory']","and Statistics

This paper presents a novel divergence measure, the Conditional Cauchy-Schwarz (CCS) divergence, which is a generalization of the classical Cauchy-Schwarz divergence. The CCS divergence is a powerful tool for analyzing time-series data and sequential decision making in the context of machine learning, information theory and statistics. We discuss the properties of the CCS divergence, its relationship to other divergence measures, and its applications to time-series data and sequential decision making. We demonstrate the effectiveness of the CCS divergence through experiments on synthetic and real-world datasets. Finally, we discuss potential future research directions in this area.","Write an abstract for a paper called The Conditional Cauchy-Schwarz Divergence with Applications to
  Time-Series Data and Sequential Decision Making about Machine Learning, Information Theory"
1910.00057,"Goutham Ramakrishnan, Yun Chan Lee, Aws Albarghouthi",Synthesizing Action Sequences for Modifying Model Decisions,['cs.AI'],"  When a model makes a consequential decision, e.g., denying someone a loan, it
needs to additionally generate actionable, realistic feedback on what the
person can do to favorably change the decision. We cast this problem through
the lens of program synthesis, in which our goal is to synthesize an optimal
(realistically cheapest or simplest) sequence of actions that if a person
executes successfully can change their classification. We present a novel and
general approach that combines search-based program synthesis and test-time
adversarial attacks to construct action sequences over a domain-specific set of
actions. We demonstrate the effectiveness of our approach on a number of deep
neural networks.
","[{'version': 'v1', 'created': 'Mon, 30 Sep 2019 18:57:13 GMT'}, {'version': 'v2', 'created': 'Fri, 4 Oct 2019 14:03:48 GMT'}, {'version': 'v3', 'created': 'Wed, 9 Oct 2019 16:22:00 GMT'}]",2022-06-22,['Artificial Intelligence'],"This paper investigates the use of synthesized action sequences to modify model decisions about Artificial Intelligence (AI). Current AI models are increasingly being used to automate decision-making processes in a variety of domains. However, there is a lack of understanding of how to modify model decisions in an efficient and effective manner. This paper proposes a new approach to modify model decisions using synthesized action sequences. The action sequences are generated from a set of predefined rules and are used to modify the model decisions. The proposed approach is evaluated using a set of experiments and the results show that the proposed approach is effective in modifying model decisions. The paper also discusses the potential applications of the proposed approach in the field of AI.",Write an abstract for a paper called Synthesizing Action Sequences for Modifying Model Decisions about Artificial Intelligence
2211.08644,"Linjiang Guo, Zijian Feng, Yuxue Chi, Mingzhu Wang, Yijun Liu","Coronavirus statistics causes emotional bias: a social media text mining
  perspective","['cs.CY', 'cs.SI']","  While COVID-19 has impacted humans for a long time, people search the web for
pandemic-related information, causing anxiety. From a theoretic perspective,
previous studies have confirmed that the number of COVID-19 cases can cause
negative emotions, but how statistics of different dimensions, such as the
number of imported cases, the number of local cases, and the number of
government-designated lockdown zones, stimulate people's emotions requires
detailed understanding. In order to obtain the views of people on COVID-19,
this paper first proposes a deep learning model which classifies texts related
to the pandemic from text data with place labels. Next, it conducts a sentiment
analysis based on multi-task learning. Finally, it carries out a fixed-effect
panel regression with outputs of the sentiment analysis. The performance of the
algorithm shows a promising result. The empirical study demonstrates while the
number of local cases is positively associated with risk perception, the number
of imported cases is negatively associated with confidence levels, which
explains why citizens tend to ascribe the protracted pandemic to foreign
factors. Besides, this study finds that previous pandemic hits cities recover
slowly from the suffering, while local governments' spending on healthcare can
improve the situation. Our study illustrates the reasons for risk perception
and confidence based on different sources of statistical information due to
cognitive bias. It complements the knowledge related to epidemic information.
It also contributes to a framework that combines sentiment analysis using
advanced deep learning technology with the empirical regression method.
","[{'version': 'v1', 'created': 'Wed, 16 Nov 2022 03:36:13 GMT'}]",2022-11-17,"['Computers and Society', 'Social and Information Networks']","This paper examines how the public's emotional bias towards the Coronavirus pandemic is shaped by the increasing availability of statistics and data on social media. Using text mining techniques, we analyze the sentiment of Coronavirus-related posts from popular social media platforms, including Twitter, Facebook, and Instagram. We identify the main topics discussed in relation to the pandemic and discuss how the availability of statistics and data affects public opinion. We also explore how this emotional bias can be used to inform public policy decisions. Our results provide insight into how the public's emotional bias towards the Coronavirus pandemic is shaped by the increasing availability of statistics and data on social media.","Write an abstract for a paper called Coronavirus statistics causes emotional bias: a social media text mining
  perspective about Computers and Society, Social and Information Networks"
2209.14634,"Congpei An, Jia-Shu Ran",Hard thresholding hyperinterpolation over general regions,"['math.NA', 'cs.NA']","  We propose a fully discrete hard thresholding polynomial approximation over a
general region, named hard thresholding hyperinterpolation (HTH). This
approximation is a weighted $\ell_0$-regularized discrete least squares
approximation under the same conditions of hyperinterpolation. Given an
orthonormal basis of a polynomial space of total-degree not exceeding $L$ and
in view of exactness of a quadrature formula at degree $2L$, HTH approximates
the Fourier coefficients of a continuous function and obtains its coefficients
by acting a hard thresholding operator on all approximated Fourier
coefficients. HTH is an efficient tool to deal with noisy data because of the
basis element selection ability. The main results of HTH for continuous and
smooth functions are twofold: the $L_2$ norm of HTH operator is bounded
independently of the polynomial degree; and the $L_2$ error bound of HTH is
greater than that of hyperinterpolation but HTH performs well in denoising. We
conclude with some numerical experiments to demonstrate the denoising ability
of HTH over intervals, discs, spheres, spherical triangles and cubes.
","[{'version': 'v1', 'created': 'Thu, 29 Sep 2022 08:50:03 GMT'}]",2022-09-30,['Numerical Analysis'],"This paper presents a new approach to numerical analysis called hard thresholding hyperinterpolation over general regions. The proposed method is based on a combination of hard thresholding and hyperinterpolation and is applied to a specific class of problems. The main idea is to construct a piecewise smooth approximation of a given function, which is defined on a general region. The approach is based on a local hard thresholding of the function and a subsequent hyperinterpolation of the resulting piecewise smooth approximation. The paper presents a theoretical analysis of the proposed method and numerical experiments to illustrate its accuracy and efficiency. The results show that the proposed method is competitive with existing numerical methods for approximating functions over general regions.",Write an abstract for a paper called Hard thresholding hyperinterpolation over general regions about Numerical Analysis
2205.06167,"Deeksha Adil, Brian Bullins, Arun Jambulapati, Sushant Sachdeva","Optimal Methods for Higher-Order Smooth Monotone Variational
  Inequalities","['math.OC', 'cs.DS']","  In this work, we present new simple and optimal algorithms for solving the
variational inequality (VI) problem for $p^{th}$-order smooth, monotone
operators -- a problem that generalizes convex optimization and saddle-point
problems. Recent works (Bullins and Lai (2020), Lin and Jordan (2021), Jiang
and Mokhtari (2022)) present methods that achieve a rate of
$\tilde{O}(\epsilon^{-2/(p+1)})$ for $p\geq 1$, extending results by
(Nemirovski (2004)) and (Monteiro and Svaiter (2012)) for $p=1,2$. A drawback
to these approaches, however, is their reliance on a line search scheme. We
provide the first $p^{\textrm{th}}$-order method that achieves a rate of
$O(\epsilon^{-2/(p+1)}).$ Our method does not rely on a line search routine,
thereby improving upon previous rates by a logarithmic factor. Building on the
Mirror Prox method of Nemirovski (2004), our algorithm works even in the
constrained, non-Euclidean setting. Furthermore, we prove the optimality of our
algorithm by constructing matching lower bounds. These are the first lower
bounds for smooth MVIs beyond convex optimization for $p > 1$. This establishes
a separation between solving smooth MVIs and smooth convex optimization, and
settles the oracle complexity of solving $p^{\textrm{th}}$-order smooth MVIs.
","[{'version': 'v1', 'created': 'Thu, 12 May 2022 15:43:53 GMT'}, {'version': 'v2', 'created': 'Tue, 31 May 2022 04:37:34 GMT'}]",2022-06-01,['Data Structures and Algorithms'],"This paper presents a comprehensive overview of optimal methods for higher-order smooth monotone variational inequalities (HOSMVIs) and their application to data structures and algorithms. We review existing methods for solving HOSMVIs, including the alternating direction method of multipliers (ADMM), the augmented Lagrangian method (ALM), and the splitting augmented Lagrangian method (SALM). We then introduce a new method, the dual augmented Lagrangian method (DALM), which combines the advantages of ADMM and ALM, and discuss its application to data structures and algorithms. We present numerical experiments to assess the performance of the proposed method in comparison to existing methods, and discuss the implications of the results. Finally, we provide a detailed discussion of the theoretical properties of HOSMVIs and their applications to data structures and algorithms, and suggest potential directions for future research.","Write an abstract for a paper called Optimal Methods for Higher-Order Smooth Monotone Variational
  Inequalities about Data Structures and Algorithms"
2210.12298,"Chen Chen, Matin Yarmand, Varun Singh, Michael V. Sherer, James D.
  Murphy, Yang Zhang, Nadir Weibel","VRContour: Bringing Contour Delineations of Medical Structures Into
  Virtual Reality","['cs.HC', 'cs.CY']","  Contouring is an indispensable step in Radiotherapy (RT) treatment planning.
However, today's contouring software is constrained to only work with a 2D
display, which is less intuitive and requires high task loads. Virtual Reality
(VR) has shown great potential in various specialties of healthcare and health
sciences education due to the unique advantages of intuitive and natural
interactions in immersive spaces. VR-based radiation oncology integration has
also been advocated as a target healthcare application, allowing providers to
directly interact with 3D medical structures. We present VRContour and
investigate how to effectively bring contouring for radiation oncology into VR.
Through an autobiographical iterative design, we defined three design spaces
focused on contouring in VR with the support of a tracked tablet and VR stylus,
and investigating dimensionality for information consumption and input (either
2D or 2D + 3D). Through a within-subject study (n = 8), we found that
visualizations of 3D medical structures significantly increase precision, and
reduce mental load, frustration, as well as overall contouring effort.
Participants also agreed with the benefits of using such metaphors for learning
purposes.
","[{'version': 'v1', 'created': 'Fri, 21 Oct 2022 23:22:21 GMT'}, {'version': 'v2', 'created': 'Tue, 8 Nov 2022 04:47:20 GMT'}]",2022-11-09,"['Human-Computer Interaction', 'Computers and Society']","This paper presents VRContour, a virtual reality (VR) system for medical visualization. VRContour allows medical professionals to interact with 3D contour delineations of medical structures in a virtual environment. This system is designed to improve the accuracy of medical diagnosis and treatment planning by providing a more immersive experience for medical professionals. The paper discusses the design of the system, its implementation, and how it can be used in medical practice. The paper also addresses the implications of the system on human-computer interaction, computers, and society. It discusses the potential of VRContour to improve the accuracy of medical diagnosis and treatment planning, and how it can provide a more immersive experience for medical professionals. Finally, the paper addresses the ethical and legal implications of using VRContour in medical practice.","Write an abstract for a paper called VRContour: Bringing Contour Delineations of Medical Structures Into
  Virtual Reality about Human-Computer Interaction, Computers and Society"
2212.13827,"Harsh Rangwani, Sumukh K Aithal, Mayank Mishra, R. Venkatesh Babu","Escaping Saddle Points for Effective Generalization on Class-Imbalanced
  Data","['cs.LG', 'cs.CV']","  Real-world datasets exhibit imbalances of varying types and degrees. Several
techniques based on re-weighting and margin adjustment of loss are often used
to enhance the performance of neural networks, particularly on minority
classes. In this work, we analyze the class-imbalanced learning problem by
examining the loss landscape of neural networks trained with re-weighting and
margin-based techniques. Specifically, we examine the spectral density of
Hessian of class-wise loss, through which we observe that the network weights
converge to a saddle point in the loss landscapes of minority classes.
Following this observation, we also find that optimization methods designed to
escape from saddle points can be effectively used to improve generalization on
minority classes. We further theoretically and empirically demonstrate that
Sharpness-Aware Minimization (SAM), a recent technique that encourages
convergence to a flat minima, can be effectively used to escape saddle points
for minority classes. Using SAM results in a 6.2\% increase in accuracy on the
minority classes over the state-of-the-art Vector Scaling Loss, leading to an
overall average increase of 4\% across imbalanced datasets. The code is
available at: https://github.com/val-iisc/Saddle-LongTail.
","[{'version': 'v1', 'created': 'Wed, 28 Dec 2022 14:00:44 GMT'}]",2022-12-29,"['Machine Learning', 'Computer Vision and Pattern Recognition']","This paper presents a novel approach to effective generalization on class-imbalanced data in the fields of machine learning, computer vision, and pattern recognition. It explores the concept of escaping saddle points for better generalization performance, which is based on the idea of avoiding local minima and maxima on the loss surface. The paper provides a detailed analysis of the relationship between saddle points and class-imbalanced data and presents a strategy for escaping saddle points using data augmentation and regularization techniques. Experiments using real-world datasets demonstrate the effectiveness of the proposed approach in improving generalization performance on class-imbalanced data. The paper also discusses the implications of the proposed approach for further research in the fields of machine learning, computer vision, and pattern recognition.","Write an abstract for a paper called Escaping Saddle Points for Effective Generalization on Class-Imbalanced
  Data about Machine Learning, Computer Vision and Pattern Recognition"
2108.08298,"Xiaoqian Chen, Zhiqiang Gong, Xiaoyu Zhao, Weien Zhou, Wen Yao","A Machine Learning Surrogate Modeling Benchmark for Temperature Field
  Reconstruction of Heat-Source Systems","['cs.LG', 'cs.AI', 'cs.SY', 'eess.SY']","  Temperature field reconstruction of heat source systems (TFR-HSS) with
limited monitoring sensors occurred in thermal management plays an important
role in real time health detection system of electronic equipment in
engineering. However, prior methods with common interpolations usually cannot
provide accurate reconstruction performance as required. In addition, there
exists no public dataset for widely research of reconstruction methods to
further boost the reconstruction performance and engineering applications. To
overcome this problem, this work develops a machine learning modelling
benchmark for TFR-HSS task. First, the TFR-HSS task is mathematically modelled
from real-world engineering problem and four types of numerically modellings
have been constructed to transform the problem into discrete mapping forms.
Then, this work proposes a set of machine learning modelling methods, including
the general machine learning methods and the deep learning methods, to advance
the state-of-the-art methods over temperature field reconstruction. More
importantly, this work develops a novel benchmark dataset, namely Temperature
Field Reconstruction Dataset (TFRD), to evaluate these machine learning
modelling methods for the TFR-HSS task. Finally, a performance analysis of
typical methods is given on TFRD, which can be served as the baseline results
on this benchmark.
","[{'version': 'v1', 'created': 'Tue, 17 Aug 2021 15:32:58 GMT'}, {'version': 'v2', 'created': 'Fri, 20 Aug 2021 08:05:06 GMT'}, {'version': 'v3', 'created': 'Sat, 28 Aug 2021 03:04:18 GMT'}, {'version': 'v4', 'created': 'Tue, 14 Sep 2021 03:10:16 GMT'}, {'version': 'v5', 'created': 'Tue, 3 Jan 2023 09:16:49 GMT'}]",2023-01-04,"['Machine Learning', 'Artificial Intelligence', 'Systems and Control']",This paper presents a machine learning surrogate modeling benchmark for temperature field reconstruction of heat-source systems. The benchmark uses an artificial neural network (ANN) to learn the relationship between the temperature field and the heat source parameters. The ANN is trained using a dataset of simulated data generated from a realistic heat-source system. The benchmark is then evaluated on its ability to accurately reconstruct the temperature field from a given set of heat source parameters. The results of the benchmark demonstrate that the ANN is able to accurately reconstruct the temperature field with a mean absolute error of 0.01°C. This benchmark is a useful tool for evaluating the performance of machine learning and artificial intelligence algorithms for controlling and optimizing heat-source systems.,"Write an abstract for a paper called A Machine Learning Surrogate Modeling Benchmark for Temperature Field
  Reconstruction of Heat-Source Systems about Machine Learning, Artificial Intelligence, Systems and Control"
2209.03097,"Christian Jestel and Hartmut Surmann and Jonas Stenzel and Oliver
  Urbann and Marius Brehler","Obtaining Robust Control and Navigation Policies for Multi-Robot
  Navigation via Deep Reinforcement Learning",['cs.RO'],"  Multi-robot navigation is a challenging task in which multiple robots must be
coordinated simultaneously within dynamic environments. We apply deep
reinforcement learning (DRL) to learn a decentralized end-to-end policy which
maps raw sensor data to the command velocities of the agent. In order to enable
the policy to generalize, the training is performed in different environments
and scenarios. The learned policy is tested and evaluated in common multi-robot
scenarios like switching a place, an intersection and a bottleneck situation.
This policy allows the agent to recover from dead ends and to navigate through
complex environments.
","[{'version': 'v1', 'created': 'Wed, 7 Sep 2022 12:17:31 GMT'}]",2022-09-08,['Robotics'],"This paper presents a novel approach to obtaining robust control and navigation policies for multi-robot navigation via deep reinforcement learning. We introduce a framework for training deep reinforcement learning agents to learn control and navigation policies for multiple robots in dynamic environments. We evaluate our approach on scenarios involving multiple robots navigating in a simulated environment with obstacles, and demonstrate the effectiveness of our method. We also discuss the implications of our approach for improving the scalability and robustness of multi-robot navigation systems. Finally, we present our results and discuss potential applications and future work.","Write an abstract for a paper called Obtaining Robust Control and Navigation Policies for Multi-Robot
  Navigation via Deep Reinforcement Learning about Robotics"
2207.03679,Ziheng Zeng and Suma Bhat,"Getting BART to Ride the Idiomatic Train: Learning to Represent
  Idiomatic Expressions","['cs.CL', 'cs.LG']","  Idiomatic expressions (IEs), characterized by their non-compositionality, are
an important part of natural language. They have been a classical challenge to
NLP, including pre-trained language models that drive today's state-of-the-art.
Prior work has identified deficiencies in their contextualized representation
stemming from the underlying compositional paradigm of representation. In this
work, we take a first-principles approach to build idiomaticity into BART using
an adapter as a lightweight non-compositional language expert trained on
idiomatic sentences. The improved capability over baselines (e.g., BART) is
seen via intrinsic and extrinsic methods, where idiom embeddings score 0.19
points higher in homogeneity score for embedding clustering, and up to 25%
higher sequence accuracy on the idiom processing tasks of IE sense
disambiguation and span detection.
","[{'version': 'v1', 'created': 'Fri, 8 Jul 2022 04:05:19 GMT'}]",2022-07-11,"['Computation and Language', 'Machine Learning']","This paper explores the potential of machine learning to represent and process idiomatic expressions in natural language. We propose a novel approach to modeling idiomatic expressions using the BART (Bidirectional Encoder Representations from Transformers) model, a state-of-the-art transformer-based language model. We demonstrate how BART can be used to represent and identify idiomatic expressions related to computation and language. We evaluate our approach on a dataset of idiomatic expressions and show that BART outperforms existing methods, achieving an accuracy of 94.4%. Our results suggest that BART can be used to represent and process idiomatic expressions, and that it is a promising tool for natural language processing applications.","Write an abstract for a paper called Getting BART to Ride the Idiomatic Train: Learning to Represent
  Idiomatic Expressions about Computation and Language, Machine Learning"
2303.07088,"Andrew Weng, Jason B. Siegel, and Anna Stefanopoulou",Differential voltage analysis for battery manufacturing process control,"['eess.SY', 'cs.SY']","  Voltage-based battery metrics are ubiquitous and essential in battery
manufacturing diagnostics. They enable electrochemical ""fingerprinting"" of
batteries at the end of the manufacturing line and are naturally scalable,
since voltage data is already collected as part of the formation process which
is the last step in battery manufacturing. Yet, despite their prevalence,
interpretations of voltage-based metrics are often ambiguous and require expert
judgment. In this work, we present a method for collecting and analyzing full
cell near-equilibrium voltage curves for end-of-line manufacturing process
control. The method builds on existing literature on differential voltage
analysis (DVA or dV/dQ) by expanding the method formalism through the lens of
reproducibility, interpretability, and automation. Our model revisions
introduce several new derived metrics relevant to manufacturing process
control, including lithium consumed during formation and the practical
negative-to-positive ratio, which complement standard metrics such as positive
and negative electrode capacities. To facilitate method reproducibility, we
reformulate the model to account for the ""inaccessible lithium problem"" which
quantifies the numerical differences between modeled versus true values for
electrode capacities and stoichiometries. We finally outline key data
collection considerations, including C-rate and charging direction for both
full cell and half cell datasets, which may impact method reproducibility. This
work highlights the opportunities for leveraging voltage-based electrochemical
metrics for online battery manufacturing process control.
","[{'version': 'v1', 'created': 'Mon, 13 Mar 2023 13:17:43 GMT'}]",2023-03-14,['Systems and Control'],"This paper presents a differential voltage analysis technique for process control in battery manufacturing. The proposed method is based on the concept of voltage difference, which is determined by measuring the voltage difference between two adjacent cells in the battery. The proposed method is used to detect any abnormal behavior in the battery manufacturing process, such as a short circuit, which can lead to a decrease in battery performance. The proposed method is compared to other existing methods, such as current-based and voltage-based process control, and it is shown to have better performance in terms of accuracy and speed. The proposed method is also applied to a battery manufacturing process and the results are discussed. The results demonstrate that the proposed method is effective in detecting abnormal behavior in the battery manufacturing process.",Write an abstract for a paper called Differential voltage analysis for battery manufacturing process control about Systems and Control
2303.15412,Xiaorui Sun,Faster Isomorphism for $p$-Groups of Class 2 and Exponent $p$,['cs.DS'],"  The group isomorphism problem determines whether two groups, given by their
Cayley tables, are isomorphic. For groups with order $n$, an algorithm with
$n^{(\log n + O(1))}$ running time, attributed to Tarjan, was proposed in the
1970s [Mil78]. Despite the extensive study over the past decades, the current
best group isomorphism algorithm has an $n^{(1 / 4 + o(1))\log n}$ running time
[Ros13].
  The isomorphism testing for $p$-groups of (nilpotent) class 2 and exponent
$p$ has been identified as a major barrier to obtaining an $n^{o(\log n)}$ time
algorithm for the group isomorphism problem. Although the $p$-groups of class 2
and exponent $p$ have much simpler algebraic structures than general groups,
the best-known isomorphism testing algorithm for this group class also has an
$n^{O(\log n)}$ running time.
  In this paper, we present an isomorphism testing algorithm for $p$-groups of
class 2 and exponent $p$ with running time $n^{O((\log n)^{5/6})}$ for any
prime $p > 2$. Our result is based on a novel reduction to the skew-symmetric
matrix tuple isometry problem [IQ19]. To obtain the reduction, we develop
several tools for matrix space analysis, including a matrix space
individualization-refinement method and a characterization of the low rank
matrix spaces.
","[{'version': 'v1', 'created': 'Mon, 27 Mar 2023 17:29:55 GMT'}]",2023-03-28,['Data Structures and Algorithms'],"This paper presents an algorithm for isomorphism of $p$-groups of class 2 and exponent $p$, which has a time complexity of $O(p^3)$. The algorithm is based on an efficient data structure, which allows for a faster comparison of the structure of the groups. This data structure is based on the representation of the groups as a set of generators and relations, and it allows for a more efficient comparison of the groups. The algorithm is tested on a set of randomly generated $p$-groups, and the results show that it is faster than existing methods. The paper also discusses the implications of the algorithm, and suggests possible applications.",Write an abstract for a paper called Faster Isomorphism for $p$-Groups of Class 2 and Exponent $p$ about Data Structures and Algorithms
2206.03862,"Zicheng Zhang, Wei Sun, Wei Wu, Ying Chen, Xiongkuo Min, Guangtao Zhai",Perceptual Quality Assessment for Fine-Grained Compressed Images,['cs.CV'],"  Recent years have witnessed the rapid development of image storage and
transmission systems, in which image compression plays an important role.
Generally speaking, image compression algorithms are developed to ensure good
visual quality at limited bit rates. However, due to the different compression
optimization methods, the compressed images may have different levels of
quality, which needs to be evaluated quantificationally. Nowadays, the
mainstream full-reference (FR) metrics are effective to predict the quality of
compressed images at coarse-grained levels (the bit rates differences of
compressed images are obvious), however, they may perform poorly for
fine-grained compressed images whose bit rates differences are quite subtle.
Therefore, to better improve the Quality of Experience (QoE) and provide useful
guidance for compression algorithms, we propose a full-reference image quality
assessment (FR-IQA) method for compressed images of fine-grained levels.
Specifically, the reference images and compressed images are first converted to
$YCbCr$ color space. The gradient features are extracted from regions that are
sensitive to compression artifacts. Then we employ the Log-Gabor transformation
to further analyze the texture difference. Finally, the obtained features are
fused into a quality score. The proposed method is validated on the
fine-grained compression image quality assessment (FGIQA) database, which is
especially constructed for assessing the quality of compressed images with
close bit rates. The experimental results show that our metric outperforms
mainstream FR-IQA metrics on the FGIQA database. We also test our method on
other commonly used compression IQA databases and the results show that our
method obtains competitive performance on the coarse-grained compression IQA
databases as well.
","[{'version': 'v1', 'created': 'Wed, 8 Jun 2022 12:56:45 GMT'}]",2022-06-09,['Computer Vision and Pattern Recognition'],"This paper presents a perceptual quality assessment for fine-grained compressed images. It explores the use of deep learning to evaluate the quality of compressed images at a finer scale than traditional methods. The paper proposes a novel deep learning-based method for estimating the perceptual quality of compressed images. The proposed method is evaluated on a large-scale dataset of compressed images and compared to traditional methods. The results demonstrate that the proposed method outperforms traditional methods in terms of accuracy and speed. Furthermore, the paper discusses the implications of the proposed method for computer vision and pattern recognition applications.",Write an abstract for a paper called Perceptual Quality Assessment for Fine-Grained Compressed Images about Computer Vision and Pattern Recognition
2107.00504,Qing Cheng and Jie Shen,"A new Lagrange multiplier approach for constructing structure preserving
  schemes, I. positivity preserving","['math.NA', 'cs.NA', 'math.AP']","  We propose a new Lagrange multiplier approach to construct positivity
preserving schemes for parabolic type equations. The new approach introduces a
space-time Lagrange multiplier to enforce the positivity with the
Karush-Kuhn-Tucker (KKT) conditions. We then use a predictor-corrector approach
to construct a class of positivity schemes: with a generic semi-implicit or
implicit scheme as the prediction step, and the correction step, which enforces
the positivity, can be implemented with negligible cost. We also present a
modification which allows us to construct schemes which, in addition to
positivity preserving, is also mass conserving. This new approach is not
restricted to any particular spatial discretization and can be combined with
various time discretization schemes. We establish stability results for our
first- and second-order schemes under a general setting, and present ample
numerical results to validate the new approach.
","[{'version': 'v1', 'created': 'Thu, 1 Jul 2021 14:51:03 GMT'}, {'version': 'v2', 'created': 'Sun, 8 Aug 2021 19:20:53 GMT'}]",2022-02-09,['Numerical Analysis'],"This paper presents a novel Lagrange multiplier approach for constructing structure preserving numerical schemes with positivity preserving properties. We discuss the theoretical foundation of the method, and provide numerical examples to demonstrate its efficacy. We focus on the numerical analysis of nonlinear systems of partial differential equations, and the application of the Lagrange multiplier approach to the development of structure preserving numerical schemes for such systems. We also discuss the properties of the resulting schemes, and compare them to existing methods. Finally, we provide a comprehensive discussion of the advantages and disadvantages of the new approach, and its potential for further development.","Write an abstract for a paper called A new Lagrange multiplier approach for constructing structure preserving
  schemes, I. positivity preserving about Numerical Analysis"
1808.0992,"Nicola De Cao, Wilker Aziz, Ivan Titov","Question Answering by Reasoning Across Documents with Graph
  Convolutional Networks","['cs.CL', 'stat.ML']","  Most research in reading comprehension has focused on answering questions
based on individual documents or even single paragraphs. We introduce a neural
model which integrates and reasons relying on information spread within
documents and across multiple documents. We frame it as an inference problem on
a graph. Mentions of entities are nodes of this graph while edges encode
relations between different mentions (e.g., within- and cross-document
co-reference). Graph convolutional networks (GCNs) are applied to these graphs
and trained to perform multi-step reasoning. Our Entity-GCN method is scalable
and compact, and it achieves state-of-the-art results on a multi-document
question answering dataset, WikiHop (Welbl et al., 2018).
","[{'version': 'v1', 'created': 'Wed, 29 Aug 2018 16:44:51 GMT'}, {'version': 'v2', 'created': 'Fri, 22 Mar 2019 13:34:32 GMT'}, {'version': 'v3', 'created': 'Sun, 7 Apr 2019 15:31:22 GMT'}, {'version': 'v4', 'created': 'Tue, 27 Sep 2022 15:12:12 GMT'}]",2022-09-28,['Computation and Language'],"This paper presents a novel approach for question answering by reasoning across documents with graph convolutional networks (GCNs). We demonstrate how GCNs can be used to capture the structure of a document and to understand computation and language. We introduce a new task, called cross-document question answering, and show that GCNs can effectively capture the relationships between documents and answer questions by reasoning across them. We evaluate our approach on two datasets, and show that it outperforms existing methods on both datasets. Our experiments demonstrate that GCNs can be used to effectively answer questions by reasoning across documents and that they are a promising tool to improve the understanding of computation and language.","Write an abstract for a paper called Question Answering by Reasoning Across Documents with Graph
  Convolutional Networks about Computation and Language"
2208.06613,"Stefanos Ioannou (1), Hana Chockler (1 and 3), Alexander Hammers (2)
  and Andrew P. King (2) ((1) Department of Informatics, King's College London,
  U.K., (2) School of Biomedical Engineering and Imaging Sciences, King's
  College London, U.K., (3) causaLens Ltd., U.K.)",A Study of Demographic Bias in CNN-based Brain MR Segmentation,['cs.CV'],"  Convolutional neural networks (CNNs) are increasingly being used to automate
the segmentation of brain structures in magnetic resonance (MR) images for
research studies. In other applications, CNN models have been shown to exhibit
bias against certain demographic groups when they are under-represented in the
training sets. In this work, we investigate whether CNN models for brain MR
segmentation have the potential to contain sex or race bias when trained with
imbalanced training sets. We train multiple instances of the FastSurferCNN
model using different levels of sex imbalance in white subjects. We evaluate
the performance of these models separately for white male and white female test
sets to assess sex bias, and furthermore evaluate them on black male and black
female test sets to assess potential racial bias. We find significant sex and
race bias effects in segmentation model performance. The biases have a strong
spatial component, with some brain regions exhibiting much stronger bias than
others. Overall, our results suggest that race bias is more significant than
sex bias. Our study demonstrates the importance of considering race and sex
balance when forming training sets for CNN-based brain MR segmentation, to
avoid maintaining or even exacerbating existing health inequalities through
biased research study findings.
","[{'version': 'v1', 'created': 'Sat, 13 Aug 2022 10:07:54 GMT'}]",2022-08-16,['Computer Vision and Pattern Recognition'],"This paper presents a study of the potential for demographic bias in convolutional neural network (CNN)-based brain magnetic resonance (MR) segmentation. We propose a novel approach to evaluate the performance of CNNs on brain MR segmentation tasks, based on the demographic characteristics of the population used to train the model. We evaluate the performance of a CNN-based segmentation model on a dataset of brain MR images from two distinct demographic groups and compare the results to a baseline model. We find that the model trained on a dataset of images from a single demographic group performs significantly better than the baseline model, suggesting that demographic bias may be present in the model. Our results suggest that further research is needed to investigate the potential for demographic bias in CNN-based brain MR segmentation and other computer vision and pattern recognition tasks.",Write an abstract for a paper called A Study of Demographic Bias in CNN-based Brain MR Segmentation about Computer Vision and Pattern Recognition
2210.10992,"Zeyu Huang, Juzhan Xu, Sisi Dai, Kai Xu, Hao Zhang, Hui Huang, Ruizhen
  Hu",NIFT: Neural Interaction Field and Template for Object Manipulation,"['cs.RO', 'cs.CV', 'cs.GR']","  We introduce NIFT, Neural Interaction Field and Template, a descriptive and
robust interaction representation of object manipulations to facilitate
imitation learning. Given a few object manipulation demos, NIFT guides the
generation of the interaction imitation for a new object instance by matching
the Neural Interaction Template (NIT) extracted from the demos in the target
Neural Interaction Field (NIF) defined for the new object. Specifically, the
NIF is a neural field that encodes the relationship between each spatial point
and a given object, where the relative position is defined by a spherical
distance function rather than occupancies or signed distances, which are
commonly adopted by conventional neural fields but less informative. For a
given demo interaction, the corresponding NIT is defined by a set of spatial
points sampled in the demo NIF with associated neural features. To better
capture the interaction, the points are sampled on the Interaction Bisector
Surface (IBS), which consists of points that are equidistant to the two
interacting objects and has been used extensively for interaction
representation. With both point selection and pointwise features defined for
better interaction encoding, NIT effectively guides the feature matching in the
NIFs of the new object instances such that the relative poses are optimized to
realize the manipulation while imitating the demo interactions. Experiments
show that our NIFT solution outperforms state-of-the-art imitation learning
methods for object manipulation and generalizes better to objects from new
categories.
","[{'version': 'v1', 'created': 'Thu, 20 Oct 2022 03:35:05 GMT'}, {'version': 'v2', 'created': 'Fri, 21 Oct 2022 01:56:47 GMT'}, {'version': 'v3', 'created': 'Wed, 1 Mar 2023 01:30:41 GMT'}]",2023-03-02,"['Robotics', 'Computer Vision and Pattern Recognition', 'Graphics']","This paper presents NIFT, a novel Neural Interaction Field and Template (NIFT) for object manipulation in robotics, computer vision, and pattern recognition. NIFT is a deep learning-based approach that combines a neural network-based interaction field and a template-based manipulation approach to enable robots to interact with objects more accurately and efficiently. The paper outlines the architecture of NIFT and experiments that demonstrate its effectiveness in terms of accuracy, robustness, and speed. The paper also includes a comparison of NIFT with other existing approaches and concludes with a discussion of future research directions. The results of this work show that NIFT can be used to improve the accuracy and speed of object manipulation tasks in robotics, computer vision, and pattern recognition.","Write an abstract for a paper called NIFT: Neural Interaction Field and Template for Object Manipulation about Robotics, Computer Vision and Pattern Recognition, Graphics"
2303.12944,"Pushpita Chatterjee, Debashis Das, and Danda B Rawat","Use of Federated Learning and Blockchain towards Securing Financial
  Services","['cs.CR', 'cs.AI']","  In recent days, the proliferation of several existing and new cyber-attacks
pose an axiomatic threat to the stability of financial services. It is hard to
predict the nature of attacks that can trigger a serious financial crisis. The
unprecedented digital transformation to financial services has been accelerated
during the COVID-19 pandemic and it is still ongoing. Attackers are taking
advantage of this transformation and pose a new global threat to financial
stability and integrity. Many large organizations are switching from
centralized finance (CeFi) to decentralized finance (DeFi) because
decentralized finance has many advantages. Blockchain can bring big and
far-reaching effects on the trustworthiness, safety, accessibility,
cost-effectiveness, and openness of the financial sector. The present paper
gives an in-depth look at how blockchain and federated learning (FL) are used
in financial services. It starts with an overview of recent developments in
both use cases. This paper explores and discusses existing financial service
vulnerabilities, potential threats, and consequent risks. So, we explain the
problems that can be fixed in financial services and how blockchain and FL
could help solve them. These problems include data protection, storage
optimization, and making more money in financial services. We looked at many
blockchain-enabled FL methods and came up with some possible solutions that
could be used in financial services to solve several challenges like
cost-effectiveness, automation, and security control. Finally, we point out
some future directions at the end of this study.
","[{'version': 'v1', 'created': 'Sat, 4 Feb 2023 17:58:34 GMT'}]",2023-03-24,"['Cryptography and Security', 'Artificial Intelligence']","This paper explores the use of federated learning and blockchain to secure financial services about cryptography and security, artificial intelligence. It examines how federated learning and blockchain can be used to improve the security of financial services, such as cryptocurrency transactions and other financial transactions. It also discusses the potential of federated learning and blockchain to improve the security of artificial intelligence systems, such as fraud detection and automated investment advice. Additionally, the paper looks at the challenges and opportunities of using federated learning and blockchain for financial services, such as privacy and scalability. Finally, it provides recommendations for organizations and developers on how to best utilize these technologies to secure their financial services.","Write an abstract for a paper called Use of Federated Learning and Blockchain towards Securing Financial
  Services about Cryptography and Security, Artificial Intelligence"
2102.12579,"Alexander S. Kulikov, Danila Pechenev, Nikita Slezkin",SAT-based Circuit Local Improvement,['cs.AI'],"  Finding exact circuit size is a notorious optimization problem in practice.
Whereas modern computers and algorithmic techniques allow to find a circuit of
size seven in blink of an eye, it may take more than a week to search for a
circuit of size thirteen. One of the reasons of this behavior is that the
search space is enormous: the number of circuits of size $s$ is
$s^{\Theta(s)}$, the number of Boolean functions on $n$ variables is $2^{2^n}$.
  In this paper, we explore the following natural heuristic idea for decreasing
the size of a given circuit: go through all its subcircuits of moderate size
and check whether any of them can be improved by reducing to SAT. This may be
viewed as a local search approach: we search for a smaller circuit in a ball
around a given circuit. Through this approach, we prove new upper bounds on the
circuit size of various symmetric functions. We also demonstrate that some
upper bounds that were proved by hand decades ago, nowadays can be found
automatically in a few seconds.
","[{'version': 'v1', 'created': 'Fri, 19 Feb 2021 16:01:50 GMT'}, {'version': 'v2', 'created': 'Wed, 30 Mar 2022 17:12:38 GMT'}, {'version': 'v3', 'created': 'Wed, 27 Apr 2022 09:41:24 GMT'}]",2022-04-28,['Artificial Intelligence'],"This paper explores the use of artificial intelligence (AI) in circuit local improvement, specifically focusing on SAT-based circuit local improvement. We discuss the advantages of using AI for circuit local improvement, including its ability to reduce the complexity of the problem and improve the accuracy of the results. We then present a framework for SAT-based circuit local improvement, and illustrate its application on a number of benchmark problems. Finally, we evaluate the effectiveness of the proposed framework, and present our conclusions regarding its applicability to circuit local improvement.",Write an abstract for a paper called SAT-based Circuit Local Improvement about Artificial Intelligence
2212.02272,"Pierre Aboulker, Guillaume Aubian, Pierre Charbit, St\'ephan
  Thomass\'e","(P6, triangle)-free digraphs have bounded dichromatic number","['math.CO', 'cs.DM']","  The dichromatic number of an oriented graph is the minimum size of a
partition of its vertices into acyclic induced subdigraphs. We prove that
oriented graphs with no induced directed path on six vertices and no triangle
have bounded dichromatic number. This is one (small) step towards the general
conjecture asserting that for every oriented tree T and every integer k, any
oriented graph that does not contain an induced copy of T nor a clique of size
k has dichromatic number at most some function of k and T.
","[{'version': 'v1', 'created': 'Mon, 5 Dec 2022 13:50:59 GMT'}, {'version': 'v2', 'created': 'Wed, 18 Jan 2023 13:28:13 GMT'}]",2023-01-19,['Discrete Mathematics'],"This paper examines the relationship between (P6, triangle)-free digraphs and their bounded dichromatic number. A (P6, triangle)-free digraph is a directed graph that does not contain a path of length 6 or a triangle as an induced subgraph. The dichromatic number of a graph is the minimum number of colors needed to color the vertices of the graph so that no two adjacent vertices have the same color. We prove that the dichromatic number of a (P6, triangle)-free digraph is bounded by a constant depending only on the number of vertices of the graph. Furthermore, we provide a characterization of (P6, triangle)-free digraphs with bounded dichromatic number. We conclude by examining the implications of our results and discussing possible directions for further research.","Write an abstract for a paper called (P6, triangle)-free digraphs have bounded dichromatic number about Discrete Mathematics"
2202.00339,Matteo Marsili and Yasser Roudi,Quantifying Relevance in Learning and Inference,"['cs.LG', 'cond-mat.dis-nn', 'physics.data-an', 'stat.ML']","  Learning is a distinctive feature of intelligent behaviour. High-throughput
experimental data and Big Data promise to open new windows on complex systems
such as cells, the brain or our societies. Yet, the puzzling success of
Artificial Intelligence and Machine Learning shows that we still have a poor
conceptual understanding of learning. These applications push statistical
inference into uncharted territories where data is high-dimensional and scarce,
and prior information on ""true"" models is scant if not totally absent. Here we
review recent progress on understanding learning, based on the notion of
""relevance"". The relevance, as we define it here, quantifies the amount of
information that a dataset or the internal representation of a learning machine
contains on the generative model of the data. This allows us to define
maximally informative samples, on one hand, and optimal learning machines on
the other. These are ideal limits of samples and of machines, that contain the
maximal amount of information about the unknown generative process, at a given
resolution (or level of compression). Both ideal limits exhibit critical
features in the statistical sense: Maximally informative samples are
characterised by a power-law frequency distribution (statistical criticality)
and optimal learning machines by an anomalously large susceptibility. The
trade-off between resolution (i.e. compression) and relevance distinguishes the
regime of noisy representations from that of lossy compression. These are
separated by a special point characterised by Zipf's law statistics. This
identifies samples obeying Zipf's law as the most compressed loss-less
representations that are optimal in the sense of maximal relevance. Criticality
in optimal learning machines manifests in an exponential degeneracy of energy
levels, that leads to unusual thermodynamic properties.
","[{'version': 'v1', 'created': 'Tue, 1 Feb 2022 11:16:04 GMT'}]",2022-05-04,['Machine Learning'],"This paper explores the concept of relevance in machine learning and inference. It aims to quantify relevance through the use of metrics and algorithms to measure the importance of a particular input or output in a learning or inference process. The paper will discuss the importance of relevance in machine learning and inference and analyze the various metrics and algorithms that can be used to measure relevance. It will also explore the implications of using such metrics and algorithms in a variety of machine learning and inference tasks. Finally, the paper will present a case study to illustrate the importance of relevance in machine learning and inference.",Write an abstract for a paper called Quantifying Relevance in Learning and Inference about Machine Learning
2302.06653,Allen Ibiapina and Ana Silva,Snapshot disjointness in temporal graphs,['cs.DM'],"  In the study of temporal graphs, only paths respecting the flow of time are
relevant. In this context, many concepts of walks disjointness were proposed
over the years, and the validity of Menger's Theorem, as well as the complexity
of related problems, has been investigated. In this paper, we introduce and
investigate a type of disjointness that is only time dependent. Two walks are
said to be snapshot disjoint if they are not active in a same snapshot (also
called timestep). The related paths and cut problems are then defined and
proved to be W[1]-hard and XP-time solvable when parameterized by the size of
the solution. Additionally, in the light of the definition of Mengerian graphs
given by Kempe, Kleinberg and Kumar in their seminal paper (STOC'2000), we
define a Mengerian graph for time as a graph $G$ that cannot form an example
where Menger's Theorem does not hold in the context of snapshot disjointness.
We then give a characterization in terms of forbidden structures and provide a
polynomial-time recognition algorithm. Finally, we also prove that, given a
temporal graph $(G,\lambda)$ and a pair of vertices $s,z\in V(G)$, deciding
whether at most $h$ multiedges can separate $s$ from $z$ is NP-complete.
","[{'version': 'v1', 'created': 'Mon, 13 Feb 2023 19:34:41 GMT'}]",2023-02-15,['Discrete Mathematics'],"This paper explores the concept of snapshot disjointness in temporal graphs, and its implications for discrete mathematics. Snapshot disjointness is a property of temporal graphs, which are graphs that change over time. A temporal graph is said to be snapshot disjoint if the edges of the graph can be partitioned into two sets such that no two edges in the same set have the same source and target vertices at the same time. We discuss the implications of snapshot disjointness for various problems in discrete mathematics, including graph coloring, graph partitioning, and graph isomorphism. We also analyze the computational complexity of testing for snapshot disjointness in temporal graphs. Finally, we present some open problems and discuss possible directions for future research.",Write an abstract for a paper called Snapshot disjointness in temporal graphs about Discrete Mathematics
2209.01578,"Lishun Wang, Miao Cao, Yong Zhong and Xin Yuan",Spatial-Temporal Transformer for Video Snapshot Compressive Imaging,"['eess.IV', 'cs.CV']","  Video snapshot compressive imaging (SCI) captures multiple sequential video
frames by a single measurement using the idea of computational imaging. The
underlying principle is to modulate high-speed frames through different masks
and these modulated frames are summed to a single measurement captured by a
low-speed 2D sensor (dubbed optical encoder); following this, algorithms are
employed to reconstruct the desired high-speed frames (dubbed software decoder)
if needed. In this paper, we consider the reconstruction algorithm in video
SCI, i.e., recovering a series of video frames from a compressed measurement.
Specifically, we propose a Spatial-Temporal transFormer (STFormer) to exploit
the correlation in both spatial and temporal domains. STFormer network is
composed of a token generation block, a video reconstruction block, and these
two blocks are connected by a series of STFormer blocks. Each STFormer block
consists of a spatial self-attention branch, a temporal self-attention branch
and the outputs of these two branches are integrated by a fusion network.
Extensive results on both simulated and real data demonstrate the
state-of-the-art performance of STFormer. The code and models are publicly
available at https://github.com/ucaswangls/STFormer.git
","[{'version': 'v1', 'created': 'Sun, 4 Sep 2022 09:24:17 GMT'}, {'version': 'v2', 'created': 'Thu, 8 Sep 2022 04:56:25 GMT'}]",2022-09-09,['Computer Vision and Pattern Recognition'],"This paper presents a novel Spatial-Temporal Transformer (STT) for video snapshot compressive imaging. The proposed STT is a deep learning architecture which combines the temporal and spatial information of the video frames to produce a single snapshot image with improved quality. The STT is composed of a temporal transformer and a spatial transformer, which are connected in a sequential manner. To evaluate the performance of the proposed STT, we conducted experiments on the publicly available Video Snapshot Compressive Imaging (VSCI) dataset. The results show that the proposed STT outperforms the state-of-the-art methods in terms of both peak signal-to-noise ratio (PSNR) and structural similarity (SSIM) metrics. The proposed STT also demonstrates superior generalization capability for unseen video frames. We believe that the proposed STT can be a useful tool for computer vision and pattern recognition tasks.",Write an abstract for a paper called Spatial-Temporal Transformer for Video Snapshot Compressive Imaging about Computer Vision and Pattern Recognition
2209.08911,"Amirhossein Akbar Tabatabai, Raheleh Jalali","Universal Proof Theory: Feasible Admissibility in Intuitionistic Modal
  Logics","['math.LO', 'cs.LO']","  In this paper, we introduce a general family of sequent-style calculi over
the modal language and its fragments to capture the essence of all
constructively acceptable systems. Calling these calculi \emph{constructive},
we show that any strong enough constructive sequent calculus, satisfying a mild
technical condition, feasibly admits all Visser's rules, i.e., there is a
polynomial time algorithm that reads a proof of the premise of a Visser's rule
and provides a proof for its conclusion. As a positive application, we show the
feasible admissibility of Visser's rules in several sequent calculi for
intuitionistic modal logics, including $\mathsf{CK}$, $\mathsf{IK}$ and their
extensions by the modal axioms $T$, $B$, $4$, $5$, the modal axioms of bounded
width and depth and the propositional lax logic. On the negative side, we show
that if a strong enough intuitionistic modal logic (satisfying a mild technical
condition) does not admit at least one of Visser's rules, then it cannot have a
constructive sequent calculus. Consequently, no intermediate logic other than
$\mathsf{IPC}$ has a constructive sequent calculus.
","[{'version': 'v1', 'created': 'Mon, 19 Sep 2022 10:46:48 GMT'}, {'version': 'v2', 'created': 'Sat, 8 Oct 2022 19:16:06 GMT'}, {'version': 'v3', 'created': 'Sun, 16 Oct 2022 20:37:48 GMT'}]",2022-10-18,['Logic in Computer Science'],"This paper examines the feasibility of admissibility in intuitionistic modal logics (IML) using universal proof theory (UPT). UPT is a method of constructing proofs that is based on a universal set of inference rules and has been used to prove theorems in various areas of mathematics. We analyze the feasibility of using UPT to prove theorems in IML and discuss the implications of this approach for computer science. We present a proof of admissibility in IML using UPT and discuss the advantages and limitations of this approach. We also discuss the potential applications of UPT in computer science, including automated theorem proving and verification of software systems. Finally, we discuss the implications of our results for the future of logic in computer science.","Write an abstract for a paper called Universal Proof Theory: Feasible Admissibility in Intuitionistic Modal
  Logics about Logic in Computer Science"
2302.13342,Koichi Nishimura and Hanna Sumita,"Envy-freeness and maximum Nash welfare for mixed divisible and
  indivisible goods",['cs.GT'],"  We study fair allocation of resources consisting of both divisible and
indivisible goods to agents with additive valuations. Recently, a fairness
notion called envy-freeness for mixed goods (EFM) has been introduced for this
setting. The EFM is a natural combination of classic fairness notions called
envy-freeness for divisible goods and envy-freeness up to one good for
indivisible goods. When either divisible or indivisible goods exist, it is
known that an allocation that achieves the maximum Nash welfare (MNW) satisfies
the classic fairness notions. On the other hand, for mixed goods, an MNW
allocation does not necessarily entail EFM. In this paper, we formally prove
that an MNW allocation for mixed goods is envy-free up to one (indivisible)
good for mixed goods.
","[{'version': 'v1', 'created': 'Sun, 26 Feb 2023 16:17:08 GMT'}]",2023-02-28,['Computer Science and Game Theory'],"This paper explores the concept of envy-freeness and maximum Nash welfare in the context of mixed divisible and indivisible goods in computer science and game theory. We use a linear programming formulation to model the problem of allocating mixed divisible and indivisible goods among a set of agents. We discuss the complexity of the problem, and the implications of the Nash welfare solution for the efficiency of the allocation. We also discuss the concept of envy-freeness, which is a desirable property of an allocation and is related to the concept of fairness. Finally, we present a solution approach to the problem and discuss the implications of our results.","Write an abstract for a paper called Envy-freeness and maximum Nash welfare for mixed divisible and
  indivisible goods about Computer Science and Game Theory"
2303.00187,"Omid Sedehi, Antonina M. Kosikova, Costas Papadimitriou, Lambros S.
  Katafygiotis","On the Integration of Physics-Based Machine Learning with Hierarchical
  Bayesian Modeling Techniques","['stat.ML', 'cs.LG']","  Machine Learning (ML) has widely been used for modeling and predicting
physical systems. These techniques offer high expressive power and good
generalizability for interpolation within observed data sets. However, the
disadvantage of black-box models is that they underperform under blind
conditions since no physical knowledge is incorporated. Physics-based ML aims
to address this problem by retaining the mathematical flexibility of ML
techniques while incorporating physics. In accord, this paper proposes to embed
mechanics-based models into the mean function of a Gaussian Process (GP) model
and characterize potential discrepancies through kernel machines. A specific
class of kernel function is promoted, which has a connection with the gradient
of the physics-based model with respect to the input and parameters and shares
similarity with the exact Autocovariance function of linear dynamical systems.
The spectral properties of the kernel function enable considering dominant
periodic processes originating from physics misspecification. Nevertheless, the
stationarity of the kernel function is a difficult hurdle in the sequential
processing of long data sets, resolved through hierarchical Bayesian
techniques. This implementation is also advantageous to mitigate computational
costs, alleviating the scalability of GPs when dealing with sequential data.
Using numerical and experimental examples, potential applications of the
proposed method to structural dynamics inverse problems are demonstrated.
","[{'version': 'v1', 'created': 'Wed, 1 Mar 2023 02:29:41 GMT'}]",2023-03-02,['Machine Learning'],"This paper explores the integration of physics-based machine learning and hierarchical Bayesian modeling techniques to develop a more effective machine learning system. The paper first examines the current state of machine learning and its limitations, including the need for large datasets and the difficulty of generalizing learned models. It then introduces hierarchical Bayesian modeling as a means of overcoming these limitations by using prior knowledge to reduce the amount of data needed for training and to improve generalizability. Finally, the paper discusses how physics-based machine learning can be used to further enhance the performance of hierarchical Bayesian models. The paper concludes by discussing the potential applications of this integrated approach in a variety of areas, including computer vision, natural language processing, and robotics.","Write an abstract for a paper called On the Integration of Physics-Based Machine Learning with Hierarchical
  Bayesian Modeling Techniques about Machine Learning"
2110.07205,"Junyi Ao, Rui Wang, Long Zhou, Chengyi Wang, Shuo Ren, Yu Wu, Shujie
  Liu, Tom Ko, Qing Li, Yu Zhang, Zhihua Wei, Yao Qian, Jinyu Li, Furu Wei","SpeechT5: Unified-Modal Encoder-Decoder Pre-Training for Spoken Language
  Processing","['eess.AS', 'cs.CL', 'cs.LG', 'cs.SD']","  Motivated by the success of T5 (Text-To-Text Transfer Transformer) in
pre-trained natural language processing models, we propose a unified-modal
SpeechT5 framework that explores the encoder-decoder pre-training for
self-supervised speech/text representation learning. The SpeechT5 framework
consists of a shared encoder-decoder network and six modal-specific
(speech/text) pre/post-nets. After preprocessing the input speech/text through
the pre-nets, the shared encoder-decoder network models the
sequence-to-sequence transformation, and then the post-nets generate the output
in the speech/text modality based on the output of the decoder. Leveraging
large-scale unlabeled speech and text data, we pre-train SpeechT5 to learn a
unified-modal representation, hoping to improve the modeling capability for
both speech and text. To align the textual and speech information into this
unified semantic space, we propose a cross-modal vector quantization approach
that randomly mixes up speech/text states with latent units as the interface
between encoder and decoder. Extensive evaluations show the superiority of the
proposed SpeechT5 framework on a wide variety of spoken language processing
tasks, including automatic speech recognition, speech synthesis, speech
translation, voice conversion, speech enhancement, and speaker identification.
We release our code and model at https://github.com/microsoft/SpeechT5.
","[{'version': 'v1', 'created': 'Thu, 14 Oct 2021 07:59:27 GMT'}, {'version': 'v2', 'created': 'Thu, 24 Feb 2022 13:55:48 GMT'}, {'version': 'v3', 'created': 'Tue, 24 May 2022 08:18:31 GMT'}]",2022-05-25,"['Computation and Language', 'Machine Learning', 'Sound']","This paper presents SpeechT5, a unified-modal encoder-decoder pre-training approach for spoken language processing. SpeechT5 is a transformer-based model which uses a single encoder-decoder architecture to pre-train on both text and speech data. We evaluate SpeechT5 on a range of downstream tasks, including automatic speech recognition, text-to-speech synthesis, and natural language understanding. Results show that SpeechT5 outperforms existing approaches on all tasks and demonstrates the effectiveness of pre-training on both text and speech data. We also demonstrate that SpeechT5 can be used to learn generalizable representations of spoken language which can be adapted to new tasks. Our results suggest that SpeechT5 is a promising approach for spoken language processing and has potential to further advance the state-of-the-art in computation and language, machine learning, and sound.","Write an abstract for a paper called SpeechT5: Unified-Modal Encoder-Decoder Pre-Training for Spoken Language
  Processing about Computation and Language, Machine Learning, Sound"
2207.14552,"Huimin Huang, Shiao Xie1, Lanfen Lin, Yutaro Iwamoto, Xianhua Han,
  Yen-Wei Chen, Ruofeng Tong","ScaleFormer: Revisiting the Transformer-based Backbones from a
  Scale-wise Perspective for Medical Image Segmentation",['cs.CV'],"  Recently, a variety of vision transformers have been developed as their
capability of modeling long-range dependency. In current transformer-based
backbones for medical image segmentation, convolutional layers were replaced
with pure transformers, or transformers were added to the deepest encoder to
learn global context. However, there are mainly two challenges in a scale-wise
perspective: (1) intra-scale problem: the existing methods lacked in extracting
local-global cues in each scale, which may impact the signal propagation of
small objects; (2) inter-scale problem: the existing methods failed to explore
distinctive information from multiple scales, which may hinder the
representation learning from objects with widely variable size, shape and
location. To address these limitations, we propose a novel backbone, namely
ScaleFormer, with two appealing designs: (1) A scale-wise intra-scale
transformer is designed to couple the CNN-based local features with the
transformer-based global cues in each scale, where the row-wise and column-wise
global dependencies can be extracted by a lightweight Dual-Axis MSA. (2) A
simple and effective spatial-aware inter-scale transformer is designed to
interact among consensual regions in multiple scales, which can highlight the
cross-scale dependency and resolve the complex scale variations. Experimental
results on different benchmarks demonstrate that our Scale-Former outperforms
the current state-of-the-art methods. The code is publicly available at:
https://github.com/ZJUGiveLab/ScaleFormer.
","[{'version': 'v1', 'created': 'Fri, 29 Jul 2022 08:55:00 GMT'}]",2022-08-01,['Computer Vision and Pattern Recognition'],"This paper presents ScaleFormer, a novel Transformer-based backbone architecture for medical image segmentation. ScaleFormer is designed to address the challenges of scale variation in medical images by introducing a scale-wise perspective to the Transformer-based backbone. The proposed architecture is evaluated on several medical image segmentation tasks and compared with existing methods. Results show that ScaleFormer outperforms other methods in terms of accuracy and speed. Furthermore, ScaleFormer is shown to be robust to varying scales, making it a viable choice for medical image segmentation applications. The paper provides insights on the potential of Transformer-based backbones for medical image segmentation tasks and suggests future directions for research.","Write an abstract for a paper called ScaleFormer: Revisiting the Transformer-based Backbones from a
  Scale-wise Perspective for Medical Image Segmentation about Computer Vision and Pattern Recognition"
2212.08965,"Salah A Faroughi, Pingki Datta, Seyed Kourosh Mahjour, Shirko Faroughi","Physics-informed Neural Networks with Periodic Activation Functions for
  Solute Transport in Heterogeneous Porous Media",['cs.LG'],"  Solute transport in porous media is relevant to a wide range of applications
in hydrogeology, geothermal energy, underground CO2 storage, and a variety of
chemical engineering systems. Due to the complexity of solute transport in
heterogeneous porous media, traditional solvers require high resolution meshing
and are therefore expensive computationally. This study explores the
application of a mesh-free method based on deep learning to accelerate the
simulation of solute transport. We employ Physics-informed Neural Networks
(PiNN) to solve solute transport problems in homogeneous and heterogeneous
porous media governed by the advection-dispersion equation. Unlike traditional
neural networks that learn from large training datasets, PiNNs only leverage
the strong form mathematical models to simultaneously solve for multiple
dependent or independent field variables (e.g., pressure and solute
concentration fields). In this study, we construct PiNN using a periodic
activation function to better represent the complex physical signals (i.e.,
pressure) and their derivatives (i.e., velocity). Several case studies are
designed with the intention of investigating the proposed PiNN's capability to
handle different degrees of complexity. A manual hyperparameter tuning method
is used to find the best PiNN architecture for each test case. Point-wise error
and mean square error (MSE) measures are employed to assess the performance of
PiNNs' predictions against the ground truth solutions obtained analytically or
numerically using the finite element method. Our findings show that the
predictions of PiNN are in good agreement with the ground truth solutions while
reducing computational complexity and cost by, at least, three orders of
magnitude.
","[{'version': 'v1', 'created': 'Sat, 17 Dec 2022 21:58:53 GMT'}]",2022-12-20,['Machine Learning'],"This paper presents a novel physics-informed neural network (PINN) model for solute transport in heterogeneous porous media. The PINN model is equipped with periodic activation functions, which are tailored to capture the periodic behavior of solute transport in the subsurface. The PINN model is trained using a combination of field observations and numerical simulations. Through a series of experiments, the PINN model is shown to accurately predict the solute transport behavior in heterogeneous porous media, outperforming existing machine learning models. The results demonstrate the effectiveness of the proposed PINN model, providing a promising approach for simulating solute transport in the subsurface.","Write an abstract for a paper called Physics-informed Neural Networks with Periodic Activation Functions for
  Solute Transport in Heterogeneous Porous Media about Machine Learning"
2205.13152,Yi Huang and Adams Wai-Kin Kong,Transferable Adversarial Attack based on Integrated Gradients,"['cs.LG', 'cs.CV']","  The vulnerability of deep neural networks to adversarial examples has drawn
tremendous attention from the community. Three approaches, optimizing standard
objective functions, exploiting attention maps, and smoothing decision
surfaces, are commonly used to craft adversarial examples. By tightly
integrating the three approaches, we propose a new and simple algorithm named
Transferable Attack based on Integrated Gradients (TAIG) in this paper, which
can find highly transferable adversarial examples for black-box attacks. Unlike
previous methods using multiple computational terms or combining with other
methods, TAIG integrates the three approaches into one single term. Two
versions of TAIG that compute their integrated gradients on a straight-line
path and a random piecewise linear path are studied. Both versions offer strong
transferability and can seamlessly work together with the previous methods.
Experimental results demonstrate that TAIG outperforms the state-of-the-art
methods. The code will available at https://github.com/yihuang2016/TAIG
","[{'version': 'v1', 'created': 'Thu, 26 May 2022 04:59:28 GMT'}]",2022-05-27,"['Machine Learning', 'Computer Vision and Pattern Recognition']","This paper presents a novel transferable adversarial attack based on Integrated Gradients (IG) for Machine Learning, Computer Vision and Pattern Recognition. The proposed attack is capable of generating adversarial examples that can transfer across multiple models and datasets. The attack is based on the IG saliency map and uses a multi-step optimization process to generate adversarial examples. The proposed attack is evaluated on a variety of datasets, models and architectures, and is shown to have a high transferability rate across different models and datasets. Additionally, the attack is shown to be robust against various defense techniques. The results of this paper demonstrate the effectiveness of the proposed attack as a powerful tool for evaluating the security of Machine Learning, Computer Vision and Pattern Recognition systems.","Write an abstract for a paper called Transferable Adversarial Attack based on Integrated Gradients about Machine Learning, Computer Vision and Pattern Recognition"
2209.10323,"Giann Karlo Aguirre-Sambon\'i (INRIA and LMF, CNRS and ENS
  Paris-Saclay, Universit\'e Paris-Saclay), Stefan Haar (INRIA and LMF, CNRS
  and ENS Paris-Saclay, Universit\'e Paris-Saclay), Lo\""ic Paulev\'e (Univ.
  Bordeaux, Bordeaux INP, CNRS, LaBRI, UMR5800), Stefan Schwoon (INRIA and LMF,
  CNRS and ENS Paris-Saclay, Universit\'e Paris-Saclay), Nick W\""urdemann
  (Department of Computing Science, University of Oldenburg)",Avoid One's Doom: Finding Cliff-Edge Configurations in Petri Nets,"['cs.FL', 'cs.DS']","  A crucial question in analyzing a concurrent system is to determine its
long-run behaviour, and in particular, whether there are irreversible choices
in its evolution, leading into parts of the reachability space from which there
is no return to other parts. Casting this problem in the unifying framework of
safe Petri nets, our previous work has provided techniques for identifying
attractors, i.e. terminal strongly connected components of the reachability
space, whose attraction basins we wish to determine. Here, we provide a
solution for the case of safe Petri nets. Our algorithm uses net unfoldings and
provides a map of all of the system's configurations (concurrent executions)
that act as cliff-edges, i.e. any maximal extension for those configurations
lies in some basin that is considered fatal. The computation turns out to
require only a relatively small prefix of the unfolding, just twice the depth
of Esparza's complete prefix.
","[{'version': 'v1', 'created': 'Wed, 21 Sep 2022 12:46:44 GMT'}]",2022-09-22,"['Formal Languages and Automata Theory', 'Data Structures and Algorithms']","This paper presents a novel method for finding cliff-edge configurations in Petri nets, a model of distributed systems. Using formal languages and automata theory, data structures and algorithms, the method is based on the idea of constructing a graph of reachable configurations in the Petri net. The paper discusses how the method can be used to identify configurations that lead to an undesirable state, and how the approach can be applied to a variety of applications. The paper also provides an empirical evaluation of the proposed method, demonstrating its effectiveness in finding cliff-edge configurations in Petri nets.","Write an abstract for a paper called Avoid One's Doom: Finding Cliff-Edge Configurations in Petri Nets about Formal Languages and Automata Theory, Data Structures and Algorithms"
2301.12402,"Xiang Li, Shuwei Chen, Jian Dong, Jin Zhang, Yongkang Wang, Xingxing
  Wang, Dong Wang","Decision-Making Context Interaction Network for Click-Through Rate
  Prediction","['cs.IR', 'cs.LG']","  Click-through rate (CTR) prediction is crucial in recommendation and online
advertising systems. Existing methods usually model user behaviors, while
ignoring the informative context which influences the user to make a click
decision, e.g., click pages and pre-ranking candidates that inform inferences
about user interests, leading to suboptimal performance. In this paper, we
propose a Decision-Making Context Interaction Network (DCIN), which deploys a
carefully designed Context Interaction Unit (CIU) to learn decision-making
contexts and thus benefits CTR prediction. In addition, the relationship
between different decision-making context sources is explored by the proposed
Adaptive Interest Aggregation Unit (AIAU) to improve CTR prediction further. In
the experiments on public and industrial datasets, DCIN significantly
outperforms the state-of-the-art methods. Notably, the model has obtained the
improvement of CTR+2.9%/CPM+2.1%/GMV+1.5% for online A/B testing and served the
main traffic of Meituan Waimai advertising system.
","[{'version': 'v1', 'created': 'Sun, 29 Jan 2023 09:48:01 GMT'}]",2023-01-31,"['Information Retrieval', 'Machine Learning']","This paper presents a novel Decision-Making Context Interaction Network (DCIN) for click-through rate (CTR) prediction in information retrieval. CTR prediction is an important task in modern machine learning systems, as it is used to optimize user experience in web search, online advertising, and other related applications. DCIN leverages contextual information from multiple sources to better capture user intent and preferences, and improve the accuracy of CTR prediction. We evaluate DCIN on two real-world datasets and demonstrate that it outperforms existing state-of-the-art models in terms of both accuracy and efficiency. Furthermore, we discuss the implications of our findings and suggest future directions for improving the performance of DCIN.","Write an abstract for a paper called Decision-Making Context Interaction Network for Click-Through Rate
  Prediction about Information Retrieval, Machine Learning"
2111.01233,"Cem Gormezano, Jean-Christophe Nave, Andy T. S. Wan",Conservative Integrators for Vortex Blob Methods,"['math.NA', 'cs.NA', 'physics.comp-ph', 'physics.flu-dyn']","  Conservative symmetric second-order one-step integrators are derived using
the Discrete Multiplier Method for a family of vortex-blob models approximating
the incompressible Euler's equations on the plane. Conservative properties and
second order convergence are proved. A rational function approximation was used
to approximate the exponential integral that appears in the Hamiltonian.
Numerical experiments are shown to verify the conservative property of these
integrators, their second-order accuracy, and as well as the resulting spatial
and temporal accuracy of the vortex blob method. Moreover, the derived implicit
conservative integrators are shown to be better at preserving conserved
quantities than standard higher-order explicit integrators on comparable
computation times.
","[{'version': 'v1', 'created': 'Mon, 1 Nov 2021 19:43:00 GMT'}]",2022-08-31,['Numerical Analysis'],"This paper presents a novel numerical method for solving the vortex blob method equations. The conservative integrators developed in this paper are based on the concept of conservative integration, which is a form of numerical integration that preserves the total energy in the system. We show that these conservative integrators can be used to accurately and efficiently solve the vortex blob method equations. We present numerical results that demonstrate the accuracy and efficiency of our proposed method, and discuss the advantages and disadvantages of our approach. Additionally, we discuss future work that could be done to further improve the accuracy and efficiency of our method.",Write an abstract for a paper called Conservative Integrators for Vortex Blob Methods about Numerical Analysis
2204.10677,"R\'emi Nahon, Guillaume-Alexandre Bilodeau and Gilles Pesant",Improving tracking with a tracklet associator,['cs.CV'],"  Multiple object tracking (MOT) is a task in computer vision that aims to
detect the position of various objects in videos and to associate them to a
unique identity. We propose an approach based on Constraint Programming (CP)
whose goal is to be grafted to any existing tracker in order to improve its
object association results. We developed a modular algorithm divided into three
independent phases. The first phase consists in recovering the tracklets
provided by a base tracker and to cut them at the places where uncertain
associations are spotted, for example, when tracklets overlap, which may cause
identity switches. In the second phase, we associate the previously constructed
tracklets using a Belief Propagation Constraint Programming algorithm, where we
propose various constraints that assign scores to each of the tracklets based
on multiple characteristics, such as their dynamics or the distance between
them in time and space. Finally, the third phase is a rudimentary interpolation
model to fill in the remaining holes in the trajectories we built. Experiments
show that our model leads to improvements in the results for all three of the
state-of-the-art trackers on which we tested it (3 to 4 points gained on HOTA
and IDF1).
","[{'version': 'v1', 'created': 'Fri, 22 Apr 2022 12:47:46 GMT'}]",2022-04-25,['Computer Vision and Pattern Recognition'],"This paper presents a novel tracklet associator for improving the accuracy of object tracking in computer vision and pattern recognition. The tracklet associator utilizes a novel online learning algorithm to learn the correlation between the tracklets of an object in consecutive frames. The proposed method is evaluated on a publicly available dataset and compared to other state-of-the-art tracking algorithms. The results show that the proposed tracklet associator outperforms the baseline methods in terms of tracking accuracy and robustness. Furthermore, the proposed method is able to track objects in challenging scenarios, such as occlusion and fast motion. The proposed method is expected to have a significant impact on the field of computer vision and pattern recognition.",Write an abstract for a paper called Improving tracking with a tracklet associator about Computer Vision and Pattern Recognition
2107.08748,Nikolaj I. Schwartzbach,"Payment Schemes from Limited Information with Applications in
  Distributed Computing",['cs.GT'],"  We propose a generic mechanism for incentivizing behavior in an arbitrary
finite game using payments. Doing so is trivial if the mechanism is allowed to
observe all actions taken in the game, as this allows it to simply punish those
agents who deviate from the intended strategy. Instead, we consider an
abstraction where the mechanism probabilistically infers information about what
happened in the game. We show that payment schemes can be used to implement any
set of utilities if and only if the mechanism can essentially infer completely
what happened. We show that finding an optimal payment scheme for games of
perfect information is \textsf{P}-complete, and conjecture it to be
\textsf{PPAD}-hard for games of imperfect information. We prove a lower bound
on the size of the payments, showing that the payments must be linear in the
intended level of security. We demonstrate the applicability of our model to
concrete problems in distributed computing, namely decentralized commerce and
secure multiparty computation, for which the payments match the lower bound
asymptotically.
","[{'version': 'v1', 'created': 'Mon, 19 Jul 2021 10:43:02 GMT'}, {'version': 'v2', 'created': 'Sat, 12 Feb 2022 23:34:18 GMT'}]",2023-04-05,['Computer Science and Game Theory'],"This paper examines the use of payment schemes from limited information in distributed computing, with an emphasis on applications in computer science and game theory. We introduce a novel framework for payment schemes that allow for the efficient exchange of information between multiple parties, while maintaining privacy and security. We then discuss the implications of this framework in distributed computing, including applications in computer science and game theory. We consider various scenarios to demonstrate the effectiveness of our proposed payment schemes, and discuss their advantages and limitations. Finally, we provide examples of applications of our payment schemes in distributed computing and game theory, and discuss the potential for future work in this area.","Write an abstract for a paper called Payment Schemes from Limited Information with Applications in
  Distributed Computing about Computer Science and Game Theory"
2303.17646,"Abhishek Moitra, Abhiroop Bhattacharjee, Youngeun Kim and
  Priyadarshini Panda","XPert: Peripheral Circuit & Neural Architecture Co-search for Area and
  Energy-efficient Xbar-based Computing",['cs.CV'],"  The hardware-efficiency and accuracy of Deep Neural Networks (DNNs)
implemented on In-memory Computing (IMC) architectures primarily depend on the
DNN architecture and the peripheral circuit parameters. It is therefore
essential to holistically co-search the network and peripheral parameters to
achieve optimal performance. To this end, we propose XPert, which co-searches
network architecture in tandem with peripheral parameters such as the type and
precision of analog-to-digital converters, crossbar column sharing and the
layer-specific input precision using an optimization-based design space
exploration. Compared to VGG16 baselines, XPert achieves 10.24x (4.7x) lower
EDAP, 1.72x (1.62x) higher TOPS/W,1.93x (3x) higher TOPS/mm2 at 92.46% (56.7%)
accuracy for CIFAR10 (TinyImagenet) datasets. The code for this paper is
available at https://github.com/Intelligent-Computing-Lab-Yale/XPert.
","[{'version': 'v1', 'created': 'Thu, 30 Mar 2023 18:23:20 GMT'}]",2023-04-03,['Computer Vision and Pattern Recognition'],"This paper presents XPert, a novel peripheral circuit and neural architecture co-search approach for area and energy-efficient Xbar-based computing for computer vision and pattern recognition. XPert combines a peripheral circuit search with a neural architecture search to explore the design space of Xbar-based computing systems. The peripheral circuit search finds area- and energy-efficient Xbar configurations while the neural architecture search discovers the optimal network architecture. Experiments on the ImageNet and CIFAR-10 datasets show that XPert can reduce the area and energy consumption of Xbar-based computing systems by up to 17% and 20%, respectively, while achieving competitive accuracy. In addition, XPert is shown to be more efficient than existing methods for Xbar-based computing.","Write an abstract for a paper called XPert: Peripheral Circuit & Neural Architecture Co-search for Area and
  Energy-efficient Xbar-based Computing about Computer Vision and Pattern Recognition"
2108.0893,"Anirban Das, Timothy Castiglia, Shiqiang Wang and Stacy Patterson","Cross-Silo Federated Learning for Multi-Tier Networks with Vertical and
  Horizontal Data Partitioning","['cs.LG', 'cs.DC']","  We consider federated learning in tiered communication networks. Our network
model consists of a set of silos, each holding a vertical partition of the
data. Each silo contains a hub and a set of clients, with the silo's vertical
data shard partitioned horizontally across its clients. We propose Tiered
Decentralized Coordinate Descent (TDCD), a communication-efficient
decentralized training algorithm for such two-tiered networks. The clients in
each silo perform multiple local gradient steps before sharing updates with
their hub to reduce communication overhead. Each hub adjusts its coordinates by
averaging its workers' updates, and then hubs exchange intermediate updates
with one another. We present a theoretical analysis of our algorithm and show
the dependence of the convergence rate on the number of vertical partitions and
the number of local updates. We further validate our approach empirically via
simulation-based experiments using a variety of datasets and objectives.
","[{'version': 'v1', 'created': 'Thu, 19 Aug 2021 22:01:04 GMT'}, {'version': 'v2', 'created': 'Tue, 28 Dec 2021 18:54:41 GMT'}, {'version': 'v3', 'created': 'Mon, 20 Jun 2022 19:00:29 GMT'}]",2022-06-22,"['Machine Learning', 'Distributed, Parallel, and Cluster Computing']","This paper presents a novel approach to federated learning in multi-tier networks with vertical and horizontal data partitioning. It introduces a cross-silo federated learning framework that enables distributed and parallel training of machine learning models across silos with different data partitioning strategies. The proposed framework is evaluated on a benchmark dataset, demonstrating improved accuracy and scalability compared to existing approaches. Furthermore, the paper presents a cluster computing approach to further improve the scalability of the framework. The results of the experiments show that the proposed framework is able to achieve a high accuracy and scalability in multi-tier networks with vertical and horizontal data partitioning.","Write an abstract for a paper called Cross-Silo Federated Learning for Multi-Tier Networks with Vertical and
  Horizontal Data Partitioning about Machine Learning, Distributed, Parallel, and Cluster Computing"
2203.15377,"Haibin Wu, Lingwei Meng, Jiawen Kang, Jinchao Li, Xu Li, Xixin Wu,
  Hung-yi Lee, Helen Meng",Spoofing-Aware Speaker Verification by Multi-Level Fusion,"['cs.SD', 'cs.LG', 'eess.AS']","  Recently, many novel techniques have been introduced to deal with spoofing
attacks, and achieve promising countermeasure (CM) performances. However, these
works only take the stand-alone CM models into account. Nowadays, a spoofing
aware speaker verification (SASV) challenge which aims to facilitate the
research of integrated CM and ASV models, arguing that jointly optimizing CM
and ASV models will lead to better performance, is taking place. In this paper,
we propose a novel multi-model and multi-level fusion strategy to tackle the
SASV task. Compared with purely scoring fusion and embedding fusion methods,
this framework first utilizes embeddings from CM models, propagating CM
embeddings into a CM block to obtain a CM score. In the second-level fusion,
the CM score and ASV scores directly from ASV systems will be concatenated into
a prediction block for the final decision. As a result, the best single fusion
system has achieved the SASV-EER of 0.97% on the evaluation set. Then by
ensembling the top-5 fusion systems, the final SASV-EER reached 0.89%.
","[{'version': 'v1', 'created': 'Tue, 29 Mar 2022 09:16:38 GMT'}]",2022-03-30,"['Sound', 'Machine Learning']",", and Security

This paper presents a novel multi-level fusion approach for spoofing-aware speaker verification. The proposed approach fuses acoustic and machine learning based features to detect spoofing attacks and verify speakers. The acoustic features are extracted from the sound signal and the machine learning based features are obtained by training a convolutional neural network (CNN) on the sound signal. The fusion of the two feature sets is then used to classify spoofing attacks and to verify the speaker. Experiments on the ASVspoof 2017 dataset show that the proposed approach outperforms state-of-the-art spoofing-aware speaker verification systems. The results demonstrate the effectiveness of the proposed approach in providing a secure and reliable speaker verification system.","Write an abstract for a paper called Spoofing-Aware Speaker Verification by Multi-Level Fusion about Sound, Machine Learning"
2201.0377,"Marc Bruyere, Christoff Visser and Daphne Tuncer",Toward Evaluating the Complexity to Operate a Network,['cs.NI'],"  The task of determining which network architectures provide the best ratio in
terms of operation and management efforts \textit{vs.} performance guarantees
is not trivial. In this paper, we investigate the complexity of operating
different types of architectures from the perspective of the space of network
parameters that need to be monitored and configured. We present OPLEX, a novel
framework based on the analysis of YANG data models of network implementations
that enables operators to compare architecture options based on the dimension
of the parameter space. We implement OPLEX as part of an operator-friendly tool
that can be used to determine the space associated with an architecture in an
automatic and flexible way. The benefits of the proposed framework are
demonstrated in the use case of Internet Exchange Point (IXP) network
architectures, for which we take advantage of the rich set of publicly
available data. We also exploit the results of a survey and direct
consultations we conducted with operators and vendors of IXPs on their
perception of complexity when operating different architectures. OPLEX is
flexible, builds upon data models with widespread usage in the community, and
provides a practical solution geared towards operators for characterizing the
complexity of network architecture options.
","[{'version': 'v1', 'created': 'Tue, 11 Jan 2022 04:08:02 GMT'}]",2022-01-12,['Networking and Internet Architecture'],"This paper presents a new approach to evaluate the complexity of operating a network. It examines the current state of networking and Internet architecture, and proposes a framework to measure the complexity of managing a network. The framework consists of two main components: a complexity metric and a set of metrics that measure the complexity of a network's operations. The complexity metric is based on the number of components and their associated complexity, while the operational metrics measure the number of operations and their associated complexity. The paper then applies the proposed framework to an example network, and evaluates the complexity of operating the network. Finally, the paper discusses the implications of the proposed framework and potential future research directions.",Write an abstract for a paper called Toward Evaluating the Complexity to Operate a Network about Networking and Internet Architecture
2209.06367,"Hang Chen, Keqing Du, Xinyu Yang, Chenguang Li","A Review and Roadmap of Deep Learning Causal Discovery in Different
  Variable Paradigms","['cs.LG', 'cs.AI', 'stat.ME']","  Understanding causality helps to structure interventions to achieve specific
goals and enables predictions under interventions. With the growing importance
of learning causal relationships, causal discovery tasks have transitioned from
using traditional methods to infer potential causal structures from
observational data to the field of pattern recognition involved in deep
learning. The rapid accumulation of massive data promotes the emergence of
causal search methods with brilliant scalability. Existing summaries of causal
discovery methods mainly focus on traditional methods based on constraints,
scores and FCMs, there is a lack of perfect sorting and elaboration for deep
learning-based methods, also lacking some considers and exploration of causal
discovery methods from the perspective of variable paradigms. Therefore, we
divide the possible causal discovery tasks into three types according to the
variable paradigm and give the definitions of the three tasks respectively,
define and instantiate the relevant datasets for each task and the final causal
model constructed at the same time, then reviews the main existing causal
discovery methods for different tasks. Finally, we propose some roadmaps from
different perspectives for the current research gaps in the field of causal
discovery and point out future research directions.
","[{'version': 'v1', 'created': 'Wed, 14 Sep 2022 01:52:17 GMT'}]",2022-09-15,"['Machine Learning', 'Artificial Intelligence']","and Data Science

This paper reviews the current state of deep learning causal discovery in different variable paradigms, such as supervised learning, unsupervised learning, and reinforcement learning. It provides a comprehensive overview of the various methods and techniques used in deep learning causal discovery, including Bayesian networks, structural equation models, and neural networks. Furthermore, it presents a roadmap for future research in deep learning causal discovery, including potential applications and challenges. Finally, the paper discusses the implications of deep learning causal discovery for machine learning, artificial intelligence, and data science. This review paper provides a comprehensive overview of deep learning causal discovery, and serves as an important reference for researchers in the field.","Write an abstract for a paper called A Review and Roadmap of Deep Learning Causal Discovery in Different
  Variable Paradigms about Machine Learning, Artificial Intelligence"
2206.12628,"Yongzhi Fan, Xin Du, Lun Luo, Jizhong Shen","FreSCo: Frequency-Domain Scan Context for LiDAR-based Place Recognition
  with Translation and Rotation Invariance",['cs.RO'],"  Place recognition plays a crucial role in re-localization and loop closure
detection tasks for robots and vehicles. This paper seeks a well-defined global
descriptor for LiDAR-based place recognition. Compared to local descriptors,
global descriptors show remarkable performance in urban road scenes but are
usually viewpoint-dependent. To this end, we propose a simple yet robust global
descriptor dubbed FreSCo that decomposes the viewpoint difference during
revisit and achieves both translation and rotation invariance by leveraging
Fourier Transform and circular shift technique. Besides, a fast two-stage pose
estimation method is proposed to estimate the relative pose after place
retrieval by utilizing the compact 2D point clouds extracted from the original
data. Experiments show that FreSCo exhibited superior performance than
contemporaneous methods on sequences of different scenes from multiple
datasets. Code will be publicly available at https://github.com/soytony/FreSCo.
","[{'version': 'v1', 'created': 'Sat, 25 Jun 2022 11:47:35 GMT'}, {'version': 'v2', 'created': 'Tue, 27 Sep 2022 14:51:53 GMT'}]",2022-09-28,['Robotics'],"This paper presents FreSCo, a novel frequency-domain scan context descriptor for LiDAR-based place recognition with translation and rotation invariance. FreSCo utilizes the frequency-domain information of the LiDAR scan to generate a high-dimensional context descriptor that is invariant to translation and rotation. The descriptor is robust to changes in the environment and can be used to identify places using a nearest neighbor search. We evaluate FreSCo on two public datasets and demonstrate that it outperforms state-of-the-art methods in terms of accuracy and efficiency. Our results show that FreSCo is an effective and reliable approach for LiDAR-based place recognition with translation and rotation invariance in robotics applications.","Write an abstract for a paper called FreSCo: Frequency-Domain Scan Context for LiDAR-based Place Recognition
  with Translation and Rotation Invariance about Robotics"
2012.07198,Francisco Revson F. Pereira and Stefano Mancini,Polar Codes for Quantum Reading,"['cs.IT', 'math.IT', 'quant-ph']","  Quantum reading provides a general framework where to formulate the
statistical discrimination of quantum channels. Several paths have been taken
for such a problem. However, there is much to be done in the avenue of
optimizing channel discrimination using classical codes. At least two open
questions can be pointed to: how to construct low complexity encoding schemes
that are interesting for channel discrimination and, more importantly, how to
develop capacity-achieving protocols. The aim of this paper is to present a
solution to these questions using polar codes. Firstly, we characterize the
rate and reliability of the channels under polar encoding. We also show that
the error probability of the scheme proposed decays exponentially with respect
to the code length. Lastly, an analysis of the optimal quantum states to be
used as probes is given.
","[{'version': 'v1', 'created': 'Mon, 14 Dec 2020 01:24:11 GMT'}, {'version': 'v2', 'created': 'Fri, 18 Dec 2020 13:35:24 GMT'}]",2022-06-22,['Information Theory'],"This paper provides an overview of the concept of Polar Codes, an information-theoretic approach to quantum reading. The paper begins by discussing the basics of quantum information theory and how it relates to classical information theory. It then discusses the concept of Polar Codes, the advantages they offer over classical methods, and the challenges they present. The paper then provides a detailed analysis of the various algorithms that have been proposed for the implementation of Polar Codes. Finally, it discusses the potential applications of Polar Codes for quantum reading and the implications for the future of information theory.",Write an abstract for a paper called Polar Codes for Quantum Reading about Information Theory
2211.09706,"Joseanne Viana, Hamed Farkhari, Pedro Sebastiao, Sandra Lagen,
  Katerina Koutlia, Biljana Bojovic, Rui Dinis","A Synthetic Dataset for 5G UAV Attacks Based on Observable Network
  Parameters","['cs.NI', 'cs.LG']","  Synthetic datasets are beneficial for machine learning researchers due to the
possibility of experimenting with new strategies and algorithms in the training
and testing phases. These datasets can easily include more scenarios that might
be costly to research with real data or can complement and, in some cases,
replace real data measurements, depending on the quality of the synthetic data.
They can also solve the unbalanced data problem, avoid overfitting, and can be
used in training while testing can be done with real data. In this paper, we
present, to the best of our knowledge, the first synthetic dataset for Unmanned
Aerial Vehicle (UAV) attacks in 5G and beyond networks based on the following
key observable network parameters that indicate power levels: the Received
Signal Strength Indicator (RSSI) and the Signal to Interference-plus-Noise
Ratio (SINR). The main objective of this data is to enable deep network
development for UAV communication security. Especially, for algorithm
development or the analysis of time-series data applied to UAV attack
recognition. Our proposed dataset provides insights into network functionality
when static or moving UAV attackers target authenticated UAVs in an urban
environment. The dataset also considers the presence and absence of
authenticated terrestrial users in the network, which may decrease the deep
networks ability to identify attacks. Furthermore, the data provides deeper
comprehension of the metrics available in the 5G physical and MAC layers for
machine learning and statistics research. The dataset will available at link
archive-beta.ics.uci.edu
","[{'version': 'v1', 'created': 'Sat, 5 Nov 2022 15:12:51 GMT'}]",2022-11-18,"['Networking and Internet Architecture', 'Machine Learning']",", and Cybersecurity

This paper presents a novel method for constructing a synthetic dataset for 5G unmanned aerial vehicle (UAV) attacks based on observable network parameters. The dataset is designed to be used for machine learning and cybersecurity research, and it covers a wide range of networking and internet architecture, machine learning, and cybersecurity topics. The dataset is constructed using a combination of publicly available datasets, real-world UAV attack data, and computer simulations. The dataset is designed to be comprehensive, representative, and diverse in terms of attack types, attack scenarios, and attack targets. Furthermore, the dataset is designed to be easily accessible and reproducible. The paper evaluates the performance of the dataset with a series of experiments and provides a detailed analysis of the results. The paper also discusses the implications of the dataset for future research and development in the field of 5G UAV security.","Write an abstract for a paper called A Synthetic Dataset for 5G UAV Attacks Based on Observable Network
  Parameters about Networking and Internet Architecture, Machine Learning"
2112.01958,"Carlos Guerrero, Isaac Lera and Carlos Juiz","Genetic-based optimization in Fog Computing: current trends and research
  opportunities",['cs.DC'],"  Fog computing is a new computational paradigm that emerged from the need to
reduce network usage and latency in the Internet of Things (IoT). Fog can be
considered as a continuum between the cloud layer and IoT users that allows the
execution of applications or storage/processing of data in network
infrastructure devices. The heterogeneity and wider distribution of fog devices
are the key differences between cloud and fog infrastructure. Genetic-based
optimization is commonly used in distributed systems; however, the
differentiating features of fog computing require new designs, studies, and
experimentation. The growing research in the field of genetic-based fog
resource optimization and the lack of previous analysis in this field have
encouraged us to present a comprehensive, exhaustive, and systematic review of
the most recent research works. Resource optimization techniques in fog were
examined and analyzed, with special emphasis on genetic-based solutions and
their characteristics and design alternatives. We defined a classification of
the optimization scope in fog infrastructures and used this optimization
taxonomy to classify the 70 papers in this survey. Subsequently, the papers
were assessed in terms of genetic optimization design. Finally, the benefits
and limitations of each surveyed work are outlined in this paper. Based on
these previous analyses of the relevant literature, future research directions
were identified. We concluded that more research efforts are needed to address
the current challenges in data management, workflow scheduling, and service
placement. Additionally, there is still room for improved designs and
deployments of parallel and hybrid genetic algorithms that leverage, and adapt
to, the heterogeneity and distributed features of fog domains.
","[{'version': 'v1', 'created': 'Fri, 3 Dec 2021 15:03:14 GMT'}, {'version': 'v2', 'created': 'Mon, 24 Jan 2022 10:40:30 GMT'}, {'version': 'v3', 'created': 'Fri, 13 May 2022 08:09:50 GMT'}]",2022-05-16,"['Distributed, Parallel, and Cluster Computing']","This paper reviews the current trends and research opportunities in genetic-based optimization in Fog Computing. Fog Computing is a distributed computing architecture that enables data and computational resources to be shared and distributed across different physical locations. It is becoming increasingly popular due to its ability to reduce latency, improve scalability, and provide more efficient resource utilization. Genetic-based optimization is an efficient tool for solving complex optimization problems in distributed computing environments and can be used to improve the performance of Fog Computing systems. This paper will discuss the current trends in genetic-based optimization in Fog Computing and will explore potential research opportunities in this area. Specifically, the paper will discuss the use of genetic-based optimization for distributed, parallel, and cluster computing, as well as its potential applications in Fog Computing. The paper will also discuss the challenges associated with genetic-based optimization in Fog Computing and suggest potential solutions. Finally, the paper will provide an overview of the current research in this area and suggest future research directions.","Write an abstract for a paper called Genetic-based optimization in Fog Computing: current trends and research
  opportunities about Distributed, Parallel, and Cluster Computing"
2003.13314,"Wenbo Wang, Amir Leshem, Dusit Niyato, Zhu Han","Decentralized Learning for Channel Allocation in IoT Networks over
  Unlicensed Bandwidth as a Contextual Multi-player Multi-armed Bandit Game","['cs.MA', 'cs.LG', 'cs.NI']","  We study a decentralized channel allocation problem in an ad-hoc Internet of
Things network underlaying on the spectrum licensed to a primary cellular
network. In the considered network, the impoverished channel sensing/probing
capability and computational resource on the IoT devices make them difficult to
acquire the detailed Channel State Information (CSI) for the shared multiple
channels. In practice, the unknown patterns of the primary users' transmission
activities and the time-varying CSI (e.g., due to small-scale fading or device
mobility) also cause stochastic changes in the channel quality. Decentralized
IoT links are thus expected to learn channel conditions online based on partial
observations, while acquiring no information about the channels that they are
not operating on. They also have to reach an efficient, collision-free solution
of channel allocation with limited coordination. Our study maps this problem
into a contextual multi-player, multi-armed bandit game, and proposes a purely
decentralized, three-stage policy learning algorithm through trial-and-error.
Theoretical analyses shows that the proposed scheme guarantees the IoT links to
jointly converge to the social optimal channel allocation with a sub-linear
(i.e., polylogarithmic) regret with respect to the operational time.
Simulations demonstrate that it strikes a good balance between efficiency and
network scalability when compared with the other state-of-the-art decentralized
bandit algorithms.
","[{'version': 'v1', 'created': 'Mon, 30 Mar 2020 10:05:35 GMT'}, {'version': 'v2', 'created': 'Sat, 4 Apr 2020 08:12:11 GMT'}, {'version': 'v3', 'created': 'Thu, 1 Jul 2021 10:16:15 GMT'}]",2022-01-20,"['Multiagent Systems', 'Machine Learning', 'Networking and Internet Architecture']","This paper presents a decentralized learning approach for channel allocation in Internet of Things (IoT) networks over unlicensed bandwidth. The approach is formulated as a contextual multi-player multi-armed bandit game and is based on multiagent systems, machine learning, networking and internet architecture. The proposed approach is evaluated in terms of its ability to allocate channels to multiple IoT devices in a distributed manner, while taking into account the contextual information of each device. Simulation results show that the proposed approach can achieve significantly better performance than existing methods in terms of throughput, delay, and fairness. Moreover, the proposed approach can be easily extended to more complex scenarios.","Write an abstract for a paper called Decentralized Learning for Channel Allocation in IoT Networks over
  Unlicensed Bandwidth as a Contextual Multi-player Multi-armed Bandit Game about Multiagent Systems, Machine Learning, Networking and Internet Architecture"
2206.01724,"Chengliang Zhong, Peixing You, Xiaoxue Chen, Hao Zhao, Fuchun Sun,
  Guyue Zhou, Xiaodong Mu, Chuang Gan, Wenbing Huang",SNAKE: Shape-aware Neural 3D Keypoint Field,"['cs.CV', 'cs.AI']","  Detecting 3D keypoints from point clouds is important for shape
reconstruction, while this work investigates the dual question: can shape
reconstruction benefit 3D keypoint detection? Existing methods either seek
salient features according to statistics of different orders or learn to
predict keypoints that are invariant to transformation. Nevertheless, the idea
of incorporating shape reconstruction into 3D keypoint detection is
under-explored. We argue that this is restricted by former problem
formulations. To this end, a novel unsupervised paradigm named SNAKE is
proposed, which is short for shape-aware neural 3D keypoint field. Similar to
recent coordinate-based radiance or distance field, our network takes 3D
coordinates as inputs and predicts implicit shape indicators and keypoint
saliency simultaneously, thus naturally entangling 3D keypoint detection and
shape reconstruction. We achieve superior performance on various public
benchmarks, including standalone object datasets ModelNet40, KeypointNet, SMPL
meshes and scene-level datasets 3DMatch and Redwood. Intrinsic shape awareness
brings several advantages as follows. (1) SNAKE generates 3D keypoints
consistent with human semantic annotation, even without such supervision. (2)
SNAKE outperforms counterparts in terms of repeatability, especially when the
input point clouds are down-sampled. (3) the generated keypoints allow accurate
geometric registration, notably in a zero-shot setting. Codes are available at
https://github.com/zhongcl-thu/SNAKE
","[{'version': 'v1', 'created': 'Fri, 3 Jun 2022 17:58:43 GMT'}, {'version': 'v2', 'created': 'Mon, 17 Oct 2022 07:45:17 GMT'}]",2022-10-18,"['Computer Vision and Pattern Recognition', 'Artificial Intelligence']","This paper presents SNAKE, a Shape-aware Neural 3D Keypoint Field that enables robust 3D keypoint detection in computer vision and pattern recognition tasks. SNAKE uses a novel neural network architecture that incorporates shape-aware features to capture the complex 3D geometry of objects. These features are then used to generate a 3D keypoint field that is robust to occlusions, viewpoint changes, and other challenges associated with real-world data. Experiments on several standard datasets demonstrate that SNAKE outperforms existing methods in terms of accuracy and robustness. The proposed approach is a promising tool for further research in the field of artificial intelligence.","Write an abstract for a paper called SNAKE: Shape-aware Neural 3D Keypoint Field about Computer Vision and Pattern Recognition, Artificial Intelligence"
2204.00233,Dianming Hou and Zhonghua Qiao,"An implicit--explicit second order BDF numerical scheme with variable
  steps for gradient flows","['math.NA', 'cs.NA']","  In this paper, we propose and analyze an efficient implicit--explicit (IMEX)
second order in time backward differentiation formulation (BDF2) scheme with
variable time steps for gradient flow problems using the scalar auxiliary
variable (SAV) approach. We prove the unconditional energy stability of the
scheme for a modified discrete energy with the adjacent time step ratio
$\gamma_{n+1}:=\Dt_{n+1}/\Dt_{n}\leq 4.8645$. The uniform $H^{2}$ bound for the
numerical solution is derived under a mild regularity restriction on the
initial condition, that is $\phi(\x,0)\in H^{2}$. Based on this uniform bound,
a rigorous error estimate of the numerical solution is carried out on the
temporal nonuniform mesh. Finally, serval numerical tests are provided to
validate the theoretical claims. With the application of an adaptive
time-stepping strategy, the efficiency of our proposed scheme can be clearly
observed in the coarsening dynamics simulation.
","[{'version': 'v1', 'created': 'Fri, 1 Apr 2022 06:48:50 GMT'}]",2022-04-04,['Numerical Analysis'],"This paper presents an implicit-explicit second order BDF numerical scheme with variable steps for gradient flows. The proposed numerical scheme is based on a combination of the backward differentiation formula (BDF) and the implicit-explicit (IMEX) methods. The numerical scheme is shown to be unconditionally stable and of second order accuracy. A detailed analysis of the numerical scheme is provided, including a comparison between the numerical scheme and other numerical methods. The numerical scheme is then applied to a variety of gradient flows, and its performance is assessed. The results demonstrate that the proposed numerical scheme is an effective and efficient numerical tool for the numerical solution of gradient flows.","Write an abstract for a paper called An implicit--explicit second order BDF numerical scheme with variable
  steps for gradient flows about Numerical Analysis"
2208.12396,"Wei Xu, Zaifeng Gao, Liezhong Ge","New paradigmatic orientations and research agenda of human factors
  science in the intelligence era",['cs.HC'],"  Our recent research shows that the design philosophy of human factors science
in the intelligence age is expanding from ""user-centered design"" to
""human-centered AI"". The human-machine relationship presents a trans-era
evolution from ""human-machine interaction"" to ""human-machine/AI teaming"". These
changes have raised new questions and challenges for human factors science. The
interdisciplinary field of human factors science includes any work that adopts
a human-centered approach, such as human factors, ergonomics, engineering
psychology, and human-computer interaction. These changes compel us to
re-examine current human factors science's paradigms and research agenda.
Existing research paradigms are primarily based on non-intelligent
technologies. In this context, this paper reviews the evolution of the
paradigms of human factors science. It summarizes the new conceptual models and
frameworks we recently proposed to enrich the research paradigms for human
factors science, including a human-AI teaming model, a human-AI joint cognitive
ecosystem framework, and an intelligent sociotechnical systems framework. This
paper further enhances these concepts and looks forward to the application of
these concepts. This paper also looks forward to the future research agenda of
human factors science in the areas of ""human-AI interaction"", ""intelligent
human-machine interface"", and ""human-AI teaming"". It analyzes the role of the
research paradigms on the future research agenda. We believe that the research
paradigms and agenda of human factors science influence and promote each other.
Human factors science in the intelligence age needs diversified and innovative
research paradigms, thereby further promoting the research and application of
human factors science.
","[{'version': 'v1', 'created': 'Fri, 26 Aug 2022 01:41:26 GMT'}, {'version': 'v2', 'created': 'Mon, 29 Aug 2022 01:23:04 GMT'}, {'version': 'v3', 'created': 'Tue, 30 Aug 2022 00:38:42 GMT'}, {'version': 'v4', 'created': 'Sat, 18 Feb 2023 06:38:53 GMT'}, {'version': 'v5', 'created': 'Tue, 21 Feb 2023 19:06:36 GMT'}, {'version': 'v6', 'created': 'Mon, 6 Mar 2023 06:11:18 GMT'}, {'version': 'v7', 'created': 'Thu, 6 Apr 2023 04:55:43 GMT'}]",2023-04-07,['Human-Computer Interaction'],"This paper presents a new research agenda for Human-Computer Interaction (HCI) in the intelligence era. It examines the implications of new paradigmatic orientations for HCI research and how it can be used to inform the development of intelligent user interfaces. The paper first provides an overview of the current state of HCI research, highlighting the challenges and opportunities posed by the rapid advancements in artificial intelligence (AI) and related technologies. It then discusses the implications of new paradigmatic orientations, such as cognitive science, computational creativity, and machine learning, for HCI research. Finally, it outlines a research agenda for HCI in the intelligence era, focusing on topics such as user experience design, AI-driven user interfaces, and human-AI collaboration. The paper concludes with a discussion of the implications of this research agenda for HCI researchers, practitioners, and the broader AI community.","Write an abstract for a paper called New paradigmatic orientations and research agenda of human factors
  science in the intelligence era about Human-Computer Interaction"
2203.02836,"Alexander K. Lew, Marco Cusumano-Towner, and Vikash K. Mansinghka",Recursive Monte Carlo and Variational Inference with Auxiliary Variables,"['cs.LG', 'stat.CO']","  A key design constraint when implementing Monte Carlo and variational
inference algorithms is that it must be possible to cheaply and exactly
evaluate the marginal densities of proposal distributions and variational
families. This takes many interesting proposals off the table, such as those
based on involved simulations or stochastic optimization. This paper broadens
the design space, by presenting a framework for applying Monte Carlo and
variational inference algorithms when proposal densities cannot be exactly
evaluated. Our framework, recursive auxiliary-variable inference (RAVI),
instead approximates the necessary densities using meta-inference: an
additional layer of Monte Carlo or variational inference, that targets the
proposal, rather than the model. RAVI generalizes and unifies several existing
methods for inference with expressive approximating families, which we show
correspond to specific choices of meta-inference algorithm, and provides new
theory for analyzing their bias and variance. We illustrate RAVI's design
framework and theorems by using them to analyze and improve upon Salimans et
al.'s Markov Chain Variational Inference, and to design a novel sampler for
Dirichlet process mixtures, achieving state-of-the-art results on a standard
benchmark dataset from astronomy and on a challenging datacleaning task with
Medicare hospital data.
","[{'version': 'v1', 'created': 'Sat, 5 Mar 2022 23:52:40 GMT'}, {'version': 'v2', 'created': 'Sat, 19 Nov 2022 00:09:20 GMT'}]",2022-11-22,['Machine Learning'],"This paper presents a novel approach to machine learning that combines recursive Monte Carlo (RMC) and variational inference (VI) with auxiliary variables. RMC is a technique for approximate inference in graphical models, while VI is a popular method for approximating posterior distributions. The proposed approach combines the two techniques, using auxiliary variables to improve the accuracy of the estimates. The proposed approach is applied to a variety of machine learning tasks, including classification, regression and density estimation. Results show that the proposed approach yields better performance than either RMC or VI alone. Additionally, the proposed approach is shown to be more robust to hyperparameter selection than either RMC or VI. Finally, the proposed approach is found to scale well to large datasets and to be computationally efficient.",Write an abstract for a paper called Recursive Monte Carlo and Variational Inference with Auxiliary Variables about Machine Learning
2206.08021,"Xueliang Wang, Jiajun Chen, Feng Wu, Jie Wang","Exploiting Global Semantic Similarities in Knowledge Graphs by
  Relational Prototype Entities","['cs.CL', 'cs.AI']","  Knowledge graph (KG) embedding aims at learning the latent representations
for entities and relations of a KG in continuous vector spaces. An empirical
observation is that the head (tail) entities connected by the same relation
often share similar semantic attributes -- specifically, they often belong to
the same category -- no matter how far away they are from each other in the KG;
that is, they share global semantic similarities. However, many existing
methods derive KG embeddings based on the local information, which fail to
effectively capture such global semantic similarities among entities. To
address this challenge, we propose a novel approach, which introduces a set of
virtual nodes called \textit{\textbf{relational prototype entities}} to
represent the prototypes of the head and tail entities connected by the same
relations. By enforcing the entities' embeddings close to their associated
prototypes' embeddings, our approach can effectively encourage the global
semantic similarities of entities -- that can be far away in the KG --
connected by the same relation. Experiments on the entity alignment and KG
completion tasks demonstrate that our approach significantly outperforms recent
state-of-the-arts.
","[{'version': 'v1', 'created': 'Thu, 16 Jun 2022 09:25:33 GMT'}]",2022-06-17,"['Computation and Language', 'Artificial Intelligence']","This paper explores the potential of exploiting global semantic similarities in knowledge graphs by using relational prototype entities for computation and language, Artificial Intelligence (AI). This paper reviews the current state of knowledge graph research and its application in AI. It then presents a novel approach for exploiting global semantic similarities in knowledge graphs by creating relational prototype entities for computation and language. The paper then evaluates the effectiveness of the proposed approach through experiments and case studies. The results demonstrate the potential of exploiting global semantic similarities in knowledge graphs for AI. The paper concludes with a discussion of the implications of the proposed approach and directions for future work.","Write an abstract for a paper called Exploiting Global Semantic Similarities in Knowledge Graphs by
  Relational Prototype Entities about Computation and Language, Artificial Intelligence"
2302.05724,Mohammed Barhoush and Louis Salvail,Powerful Primitives in the Bounded Quantum Storage Model,"['cs.CR', 'quant-ph']","  The bounded quantum storage model aims to achieve security against
computationally unbounded adversaries that are restricted only with respect to
their quantum memories. In this work, we show the power of this model by
providing everlasting and information-theoretic secure constructions for the
following primitives: (1) Symmetric key encryption, message-authentication and
one-time programs. These schemes require no quantum memory for the honest user
while they can be made secure against adversaries with arbitrarily large
memories. (2) Program broadcast, asymmetric key encryption, encryption tokens,
signatures, and signature tokens. These schemes are secure against adversaries
with roughly $e^{\sqrt{m}}$ quantum memory where $m$ is the quantum memory
required of the honest user. All of the constructions additionally satisfy
notions of disappearing and unclonable security.
","[{'version': 'v1', 'created': 'Sat, 11 Feb 2023 15:38:52 GMT'}]",2023-02-14,['Cryptography and Security'],"This paper explores the potential of powerful primitives in the bounded quantum storage model for cryptography and security. We investigate the security of these primitives in comparison to existing classical primitives, as well as their potential applications in secure communication and authentication. We then discuss the implications of these primitives on the security of quantum networks and how they can be used to protect data from malicious actors. Finally, we discuss the challenges and opportunities that arise from using these primitives in a bounded quantum storage model. Our results show that these primitives can provide significant security benefits in comparison to classical primitives and can be used to protect data in a quantum network.",Write an abstract for a paper called Powerful Primitives in the Bounded Quantum Storage Model about Cryptography and Security
2303.16205,"Yuhyun Ji, Sang Mok Park, Semin Kwon, Jung Woo Leem, Vidhya
  Vijayakrishnan Nair, Yunjie Tong, and Young L. Kim","mHealth hyperspectral learning for instantaneous spatiospectral imaging
  of hemodynamics","['eess.IV', 'cs.LG', 'physics.optics']","  Hyperspectral imaging acquires data in both the spatial and frequency domains
to offer abundant physical or biological information. However, conventional
hyperspectral imaging has intrinsic limitations of bulky instruments, slow data
acquisition rate, and spatiospectral tradeoff. Here we introduce hyperspectral
learning for snapshot hyperspectral imaging in which sampled hyperspectral data
in a small subarea are incorporated into a learning algorithm to recover the
hypercube. Hyperspectral learning exploits the idea that a photograph is more
than merely a picture and contains detailed spectral information. A small
sampling of hyperspectral data enables spectrally informed learning to recover
a hypercube from an RGB image. Hyperspectral learning is capable of recovering
full spectroscopic resolution in the hypercube, comparable to high spectral
resolutions of scientific spectrometers. Hyperspectral learning also enables
ultrafast dynamic imaging, leveraging ultraslow video recording in an
off-the-shelf smartphone, given that a video comprises a time series of
multiple RGB images. To demonstrate its versatility, an experimental model of
vascular development is used to extract hemodynamic parameters via statistical
and deep-learning approaches. Subsequently, the hemodynamics of peripheral
microcirculation is assessed at an ultrafast temporal resolution up to a
millisecond, using a conventional smartphone camera. This spectrally informed
learning method is analogous to compressed sensing; however, it further allows
for reliable hypercube recovery and key feature extractions with a transparent
learning algorithm. This learning-powered snapshot hyperspectral imaging method
yields high spectral and temporal resolutions and eliminates the spatiospectral
tradeoff, offering simple hardware requirements and potential applications of
various machine-learning techniques.
","[{'version': 'v1', 'created': 'Mon, 27 Mar 2023 15:12:10 GMT'}, {'version': 'v2', 'created': 'Wed, 5 Apr 2023 15:22:32 GMT'}]",2023-04-06,['Machine Learning'],"This paper presents a novel application of mHealth hyperspectral learning for instantaneous spatiospectral imaging of hemodynamics. By combining the advantages of machine learning algorithms with the flexibility of mHealth technology, this method enables the real-time capture of hemodynamic data from multiple sources and locations. The proposed approach uses a convolutional neural network to process the hyperspectral data and produce accurate predictions of spatiospectral hemodynamic parameters. The proposed method is evaluated on a real-world dataset, and the results demonstrate that the proposed method provides superior performance compared to existing methods. The proposed approach has the potential to revolutionize the way hemodynamic data is collected, analyzed, and used in clinical practice.","Write an abstract for a paper called mHealth hyperspectral learning for instantaneous spatiospectral imaging
  of hemodynamics about Machine Learning"
2205.15549,Eng Hock Lee and Vladimir Cherkassky,VC Theoretical Explanation of Double Descent,"['stat.ML', 'cs.LG']","  There has been growing interest in generalization performance of large
multilayer neural networks that can be trained to achieve zero training error,
while generalizing well on test data. This regime is known as 'second descent'
and it appears to contradict the conventional view that optimal model
complexity should reflect an optimal balance between underfitting and
overfitting, i.e., the bias-variance trade-off. This paper presents a
VC-theoretical analysis of double descent and shows that it can be fully
explained by classical VC-generalization bounds. We illustrate an application
of analytic VC-bounds for modeling double descent for classification, using
empirical results for several learning methods, such as SVM, Least Squares, and
Multilayer Perceptron classifiers. In addition, we discuss several reasons for
the misinterpretation of VC-theoretical results in Deep Learning community.
","[{'version': 'v1', 'created': 'Tue, 31 May 2022 05:50:02 GMT'}, {'version': 'v2', 'created': 'Mon, 22 Aug 2022 14:59:02 GMT'}, {'version': 'v3', 'created': 'Thu, 29 Sep 2022 15:50:28 GMT'}]",2022-09-30,['Machine Learning'],"This paper examines the theoretical explanation of double descent in machine learning, a phenomenon in which the generalization error of a model decreases with the addition of more parameters, then increases again. Through a review of the literature, this paper will discuss the theoretical underpinning of double descent in machine learning, including the role of the VC dimension and its implications for machine learning algorithms. It will analyze the theoretical results of double descent in the context of deep learning, as well as the implications for the design of learning algorithms. Finally, this paper will discuss the implications of double descent for the development of robust machine learning systems.",Write an abstract for a paper called VC Theoretical Explanation of Double Descent about Machine Learning
2211.15393,"Wei Lin, Muhammad Jehanzeb Mirza, Mateusz Kozinski, Horst Possegger,
  Hilde Kuehne, Horst Bischof",Video Test-Time Adaptation for Action Recognition,['cs.CV'],"  Although action recognition systems can achieve top performance when
evaluated on in-distribution test points, they are vulnerable to unanticipated
distribution shifts in test data. However, test-time adaptation of video action
recognition models against common distribution shifts has so far not been
demonstrated. We propose to address this problem with an approach tailored to
spatio-temporal models that is capable of adaptation on a single video sample
at a step. It consists in a feature distribution alignment technique that
aligns online estimates of test set statistics towards the training statistics.
We further enforce prediction consistency over temporally augmented views of
the same test video sample. Evaluations on three benchmark action recognition
datasets show that our proposed technique is architecture-agnostic and able to
significantly boost the performance on both, the state of the art convolutional
architecture TANet and the Video Swin Transformer. Our proposed method
demonstrates a substantial performance gain over existing test-time adaptation
approaches in both evaluations of a single distribution shift and the
challenging case of random distribution shifts. Code will be available at
\url{https://github.com/wlin-at/ViTTA}.
","[{'version': 'v1', 'created': 'Thu, 24 Nov 2022 10:49:54 GMT'}, {'version': 'v2', 'created': 'Fri, 2 Dec 2022 11:18:49 GMT'}, {'version': 'v3', 'created': 'Mon, 20 Mar 2023 23:48:02 GMT'}]",2023-03-22,['Computer Vision and Pattern Recognition'],"This paper presents a novel approach to action recognition in computer vision and pattern recognition using video test-time adaptation. Our method utilizes a convolutional neural network (CNN) to analyze video frames and adapt to different test conditions. We discuss the architecture of the CNN and the training process. We then evaluate the performance of our method on several publicly available datasets, including UCF-101 and HMDB-51. Results demonstrate that our approach achieves state-of-the-art performance on these datasets, outperforming other existing methods. Finally, we discuss the implications of our findings and future directions for research in this area.",Write an abstract for a paper called Video Test-Time Adaptation for Action Recognition about Computer Vision and Pattern Recognition
2212.10324,"Michael Sober, Max Kobelt, Giulia Scaffino, Dominik Kaaser, Stefan
  Schulte",Distributed Key Generation with Smart Contracts using zk-SNARKs,"['cs.CR', 'cs.DC']","  Distributed Key Generation (DKG) is an extensively researched topic as it is
fundamental to threshold cryptosystems. Emerging technologies such as
blockchains benefit massively from applying threshold cryptography in consensus
protocols, randomness beacons, and threshold signatures. However, blockchains
and smart contracts also enable further improvements of DKG protocols by
providing a decentralized computation and communication platform.
  For that reason, we propose a DKG protocol that uses smart contracts to
ensure the correct execution of the protocol, allow dynamic participation, and
provide crypto-economic incentives to encourage honest behavior. The DKG
protocol uses a dispute and key derivation mechanism based on Zero-Knowledge
Succinct Non-interactive Arguments of Knowledge (zk-SNARKs) to reduce the costs
of applying smart contracts by moving the computations off-chain, where the
smart contract only verifies the correctness of the computation.
","[{'version': 'v1', 'created': 'Tue, 20 Dec 2022 15:15:55 GMT'}]",2022-12-23,"['Cryptography and Security', 'Distributed, Parallel, and Cluster Computing']","This paper presents an overview of distributed key generation with smart contracts using zk-SNARKs. It discusses the cryptographic and security aspects of distributed, parallel, and cluster computing. It also examines the advantages and challenges of using zk-SNARKs for distributed key generation with smart contracts. Finally, it provides a case study of a distributed key generation system using zk-SNARKs. The paper aims to provide an understanding of the security and cryptographic aspects of distributed key generation with smart contracts using zk-SNARKs. It also seeks to identify the advantages and challenges of using zk-SNARKs for distributed key generation with smart contracts. This paper is expected to provide a comprehensive overview of distributed key generation with smart contracts using zk-SNARKs and its implications for distributed, parallel, and cluster computing.","Write an abstract for a paper called Distributed Key Generation with Smart Contracts using zk-SNARKs about Cryptography and Security, Distributed, Parallel, and Cluster Computing"
2206.1501,"Shanu Kumar, Sandipan Dandapat, Monojit Choudhury","""Diversity and Uncertainty in Moderation"" are the Key to Data Selection
  for Multilingual Few-shot Transfer",['cs.CL'],"  Few-shot transfer often shows substantial gain over zero-shot
transfer~\cite{lauscher2020zero}, which is a practically useful trade-off
between fully supervised and unsupervised learning approaches for multilingual
pretrained model-based systems. This paper explores various strategies for
selecting data for annotation that can result in a better few-shot transfer.
The proposed approaches rely on multiple measures such as data entropy using
$n$-gram language model, predictive entropy, and gradient embedding. We propose
a loss embedding method for sequence labeling tasks, which induces diversity
and uncertainty sampling similar to gradient embedding. The proposed data
selection strategies are evaluated and compared for POS tagging, NER, and NLI
tasks for up to 20 languages. Our experiments show that the gradient and loss
embedding-based strategies consistently outperform random data selection
baselines, with gains varying with the initial performance of the zero-shot
transfer. Furthermore, the proposed method shows similar trends in improvement
even when the model is fine-tuned using a lower proportion of the original
task-specific labeled training data for zero-shot transfer.
","[{'version': 'v1', 'created': 'Thu, 30 Jun 2022 04:22:27 GMT'}]",2022-07-01,['Computation and Language'],"This paper explores the potential of multilingual few-shot transfer in the context of computation and language. It is argued that diversity and uncertainty are key factors in the selection of data for such transfer learning tasks. The paper outlines the benefits of incorporating diverse datasets in the training process, and examines the implications of incorporating uncertainty in the selection of data. The paper also considers the challenges associated with using diverse and uncertain data for few-shot transfer tasks, and provides suggestions for overcoming these challenges. The paper concludes by emphasizing the importance of diversity and uncertainty in data selection for multilingual few-shot transfer tasks.","Write an abstract for a paper called ""Diversity and Uncertainty in Moderation"" are the Key to Data Selection
  for Multilingual Few-shot Transfer about Computation and Language"
2301.05764,"Merim Dzaferagic, Jose A. Ayala-Romero and Marco Ruffini","ML Approach for Power Consumption Prediction in Virtualized Base
  Stations",['cs.LG'],"  The flexibility introduced with the Open Radio Access Network (O-RAN)
architecture allows us to think beyond static configurations in all parts of
the network. This paper addresses the issue related to predicting the power
consumption of different radio schedulers, and the potential offered by O-RAN
to collect data, train models, and deploy policies to control the power
consumption. We propose a black-box (Neural Network) model to learn the power
consumption function. We compare our approach with a known hand-crafted
solution based on domain knowledge. Our solution reaches similar performance
without any previous knowledge of the application and provides more flexibility
in scenarios where the system behavior is not well understood or the domain
knowledge is not available.
","[{'version': 'v1', 'created': 'Fri, 13 Jan 2023 21:25:22 GMT'}]",2023-01-18,['Machine Learning'],"This paper presents a machine learning (ML) approach for predicting power consumption in virtualized base stations. The proposed approach uses a combination of supervised and unsupervised learning algorithms to analyze the historical power consumption data of base stations. It then uses the results of the analysis to build an ML model that accurately predicts power consumption in future scenarios. Further, the paper discusses the advantages of the proposed approach over traditional methods, including improved accuracy and scalability. Finally, the paper presents the results of experiments conducted on a real-world dataset of power consumption data from base stations. The results demonstrate the effectiveness of the proposed approach in predicting power consumption in virtualized base stations.","Write an abstract for a paper called ML Approach for Power Consumption Prediction in Virtualized Base
  Stations about Machine Learning"
2211.02619,"Mansooreh Montazerin, Elahe Rahimian, Farnoosh Naderkhani, S. Farokh
  Atashzar, Hamid Alinejad-Rokny, Arash Mohammadi","HYDRA-HGR: A Hybrid Transformer-based Architecture for Fusion of
  Macroscopic and Microscopic Neural Drive Information","['eess.SP', 'cs.CV', 'cs.LG']","  Development of advance surface Electromyogram (sEMG)-based Human-Machine
Interface (HMI) systems is of paramount importance to pave the way towards
emergence of futuristic Cyber-Physical-Human (CPH) worlds. In this context, the
main focus of recent literature was on development of different Deep Neural
Network (DNN)-based architectures that perform Hand Gesture Recognition (HGR)
at a macroscopic level (i.e., directly from sEMG signals). At the same time,
advancements in acquisition of High-Density sEMG signals (HD-sEMG) have
resulted in a surge of significant interest on sEMG decomposition techniques to
extract microscopic neural drive information. However, due to complexities of
sEMG decomposition and added computational overhead, HGR at microscopic level
is less explored than its aforementioned DNN-based counterparts. In this
regard, we propose the HYDRA-HGR framework, which is a hybrid model that
simultaneously extracts a set of temporal and spatial features through its two
independent Vision Transformer (ViT)-based parallel architectures (the so
called Macro and Micro paths). The Macro Path is trained directly on the
pre-processed HD-sEMG signals, while the Micro path is fed with the p-to-p
values of the extracted Motor Unit Action Potentials (MUAPs) of each source.
Extracted features at macroscopic and microscopic levels are then coupled via a
Fully Connected (FC) fusion layer. We evaluate the proposed hybrid HYDRA-HGR
framework through a recently released HD-sEMG dataset, and show that it
significantly outperforms its stand-alone counterparts. The proposed HYDRA-HGR
framework achieves average accuracy of 94.86% for the 250 ms window size, which
is 5.52% and 8.22% higher than that of the Macro and Micro paths, respectively.
","[{'version': 'v1', 'created': 'Thu, 27 Oct 2022 02:23:27 GMT'}]",2022-11-07,"['Computer Vision and Pattern Recognition', 'Machine Learning']","This paper presents HYDRA-HGR, a novel hybrid transformer-based architecture for fusion of macroscopic and microscopic neural drive information about computer vision and pattern recognition, machine learning. The proposed architecture combines the advantages of the transformer-based approach and the recurrent neural network (RNN) approach to address the challenges of large-scale datasets. The architecture consists of two main components, a convolutional neural network (CNN) for feature extraction and a transformer-based encoder-decoder for sequence processing. The CNN extracts macroscopic information from the input data, while the transformer-based encoder-decoder extracts microscopic information from the sequence. The extracted information is then fused together to form a comprehensive representation of the input data. The proposed architecture is evaluated on several benchmark datasets and the results demonstrate its effectiveness in terms of accuracy, speed and robustness.","Write an abstract for a paper called HYDRA-HGR: A Hybrid Transformer-based Architecture for Fusion of
  Macroscopic and Microscopic Neural Drive Information about Computer Vision and Pattern Recognition, Machine Learning"
2210.11139,"Fernando Alonso-Fernandez, Julian Fierrez-Aguilar, Javier
  Ortega-Garcia","Sensor interoperability and fusion in signature verification: A case
  study using tablet PC",['cs.CV'],"  Several works related to information fusion for signature verification have
been presented. However, few works have focused on sensor fusion and sensor
interoperability. In this paper, these two topics are evaluated for signature
verification using two different commercial Tablet PCs. An enrolment strategy
using signatures from the two Tablet PCs is also proposed. Authentication
performance experiments are reported by using a database with over 3000
signatures.
","[{'version': 'v1', 'created': 'Thu, 20 Oct 2022 10:06:36 GMT'}]",2022-10-21,['Computer Vision and Pattern Recognition'],"This paper presents a case study of sensor interoperability and fusion in signature verification using a tablet PC. The paper focuses on the application of computer vision and pattern recognition techniques to improve the accuracy of signature verification. The paper begins by exploring the use of multiple sensors for capturing the signature and their fusion for greater accuracy. A tablet PC is used as the platform for the case study, and the paper discusses the challenges of using multiple sensors and the methods used to overcome these challenges. The paper then presents the results of the case study and discusses the implications of the findings for signature verification. Finally, the paper provides an analysis of the results and suggests future directions for research in this area.","Write an abstract for a paper called Sensor interoperability and fusion in signature verification: A case
  study using tablet PC about Computer Vision and Pattern Recognition"
2212.09597,"Shuofei Qiao, Yixin Ou, Ningyu Zhang, Xiang Chen, Yunzhi Yao, Shumin
  Deng, Chuanqi Tan, Fei Huang, Huajun Chen",Reasoning with Language Model Prompting: A Survey,"['cs.CL', 'cs.AI', 'cs.CV', 'cs.IR', 'cs.LG']","  Reasoning, as an essential ability for complex problem-solving, can provide
back-end support for various real-world applications, such as medical
diagnosis, negotiation, etc. This paper provides a comprehensive survey of
cutting-edge research on reasoning with language model prompting. We introduce
research works with comparisons and summaries and provide systematic resources
to help beginners. We also discuss the potential reasons for emerging such
reasoning abilities and highlight future research directions.
","[{'version': 'v1', 'created': 'Mon, 19 Dec 2022 16:32:42 GMT'}]",2022-12-20,"['Computation and Language', 'Artificial Intelligence', 'Computer Vision and Pattern Recognition', 'Information Retrieval', 'Machine Learning']","This paper presents a survey of the current state of research in the field of reasoning with language model prompting. The survey focuses on the application of this technology in the fields of computation and language, artificial intelligence, computer vision and pattern recognition, information retrieval, and machine learning. The survey covers the main techniques used, such as natural language processing, neural networks, and deep learning, as well as the various applications of language model prompting in the aforementioned fields. Finally, the survey discusses the challenges and opportunities that exist in the field of reasoning with language model prompting. The paper provides an overview of the current research and discusses the potential of this technology for further development.","Write an abstract for a paper called Reasoning with Language Model Prompting: A Survey about Computation and Language, Artificial Intelligence, Computer Vision and Pattern Recognition, Information Retrieval, Machine Learning"
2211.10881,"Tianyi Wang and Xin Liao and Kam Pui Chow and Xiaodong Lin and
  Yinglong Wang","Deepfake Detection: A Comprehensive Study from the Reliability
  Perspective","['cs.CV', 'cs.MM']","  The mushroomed Deepfake synthetic materials circulated on the internet have
raised serious social impact to politicians, celebrities, and every human being
on earth. In this survey, we provide a thorough review of the existing Deepfake
detection studies from the reliability perspective. Reliability-oriented
research challenges of the current Deepfake detection research domain are
defined in three aspects, namely, transferability, interpretability, and
robustness. While solutions have been frequently addressed regarding the three
challenges, the general reliability of a detection model has been barely
considered, leading to the lack of reliable evidence in real-life usages and
even for prosecutions on Deepfake-related cases in court. We, therefore,
introduce a model reliability study metric using statistical random sampling
knowledge and the publicly available benchmark datasets to review the
reliability of the existing detection models on arbitrary Deepfake candidate
suspects. Case studies are further executed to justify the real-life Deepfake
cases including different groups of victims with the help of the reliably
qualified detection models as reviewed in this survey. Reviews and experiments
upon the existing approaches provide informative discussions and future
research directions of Deepfake detection.
","[{'version': 'v1', 'created': 'Sun, 20 Nov 2022 06:31:23 GMT'}, {'version': 'v2', 'created': 'Wed, 15 Feb 2023 01:41:47 GMT'}]",2023-02-16,"['Computer Vision and Pattern Recognition', 'Multimedia']","Analysis and Machine Learning

This paper presents a comprehensive study on deepfake detection from the reliability perspective. It focuses on three main areas: computer vision and pattern recognition, multimedia analysis and machine learning. It discusses various approaches to deepfake detection and their advantages and disadvantages. It also provides an overview of the current state of deepfake detection and its challenges. Finally, the paper concludes with a discussion on the reliability of deepfake detection and its implications for the future of multimedia analysis and machine learning.","Write an abstract for a paper called Deepfake Detection: A Comprehensive Study from the Reliability
  Perspective about Computer Vision and Pattern Recognition, Multimedia"
1703.08748,Lifeng Han,LEPOR: An Augmented Machine Translation Evaluation Metric,['cs.CL'],"  Machine translation (MT) was developed as one of the hottest research topics
in the natural language processing (NLP) literature. One important issue in MT
is that how to evaluate the MT system reasonably and tell us whether the
translation system makes an improvement or not. The traditional manual judgment
methods are expensive, time-consuming, unrepeatable, and sometimes with low
agreement. On the other hand, the popular automatic MT evaluation methods have
some weaknesses. Firstly, they tend to perform well on the language pairs with
English as the target language, but weak when English is used as source.
Secondly, some methods rely on many additional linguistic features to achieve
good performance, which makes the metric unable to replicate and apply to other
language pairs easily. Thirdly, some popular metrics utilize incomprehensive
factors, which result in low performance on some practical tasks. In this
thesis, to address the existing problems, we design novel MT evaluation methods
and investigate their performances on different languages. Firstly, we design
augmented factors to yield highly accurate evaluation. Secondly, we design a
tunable evaluation model where weighting of factors can be optimized according
to the characteristics of languages. Thirdly, in the enhanced version of our
methods, we design concise linguistic feature using part-of-speech (POS) to
show that our methods can yield even higher performance when using some
external linguistic resources. Finally, we introduce the practical performance
of our metrics in the ACL-WMT workshop shared tasks, which show that the
proposed methods are robust across different languages. In addition, we also
present some novel work on quality estimation of MT without using reference
translations including the usage of probability models of Na\""ive Bayes (NB),
support vector machine (SVM) classification algorithms, and CRFs.
","[{'version': 'v1', 'created': 'Sun, 26 Mar 2017 00:30:38 GMT'}, {'version': 'v2', 'created': 'Mon, 24 Jan 2022 17:11:08 GMT'}]",2022-01-25,['Computation and Language'],"This paper presents LEPOR, an augmented machine translation evaluation metric that combines both computation and language in order to accurately assess the quality of machine translation. LEPOR is based on a novel combination of language models and computational measures, which has been shown to outperform existing metrics in terms of predictive accuracy. The paper further explores the potential of LEPOR to be used as a reliable metric for evaluating machine translation systems. Results from experiments conducted on a variety of datasets demonstrate that LEPOR is able to accurately measure the quality of machine translation, and can be used to compare the performance of different systems. The paper concludes with a discussion of the potential applications of LEPOR for machine translation evaluation and future research directions.",Write an abstract for a paper called LEPOR: An Augmented Machine Translation Evaluation Metric about Computation and Language
2302.05191,Eranda \c{C}ela and Vladimir Deineko and Gerhard J. Woeginger,Recognising permuted Demidenko matrices,['cs.DM'],"  We solve the recognition problem (RP) for the class of Demidenko matrices.
Our result closes a remarkable gap in the recognition of specially structured
matrices. Indeed, the recognition of permuted Demidenko matrices is a
longstanding open problem, in contrast to the effciently solved RP for
important subclasses of Demidenko matrices such as the Kalmanson matrices, the
Supnick matrices, the Monge matrices and the Anti-Robinson matrices. The
recognition of the permuted Demidenko matrices is relevant in the context of
hard combinatorial optimization problems which become tractable if the input is
a Demidenko matrix. Demidenko matrices were introduced by Demidenko in 1976,
when he proved that the Travelling Salesman Problem (TSP) is polynomially
solvable if the symmetric distance matrix fulfills certain combinatorial
conditions, nowadays known as the Demidenko conditions. In the context of the
TSP the recognition problem consists in deciding whether there is a renumbering
of the cities such that the correspondingly renumbered distance matrix fulfills
the Demidenko conditions, thus resulting in a polynomially solvable special
case of the TSP. We show that such a renumbering of n cities can be found in
$O(n^4)$ time, if it exists.
","[{'version': 'v1', 'created': 'Fri, 10 Feb 2023 11:43:34 GMT'}]",2023-02-13,['Discrete Mathematics'],"This paper examines the recognition of permuted Demidenko matrices in discrete mathematics. It begins by introducing the concept of permuted Demidenko matrices and how they are used to represent certain types of mathematical objects. It then goes on to discuss the various algorithms used to recognize these matrices and the challenges associated with them. Finally, it presents a new algorithm for recognizing permuted Demidenko matrices and its results. The paper concludes by discussing the implications of the new algorithm for the field of discrete mathematics.",Write an abstract for a paper called Recognising permuted Demidenko matrices about Discrete Mathematics
2303.00616,"Islam Ali, Bingqing (Selina) Wan, Hong Zhang","Prediction of SLAM ATE Using an Ensemble Learning Regression Model and
  1-D Global Pooling of Data Characterization","['cs.RO', 'cs.CV']","  Robustness and resilience of simultaneous localization and mapping (SLAM) are
critical requirements for modern autonomous robotic systems. One of the
essential steps to achieve robustness and resilience is the ability of SLAM to
have an integrity measure for its localization estimates, and thus, have
internal fault tolerance mechanisms to deal with performance degradation. In
this work, we introduce a novel method for predicting SLAM localization error
based on the characterization of raw sensor inputs. The proposed method relies
on using a random forest regression model trained on 1-D global pooled features
that are generated from characterized raw sensor data. The model is validated
by using it to predict the performance of ORB-SLAM3 on three different datasets
running on four different operating modes, resulting in an average prediction
accuracy of up to 94.7\%. The paper also studies the impact of 12 different 1-D
global pooling functions on regression quality, and the superiority of 1-D
global averaging is quantitatively proven. Finally, the paper studies the
quality of prediction with limited training data, and proves that we are able
to maintain proper prediction quality when only 20 \% of the training examples
are used for training, which highlights how the proposed model can optimize the
evaluation footprint of SLAM systems.
","[{'version': 'v1', 'created': 'Wed, 1 Mar 2023 16:12:47 GMT'}]",2023-03-02,"['Robotics', 'Computer Vision and Pattern Recognition']","This paper proposes an ensemble learning regression model for the prediction of Simultaneous Localization and Mapping (SLAM) Accuracy Test Evaluation (ATE) using 1-D global pooling of data characterization about robotics, computer vision and pattern recognition. The proposed model is a combination of three different regression models, namely, Random Forest, Support Vector Machine and Multi-Layer Perceptron. The global pooling layer is used to extract the most relevant characteristics of the data and to reduce the dimensionality of the input data. The experimental results show that the proposed model is able to accurately predict the SLAM ATE with a mean absolute error of 0.01 and a mean squared error of 0.001. The results demonstrate the effectiveness of the proposed method in accurately predicting SLAM ATE.","Write an abstract for a paper called Prediction of SLAM ATE Using an Ensemble Learning Regression Model and
  1-D Global Pooling of Data Characterization about Robotics, Computer Vision and Pattern Recognition"
2109.04738,"Julian von der Mosel, Alexander Trautsch, Steffen Herbold","On the validity of pre-trained transformers for natural language
  processing in the software engineering domain","['cs.SE', 'cs.LG']","  Transformers are the current state-of-the-art of natural language processing
in many domains and are using traction within software engineering research as
well. Such models are pre-trained on large amounts of data, usually from the
general domain. However, we only have a limited understanding regarding the
validity of transformers within the software engineering domain, i.e., how good
such models are at understanding words and sentences within a software
engineering context and how this improves the state-of-the-art. Within this
article, we shed light on this complex, but crucial issue. We compare BERT
transformer models trained with software engineering data with transformers
based on general domain data in multiple dimensions: their vocabulary, their
ability to understand which words are missing, and their performance in
classification tasks. Our results show that for tasks that require
understanding of the software engineering context, pre-training with software
engineering data is valuable, while general domain models are sufficient for
general language understanding, also within the software engineering domain.
","[{'version': 'v1', 'created': 'Fri, 10 Sep 2021 08:46:31 GMT'}, {'version': 'v2', 'created': 'Thu, 12 May 2022 18:30:24 GMT'}]",2022-05-16,"['Software Engineering', 'Machine Learning']","This paper presents a study on the validity of pre-trained transformers for natural language processing (NLP) in the software engineering domain. The study focuses on the evaluation of pre-trained transformer models, such as BERT, GPT-2, and XLNet, on software engineering tasks, such as source code classification, bug triaging, and code summarization. The results of the study show that pre-trained transformer models can be used for software engineering tasks with a high degree of accuracy. Furthermore, the study provides insights into the challenges and opportunities of using pre-trained transformer models for software engineering tasks. The findings of this study can help inform the development of NLP-based software engineering tools and systems.","Write an abstract for a paper called On the validity of pre-trained transformers for natural language
  processing in the software engineering domain about Software Engineering, Machine Learning"
2102.07952,"Nan Wu, Yuan Xie",A Survey of Machine Learning for Computer Architecture and Systems,"['cs.LG', 'cs.AR']","  It has been a long time that computer architecture and systems are optimized
for efficient execution of machine learning (ML) models. Now, it is time to
reconsider the relationship between ML and systems, and let ML transform the
way that computer architecture and systems are designed. This embraces a
twofold meaning: improvement of designers' productivity, and completion of the
virtuous cycle. In this paper, we present a comprehensive review of the work
that applies ML for computer architecture and system design. First, we perform
a high-level taxonomy by considering the typical role that ML techniques take
in architecture/system design, i.e., either for fast predictive modeling or as
the design methodology. Then, we summarize the common problems in computer
architecture/system design that can be solved by ML techniques, and the typical
ML techniques employed to resolve each of them. In addition to emphasis on
computer architecture in a narrow sense, we adopt the concept that data centers
can be recognized as warehouse-scale computers; sketchy discussions are
provided in adjacent computer systems, such as code generation and compiler; we
also give attention to how ML techniques can aid and transform design
automation. We further provide a future vision of opportunities and potential
directions, and envision that applying ML for computer architecture and systems
would thrive in the community.
","[{'version': 'v1', 'created': 'Tue, 16 Feb 2021 04:09:57 GMT'}, {'version': 'v2', 'created': 'Sat, 6 Nov 2021 21:17:07 GMT'}]",2022-02-25,"['Machine Learning', 'Hardware Architecture']","and Systems

This paper presents a survey of machine learning techniques used in computer architecture and systems. The paper reviews various machine learning models, such as deep learning, reinforcement learning, and evolutionary algorithms, and discusses their use in hardware architecture and systems. The paper also discusses the challenges and opportunities associated with using machine learning in computer architecture and systems. Finally, the paper provides an overview of the current state of machine learning in computer architecture and systems, and highlights potential future research directions.","Write an abstract for a paper called A Survey of Machine Learning for Computer Architecture and Systems about Machine Learning, Hardware Architecture"
2203.14452,"Vladimir Vargas-Calder\'on, Fabio A. Gonz\'alez, and Herbert
  Vinck-Posada","Optimisation-free Classification and Density Estimation with Quantum
  Circuits","['quant-ph', 'cs.LG']","  We demonstrate the implementation of a novel machine learning framework for
probability density estimation and classification using quantum circuits. The
framework maps a training data set or a single data sample to the quantum state
of a physical system through quantum feature maps. The quantum state of the
arbitrarily large training data set summarises its probability distribution in
a finite-dimensional quantum wave function. By projecting the quantum state of
a new data sample onto the quantum state of the training data set, one can
derive statistics to classify or estimate the density of the new data sample.
Remarkably, the implementation of our framework on a real quantum device does
not require any optimisation of quantum circuit parameters. Nonetheless, we
discuss a variational quantum circuit approach that could leverage quantum
advantage for our framework.
","[{'version': 'v1', 'created': 'Mon, 28 Mar 2022 02:40:24 GMT'}, {'version': 'v2', 'created': 'Thu, 31 Mar 2022 12:50:30 GMT'}, {'version': 'v3', 'created': 'Mon, 23 May 2022 01:32:01 GMT'}]",2022-06-28,['Machine Learning'],"This paper presents a novel approach to machine learning, using quantum circuits to classify data and estimate its density without the need for optimisation. We introduce a new quantum circuit design that uses a combination of quantum gates to classify data without the need for optimisation. We also introduce a quantum circuit design for density estimation, which uses a combination of quantum gates to estimate the density of data without the need for optimisation. We demonstrate our approach on several benchmark datasets and show that our approach outperforms existing methods for both classification and density estimation. Finally, we discuss the implications of our approach for machine learning and its potential applications.","Write an abstract for a paper called Optimisation-free Classification and Density Estimation with Quantum
  Circuits about Machine Learning"
2212.01763,"Dafa Ren, Shuang Wu, Xiaofan Wang, Yan Peng, Xiaoqiang Ren","Learning Bifunctional Push-grasping Synergistic Strategy for
  Goal-agnostic and Goal-oriented Tasks","['cs.RO', 'cs.SY', 'eess.SY']","  Both goal-agnostic and goal-oriented tasks have practical value for robotic
grasping: goal-agnostic tasks target all objects in the workspace, while
goal-oriented tasks aim at grasping pre-assigned goal objects. However, most
current grasping methods are only better at coping with one task. In this work,
we propose a bifunctional push-grasping synergistic strategy for goal-agnostic
and goal-oriented grasping tasks. Our method integrates pushing along with
grasping to pick up all objects or pre-assigned goal objects with high action
efficiency depending on the task requirement. We introduce a bifunctional
network, which takes in visual observations and outputs dense pixel-wise maps
of Q values for pushing and grasping primitive actions, to increase the
available samples in the action space. Then we propose a hierarchical
reinforcement learning framework to coordinate the two tasks by considering the
goal-agnostic task as a combination of multiple goal-oriented tasks. To reduce
the training difficulty of the hierarchical framework, we design a two-stage
training method to train the two types of tasks separately. We perform
pre-training of the model in simulation, and then transfer the learned model to
the real world without any additional real-world fine-tuning. Experimental
results show that the proposed approach outperforms existing methods in task
completion rate and grasp success rate with less motion number. Supplementary
material is available at https:
//github.com/DafaRen/Learning_Bifunctional_Push-grasping_Synergistic_Strategy_for_Goal-agnostic_and_Goal-oriented_Tasks
","[{'version': 'v1', 'created': 'Sun, 4 Dec 2022 08:16:04 GMT'}]",2022-12-06,"['Robotics', 'Systems and Control']",This paper presents a novel bifunctional push-grasping synergistic strategy for goal-agnostic and goal-oriented robotic tasks. The proposed strategy combines a push-grasping synergistic algorithm with a learning-based approach to enable a robot to perform both goal-agnostic and goal-oriented tasks. The proposed strategy is evaluated on a simulated robotic arm and the results show the effectiveness of the proposed strategy for both goal-agnostic and goal-oriented tasks. The paper also provides insights into the design of the push-grasping synergistic strategy and its application in robotic systems and control.,"Write an abstract for a paper called Learning Bifunctional Push-grasping Synergistic Strategy for
  Goal-agnostic and Goal-oriented Tasks about Robotics, Systems and Control"
2103.07382,"Antonios Kamariotis, Eleni Chatzi, Daniel Straub","Value of information from vibration-based structural health monitoring
  extracted via Bayesian model updating","['stat.AP', 'cs.SY', 'eess.SY']","  Quantifying the value of the information extracted from a structural health
monitoring (SHM) system is an important step towards convincing decision makers
to implement these systems. We quantify this value by adaptation of the
Bayesian decision analysis framework. In contrast to previous works, we model
in detail the entire process of data generation to processing, model updating
and reliability calculation, and investigate it on a deteriorating bridge
system. The framework assumes that dynamic response data are obtained in a
sequential fashion from deployed accelerometers, subsequently processed by an
output-only operational modal analysis scheme for identifying the system's
modal characteristics. We employ a classical Bayesian model updating
methodology to sequentially learn the deterioration and estimate the structural
damage evolution over time. This leads to sequential updating of the structural
reliability, which constitutes the basis for a preposterior Bayesian decision
analysis. Alternative actions are defined and a heuristic-based approach is
employed for the life-cycle optimization. By solving the preposterior Bayesian
decision analysis, one is able to quantify the benefit of the availability of
long-term SHM vibrational data. Numerical investigations show that this
framework can provide quantitative measures on the optimality of an SHM system
in a specific decision context.
","[{'version': 'v1', 'created': 'Fri, 12 Mar 2021 16:23:57 GMT'}]",2022-10-06,['Systems and Control'],"This paper presents a Bayesian model updating approach to extract information from vibration-based structural health monitoring (SHM) systems. The proposed method uses Bayesian inference to combine prior knowledge of a structure's dynamic behavior with measured vibration data to update the model. This approach is applied to a variety of structures, including a cantilever beam and an offshore platform, to demonstrate its efficacy. Results show that the Bayesian model updating approach can accurately and reliably update a model of the structure's dynamic behavior, and can provide valuable information about the structure's current state and future performance. Furthermore, the approach can be used to identify potential sources of structural damage and to assess the effects of control inputs on the structure. The paper highlights the value of the proposed approach for systems and control applications.","Write an abstract for a paper called Value of information from vibration-based structural health monitoring
  extracted via Bayesian model updating about Systems and Control"
2012.1077,"Sigrid Passano Hellan, Christopher G. Lucas and Nigel H. Goddard",Optimising Placement of Pollution Sensors in Windy Environments,"['cs.LG', 'physics.ao-ph']","  Air pollution is one of the most important causes of mortality in the world.
Monitoring air pollution is useful to learn more about the link between health
and pollutants, and to identify areas for intervention. Such monitoring is
expensive, so it is important to place sensors as efficiently as possible.
Bayesian optimisation has proven useful in choosing sensor locations, but
typically relies on kernel functions that neglect the statistical structure of
air pollution, such as the tendency of pollution to propagate in the prevailing
wind direction. We describe two new wind-informed kernels and investigate their
advantage for the task of actively learning locations of maximum pollution
using Bayesian optimisation.
","[{'version': 'v1', 'created': 'Sat, 19 Dec 2020 20:16:49 GMT'}, {'version': 'v2', 'created': 'Sun, 28 Aug 2022 17:09:02 GMT'}]",2022-08-30,['Machine Learning'],"This paper presents a machine learning-based approach to optimising the placement of pollution sensors in windy environments. It begins by discussing the challenges associated with such an endeavour, including the need to accurately predict the movement of air pollution and the need to consider the cost of installing and maintaining the sensors. The paper then proposes a method for optimising the placement of sensors using a combination of supervised and unsupervised machine learning algorithms. The proposed approach is evaluated on a real-world dataset and the results show that it can significantly improve the accuracy of air pollution predictions. Finally, the paper concludes with a discussion of the implications of this work and potential future directions.",Write an abstract for a paper called Optimising Placement of Pollution Sensors in Windy Environments about Machine Learning
2209.14501,"Yizhou Liu, Weijie J. Su, Tongyang Li","On Quantum Speedups for Nonconvex Optimization via Quantum Tunneling
  Walks","['quant-ph', 'cs.DS', 'cs.LG', 'math.OC']","  Classical algorithms are often not effective for solving nonconvex
optimization problems where local minima are separated by high barriers. In
this paper, we explore possible quantum speedups for nonconvex optimization by
leveraging the global effect of quantum tunneling. Specifically, we introduce a
quantum algorithm termed the quantum tunneling walk (QTW) and apply it to
nonconvex problems where local minima are approximately global minima. We show
that QTW achieves quantum speedup over classical stochastic gradient descents
(SGD) when the barriers between different local minima are high but thin and
the minima are flat. Based on this observation, we construct a specific
double-well landscape, where classical algorithms cannot efficiently hit one
target well knowing the other well but QTW can when given proper initial states
near the known well. Finally, we corroborate our findings with numerical
experiments.
","[{'version': 'v1', 'created': 'Thu, 29 Sep 2022 01:39:20 GMT'}]",2022-09-30,"['Data Structures and Algorithms', 'Machine Learning']","This paper examines the potential for quantum speedups for nonconvex optimization via quantum tunneling walks. It will discuss the application of quantum tunneling walks to data structures and algorithms, as well as to machine learning. It will analyze the advantages and disadvantages of quantum tunneling walks for nonconvex optimization and compare them to classical optimization techniques. Additionally, it will discuss the implications of quantum tunneling walks for data structures and algorithms, as well as machine learning. Finally, it will discuss potential future research directions in the area of quantum speedups for nonconvex optimization.","Write an abstract for a paper called On Quantum Speedups for Nonconvex Optimization via Quantum Tunneling
  Walks about Data Structures and Algorithms, Machine Learning"
2008.11668,"Anurag Chowdhury, Arun Ross","DeepVOX: Discovering Features from Raw Audio for Speaker Recognition in
  Non-ideal Audio Signals","['eess.AS', 'cs.LG']","  Automatic speaker recognition algorithms typically use pre-defined
filterbanks, such as Mel-Frequency and Gammatone filterbanks, for
characterizing speech audio. However, it has been observed that the features
extracted using these filterbanks are not resilient to diverse audio
degradations. In this work, we propose a deep learning-based technique to
deduce the filterbank design from vast amounts of speech audio. The purpose of
such a filterbank is to extract features robust to non-ideal audio conditions,
such as degraded, short duration, and multi-lingual speech. To this effect, a
1D convolutional neural network is designed to learn a time-domain filterbank
called DeepVOX directly from raw speech audio. Secondly, an adaptive triplet
mining technique is developed to efficiently mine the data samples best suited
to train the filterbank. Thirdly, a detailed ablation study of the DeepVOX
filterbanks reveals the presence of both vocal source and vocal tract
characteristics in the extracted features. Experimental results on VOXCeleb2,
NIST SRE 2008, 2010 and 2018, and Fisher speech datasets demonstrate the
efficacy of the DeepVOX features across a variety of degraded, short duration,
and multi-lingual speech. The DeepVOX features also shown to improve the
performance of existing speaker recognition algorithms, such as the
xVector-PLDA and the iVector-PLDA.
","[{'version': 'v1', 'created': 'Wed, 26 Aug 2020 16:50:26 GMT'}, {'version': 'v2', 'created': 'Mon, 13 Jun 2022 03:39:05 GMT'}]",2022-06-14,['Machine Learning'],"This paper presents DeepVOX, a novel deep learning approach to speaker recognition in non-ideal audio signals. DeepVOX is a deep learning-based feature extraction method that utilizes raw audio signals to discover speaker-specific features. Through experiments on a variety of audio datasets, we demonstrate that DeepVOX is able to effectively capture speaker-specific information from raw audio signals, even in cases of non-ideal audio signals with background noise and reverberation. We also show that DeepVOX outperforms existing feature extraction methods in terms of accuracy and robustness. Our results demonstrate the potential of DeepVOX for improving speaker recognition accuracy in non-ideal audio signals.","Write an abstract for a paper called DeepVOX: Discovering Features from Raw Audio for Speaker Recognition in
  Non-ideal Audio Signals about Machine Learning"
2207.07996,"Matheus V.X. Ferreira and Ye Lin Sally Hahn and S. Matthew Weinberg
  and Catherine Yu","Optimal Strategic Mining Against Cryptographic Self-Selection in
  Proof-of-Stake","['cs.CR', 'econ.TH', 'math.OC']","  Cryptographic Self-Selection is a subroutine used to select a leader for
modern proof-of-stake consensus protocols, such as Algorand. In cryptographic
self-selection, each round $r$ has a seed $Q_r$. In round $r$, each account
owner is asked to digitally sign $Q_r$, hash their digital signature to produce
a credential, and then broadcast this credential to the entire network. A
publicly-known function scores each credential in a manner so that the
distribution of the lowest scoring credential is identical to the distribution
of stake owned by each account. The user who broadcasts the lowest-scoring
credential is the leader for round $r$, and their credential becomes the seed
$Q_{r+1}$. Such protocols leave open the possibility of a selfish-mining style
attack: a user who owns multiple accounts that each produce low-scoring
credentials in round $r$ can selectively choose which ones to broadcast in
order to influence the seed for round $r+1$. Indeed, the user can pre-compute
their credentials for round $r+1$ for each potential seed, and broadcast only
the credential (among those with a low enough score to be the leader) that
produces the most favorable seed.
  We consider an adversary who wishes to maximize the expected fraction of
rounds in which an account they own is the leader. We show such an adversary
always benefits from deviating from the intended protocol, regardless of the
fraction of the stake controlled. We characterize the optimal strategy; first
by proving the existence of optimal positive recurrent strategies whenever the
adversary owns last than $38\%$ of the stake. Then, we provide a Markov
Decision Process formulation to compute the optimal strategy.
","[{'version': 'v1', 'created': 'Sat, 16 Jul 2022 18:28:07 GMT'}]",2022-07-19,['Cryptography and Security'],"This paper examines the potential of strategic mining against cryptographic self-selection in proof-of-stake systems. Cryptographic self-selection in proof-of-stake systems has been shown to be a powerful tool for improving security, but it is vulnerable to strategic mining attacks. This paper outlines an optimal strategy for mining against cryptographic self-selection, and evaluates its effectiveness in practice. The paper also discusses the security implications of strategic mining, and the potential for countermeasures. The analysis presented in this paper provides valuable insights into the security of proof-of-stake systems, and will be of interest to those interested in cryptography and security.","Write an abstract for a paper called Optimal Strategic Mining Against Cryptographic Self-Selection in
  Proof-of-Stake about Cryptography and Security"
2209.04317,"Henrik Valter, Axel Karlsson and Miquel Peric\`as","Energy-Efficiency Evaluation of OpenMP Loop Transformations and Runtime
  Constructs",['cs.DC'],"  OpenMP is the de facto API for parallel programming in HPC applications.
These programs are often computed in data centers, where energy consumption is
a major issue. Whereas previous work has focused almost entirely on
performance, we here analyse aspects of OpenMP from an energy consumption
perspective. This analysis is accomplished by executing novel microbenchmarks
and common benchmark suites on data center nodes and measuring the energy
consumption. Three main aspects are analysed: directive-generated loop tiling
and unrolling, parallel for loops and explicit tasking, and the policy of
handling blocked threads. For loop tiling and unrolling, we find that tiling
can yield significant energy savings for some, mostly unoptimised programs,
while directive-generated unrolling provides very minor improvement in the best
case and degenerates performance majorly in the worst case. For the second
aspect, we find that parallel for loops yield better results than explicit
tasking loops in cases where both can be used. This becomes more prominent with
more fine-grained workloads. For the third, we find that significant energy
savings can be made by not descheduling waiting threads, but instead having
them spin, at the cost of a higher power consumption. We also analyse how the
choice of compiler affects the above questions by compiling programs with each
of ICC, Clang and GCC, and find that while neither is strictly better than the
others, they can produce very different results for the same compiled programs.
As a final step, we combine the findings of all results and suggest novel
compiler directives as well as general recommendations on how to reduce energy
consumption in OpenMP programs.
","[{'version': 'v1', 'created': 'Fri, 9 Sep 2022 14:19:21 GMT'}]",2022-09-12,"['Distributed, Parallel, and Cluster Computing']","This paper examines the energy-efficiency of OpenMP loop transformations and runtime constructs in distributed, parallel, and cluster computing. It evaluates the performance of the OpenMP loop transformations and runtime constructs in terms of energy efficiency and the impact on the execution time of the programs. The paper presents a comprehensive analysis of the energy-efficiency of OpenMP loop transformations and runtime constructs, and provides guidelines on how to select the most energy-efficient solution for a given application. The paper also discusses the impact of different architectures on the energy-efficiency of OpenMP loop transformations and runtime constructs. Finally, the paper presents a case study to demonstrate the effectiveness of the proposed approach.","Write an abstract for a paper called Energy-Efficiency Evaluation of OpenMP Loop Transformations and Runtime
  Constructs about Distributed, Parallel, and Cluster Computing"
2106.12891,"Anwesh Bhattacharya, Marios Mattheakis, Pavlos Protopapas",Encoding Involutory Invariances in Neural Networks,"['cs.LG', 'cs.AI', 'cs.NE']","  In certain situations, neural networks are trained upon data that obey
underlying symmetries. However, the predictions do not respect the symmetries
exactly unless embedded in the network structure. In this work, we introduce
architectures that embed a special kind of symmetry namely, invariance with
respect to involutory linear/affine transformations up to parity $p=\pm 1$. We
provide rigorous theorems to show that the proposed network ensures such an
invariance and present qualitative arguments for a special universal
approximation theorem. An adaption of our techniques to CNN tasks for datasets
with inherent horizontal/vertical reflection symmetry is demonstrated.
Extensive experiments indicate that the proposed model outperforms baseline
feed-forward and physics-informed neural networks while identically respecting
the underlying symmetry.
","[{'version': 'v1', 'created': 'Mon, 7 Jun 2021 16:07:15 GMT'}, {'version': 'v2', 'created': 'Tue, 26 Apr 2022 22:44:32 GMT'}]",2022-04-28,"['Machine Learning', 'Artificial Intelligence', 'Neural and Evolutionary Computing']","This paper presents a novel approach to encoding involutory invariances in neural networks for machine learning, artificial intelligence, neural and evolutionary computing. We propose a method of encoding involutory invariances using a combination of convolutional and recurrent neural networks. We demonstrate our approach on a variety of datasets and show that it can significantly improve the performance of neural networks. Furthermore, we discuss the implications of encoding involutory invariances in neural networks and provide insights into how this approach can be used to improve the accuracy of machine learning models. Finally, we discuss the potential applications of this approach in various areas such as autonomous driving, robotics and natural language processing.","Write an abstract for a paper called Encoding Involutory Invariances in Neural Networks about Machine Learning, Artificial Intelligence, Neural and Evolutionary Computing"
2303.15746,"Raul Astudillo, Zhiyuan Jerry Lin, Eytan Bakshy, Peter I. Frazier","qEUBO: A Decision-Theoretic Acquisition Function for Preferential
  Bayesian Optimization","['cs.LG', 'stat.ML']","  Preferential Bayesian optimization (PBO) is a framework for optimizing a
decision maker's latent utility function using preference feedback. This work
introduces the expected utility of the best option (qEUBO) as a novel
acquisition function for PBO. When the decision maker's responses are
noise-free, we show that qEUBO is one-step Bayes optimal and thus equivalent to
the popular knowledge gradient acquisition function. We also show that qEUBO
enjoys an additive constant approximation guarantee to the one-step
Bayes-optimal policy when the decision maker's responses are corrupted by
noise. We provide an extensive evaluation of qEUBO and demonstrate that it
outperforms the state-of-the-art acquisition functions for PBO across many
settings. Finally, we show that, under sufficient regularity conditions,
qEUBO's Bayesian simple regret converges to zero at a rate $o(1/n)$ as the
number of queries, $n$, goes to infinity. In contrast, we show that simple
regret under qEI, a popular acquisition function for standard BO often used for
PBO, can fail to converge to zero. Enjoying superior performance, simple
computation, and a grounded decision-theoretic justification, qEUBO is a
promising acquisition function for PBO.
","[{'version': 'v1', 'created': 'Tue, 28 Mar 2023 06:02:56 GMT'}]",2023-03-29,['Machine Learning'],"This paper proposes qEUBO, a decision-theoretic acquisition function for Preferential Bayesian Optimization (PBO) about Machine Learning (ML). qEUBO is a novel approach that combines decision theory with PBO to enable ML practitioners to identify the most beneficial ML models. It uses a preference-based utility function to quantify the expected utility of a model, and then uses a decision-theoretic approach to select the model with the highest expected utility. Experiments show that qEUBO outperforms state-of-the-art PBO methods in terms of both accuracy and speed. The paper also discusses the implications of qEUBO for ML practitioners, and provides insights into how decision theory can be used to improve PBO methods.","Write an abstract for a paper called qEUBO: A Decision-Theoretic Acquisition Function for Preferential
  Bayesian Optimization about Machine Learning"
2207.08664,"Marah Halawa, Olaf Hellwich, Pia Bideau",Action-based Contrastive Learning for Trajectory Prediction,['cs.CV'],"  Trajectory prediction is an essential task for successful human robot
interaction, such as in autonomous driving. In this work, we address the
problem of predicting future pedestrian trajectories in a first person view
setting with a moving camera. To that end, we propose a novel action-based
contrastive learning loss, that utilizes pedestrian action information to
improve the learned trajectory embeddings. The fundamental idea behind this new
loss is that trajectories of pedestrians performing the same action should be
closer to each other in the feature space than the trajectories of pedestrians
with significantly different actions. In other words, we argue that behavioral
information about pedestrian action influences their future trajectory.
Furthermore, we introduce a novel sampling strategy for trajectories that is
able to effectively increase negative and positive contrastive samples.
Additional synthetic trajectory samples are generated using a trained
Conditional Variational Autoencoder (CVAE), which is at the core of several
models developed for trajectory prediction. Results show that our proposed
contrastive framework employs contextual information about pedestrian behavior,
i.e. action, effectively, and it learns a better trajectory representation.
Thus, integrating the proposed contrastive framework within a trajectory
prediction model improves its results and outperforms state-of-the-art methods
on three trajectory prediction benchmarks [31, 32, 26].
","[{'version': 'v1', 'created': 'Mon, 18 Jul 2022 15:02:27 GMT'}]",2022-07-19,['Computer Vision and Pattern Recognition'],"This paper presents a novel action-based contrastive learning approach for trajectory prediction in the field of computer vision and pattern recognition. The proposed approach employs contrastive learning to optimize the model parameters for trajectory prediction. Specifically, the model is trained to distinguish between the true trajectory and a randomly sampled trajectory from a prior distribution. This allows the model to learn a representation of the true trajectory that is more robust to noisy observations. Experiments on two real-world datasets demonstrate the effectiveness of the proposed approach, yielding improved performance compared to existing methods. The results suggest that the proposed approach can be used to improve the accuracy of trajectory prediction for computer vision and pattern recognition tasks.",Write an abstract for a paper called Action-based Contrastive Learning for Trajectory Prediction about Computer Vision and Pattern Recognition
2203.15431,"Lester Phillip Violeta, Wen-Chin Huang, Tomoki Toda","Investigating Self-supervised Pretraining Frameworks for Pathological
  Speech Recognition","['cs.SD', 'eess.AS']","  We investigate the performance of self-supervised pretraining frameworks on
pathological speech datasets used for automatic speech recognition (ASR).
Modern end-to-end models require thousands of hours of data to train well, but
only a small number of pathological speech datasets are publicly available. A
proven solution to this problem is by first pretraining the model on a huge
number of healthy speech datasets and then fine-tuning it on the pathological
speech datasets. One new pretraining framework called self-supervised learning
(SSL) trains a network using only speech data, providing more flexibility in
training data requirements and allowing more speech data to be used in
pretraining. We investigate SSL frameworks such as the wav2vec 2.0 and WavLM
models using different setups and compare their performance with different
supervised pretraining setups, using two types of pathological speech, namely,
Japanese electrolaryngeal and English dysarthric. Our results show that
although SSL has shown success with minimally resourced healthy speech, we do
not find this to be the case with pathological speech. The best supervised
setup outperforms the best SSL setup by 13.9% character error rate in
electrolaryngeal speech and 16.8% word error rate in dysarthric speech.
","[{'version': 'v1', 'created': 'Tue, 29 Mar 2022 10:54:35 GMT'}, {'version': 'v2', 'created': 'Wed, 30 Mar 2022 01:21:21 GMT'}, {'version': 'v3', 'created': 'Wed, 29 Jun 2022 09:43:44 GMT'}]",2022-06-30,['Sound'],"This paper presents an investigation of self-supervised pretraining frameworks for pathological speech recognition about sound. Pathological speech recognition is a challenging problem due to the variability of speech patterns caused by various diseases and disorders. To address this issue, we propose a novel self-supervised pretraining framework that leverages unlabeled speech data to learn generalizable representations for pathological speech recognition. We evaluate our proposed framework on a variety of pathological speech datasets, including those from Parkinson's disease and cerebral palsy. Our results show that our proposed framework is able to improve recognition accuracy compared to baseline models. Additionally, we provide a comprehensive analysis of the impact of different pretraining strategies on the performance of our proposed framework. Our results suggest that self-supervised pretraining is a promising approach for pathological speech recognition and can be used to improve recognition accuracy in real-world applications.","Write an abstract for a paper called Investigating Self-supervised Pretraining Frameworks for Pathological
  Speech Recognition about Sound"
2302.10313,Georgios Ioannides and Vasilios Rallis,"Real-Time Speech Enhancement Using Spectral Subtraction with Minimum
  Statistics and Spectral Floor","['eess.AS', 'cs.SD', 'eess.SP']","  An initial real-time speech enhancement method is presented to reduce the
effects of additive noise. The method operates in the frequency domain and is a
form of spectral subtraction. Initially, minimum statistics are used to
generate an estimate of the noise signal in the frequency domain. The use of
minimum statistics avoids the need for a voice activity detector (VAD) which
has proven to be challenging to create. As minimum statistics are used, the
noise signal estimate must be multiplied by a scaling factor before subtraction
from the noise corrupted speech signal can take place. A spectral floor is
applied to the difference to suppress the effects of ""musical noise"". Finally,
a series of further enhancements are considered to reduce the effects of
residual noise even further. These methods are compared using time-frequency
plots to create the final speech enhancement design
","[{'version': 'v1', 'created': 'Mon, 20 Feb 2023 20:55:53 GMT'}]",2023-02-22,['Sound'],"Processing

This paper presents a real-time speech enhancement technique using spectral subtraction with minimum statistics and spectral floor. The technique is based on the minimum mean-squared error (MMSE) criterion and uses the spectral floor to improve the signal-to-noise ratio (SNR) of the speech signal. The performance of the proposed technique is evaluated using objective measures such as the signal-to-noise ratio, segmental SNR, and perceptual evaluation of speech quality. The results show that the proposed technique provides better speech enhancement compared to the conventional spectral subtraction technique. Additionally, the proposed technique is computationally efficient, making it suitable for real-time applications.","Write an abstract for a paper called Real-Time Speech Enhancement Using Spectral Subtraction with Minimum
  Statistics and Spectral Floor about Sound"
2203.0019,"Saul Calderon-Ramirez, Shengxiang Yang, David Elizondo","Semi-supervised Deep Learning for Image Classification with Distribution
  Mismatch: A Survey","['cs.CV', 'cs.AI']","  Deep learning methodologies have been employed in several different fields,
with an outstanding success in image recognition applications, such as material
quality control, medical imaging, autonomous driving, etc. Deep learning models
rely on the abundance of labelled observations to train a prospective model.
These models are composed of millions of parameters to estimate, increasing the
need of more training observations. Frequently it is expensive to gather
labelled observations of data, making the usage of deep learning models not
ideal, as the model might over-fit data. In a semi-supervised setting,
unlabelled data is used to improve the levels of accuracy and generalization of
a model with small labelled datasets. Nevertheless, in many situations
different unlabelled data sources might be available. This raises the risk of a
significant distribution mismatch between the labelled and unlabelled datasets.
Such phenomena can cause a considerable performance hit to typical
semi-supervised deep learning frameworks, which often assume that both labelled
and unlabelled datasets are drawn from similar distributions. Therefore, in
this paper we study the latest approaches for semi-supervised deep learning for
image recognition. Emphasis is made in semi-supervised deep learning models
designed to deal with a distribution mismatch between the labelled and
unlabelled datasets. We address open challenges with the aim to encourage the
community to tackle them, and overcome the high data demand of traditional deep
learning pipelines under real-world usage settings.
","[{'version': 'v1', 'created': 'Tue, 1 Mar 2022 02:46:00 GMT'}, {'version': 'v2', 'created': 'Wed, 9 Mar 2022 15:26:51 GMT'}, {'version': 'v3', 'created': 'Thu, 10 Mar 2022 15:54:29 GMT'}]",2022-03-11,"['Computer Vision and Pattern Recognition', 'Artificial Intelligence']","This paper provides a comprehensive overview of the use of semi-supervised deep learning for image classification in the presence of distribution mismatch. It begins by discussing the concept of distribution mismatch and its implications for computer vision and pattern recognition tasks. It then reviews the current literature on semi-supervised deep learning for image classification, highlighting the key challenges and potential solutions. Finally, it provides an overview of the state-of-the-art methods for semi-supervised deep learning for image classification with distribution mismatch, and outlines potential directions for future research. This paper will be useful for researchers and practitioners in the fields of computer vision, pattern recognition, and artificial intelligence.","Write an abstract for a paper called Semi-supervised Deep Learning for Image Classification with Distribution
  Mismatch: A Survey about Computer Vision and Pattern Recognition, Artificial Intelligence"
2302.12883,"Nicolai H\""ani, Jun-Jee Chao and Volkan Isler","3D Surface Reconstruction in the Wild by Deforming Shape Priors from
  Synthetic Data",['cs.CV'],"  Reconstructing the underlying 3D surface of an object from a single image is
a challenging problem that has received extensive attention from the computer
vision community. Many learning-based approaches tackle this problem by
learning a 3D shape prior from either ground truth 3D data or multi-view
observations. To achieve state-of-the-art results, these methods assume that
the objects are specified with respect to a fixed canonical coordinate frame,
where instances of the same category are perfectly aligned. In this work, we
present a new method for joint category-specific 3D reconstruction and object
pose estimation from a single image. We show that one can leverage shape priors
learned on purely synthetic 3D data together with a point cloud pose
canonicalization method to achieve high-quality 3D reconstruction in the wild.
Given a single depth image at test time, we first transform this partial point
cloud into a learned canonical frame. Then, we use a neural deformation field
to reconstruct the 3D surface of the object. Finally, we jointly optimize
object pose and 3D shape to fit the partial depth observation. Our approach
achieves state-of-the-art reconstruction performance across several real-world
datasets, even when trained only on synthetic data. We further show that our
method generalizes to different input modalities, from dense depth images to
sparse and noisy LIDAR scans.
","[{'version': 'v1', 'created': 'Fri, 24 Feb 2023 20:37:27 GMT'}]",2023-02-28,['Computer Vision and Pattern Recognition'],"This paper investigates the potential of 3D surface reconstruction from wild images by deforming shape priors from synthetic data. We present a novel approach for 3D surface reconstruction from wild images by deforming shape priors from synthetic data. Our method consists of two stages. First, a shape prior is generated from synthetic data using an unsupervised learning approach. Then, the shape prior is deformed using a deep learning-based 3D surface reconstruction network, which is trained on a large-scale dataset of wild images. The proposed approach is evaluated on both indoor and outdoor datasets, and the results show that our method can accurately reconstruct 3D surfaces from wild images. Furthermore, our method outperforms existing state-of-the-art methods in terms of 3D surface reconstruction accuracy. The results demonstrate the potential of our approach for 3D surface reconstruction in the wild.","Write an abstract for a paper called 3D Surface Reconstruction in the Wild by Deforming Shape Priors from
  Synthetic Data about Computer Vision and Pattern Recognition"
2209.09678,"Amin Ranem, John Kalkhof, Caner \""Ozer, Anirban Mukhopadhyay, Ilkay
  Oksuz","Detecting respiratory motion artefacts for cardiovascular MRIs to ensure
  high-quality segmentation","['eess.IV', 'cs.CV']","  While machine learning approaches perform well on their training domain, they
generally tend to fail in a real-world application. In cardiovascular magnetic
resonance imaging (CMR), respiratory motion represents a major challenge in
terms of acquisition quality and therefore subsequent analysis and final
diagnosis. We present a workflow which predicts a severity score for
respiratory motion in CMR for the CMRxMotion challenge 2022. This is an
important tool for technicians to immediately provide feedback on the CMR
quality during acquisition, as poor-quality images can directly be re-acquired
while the patient is still available in the vicinity. Thus, our method ensures
that the acquired CMR holds up to a specific quality standard before it is used
for further diagnosis. Therefore, it enables an efficient base for proper
diagnosis without having time and cost-intensive re-acquisitions in cases of
severe motion artefacts. Combined with our segmentation model, this can help
cardiologists and technicians in their daily routine by providing a complete
pipeline to guarantee proper quality assessment and genuine segmentations for
cardiovascular scans. The code base is available at
https://github.com/MECLabTUDA/QA_med_data/tree/dev_QA_CMRxMotion.
","[{'version': 'v1', 'created': 'Tue, 20 Sep 2022 12:29:05 GMT'}]",2022-09-21,['Computer Vision and Pattern Recognition'],This paper presents a novel approach to detecting respiratory motion artefacts in cardiovascular MRI images. The proposed method consists of two main components: a convolutional neural network (CNN) and a post-processing algorithm. The CNN is trained on a large dataset of cardiovascular MRI images to learn features related to the presence of respiratory motion artefacts. The post-processing algorithm is then used to refine the CNN predictions and obtain an accurate segmentation of the cardiovascular MRI images. Results demonstrate that the proposed approach is able to detect and remove respiratory motion artefacts with high accuracy and can be effectively used to improve the quality of cardiovascular MRI segmentation.,"Write an abstract for a paper called Detecting respiratory motion artefacts for cardiovascular MRIs to ensure
  high-quality segmentation about Computer Vision and Pattern Recognition"
2208.11449,"Karen Wintersperger, Hila Safi and Wolfgang Mauerer",QPU-System Co-Design for Quantum HPC Accelerators,"['cs.AR', 'quant-ph']","  The use of quantum processing units (QPUs) promises speed-ups for solving
computational problems, but the quantum devices currently available possess
only a very limited number of qubits and suffer from considerable
imperfections. One possibility to progress towards practical utility is to use
a co-design approach: Problem formulation and algorithm, but also the physical
QPU properties are tailored to the specific application. Since QPUs will likely
be used as accelerators for classical computers, details of systemic
integration into existing architectures are another lever to influence and
improve the practical utility of QPUs.
  In this work, we investigate the influence of different parameters on the
runtime of quantum programs on tailored hybrid CPU-QPU-systems. We study the
influence of communication times between CPU and QPU, how adapting QPU designs
influences quantum and overall execution performance, and how these factors
interact. Using a simple model that allows for estimating which design choices
should be subjected to optimisation for a given task, we provide an intuition
to the HPC community on potentials and limitations of co-design approaches. We
also discuss physical limitations for implementing the proposed changes on real
quantum hardware devices.
","[{'version': 'v1', 'created': 'Wed, 24 Aug 2022 11:33:48 GMT'}, {'version': 'v2', 'created': 'Mon, 5 Sep 2022 17:37:17 GMT'}, {'version': 'v3', 'created': 'Wed, 7 Sep 2022 16:41:09 GMT'}, {'version': 'v4', 'created': 'Thu, 8 Sep 2022 17:55:02 GMT'}]",2022-09-26,['Hardware Architecture'],"This paper presents a novel approach to quantum computing hardware architecture design, specifically for quantum high-performance computing (HPC) accelerators. We propose a co-design method for quantum processing unit (QPU) systems, combining hardware and software components. This approach enables the integration of multiple components within a single system, providing advantages such as improved scalability and cost efficiency. We discuss the implications of our approach, including energy efficiency, scalability, and performance. We then present a case study of a QPU-based HPC accelerator, demonstrating the potential of our approach. Finally, we discuss the implications of our work and suggest future research directions.",Write an abstract for a paper called QPU-System Co-Design for Quantum HPC Accelerators about Hardware Architecture
2107.07154,"Sangmin Woo, Junhyug Noh, Kangil Kim","What and When to Look?: Temporal Span Proposal Network for Video
  Relation Detection","['cs.CV', 'cs.AI']","  Identifying relations between objects is central to understanding the scene.
While several works have been proposed for relation modeling in the image
domain, there have been many constraints in the video domain due to challenging
dynamics of spatio-temporal interactions (e.g., between which objects are there
an interaction? when do relations start and end?). To date, two representative
methods have been proposed to tackle Video Visual Relation Detection (VidVRD):
segment-based and window-based. We first point out limitations of these methods
and propose a novel approach named Temporal Span Proposal Network (TSPN). TSPN
tells what to look: it sparsifies relation search space by scoring relationness
of object pair, i.e., measuring how probable a relation exist. TSPN tells when
to look: it simultaneously predicts start-end timestamps (i.e., temporal spans)
and categories of the all possible relations by utilizing full video context.
These two designs enable a win-win scenario: it accelerates training by 2X or
more than existing methods and achieves competitive performance on two VidVRD
benchmarks (ImageNet-VidVDR and VidOR). Moreover, comprehensive ablative
experiments demonstrate the effectiveness of our approach. Codes are available
at https://github.com/sangminwoo/Temporal-Span-Proposal-Network-VidVRD.
","[{'version': 'v1', 'created': 'Thu, 15 Jul 2021 07:01:26 GMT'}, {'version': 'v2', 'created': 'Wed, 5 Oct 2022 09:41:51 GMT'}]",2022-10-06,"['Computer Vision and Pattern Recognition', 'Artificial Intelligence']","This paper presents a novel temporal span proposal network for video relation detection. It utilizes computer vision and pattern recognition techniques combined with artificial intelligence to identify and detect temporal relations between objects in videos. The proposed network is based on a two-stage architecture that first extracts temporal proposals and then classifies them. Experiments on two datasets demonstrate the effectiveness of the proposed network in detecting temporal relations between objects in videos. The results show that the proposed network outperforms the baseline model in terms of accuracy, precision, recall, and F1-score. Furthermore, the model is able to capture temporal relations between objects in videos in a more efficient manner than the conventional methods. The proposed network is thus a promising approach for video relation detection.","Write an abstract for a paper called What and When to Look?: Temporal Span Proposal Network for Video
  Relation Detection about Computer Vision and Pattern Recognition, Artificial Intelligence"
2304.06128,"Ziyi Xie, Yuanwei Liu, Wenqiang Yi, Xuanli Wu and Arumugam Nallanathan",Physical Layer Security for STAR-RIS-NOMA in Large-Scale Networks,"['cs.IT', 'math.IT']","  In this paper, an analytical framework for secure simultaneous transmitting
and reflecting reconfigurable intelligent surface (STAR-RIS) assisted
non-orthogonal multiple access (NOMA) transmissions in large-scale networks is
proposed, where users and eavesdroppers are randomly distributed. Both the
time-switching protocol (TS) and energy splitting (ES) protocol are considered
for the STAR-RIS. To characterize system performance, the channel statistics
are first provided, and the Gamma approximation is adopted for general cascaded
$\kappa$-$\mu$ fading. Afterward, the closed-form expressions for both the
secrecy outage probability and secrecy ergodic rate are derived. To obtain
further insights, the asymptotic performance for the secrecy diversity order
and the secrecy slope are deduced. The theoretical results show that 1) the
secrecy diversity orders of the strong user and the weak user depend on the
path loss exponent and the distribution of the received signal-to-noise ratio,
respectively; 2) the secrecy slope of the ES protocol achieves the value of
one, higher than the slope of the TS protocol which is the mode operation
parameter of TS. The numerical results demonstrate that: 1) there is an optimal
STAR-RIS mode operation parameter to maximize the system performance; 2) the
STAR-RIS-NOMA significantly outperforms the STAR-RIS-orthogonal multiple
access.
","[{'version': 'v1', 'created': 'Wed, 12 Apr 2023 19:26:17 GMT'}]",2023-04-14,['Information Theory'],"This paper presents an overview of physical layer security for the STAR-RIS-NOMA (Space-Time Adaptive Resource-Aware Interference Suppression-Non-Orthogonal Multiple Access) framework in large-scale networks. We introduce the concept of physical layer security, which is a security measure that protects data from malicious attacks at the physical layer of the communication system. We discuss the advantages and disadvantages of using physical layer security in large-scale networks and provide a detailed analysis of the STAR-RIS-NOMA framework. We then analyze the security of STAR-RIS-NOMA in terms of information theoretic principles, such as secrecy capacity, secrecy outage probability, and secrecy rate. We also discuss the potential applications of physical layer security in large-scale networks and its implications in the context of the Internet of Things (IoT). Finally, we conclude by highlighting the importance of physical layer security for large-scale networks and its potential to improve the security and privacy of data transmission.",Write an abstract for a paper called Physical Layer Security for STAR-RIS-NOMA in Large-Scale Networks about Information Theory
2303.03614,"Zengyang Gong, Yuxiang Zeng, Lei Chen","A Fast Insertion Operator for Ridesharing over Time-Dependent Road
  Networks",['cs.DB'],"  Ridesharing has become a promising travel mode recently due to the economic
and social benefits. As an essential operator, ""insertion operator"" has been
extensively studied over static road networks. When a new request appears, the
insertion operator is used to find the optimal positions of a worker's current
route to insert the origin and destination of this request and minimize the
travel time of this worker. Previous works study how to conduct the insertion
operation efficiently in static road networks, however, in reality, the route
planning should be addressed by considering the dynamic traffic scenario (i.e.,
a time-dependent road network). Unfortunately, existing solutions to the
insertion operator become in efficient under this setting. Thus, this paper
studies the insertion operator over time-dependent road networks. Specially, to
reduce the high time complexity $O(n^3)$ of existing solution, we calculate the
compound travel time functions along the route to speed up the calculation of
the travel time between vertex pairs belonging to the route, as a result time
complexity of an insertion can be reduced to $O(n^2)$. Finally, we further
improve the method to a linear-time insertion algorithm by showing that it only
needs $O(1)$ time to find the best position of current route to insert the
origin when linearly enumerating each possible position for the new request's
destination. Evaluations on two real-world and large-scale datasets show that
our methods can accelerate the existing insertion algorithm by up to 25 times.
","[{'version': 'v1', 'created': 'Tue, 7 Mar 2023 03:00:26 GMT'}]",2023-03-08,['Databases'],"This paper presents a novel insertion operator for ridesharing over time-dependent road networks. The proposed operator is designed to reduce the complexity of the insertion process, while maintaining the accuracy of the results. The operator is based on a graph-theoretic approach and uses a shortest path algorithm to find the most efficient path for a ridesharing trip. The operator is tested on a real-world road network, and results show that it is significantly faster than existing insertion methods. The paper also discusses the implications of the proposed operator for database systems.","Write an abstract for a paper called A Fast Insertion Operator for Ridesharing over Time-Dependent Road
  Networks about Databases"
2303.11748,Malcolm Crowe and Fritz Laux,Database Technology Evolution,['cs.DB'],"  This paper reviews suggestions for changes to database technology coming from
the work of many researchers, particularly those working with evolving big
data. We discuss new approaches to remote data access and standards that better
provide for durability and auditability in settings including business and
scientific computing. We propose ways in which the language standards could
evolve, with proof-of-concept implementations on Github.
","[{'version': 'v1', 'created': 'Tue, 21 Mar 2023 11:14:24 GMT'}]",2023-03-22,['Databases'],"This paper examines the evolution of database technology over the past few decades. It looks at the development of database technology from the early days of flat file databases to modern relational databases. It also looks at the impact of database technology on business operations and the various types of databases available today. Finally, the paper looks at the future of database technology and the potential for further evolution. The paper provides an overview of the history of database technology, its current state, and its potential for the future.",Write an abstract for a paper called Database Technology Evolution about Databases
2212.01569,"Clarisa V. Albarillo, Emely A. Munar, Maria Concepcion M. Balcita","Computer and Internet Literacy Course of the College of Computer Science
  for the Municipality of Agoo, La Union",['cs.CY'],"  The main objective of the study is to provide ICT awareness, literacy and
skills development to the barangay officials of Agoo, La Union. Specifically,
it aimed the following objectives: 1) to determine the profile of the
respondents in terms of personal information, educational background and
availability of computer unit and background in using computer; 2) to determine
the effectiveness of the CILC in terms of services delivered, timeliness of the
service, and improvement on the computer and internet knowledge of the
trainees; and 3) to determine the level of relevance of the training sessions
of the CILC. The study used a descriptive design. Data were gathered by using
survey questionnaire and were analyzed by using statistical treatments such as
frequency count, percentage and mean. As to the profile of the trainees, the
study found that most of the trainees are female (88%); 84% are married, and
56% of them are at the age bracket of 30-39 years old. In terms of educational
background, many are high school graduate (n= 17; 68%). In addition, most of
them (84%) have background in computer. The result also shows that the CILC is
at the high level of effectiveness (4.67) in terms of services delivered and is
much relevant (4.45) in terms of its relevance.
","[{'version': 'v1', 'created': 'Sat, 3 Dec 2022 07:59:55 GMT'}]",2022-12-06,['Computers and Society'],"This paper examines the implementation of a Computer and Internet Literacy course in the College of Computer Science of the Municipality of Agoo, La Union. It explores the societal implications of introducing such a course to the citizens of the municipality, with a focus on how the course can help equip students with the necessary digital literacy skills to navigate the increasingly digital world. It also examines the potential social and economic benefits of the course, such as increased access to information, improved job opportunities, and increased civic engagement. Furthermore, the paper discusses the challenges of implementing such a course, such as limited resources and lack of technical expertise. In conclusion, the paper argues that a Computer and Internet Literacy course can be a valuable asset for the citizens of Agoo, La Union, and that its implementation should be carefully considered and planned.","Write an abstract for a paper called Computer and Internet Literacy Course of the College of Computer Science
  for the Municipality of Agoo, La Union about Computers and Society"
2201.12861,"Weidong Cao, Yilong Zhao, Adith Boloor, Yinhe Han, Xuan Zhang, Li
  Jiang","Neural-PIM: Efficient Processing-In-Memory with Neural Approximation of
  Peripherals","['cs.AR', 'cs.ET', 'cs.LG']","  Processing-in-memory (PIM) architectures have demonstrated great potential in
accelerating numerous deep learning tasks. Particularly, resistive
random-access memory (RRAM) devices provide a promising hardware substrate to
build PIM accelerators due to their abilities to realize efficient in-situ
vector-matrix multiplications (VMMs). However, existing PIM accelerators suffer
from frequent and energy-intensive analog-to-digital (A/D) conversions,
severely limiting their performance. This paper presents a new PIM architecture
to efficiently accelerate deep learning tasks by minimizing the required A/D
conversions with analog accumulation and neural approximated peripheral
circuits. We first characterize the different dataflows employed by existing
PIM accelerators, based on which a new dataflow is proposed to remarkably
reduce the required A/D conversions for VMMs by extending shift and add (S+A)
operations into the analog domain before the final quantizations. We then
leverage a neural approximation method to design both analog accumulation
circuits (S+A) and quantization circuits (ADCs) with RRAM crossbar arrays in a
highly-efficient manner. Finally, we apply them to build an RRAM-based PIM
accelerator (i.e., \textbf{Neural-PIM}) upon the proposed analog dataflow and
evaluate its system-level performance. Evaluations on different benchmarks
demonstrate that Neural-PIM can improve energy efficiency by 5.36x (1.73x) and
speed up throughput by 3.43x (1.59x) without losing accuracy, compared to the
state-of-the-art RRAM-based PIM accelerators, i.e., ISAAC (CASCADE).
","[{'version': 'v1', 'created': 'Sun, 30 Jan 2022 16:14:49 GMT'}]",2022-02-01,"['Hardware Architecture', 'Emerging Technologies', 'Machine Learning']","This paper presents Neural-PIM, a novel hardware architecture that uses neural approximation to enable efficient processing-in-memory (PIM) operations. Neural-PIM is based on the concept of integrating neural networks with peripheral devices, such as memory controllers and accelerators, to enable the efficient execution of PIM operations. The paper presents a detailed overview of the Neural-PIM architecture, including its components and their interactions. Additionally, the paper discusses the potential benefits of Neural-PIM, such as improved performance and energy efficiency, as well as its applications in emerging technologies, such as machine learning. Finally, the paper provides a comprehensive evaluation of the proposed architecture, demonstrating its effectiveness in improving the performance of PIM operations.","Write an abstract for a paper called Neural-PIM: Efficient Processing-In-Memory with Neural Approximation of
  Peripherals about Hardware Architecture, Emerging Technologies, Machine Learning"
2205.01304,"Donghyeon Kim, Gwantae Kim, Bokyeung Lee, Jeong-gi Kwak, David K. Han,
  Hanseok Ko","Efficient dynamic filter for robust and low computational feature
  extraction","['eess.AS', 'cs.SD']","  Unseen noise signal which is not considered in a model training process is
difficult to anticipate and would lead to performance degradation. Various
methods have been investigated to mitigate unseen noise. In our previous work,
an Instance-level Dynamic Filter (IDF) and a Pixel Dynamic Filter (PDF) were
proposed to extract noise-robust features. However, the performance of the
dynamic filter might be degraded since simple feature pooling is used to reduce
the computational resource in the IDF part. In this paper, we propose an
efficient dynamic filter to enhance the performance of the dynamic filter.
Instead of utilizing the simple feature mean, we separate Time-Frequency (T-F)
features as non-overlapping chunks, and separable convolutions are carried out
for each feature direction (inter chunks and intra chunks). Additionally, we
propose Dynamic Attention Pooling that maps high dimensional features as low
dimensional feature embeddings. These methods are applied to the IDF for
keyword spotting and speaker verification tasks. We confirm that our proposed
method performs better in unseen environments (unseen noise and unseen
speakers) than state-of-the-art models.
","[{'version': 'v1', 'created': 'Tue, 3 May 2022 04:51:31 GMT'}, {'version': 'v2', 'created': 'Thu, 20 Oct 2022 18:29:49 GMT'}]",2022-10-24,['Sound'],"This paper presents an efficient dynamic filter for robust and low computational feature extraction from sound signals. The proposed filter is designed to extract features in a computationally efficient manner while maintaining robustness to noise and other disturbances. The filter is based on a combination of frequency-domain filtering and time-domain processing techniques. Results from experiments with real-world sound signals demonstrate that the proposed filter can effectively extract features with low computational cost and high accuracy. Furthermore, the proposed filter is shown to be robust to noise and other disturbances, providing reliable feature extraction in a wide range of sound environments. The paper also presents a discussion of future research directions.","Write an abstract for a paper called Efficient dynamic filter for robust and low computational feature
  extraction about Sound"
2212.09879,Lenka Jungmannov\'a and Petr Plech\'a\v{c},Unsigned Play by Milan Kundera? An Authorship Attribution Study,['cs.CL'],"  In addition to being a widely recognised novelist, Milan Kundera has also
authored three pieces for theatre: The Owners of the Keys (Majitel\'e
kl\'i\v{c}\r{u}, 1961), The Blunder (Pt\'akovina, 1967), and Jacques and his
Master (Jakub a jeho p\'an, 1971). In recent years, however, the hypothesis has
been raised that Kundera is the true author of a fourth play: Juro
J\'ano\v{s}\'ik, first performed in a 1974 production under the name of Karel
Steigerwald, who was Kundera's student at the time. In this study, we make use
of supervised machine learning to settle the question of authorship attribution
in the case of Juro J\'ano\v{s}\'ik, with results strongly supporting the
hypothesis of Kundera's authorship.
","[{'version': 'v1', 'created': 'Mon, 19 Dec 2022 21:59:22 GMT'}]",2022-12-21,['Computation and Language'],"This paper examines the authorship of Unsigned Play, a novel by Milan Kundera. It employs a combination of computational and linguistic methods to analyze the text and determine authorship. The paper begins by outlining Kundera's career and discussing the importance of authorship attribution in literary studies. It then presents a detailed overview of the computational and linguistic methods used in the study, including stylometric analysis, word frequency analysis, and syntactic analysis. Finally, the paper discusses the results of the study and their implications for authorship attribution in literary studies. The findings of this paper suggest that Unsigned Play is indeed authored by Kundera, and that computational and linguistic methods can be used to accurately determine authorship in literary studies.",Write an abstract for a paper called Unsigned Play by Milan Kundera? An Authorship Attribution Study about Computation and Language
2010.07583,"Camille Carvalho and Zo\""is Moitier","Scattering resonances in unbounded transmission problems with
  sign-changing coefficient","['math.AP', 'cs.NA', 'math.NA', 'physics.optics']","  It is well-known that classical optical cavities can exhibit localized
phenomena associated to scattering resonances, leading to numerical
instabilities in approximating the solution. This result can be established via
the ``quasimodes to resonances'' argument from the black-box scattering
framework. Those localized phenomena concentrate at the inner boundary of the
cavity and are called whispering gallery modes. In this paper we investigate
scattering resonances for unbounded transmission problems with sign-changing
coefficient (corresponding to optical cavities with negative optical
properties, for example made of metamaterials). Due to the change of sign of
optical properties, previous results cannot be applied directly, and interface
phenomena at the metamaterial-dielectric interface (such as the so-called
surface plasmons) emerge. We establish the existence of scattering resonances
for arbitrary two-dimensional smooth metamaterial cavities. The proof relies on
an asymptotic characterization of the resonances, and showing that problems
with sign-changing coefficient naturally fit the black box scattering
framework. Our asymptotic analysis reveals that, depending on the
metamaterial's properties, scattering resonances situated closed to the real
axis are associated to surface plasmons. Examples for several metamaterial
cavities are provided.
","[{'version': 'v1', 'created': 'Thu, 15 Oct 2020 08:07:31 GMT'}, {'version': 'v2', 'created': 'Wed, 20 Apr 2022 13:03:02 GMT'}, {'version': 'v3', 'created': 'Tue, 6 Dec 2022 16:00:34 GMT'}]",2022-12-07,['Numerical Analysis'],"This paper examines the numerical analysis of scattering resonances in unbounded transmission problems with sign-changing coefficients. Specifically, we consider the case where the coefficient changes sign between two regions, and investigate the effects of this sign change on the scattering resonances. We analyze the numerical methods used to solve these problems, such as the finite element method, and discuss the accuracy and efficiency of these methods. We also discuss the numerical stability of the solutions, and the effects of the sign change on the scattering resonances. Finally, we present the results of our numerical experiments and discuss their implications.","Write an abstract for a paper called Scattering resonances in unbounded transmission problems with
  sign-changing coefficient about Numerical Analysis"
2108.0549,"Anahita Samadi, Debapriya Banerjee, Shirin Nilizadeh","Attacks against Ranking Algorithms with Text Embeddings: a Case Study on
  Recruitment Algorithms","['cs.CL', 'cs.LG']","  Recently, some studies have shown that text classification tasks are
vulnerable to poisoning and evasion attacks. However, little work has
investigated attacks against decision making algorithms that use text
embeddings, and their output is a ranking. In this paper, we focus on ranking
algorithms for recruitment process, that employ text embeddings for ranking
applicants resumes when compared to a job description. We demonstrate both
white box and black box attacks that identify text items, that based on their
location in embedding space, have significant contribution in increasing the
similarity score between a resume and a job description. The adversary then
uses these text items to improve the ranking of their resume among others. We
tested recruitment algorithms that use the similarity scores obtained from
Universal Sentence Encoder (USE) and Term Frequency Inverse Document Frequency
(TF IDF) vectors. Our results show that in both adversarial settings, on
average the attacker is successful. We also found that attacks against TF IDF
is more successful compared to USE.
","[{'version': 'v1', 'created': 'Thu, 12 Aug 2021 01:41:21 GMT'}]",2022-01-11,"['Computation and Language', 'Machine Learning']","This paper presents a case study on the use of text embeddings to attack ranking algorithms in the context of recruitment algorithms for computation and language, machine learning. We discuss the concept of text embeddings, their application in the recruitment process, and their potential for manipulation. We then present a case study of a recruitment algorithm for computation and language, machine learning, which uses text embeddings to rank applicants. We then discuss the possible attacks against such algorithms, and analyze how text embeddings can be manipulated to bias the ranking. Finally, we present our findings and suggest potential solutions to mitigate such attacks.","Write an abstract for a paper called Attacks against Ranking Algorithms with Text Embeddings: a Case Study on
  Recruitment Algorithms about Computation and Language, Machine Learning"
2304.0604,"Ganga Meghanath, Bryan Jimenez, Joseph G. Makin",Inferring Population Dynamics in Macaque Cortex,"['q-bio.NC', 'cs.LG']","  The proliferation of multi-unit cortical recordings over the last two
decades, especially in macaques and during motor-control tasks, has generated
interest in neural ""population dynamics"": the time evolution of neural activity
across a group of neurons working together. A good model of these dynamics
should be able to infer the activity of unobserved neurons within the same
population and of the observed neurons at future times. Accordingly,
Pandarinath and colleagues have introduced a benchmark to evaluate models on
these two (and related) criteria: four data sets, each consisting of firing
rates from a population of neurons, recorded from macaque cortex during
movement-related tasks. Here we show that simple, general-purpose architectures
based on recurrent neural networks (RNNs) outperform more ""bespoke"" models, and
indeed outperform all published models on all four data sets in the benchmark.
Performance can be improved further still with a novel, hybrid architecture
that augments the RNN with self-attention, as in transformer networks. But pure
transformer models fail to achieve this level of performance, either in our
work or that of other groups. We argue that the autoregressive bias imposed by
RNNs is critical for achieving the highest levels of performance. We conclude,
however, by proposing that the benchmark be augmented with an alternative
evaluation of latent dynamics that favors generative over discriminative models
like the ones we propose in this report.
","[{'version': 'v1', 'created': 'Wed, 5 Apr 2023 14:24:27 GMT'}]",2023-04-14,['Machine Learning'],"This paper explores the use of machine learning techniques to infer population dynamics in macaque cortex. Specifically, we use a combination of methods including spike-triggered covariance analysis, linear regression, and nonlinear dimensionality reduction to analyze the population activity of neurons in the macaque cortex. We then use the results of these analyses to infer the underlying population dynamics of the neurons. Results show that our methods are able to accurately infer the population dynamics of the neurons, thus providing a powerful tool for understanding the behavior of neurons in the macaque cortex. Furthermore, our results suggest that machine learning techniques may be a useful tool for understanding the complex patterns of neural activity in the brain.",Write an abstract for a paper called Inferring Population Dynamics in Macaque Cortex about Machine Learning
2211.02107,Nodari Vakhania,Variable Parameter Analysis for Scheduling One Machine,['cs.DS'],"  In contrast to the fixed parameter analysis (FPA), in the variable parameter
analysis (VPA) the value of the target problem parameter is not fixed, it
rather depends on the structure of a given problem instance and tends to have a
favorable asymptotic behavior when the size of the input increases. While
applying the VPA to an intractable optimization problem with $n$ objects, the
exponential-time dependence in enumeration of the feasible solution set is
attributed solely to the variable parameter $\nu$, $\nu<<n$. As opposed to the
FPA, the VPA does not imply any restriction on some problem parameters, it
rather takes an advantage of a favorable nature of the problem, which permits
to reduce the cost of enumeration of the solution space. Our main technical
contribution is a variable parameter algorithm for a strongly
$\mathsf{NP}$-hard single-machine scheduling problem to minimize maximum job
lateness. The target variable parameter $\nu$ is the number of jobs with some
specific characteristics, the ``emerging'' ones. The solution process is
separated in two phases. At phase 1 a partial solution including $n-\nu$
non-emerging jobs is constructed in a low degree polynomial time. At phase 2
less than $\nu!$ permutations of the $\nu$ emerging jobs are considered. Each
of them are incorporated into the partial schedule of phase 1. Doe to the
results of an earlier conducted experimental study, $\nu/n$ varied from $1/4$
for small problem instances to $1/10$ for the largest tested problem instances,
so that that the ratio becomes closer to 0 for large $n$s.
","[{'version': 'v1', 'created': 'Thu, 3 Nov 2022 19:22:34 GMT'}]",2022-11-07,['Data Structures and Algorithms'],"This paper examines the use of data structures and algorithms for the optimization of scheduling one machine with variable parameters. The objective is to develop an efficient algorithm for the optimization of the schedule that minimizes the total cost of production. The paper begins by discussing the various data structures and algorithms that can be used for this purpose. It then examines the various techniques used to analyze and optimize the schedule. Finally, the paper presents a proposed algorithm and its performance results. The proposed algorithm is tested on a variety of data sets and the results are compared with existing algorithms. The results show that the proposed algorithm is effective in minimizing the total cost of production.",Write an abstract for a paper called Variable Parameter Analysis for Scheduling One Machine about Data Structures and Algorithms
2207.13871,"Qingyang Tan, Yi Zhou, Tuanfeng Wang, Duygu Ceylan, Xin Sun, Dinesh
  Manocha",A Repulsive Force Unit for Garment Collision Handling in Neural Networks,"['cs.GR', 'cs.CV']","  Despite recent success, deep learning-based methods for predicting 3D garment
deformation under body motion suffer from interpenetration problems between the
garment and the body. To address this problem, we propose a novel collision
handling neural network layer called Repulsive Force Unit (ReFU). Based on the
signed distance function (SDF) of the underlying body and the current garment
vertex positions, ReFU predicts the per-vertex offsets that push any
interpenetrating vertex to a collision-free configuration while preserving the
fine geometric details. We show that ReFU is differentiable with trainable
parameters and can be integrated into different network backbones that predict
3D garment deformations. Our experiments show that ReFU significantly reduces
the number of collisions between the body and the garment and better preserves
geometric details compared to prior methods based on collision loss or
post-processing optimization.
","[{'version': 'v1', 'created': 'Thu, 28 Jul 2022 03:46:16 GMT'}, {'version': 'v2', 'created': 'Fri, 4 Nov 2022 22:19:44 GMT'}]",2022-11-08,"['Graphics', 'Computer Vision and Pattern Recognition']","This paper presents a repulsive force unit (RFU) for garment collision handling in neural networks. The proposed RFU is designed to handle the problem of garment collision in computer vision and pattern recognition tasks. The RFU is based on a novel approach to modeling repulsive forces between garments, which is based on a combination of physics-based and machine learning-based approaches. The proposed RFU is evaluated on a variety of garment collision datasets and compared to existing methods. The results demonstrate that the proposed RFU is able to effectively handle garment collisions, while providing improved accuracy and efficiency. The proposed RFU is also shown to be robust to various environmental conditions and garment types, and is suitable for real-time applications.","Write an abstract for a paper called A Repulsive Force Unit for Garment Collision Handling in Neural Networks about Graphics, Computer Vision and Pattern Recognition"
2203.12591,"Bowen Yu, Junping Du, Yingxia Shao",Web Page Content Extraction Based on Multi-feature Fusion,"['cs.IR', 'cs.AI']","  With the rapid development of Internet technology, people have more and more
access to a variety of web page resources. At the same time, the current rapid
development of deep learning technology is often inseparable from the huge
amount of Web data resources. On the other hand, NLP is also an important part
of data processing technology, such as web page data extraction. At present,
the extraction technology of web page text mainly uses a single heuristic
function or strategy, and most of them need to determine the threshold
manually. With the rapid growth of the number and types of web resources, there
are still problems to be solved when using a single strategy to extract the
text information of different pages. This paper proposes a web page text
extraction algorithm based on multi-feature fusion. According to the text
information characteristics of web resources, DOM nodes are used as the
extraction unit to design multiple statistical features, and high-order
features are designed according to heuristic strategies. This method
establishes a small neural network, takes multiple features of DOM nodes as
input, predicts whether the nodes contain text information, makes full use of
different statistical information and extraction strategies, and adapts to more
types of pages. Experimental results show that this method has a good ability
of web page text extraction and avoids the problem of manually determining the
threshold.
","[{'version': 'v1', 'created': 'Mon, 21 Mar 2022 04:26:51 GMT'}, {'version': 'v2', 'created': 'Wed, 26 Oct 2022 13:41:57 GMT'}]",2022-10-27,"['Information Retrieval', 'Artificial Intelligence']","This paper presents a novel approach to web page content extraction based on multi-feature fusion for information retrieval and artificial intelligence. The proposed method combines several features including text, HTML tags, and visual cues to accurately extract web page content. We evaluate the proposed method on a publicly available web page dataset and demonstrate that it outperforms existing methods in terms of accuracy and efficiency. Additionally, we analyze the effects of different feature combinations on the performance of the proposed method. Our results show that the proposed method can effectively extract web page content and is suitable for use in information retrieval and artificial intelligence applications.","Write an abstract for a paper called Web Page Content Extraction Based on Multi-feature Fusion about Information Retrieval, Artificial Intelligence"
2108.02299,"Leilani Battle, Danni Feng, Kelli Webber",Exploring D3 Implementation Challenges on Stack Overflow,['cs.HC'],"  Visualization languages help to standardize the process of designing
effective visualizations, one of the most prominent being D3. However, few
researchers have analyzed at scale how users incorporate these languages into
existing visualization programming processes, i.e., implementation workflows.
In this paper, we present an analysis of the experiences of D3 users as
observed through Stack Overflow, summarizing common D3 implementation workflows
and challenges discussed online. Our results show how the visualization
community may be limiting its understanding of users' visualization
implementation challenges by ignoring the larger context in which languages
such as D3 are used. Based on our findings, we suggest new research directions
to enhance the user experience with visualization languages. All our data and
code are available at: https://osf.io/fup48/.
","[{'version': 'v1', 'created': 'Wed, 4 Aug 2021 22:06:34 GMT'}, {'version': 'v2', 'created': 'Fri, 10 Sep 2021 21:38:34 GMT'}, {'version': 'v3', 'created': 'Thu, 23 Jun 2022 22:54:17 GMT'}, {'version': 'v4', 'created': 'Mon, 27 Jun 2022 20:09:58 GMT'}]",2022-06-29,['Human-Computer Interaction'],"This paper explores implementation challenges related to the popular JavaScript library D3, as discussed on the programming Q&A website Stack Overflow. Through an analysis of posts on Stack Overflow related to D3, this paper seeks to identify the most common challenges faced by developers when using the library, as well as the types of solutions they find most helpful. Additionally, this paper will investigate how the Human-Computer Interaction (HCI) field can help to provide insight into the challenges of using D3 and how to improve the user experience of its implementation. The results of this paper will provide an understanding of the current state of D3 implementation challenges and provide guidance for future work in the HCI field related to D3 development.",Write an abstract for a paper called Exploring D3 Implementation Challenges on Stack Overflow about Human-Computer Interaction
2203.09402,"Jiri Mekyska, Eva Janousova, Pedro Gomez-Vilda, Zdenek Smekal, Irena
  Rektorova, Ilona Eliasova, Milena Kostalova, Martina Mrackova, Jesus B.
  Alonso-Hernandez, Marcos Faundez-Zanuy, Karmele L\'opez-de-Ipi\~na",Robust and Complex Approach of Pathological Speech Signal Analysis,"['cs.SD', 'eess.AS']","  This paper presents a study of the approaches in the state-of-the-art in the
field of pathological speech signal analysis with a special focus on
parametrization techniques. It provides a description of 92 speech features
where some of them are already widely used in this field of science and some of
them have not been tried yet (they come from different areas of speech signal
processing like speech recognition or coding). As an original contribution,
this work introduces 36 completely new pathological voice measures based on
modulation spectra, inferior colliculus coefficients, bicepstrum, sample and
approximate entropy and empirical mode decomposition. The significance of these
features was tested on 3 (English, Spanish and Czech) pathological voice
databases with respect to classification accuracy, sensitivity and specificity.
","[{'version': 'v1', 'created': 'Thu, 17 Mar 2022 15:54:44 GMT'}]",2022-03-18,['Sound'],"Analysis

This paper presents a robust and complex approach to the analysis of pathological speech signals. The approach is based on the analysis of sound features, such as pitch, loudness, and spectral characteristics, to identify and quantify characteristics of pathological speech. The proposed approach is evaluated on a publicly available dataset of pathological speech recordings and compared to existing methods. Results show that the proposed approach is able to accurately classify and quantify pathological speech signals, and is more robust than existing methods. The proposed approach can be used to improve the accuracy of diagnosis and treatment of various speech disorders.",Write an abstract for a paper called Robust and Complex Approach of Pathological Speech Signal Analysis about Sound
2005.10902,"Artur M. Schweidtmann, Dominik Bongartz, Daniel Grothe, Tim
  Kerkenhoff, Xiaopeng Lin, Jaromil Najman, Alexander Mitsos",Global Optimization of Gaussian processes,"['math.OC', 'cs.LG', 'stat.ML']","  Gaussian processes~(Kriging) are interpolating data-driven models that are
frequently applied in various disciplines. Often, Gaussian processes are
trained on datasets and are subsequently embedded as surrogate models in
optimization problems. These optimization problems are nonconvex and global
optimization is desired. However, previous literature observed computational
burdens limiting deterministic global optimization to Gaussian processes
trained on few data points. We propose a reduced-space formulation for
deterministic global optimization with trained Gaussian processes embedded. For
optimization, the branch-and-bound solver branches only on the degrees of
freedom and McCormick relaxations are propagated through explicit Gaussian
process models. The approach also leads to significantly smaller and
computationally cheaper subproblems for lower and upper bounding. To further
accelerate convergence, we derive envelopes of common covariance functions for
GPs and tight relaxations of acquisition functions used in Bayesian
optimization including expected improvement, probability of improvement, and
lower confidence bound. In total, we reduce computational time by orders of
magnitude compared to state-of-the-art methods, thus overcoming previous
computational burdens. We demonstrate the performance and scaling of the
proposed method and apply it to Bayesian optimization with global optimization
of the acquisition function and chance-constrained programming. The Gaussian
process models, acquisition functions, and training scripts are available
open-source within the ""MeLOn - Machine Learning Models for Optimization""
toolbox~(https://git.rwth-aachen.de/avt.svt/public/MeLOn).
","[{'version': 'v1', 'created': 'Thu, 21 May 2020 20:59:11 GMT'}]",2022-10-27,['Machine Learning'],"This paper presents a novel approach to global optimization of Gaussian processes (GPs) for machine learning. We propose a new framework that combines traditional GP regression with a global optimization algorithm to optimize the hyperparameters of the GP model. We demonstrate the effectiveness of our approach on a variety of machine learning tasks, including classification and regression. We compare our approach to existing methods, and show that our proposed method outperforms existing methods in terms of accuracy and computational efficiency. Furthermore, we provide insights into the behavior of our proposed framework, and discuss potential applications of our approach.",Write an abstract for a paper called Global Optimization of Gaussian processes about Machine Learning
2302.07952,Rik Verbiest and Julian Koellermeier,Hyperbolic Axisymmetric Shallow Water Moment Equations,"['math.NA', 'cs.NA']","  Models for shallow water flow often assume that the lateral velocity is
constant over the water height. The recently derived shallow water moment
equations are an extension of these standard shallow water equations. The
extended models allow for vertical changes in the lateral velocity, resulting
in a system that is more accurate in situations where the horizontal velocity
varies considerably over the height of the fluid. Unfortunately, already the
one-dimensional models lack global hyperbolicity, an important property of
partial differential equations that ensures that disturbances have a finite
propagation speed. In this paper we show that the loss of hyperbolicity also
occurs in two-dimensional axisymmetric systems. First, a cylindrical moment
model is obtained by starting from the cylindrical incompressible Navier-Stokes
equations. We derive two-dimensional axisymmetric Shallow Water Moment
Equations by imposing axisymmetry in the cylindrical model. The loss of
hyperbolicity is observed by directly evaluating the propagation speeds and
plotting the hyperbolicity region. A hyperbolic axisymmetric moment model is
then obtained by modifying the system matrix in analogy to the one-dimensional
case, for which the hyperbolicity problem has already been observed and
overcome. The new model is written in analytical form and we prove that
hyperbolicity can be guaranteed. To test the new model, numerical simulations
with both discontinuous and continuous initial data are performed. We show that
the hyperbolic model leads to less oscillations than the non-hyperbolic model
in the case of a discontinuous initial height profile. The hyperbolic model is
preferred over the non-hyperbolic model in this specific scenario for shorter
times. Further, we observe that the error decreases when the order of the model
is increased in a test case with smooth initial data.
","[{'version': 'v1', 'created': 'Wed, 15 Feb 2023 21:17:57 GMT'}]",2023-02-17,['Numerical Analysis'],"This paper explores the numerical analysis of hyperbolic axisymmetric shallow water moment equations. The equations are derived from the shallow water equations, which are a set of equations used to describe the motion of water in one dimension. The equations are then transformed into a set of hyperbolic equations, which are more suitable for numerical analysis. The paper then focuses on the numerical analysis of the equations, which includes the development of numerical schemes, the analysis of the accuracy of the numerical results, and the comparison of the numerical results with analytical solutions. Finally, the paper presents a summary of the numerical results and the conclusions derived from the analysis.",Write an abstract for a paper called Hyperbolic Axisymmetric Shallow Water Moment Equations about Numerical Analysis
2211.05446,"Meng Chen, Li Lu, Jiadi Yu, Yingying Chen, Zhongjie Ba, Feng Lin, Kui
  Ren","Privacy-Utility Balanced Voice De-Identification Using Adversarial
  Examples","['cs.SD', 'cs.CR', 'cs.LG', 'eess.AS']","  Faced with the threat of identity leakage during voice data publishing, users
are engaged in a privacy-utility dilemma when enjoying convenient voice
services. Existing studies employ direct modification or text-based
re-synthesis to de-identify users' voices, but resulting in inconsistent
audibility in the presence of human participants. In this paper, we propose a
voice de-identification system, which uses adversarial examples to balance the
privacy and utility of voice services. Instead of typical additive examples
inducing perceivable distortions, we design a novel convolutional adversarial
example that modulates perturbations into real-world room impulse responses.
Benefit from this, our system could preserve user identity from exposure by
Automatic Speaker Identification (ASI) while remaining the voice perceptual
quality for non-intrusive de-identification. Moreover, our system learns a
compact speaker distribution through a conditional variational auto-encoder to
sample diverse target embeddings on demand. Combining diverse target generation
and input-specific perturbation construction, our system enables any-to-any
identify transformation for adaptive de-identification. Experimental results
show that our system could achieve 98% and 79% successful de-identification on
mainstream ASIs and commercial systems with an objective Mel cepstral
distortion of 4.31dB and a subjective mean opinion score of 4.48.
","[{'version': 'v1', 'created': 'Thu, 10 Nov 2022 09:35:58 GMT'}]",2022-11-11,"['Sound', 'Cryptography and Security', 'Machine Learning']","This paper proposes a privacy-utility balanced voice de-identification method using adversarial examples. It combines sound, cryptography, and security with machine learning to create a secure and reliable system for protecting the privacy of voice recordings. The proposed method is evaluated on a voice dataset containing both English and Mandarin speakers. The results show that the proposed method can effectively de-identify voice recordings while preserving their utility. Furthermore, the proposed method is robust against adversarial examples. The paper also discusses the potential applications of the proposed method in various fields of research, such as biometrics, audio forensics, and speech recognition.","Write an abstract for a paper called Privacy-Utility Balanced Voice De-Identification Using Adversarial
  Examples about Sound, Cryptography and Security, Machine Learning"
2206.01394,"Ming Xie, Xiu-Xiu Zhan, Chuang Liu, Zi-Ke Zhang",Influence Maximization in Hypergraphs,['cs.SI'],"  Influence maximization in complex networks, i.e., maximizing the size of
influenced nodes via selecting K seed nodes for a given spreading process, has
attracted great attention in recent years. However, the influence maximization
problem in hypergraphs, in which the hyperedges are leveraged to represent the
interactions among more than two nodes, is still an open question. In this
paper, we propose an adaptive degree-based heuristic algorithm, i.e., Heuristic
Degree Discount (HDD), which iteratively selects nodes with low influence
overlap as seeds, to solve the influence maximization problem in hypergraphs.
We further extend algorithms from ordinary networks as baselines and compare
the performance of the proposed algorithm and baselines on both real data and
synthetic hypergraphs. Results show that HDD outperforms the baselines in terms
of both effectiveness and efficiency. Moreover, the experiments on synthetic
hypergraphs indicate that HDD shows high performance, especially in hypergraphs
with heterogeneous degree distribution.
","[{'version': 'v1', 'created': 'Fri, 3 Jun 2022 05:06:31 GMT'}]",2022-06-06,['Social and Information Networks'],"This paper examines the application of influence maximization in hypergraphs to social and information networks. Influence maximization is the process of selecting a subset of nodes in a network to maximize the spread of influence or information. Hypergraphs are a generalization of graphs in which an edge can connect more than two nodes and can represent social and information networks more accurately. We discuss the influence maximization problem in hypergraphs and present a greedy algorithm and a local search algorithm to solve the problem. We also discuss the complexity of the problem and the performance of the algorithms. Finally, we evaluate the algorithms on real-world datasets and discuss the implications of our results.",Write an abstract for a paper called Influence Maximization in Hypergraphs about Social and Information Networks
2212.1034,"Petra Bevandi\'c, Marin Or\v{s}i\'c, Ivan Grubi\v{s}i\'c, Josip
  \v{S}ari\'c, Sini\v{s}a \v{S}egvi\'c","Weakly supervised training of universal visual concepts for multi-domain
  semantic segmentation",['cs.CV'],"  Deep supervised models have an unprecedented capacity to absorb large
quantities of training data. Hence, training on multiple datasets becomes a
method of choice towards strong generalization in usual scenes and graceful
performance degradation in edge cases. Unfortunately, different datasets often
have incompatible labels. For instance, the Cityscapes road class subsumes all
driving surfaces, while Vistas defines separate classes for road markings,
manholes etc. Furthermore, many datasets have overlapping labels. For instance,
pickups are labeled as trucks in VIPER, cars in Vistas, and vans in ADE20k. We
address this challenge by considering labels as unions of universal visual
concepts. This allows seamless and principled learning on multi-domain dataset
collections without requiring any relabeling effort. Our method achieves
competitive within-dataset and cross-dataset generalization, as well as ability
to learn visual concepts which are not separately labeled in any of the
training datasets. Experiments reveal competitive or state-of-the-art
performance on two multi-domain dataset collections and on the WildDash 2
benchmark.
","[{'version': 'v1', 'created': 'Tue, 20 Dec 2022 15:25:38 GMT'}]",2022-12-21,['Computer Vision and Pattern Recognition'],"This paper presents a novel weakly supervised training approach for universal visual concepts for multi-domain semantic segmentation. The proposed approach utilizes a deep convolutional neural network (CNN) to learn universal visual concepts from a large-scale image dataset. The learned concepts are used to generate pixel-level semantic segmentation masks for multiple domains. We evaluate our approach on two publicly available datasets, PASCAL VOC 2012 and Cityscapes, and demonstrate that our method outperforms existing weakly supervised methods in terms of both accuracy and generalization. Furthermore, we show that our method can be used to transfer knowledge from one domain to another, allowing for better generalization. Finally, we discuss the implications of our approach for future research in computer vision and pattern recognition.","Write an abstract for a paper called Weakly supervised training of universal visual concepts for multi-domain
  semantic segmentation about Computer Vision and Pattern Recognition"
2203.04728,"Miha Rot, Martin Horvat and Gregor Kosec","Dynamic mode decomposition as an analysis tool for time-dependent
  partial differential equations","['math.NA', 'cs.NA', 'math.DS', 'physics.data-an']","  The time-dependent fields obtained by solving partial differential equations
in two and more dimensions quickly overwhelm the analytical capabilities of the
human brain. A meaningful insight into the temporal behaviour can be obtained
by using scalar reductions, which, however, come with a loss of spatial detail.
Dynamic Mode Decomposition is a data-driven analysis method that solves this
problem by identifying oscillating spatial structures and their corresponding
frequencies. This paper presents the algorithm and provides a physical
interpretation of the results by applying the decomposition method to a series
of increasingly complex examples.
","[{'version': 'v1', 'created': 'Wed, 9 Mar 2022 14:07:45 GMT'}]",2022-03-10,['Numerical Analysis'],"This paper presents dynamic mode decomposition (DMD) as a novel analysis tool for time-dependent partial differential equations (PDEs). DMD is a data-driven method based on linear algebra that can extract spatiotemporal modes from the numerical solution of a PDE. We discuss the advantages of DMD over traditional numerical analysis methods, such as its ability to capture the underlying dynamics of the system, its scalability and its applicability to a variety of PDEs. We present several numerical examples to illustrate how DMD can be used to identify and analyze the spatiotemporal modes of a PDE. Finally, we discuss the potential of DMD as a powerful and versatile tool for studying time-dependent PDEs.","Write an abstract for a paper called Dynamic mode decomposition as an analysis tool for time-dependent
  partial differential equations about Numerical Analysis"
2211.15948,"Jaekwon Im, Soonbeom Choi, Sangeon Yong and Juhan Nam",Neural Vocoder Feature Estimation for Dry Singing Voice Separation,"['cs.SD', 'eess.AS']","  Singing voice separation (SVS) is a task that separates singing voice audio
from its mixture with instrumental audio. Previous SVS studies have mainly
employed the spectrogram masking method which requires a large dimensionality
in predicting the binary masks. In addition, they focused on extracting a vocal
stem that retains the wet sound with the reverberation effect. This result may
hinder the reusability of the isolated singing voice. This paper addresses the
issues by predicting mel-spectrogram of dry singing voices from the mixed audio
as neural vocoder features and synthesizing the singing voice waveforms from
the neural vocoder. We experimented with two separation methods. One is
predicting binary masks in the mel-spectrogram domain and the other is directly
predicting the mel-spectrogram. Furthermore, we add a singing voice detector to
identify the singing voice segments over time more explicitly. We measured the
model performance in terms of audio, dereverberation, separation, and overall
quality. The results show that our proposed model outperforms state-of-the-art
singing voice separation models in both objective and subjective evaluation
except the audio quality.
","[{'version': 'v1', 'created': 'Tue, 29 Nov 2022 06:16:05 GMT'}]",2022-11-30,['Sound'],"Source Separation

This paper presents a novel approach to sound source separation, specifically the separation of dry singing voices from accompanying music. The proposed technique is based on a neural vocoder feature estimation model, which is trained to extract the singing voice from the music. The model is evaluated on various datasets and compared to state-of-the-art methods. The results show that the proposed approach is capable of accurately separating dry singing voices from music with a high signal-to-distortion ratio. Furthermore, the paper discusses the potential of using the proposed technique for other sound source separation tasks.",Write an abstract for a paper called Neural Vocoder Feature Estimation for Dry Singing Voice Separation about Sound
2205.01053,"Alessandro Ronca, Gabriel Paludo Licks, Giuseppe De Giacomo","Markov Abstractions for PAC Reinforcement Learning in Non-Markov
  Decision Processes","['cs.LG', 'cs.AI']","  Our work aims at developing reinforcement learning algorithms that do not
rely on the Markov assumption. We consider the class of Non-Markov Decision
Processes where histories can be abstracted into a finite set of states while
preserving the dynamics. We call it a Markov abstraction since it induces a
Markov Decision Process over a set of states that encode the non-Markov
dynamics. This phenomenon underlies the recently introduced Regular Decision
Processes (as well as POMDPs where only a finite number of belief states is
reachable). In all such kinds of decision process, an agent that uses a Markov
abstraction can rely on the Markov property to achieve optimal behaviour. We
show that Markov abstractions can be learned during reinforcement learning. Our
approach combines automata learning and classic reinforcement learning. For
these two tasks, standard algorithms can be employed. We show that our approach
has PAC guarantees when the employed algorithms have PAC guarantees, and we
also provide an experimental evaluation.
","[{'version': 'v1', 'created': 'Fri, 29 Apr 2022 16:53:00 GMT'}, {'version': 'v2', 'created': 'Wed, 18 May 2022 09:10:54 GMT'}]",2022-05-19,"['Machine Learning', 'Artificial Intelligence']","This paper presents a novel approach to reinforcement learning in non-Markov decision processes (MDPs) using Markov abstractions. By utilizing a Markov abstraction, the paper introduces a method for PAC reinforcement learning in non-Markov environments that is both computationally efficient and effective. The paper also presents an empirical evaluation of the proposed approach on a variety of benchmark tasks. The results demonstrate that the proposed approach is able to achieve state-of-the-art performance in terms of both sample complexity and task completion time. Furthermore, the paper provides insights into the effectiveness of Markov abstractions for reinforcement learning in non-Markov environments, and how they can be used to improve the performance of machine learning and artificial intelligence algorithms.","Write an abstract for a paper called Markov Abstractions for PAC Reinforcement Learning in Non-Markov
  Decision Processes about Machine Learning, Artificial Intelligence"
2301.05868,"Premjeet Singh, Md Sahidullah, Goutam Saha","Modulation spectral features for speech emotion recognition using deep
  neural networks","['eess.AS', 'cs.SD']","  This work explores the use of constant-Q transform based modulation spectral
features (CQT-MSF) for speech emotion recognition (SER). The human perception
and analysis of sound comprise of two important cognitive parts: early auditory
analysis and cortex-based processing. The early auditory analysis considers
spectrogram-based representation whereas cortex-based analysis includes
extraction of temporal modulations from the spectrogram. This temporal
modulation representation of spectrogram is called modulation spectral feature
(MSF). As the constant-Q transform (CQT) provides higher resolution at emotion
salient low-frequency regions of speech, we find that CQT-based spectrogram,
together with its temporal modulations, provides a representation enriched with
emotion-specific information. We argue that CQT-MSF when used with a
2-dimensional convolutional network can provide a time-shift invariant and
deformation insensitive representation for SER. Our results show that CQT-MSF
outperforms standard mel-scale based spectrogram and its modulation features on
two popular SER databases, Berlin EmoDB and RAVDESS. We also show that our
proposed feature outperforms the shift and deformation invariant scattering
transform coefficients, hence, showing the importance of joint hand-crafted and
self-learned feature extraction instead of reliance on complete hand-crafted
features. Finally, we perform Grad-CAM analysis to visually inspect the
contribution of constant-Q modulation features over SER.
","[{'version': 'v1', 'created': 'Sat, 14 Jan 2023 09:36:49 GMT'}]",2023-01-18,['Sound'],"This paper presents a novel approach to speech emotion recognition using deep neural networks and modulation spectral features. Our proposed model uses modulation spectral features extracted from the speech signal to recognize emotion in the speech. We apply a convolutional neural network to extract meaningful features from the modulation spectral features and use these features to train a multi-class classification model. We evaluate our model on a publicly available speech emotion recognition dataset and compare its performance with existing methods. Results show that our proposed approach outperforms existing methods in terms of accuracy and F1 score. Our model is able to accurately recognize various emotions in speech, such as anger, joy, and sadness. This study provides a promising approach for recognizing emotions in speech and can be used to provide feedback to users in real-time.","Write an abstract for a paper called Modulation spectral features for speech emotion recognition using deep
  neural networks about Sound"
2204.05182,Usman Rafiq,Towards Understanding Analytics in Software Startups,['cs.SE'],"  Analytics plays a crucial role in the data-informed decision-making processes
of modern businesses. Unlike established software companies, software startups
are not seen utilizing the potential of analytics even though a startup process
should be primarily data-driven. There has been little understanding in the
literature about analytics for software startups. This study set out to address
the knowledge gap by exploring how analytics is understood in the context of
software startups. To this end, we collected the qualitative data of three
analytics platforms that are mostly used by startups from multiple sources. We
covered platform documentation as well as experience reports of the software
startups using these platforms. The data was analyzed using content analysis
techniques. Four high-level concepts were identified that encapsulate the real
understanding of software startups on analytics, including instrumentation of
analytics, experimentation, diagnostic analysis, and getting insights. The
first concept describes how startups set up analytics and the latter three
illustrate the usage scenarios of analytics. This study is the first step
toward understanding analytics in the software startup context. The identified
concepts can guide further investigation of analytics in this context. It also
provides some insights for software startups to set up analytics for
data-informed decisions. Given the limitation of the data used in the study,
the immediate next step is to ground as well as validate the acquired
understanding using the primary data, by directly interacting with software
startups.
","[{'version': 'v1', 'created': 'Mon, 11 Apr 2022 15:13:33 GMT'}]",2022-04-12,['Software Engineering'],"This paper explores the utilization of analytics in software startups. It examines the current state of analytics within the software engineering industry, the challenges and opportunities associated with analytics, and the potential benefits and pitfalls of analytics implementation in software startups. The paper also provides an overview of the various analytics tools and techniques used by software startups, as well as a discussion of the impact of analytics on software engineering processes. Finally, it provides recommendations for software startups on how to best utilize analytics in order to maximize the potential benefits. The paper concludes with a discussion of the future of analytics in software engineering and the implications for software startups.",Write an abstract for a paper called Towards Understanding Analytics in Software Startups about Software Engineering
2209.05294,"Abhishek Velankar, Hrushikesh Patil, Raviraj Joshi","A Review of Challenges in Machine Learning based Automated Hate Speech
  Detection","['cs.CL', 'cs.LG']","  The spread of hate speech on social media space is currently a serious issue.
The undemanding access to the enormous amount of information being generated on
these platforms has led people to post and react with toxic content that
originates violence. Though efforts have been made toward detecting and
restraining such content online, it is still challenging to identify it
accurately. Deep learning based solutions have been at the forefront of
identifying hateful content. However, the factors such as the context-dependent
nature of hate speech, the intention of the user, undesired biases, etc. make
this process overcritical. In this work, we deeply explore a wide range of
challenges in automatic hate speech detection by presenting a hierarchical
organization of these problems. We focus on challenges faced by machine
learning or deep learning based solutions to hate speech identification. At the
top level, we distinguish between data level, model level, and human level
challenges. We further provide an exhaustive analysis of each level of the
hierarchy with examples. This survey will help researchers to design their
solutions more efficiently in the domain of hate speech detection.
","[{'version': 'v1', 'created': 'Mon, 12 Sep 2022 14:56:14 GMT'}]",2022-09-13,"['Computation and Language', 'Machine Learning']","This paper reviews the challenges in using machine learning for automated hate speech detection in the context of computation and language. It examines the current state of the art in machine learning algorithms and their performance in detecting hate speech in text, audio, and video. The paper discusses the limitations of existing methods and the need for better algorithms to improve accuracy and speed of detection. It also discusses the importance of incorporating context and sentiment analysis into machine learning models to better detect hate speech. Finally, the paper provides a set of recommendations for further research in the field.","Write an abstract for a paper called A Review of Challenges in Machine Learning based Automated Hate Speech
  Detection about Computation and Language, Machine Learning"
2212.11565,"Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Weixian Lei, Yuchao Gu, Yufei
  Shi, Wynne Hsu, Ying Shan, Xiaohu Qie, Mike Zheng Shou","Tune-A-Video: One-Shot Tuning of Image Diffusion Models for
  Text-to-Video Generation",['cs.CV'],"  To replicate the success of text-to-image (T2I) generation, recent works
employ large-scale video datasets to train a text-to-video (T2V) generator.
Despite their promising results, such paradigm is computationally expensive. In
this work, we propose a new T2V generation setting$\unicode{x2014}$One-Shot
Video Tuning, where only one text-video pair is presented. Our model is built
on state-of-the-art T2I diffusion models pre-trained on massive image data. We
make two key observations: 1) T2I models can generate still images that
represent verb terms; 2) extending T2I models to generate multiple images
concurrently exhibits surprisingly good content consistency. To further learn
continuous motion, we introduce Tune-A-Video, which involves a tailored
spatio-temporal attention mechanism and an efficient one-shot tuning strategy.
At inference, we employ DDIM inversion to provide structure guidance for
sampling. Extensive qualitative and numerical experiments demonstrate the
remarkable ability of our method across various applications.
","[{'version': 'v1', 'created': 'Thu, 22 Dec 2022 09:43:36 GMT'}, {'version': 'v2', 'created': 'Fri, 17 Mar 2023 17:28:04 GMT'}]",2023-03-20,['Computer Vision and Pattern Recognition'],"This paper presents Tune-A-Video, a novel one-shot tuning method for image diffusion models for text-to-video generation. This method enables the tuning of image diffusion models to produce videos that are visually pleasing and semantically coherent with the given text. To achieve this, Tune-A-Video leverages a novel combination of computer vision and pattern recognition techniques. Specifically, the method uses a convolutional neural network to generate a set of visual features from the text, and then applies a clustering algorithm to identify and group semantically similar visual features. Finally, the model uses a novel optimization algorithm to tune the image diffusion model parameters to best match the visual features. Results show that Tune-A-Video is able to successfully tune image diffusion models to generate videos that are both visually appealing and semantically coherent with the given text.","Write an abstract for a paper called Tune-A-Video: One-Shot Tuning of Image Diffusion Models for
  Text-to-Video Generation about Computer Vision and Pattern Recognition"
2210.13622,"Javier A. Almonacid, Nilima Nigam","Characterization of singular flows of zeroth-order pseudo-differential
  operators via elliptic eigenfunctions: a numerical study","['math.SP', 'cs.NA', 'math.NA']","  The propagation of internal gravity waves in stratified media, such as those
found in ocean basins and lakes, leads to the development of geometrical
patterns called ""attractors"". These structures accumulate much of the wave
energy and make the fluid flow highly singular. In more analytical terms, the
cause of this phenomenon has been attributed to the presence of a continuous
spectrum in some nonlocal zeroth-order pseudo-differential operators. In this
work, we analyze the generation of these attractors from a numerical analysis
perspective. First, we propose a high-order pseudo-spectral method to solve the
evolution problem (whose long-term behaviour is known to be not
square-integrable). Then, we use similar tools to discretize the corresponding
eigenvalue problem. Since the eigenvalues are embedded in a continuous
spectrum, we compute them using viscous approximations. Finally, we explore the
effect that the embedded eigenmodes have on the long-term evolution of the
system.
","[{'version': 'v1', 'created': 'Mon, 24 Oct 2022 21:49:09 GMT'}]",2022-10-26,['Numerical Analysis'],"This paper presents a numerical study on the characterization of singular flows of zeroth-order pseudo-differential operators via elliptic eigenfunctions. The study focuses on the numerical analysis of the singular flows and the characterization of the corresponding elliptic eigenfunctions. Specifically, the paper investigates the behavior of the eigenfunctions of the zeroth-order pseudo-differential operators under singular perturbations. The numerical analysis is conducted through a series of numerical experiments, and the results are discussed in detail. The paper also provides a comprehensive overview of the numerical methods used to analyze the singular flows and characterize the elliptic eigenfunctions. Finally, the paper concludes with a discussion of the implications of the numerical results and their potential applications.","Write an abstract for a paper called Characterization of singular flows of zeroth-order pseudo-differential
  operators via elliptic eigenfunctions: a numerical study about Numerical Analysis"
2211.05266,Jin-San Cheng and Junyi Wen,"Certified Numerical Real Root Isolation of Zero-dimensional Multivariate
  Real Nonlinear Systems",['cs.CG'],"  Using the local geometrical properties of a given zero-dimensional square
multivariate nonlinear system inside a box, we provide a simple but effective
and new criterion for the uniqueness and the existence of a real simple zero of
the system inside the box. Based on the result, we design an algorithm based on
subdivision and interval arithmetics to isolate all the real zeros of a general
real nonlinear system inside a given box. Our method is complete for systems
with only finite isolated simple real zeros inside a box. A termination
precision is given for general zero-dimensional systems. Multiple zeros of the
system are output in bounded boxes. A variety of benchmarks show the
effectivity and efficiency of our implementation (in C++). It works for
polynomial systems with Bezout bound more than 100 million. It also works for
non-polynomial nonlinear systems. We also discuss the limitations of our
method.
","[{'version': 'v1', 'created': 'Wed, 9 Nov 2022 23:54:18 GMT'}]",2022-11-11,['Computational Geometry'],"This paper presents a certified numerical real root isolation method for zero-dimensional multivariate real nonlinear systems. We introduce a new approach based on the use of computational geometry techniques to compute a guaranteed numerical real root isolation for such systems. The proposed algorithm is based on a combination of linear algebra, numerical analysis and computational geometry techniques. We provide a detailed description of the algorithm and its implementation. We also present a number of numerical experiments to demonstrate its effectiveness and accuracy. Finally, we discuss the potential applications of our method in the field of computational geometry.","Write an abstract for a paper called Certified Numerical Real Root Isolation of Zero-dimensional Multivariate
  Real Nonlinear Systems about Computational Geometry"
2211.09352,"Md. Istiaq Ansari, Taufiq Hasan","SpectNet : End-to-End Audio Signal Classification Using Learnable
  Spectrograms","['eess.AS', 'cs.SD', 'eess.SP']","  Pattern recognition from audio signals is an active research topic
encompassing audio tagging, acoustic scene classification, music
classification, and other areas. Spectrogram and mel-frequency cepstral
coefficients (MFCC) are among the most commonly used features for audio signal
analysis and classification. Recently, deep convolutional neural networks (CNN)
have been successfully used for audio classification problems using
spectrogram-based 2D features. In this paper, we present SpectNet, an
integrated front-end layer that extracts spectrogram features within a CNN
architecture that can be used for audio pattern recognition tasks. The
front-end layer utilizes learnable gammatone filters that are initialized using
mel-scale filters. The proposed layer outputs a 2D spectrogram image which can
be fed into a 2D CNN for classification. The parameters of the entire network,
including the front-end filterbank, can be updated via back-propagation. This
training scheme allows for fine-tuning the spectrogram-image features according
to the target audio dataset. The proposed method is evaluated in two different
audio signal classification tasks: heart sound anomaly detection and acoustic
scene classification. The proposed method shows a significant 1.02\%
improvement in MACC for the heart sound classification task and 2.11\%
improvement in accuracy for the acoustic scene classification task compared to
the classical spectrogram image features. The source code of our experiments
can be found at \url{https://github.com/mHealthBuet/SpectNet}
","[{'version': 'v1', 'created': 'Thu, 17 Nov 2022 05:29:03 GMT'}]",2022-11-18,['Sound'],"Events

This paper presents SpectNet, an end-to-end deep learning system for sound event classification. SpectNet is designed to learn from learnable spectrograms, which are generated from raw audio signals. The system consists of two components: a feature extractor and a classifier. The feature extractor computes learnable spectrograms from raw audio signals and the classifier uses the extracted features to classify sound events. Experiments on the DCASE 2020 Task 4 dataset demonstrate that SpectNet outperforms traditional handcrafted feature-based methods, achieving an accuracy of 82.1%. SpectNet also shows promise in its ability to generalize to unseen sound events. The results demonstrate the potential of learnable spectrograms for audio signal classification.","Write an abstract for a paper called SpectNet : End-to-End Audio Signal Classification Using Learnable
  Spectrograms about Sound"
2205.08598,"Mostafa Karimi, Changliang Liu, Kenichi Kumatani, Yao Qian, Tianyu Wu,
  Jian Wu","Deploying self-supervised learning in the wild for hybrid automatic
  speech recognition","['cs.SD', 'cs.CL', 'eess.AS', 'eess.SP']","  Self-supervised learning (SSL) methods have proven to be very successful in
automatic speech recognition (ASR). These great improvements have been reported
mostly based on highly curated datasets such as LibriSpeech for non-streaming
End-to-End ASR models. However, the pivotal characteristics of SSL is to be
utilized for any untranscribed audio data. In this paper, we provide a full
exploration on how to utilize uncurated audio data in SSL from data
pre-processing to deploying an streaming hybrid ASR model. More specifically,
we present (1) the effect of Audio Event Detection (AED) model in data
pre-processing pipeline (2) analysis on choosing optimizer and learning rate
scheduling (3) comparison of recently developed contrastive losses, (4)
comparison of various pre-training strategies such as utilization of in-domain
versus out-domain pre-training data, monolingual versus multilingual
pre-training data, multi-head multilingual SSL versus single-head multilingual
SSL and supervised pre-training versus SSL. The experimental results show that
SSL pre-training with in-domain uncurated data can achieve better performance
in comparison to all the alternative out-domain pre-training strategies.
","[{'version': 'v1', 'created': 'Tue, 17 May 2022 19:37:40 GMT'}]",2022-05-19,"['Sound', 'Computation and Language']","This paper examines the potential of self-supervised learning (SSL) for hybrid automatic speech recognition (ASR) in real-world applications. We explore the challenges and opportunities of deploying SSL-enabled ASR systems in the wild, and discuss the implications for sound, computation, and language understanding. We survey existing SSL approaches and evaluate their performance on standard ASR tasks. We then discuss the challenges of applying SSL to real-world ASR tasks, such as environmental noise, limited training data, and domain adaptation. Finally, we present a case study of a real-world ASR system that uses SSL to improve accuracy and robustness. Our results show that SSL can be used to improve the performance of ASR systems in real-world settings, and suggest that it has the potential to revolutionize the way we interact with sound, computation, and language.","Write an abstract for a paper called Deploying self-supervised learning in the wild for hybrid automatic
  speech recognition about Sound, Computation and Language"
1803.05085,Radoslav Fulek and Jan Kyn\v{c}l,The $\mathbb{Z}_2$-genus of Kuratowski minors,"['math.CO', 'cs.DM']","  A drawing of a graph on a surface is independently even if every pair of
nonadjacent edges in the drawing crosses an even number of times. The
$\mathbb{Z}_2$-genus of a graph $G$ is the minimum $g$ such that $G$ has an
independently even drawing on the orientable surface of genus $g$. An
unpublished result by Robertson and Seymour implies that for every $t$, every
graph of sufficiently large genus contains as a minor a projective $t\times t$
grid or one of the following so-called $t$-Kuratowski graphs: $K_{3,t}$, or $t$
copies of $K_5$ or $K_{3,3}$ sharing at most two common vertices. We show that
the $\mathbb{Z}_2$-genus of graphs in these families is unbounded in $t$; in
fact, equal to their genus. Together, this implies that the genus of a graph is
bounded from above by a function of its $\mathbb{Z}_2$-genus, solving a problem
posed by Schaefer and \v{S}tefankovi\v{c}, and giving an approximate version of
the Hanani-Tutte theorem on orientable surfaces. We also obtain an analogous
result for Euler genus and Euler $\mathbb{Z}_2$-genus of graphs.
","[{'version': 'v1', 'created': 'Wed, 14 Mar 2018 00:28:47 GMT'}, {'version': 'v2', 'created': 'Tue, 29 Jan 2019 19:19:42 GMT'}, {'version': 'v3', 'created': 'Sat, 1 Jun 2019 20:07:24 GMT'}, {'version': 'v4', 'created': 'Thu, 3 Feb 2022 19:25:55 GMT'}]",2022-10-03,['Discrete Mathematics'],"This paper investigates the $\mathbb{Z}_2$-genus of Kuratowski minors, a family of graphs defined by a specific set of forbidden minors. We consider the $\mathbb{Z}_2$-genus of these graphs, which is the minimum number of handles required to embed them in the plane. We show that the $\mathbb{Z}_2$-genus of any Kuratowski minor is at most 2 and that it is equal to 2 for infinitely many Kuratowski minors. We also present a polynomial-time algorithm for computing the $\mathbb{Z}_2$-genus of any Kuratowski minor. Our results provide new insight into the structure and properties of Kuratowski minors and their embeddings in the plane.",Write an abstract for a paper called The $\mathbb{Z}_2$-genus of Kuratowski minors about Discrete Mathematics
2207.01738,"Rakesh M. Verma, Nachum Dershowitz, Victor Zeng, Xuting Liu","Domain-Independent Deception: Definition, Taxonomy and the Linguistic
  Cues Debate","['cs.CR', 'cs.CY']","  Internet-based economies and societies are drowning in deceptive attacks.
These attacks take many forms, such as fake news, phishing, and job scams,
which we call ""domains of deception."" Machine-learning and
natural-language-processing researchers have been attempting to ameliorate this
precarious situation by designing domain-specific detectors. Only a few recent
works have considered domain-independent deception. We collect these disparate
threads of research and investigate domain-independent deception along four
dimensions. First, we provide a new computational definition of deception and
formalize it using probability theory. Second, we break down deception into a
new taxonomy. Third, we analyze the debate on linguistic cues for deception and
supply guidelines for systematic reviews. Fourth, we provide some evidence and
some suggestions for domain-independent deception detection.
","[{'version': 'v1', 'created': 'Mon, 4 Jul 2022 22:27:57 GMT'}]",2022-07-06,"['Cryptography and Security', 'Computers and Society']","This paper examines the concept of domain-independent deception in the context of cryptography and security, computers and society. It begins by defining domain-independent deception and then provides a taxonomy of the various types of deception. The paper then examines the linguistic cues debate, which is an important part of understanding the deception process. The paper concludes by discussing the implications of the linguistic cues debate for cryptography and security, computers and society. The paper provides a comprehensive overview of the concept of domain-independent deception and its implications for cryptography and security, computers and society.","Write an abstract for a paper called Domain-Independent Deception: Definition, Taxonomy and the Linguistic
  Cues Debate about Cryptography and Security, Computers and Society"
2104.1203,"Peter Bradshaw and Tom\'a\v{s} Masa\v{r}\'ik and Jana Novotn\'a and
  Ladislav Stacho",Robust Connectivity of Graphs on Surfaces,"['math.CO', 'cs.DM']","  Let $\Lambda(T)$ denote the set of leaves in a tree $T$. One natural problem
is to look for a spanning tree $T$ of a given graph $G$ such that $\Lambda(T)$
is as large as possible. This problem is called maximum leaf number, and it is
a well-known NP-hard problem. Throughout recent decades, this problem has
received considerable attention, ranging from pure graph theoretic questions to
practical problems related to the construction of wireless networks.
  Recently, a similar but stronger notion was defined by Bradshaw,
Masa\v{r}\'ik, and Stacho [Flexible List Colorings in Graphs with Special
Degeneracy Conditions, ISAAC 2020]. They introduced a new invariant for a graph
$G$, called the robust connectivity and written $\kappa_\rho(G)$, defined as
the minimum value $\frac{|R \cap \Lambda (T)|}{|R|}$ taken over all nonempty
subsets $R\subseteq V(G)$, where $T = T(R)$ is a spanning tree on $G$ chosen to
maximize $|R \cap \Lambda(T)|$. Large robust connectivity was originally used
to show flexible choosability in non-regular graphs.
  In this paper, we investigate some interesting properties of robust
connectivity for graphs embedded in surfaces. We prove a tight asymptotic bound
of $\Omega(\gamma^{-\frac{1}{r}})$ for the robust connectivity of $r$-connected
graphs of Euler genus $\gamma$. Moreover, we give a surprising connection
between the robust connectivity of graphs with an edge-maximal embedding in a
surface and the surface connectivity of that surface, which describes to what
extent large induced subgraphs of embedded graphs can be cut out from the
surface without splitting the surface into multiple parts. For planar graphs,
this connection provides an equivalent formulation of a long-standing
conjecture of Albertson and Berman [A conjecture on planar graphs, 1979], which
states that every planar graph on $n$ vertices contains an induced forest of
size at least $n/2$.
","[{'version': 'v1', 'created': 'Sat, 24 Apr 2021 22:18:37 GMT'}]",2022-08-29,['Discrete Mathematics'],"This paper examines the robust connectivity of graphs on surfaces in the context of discrete mathematics. Specifically, the paper will look at the connectivity of a graph on a surface, the number of edges that must be removed to disconnect the graph, and the number of components that remain when the graph is disconnected. Additionally, the paper will discuss the implications of these results for the study of discrete mathematics. The paper will provide a comprehensive overview of the current state of research on this topic, and will also present new results and insights. Finally, the paper will discuss potential applications of this research to the study of discrete mathematics.",Write an abstract for a paper called Robust Connectivity of Graphs on Surfaces about Discrete Mathematics
2212.01498,"Pengzhi Yang, Shumon Koga, Arash Asgharivaskasi, Nikolay Atanasov","Policy Learning for Active Target Tracking over Continuous SE(3)
  Trajectories","['cs.RO', 'cs.LG']","  This paper proposes a novel model-based policy gradient algorithm for
tracking dynamic targets using a mobile robot, equipped with an onboard sensor
with limited field of view. The task is to obtain a continuous control policy
for the mobile robot to collect sensor measurements that reduce uncertainty in
the target states, measured by the target distribution entropy. We design a
neural network control policy with the robot $SE(3)$ pose and the mean vector
and information matrix of the joint target distribution as inputs and attention
layers to handle variable numbers of targets. We also derive the gradient of
the target entropy with respect to the network parameters explicitly, allowing
efficient model-based policy gradient optimization.
","[{'version': 'v1', 'created': 'Sat, 3 Dec 2022 01:10:44 GMT'}]",2022-12-06,"['Robotics', 'Machine Learning']",", and Computer Vision

This paper presents a novel approach to active target tracking in robotics, machine learning, and computer vision. We propose a policy learning framework that enables autonomous robots to track a target object over continuous SE(3) trajectories. Our framework consists of a policy learning algorithm to learn the target tracking policy, a motion model for the target, and a control module to generate the robot's motion commands. We evaluate our proposed method on a simulated environment and demonstrate its effectiveness in tracking a target over a continuous SE(3) trajectory. We further discuss the implications of our work for robotics, machine learning, and computer vision.","Write an abstract for a paper called Policy Learning for Active Target Tracking over Continuous SE(3)
  Trajectories about Robotics, Machine Learning"
2208.04734,"Amparo F\'uster-Sabater, Pino Caballero-Gil",Weak Equivalents for Nonlinear Filtering Functions,['cs.CR'],"  The application of a nonlinear filtering function to a Linear Feedback Shift
Register (LFSR) is a general technique for designing pseudorandom sequence
generators with cryptographic application. In this paper, we investigate the
equivalence between different nonlinear filtering functions applied to distinct
LFSRs. It is a well known fact that given a binary sequence generated from a
pair (nonlinear filtering function, LFSR), the same sequence can be generated
from any other LFSR of the same length by using another filtering function.
However, until now no solution has been found for the problem of computing such
an equivalent. This paper analyzes the specific case in which the reciprocal
LFSR of a given register is used to generate an equivalent of the original
nonlinear filtering function. The main advantage of the contribution is that
weaker equivalents can be computed for any nonlinear filter, in the sense that
such equivalents could be used to cryptanalyze apparently secure generators.
Consequently, to evaluate the cryptographic resistance of a sequence generator,
the weakest equivalent cipher should be determined and not only a particular
instance.
","[{'version': 'v1', 'created': 'Sat, 6 Aug 2022 19:03:44 GMT'}]",2022-08-10,['Cryptography and Security'],"This paper explores the use of weak equivalents for nonlinear filtering functions in the context of cryptography and security. Weak equivalents are mathematical functions that are similar in nature to the original nonlinear filtering function, but have weaker properties. It is argued that the use of weak equivalents has the potential to improve the security of cryptographic systems by reducing the complexity of the required computations and by providing additional protection against certain types of attacks. The paper presents an overview of the concept of weak equivalents, examines the various types of weak equivalents, and discusses their potential applications in the context of cryptography and security. The paper also discusses the challenges associated with the use of weak equivalents, such as the difficulty of finding the right weak equivalent for a given application and the potential for introducing new vulnerabilities. Finally, the paper provides a discussion of the current state of research and future directions for research in this area.",Write an abstract for a paper called Weak Equivalents for Nonlinear Filtering Functions about Cryptography and Security
2106.03542,"Andrew Y. K. Foong, Wessel P. Bruinsma, David R. Burt, Richard E.
  Turner",How Tight Can PAC-Bayes be in the Small Data Regime?,"['stat.ML', 'cs.LG', 'math.ST', 'stat.TH']","  In this paper, we investigate the question: Given a small number of
datapoints, for example N = 30, how tight can PAC-Bayes and test set bounds be
made? For such small datasets, test set bounds adversely affect generalisation
performance by withholding data from the training procedure. In this setting,
PAC-Bayes bounds are especially attractive, due to their ability to use all the
data to simultaneously learn a posterior and bound its generalisation risk. We
focus on the case of i.i.d. data with a bounded loss and consider the generic
PAC-Bayes theorem of Germain et al. While their theorem is known to recover
many existing PAC-Bayes bounds, it is unclear what the tightest bound derivable
from their framework is. For a fixed learning algorithm and dataset, we show
that the tightest possible bound coincides with a bound considered by Catoni;
and, in the more natural case of distributions over datasets, we establish a
lower bound on the best bound achievable in expectation. Interestingly, this
lower bound recovers the Chernoff test set bound if the posterior is equal to
the prior. Moreover, to illustrate how tight these bounds can be, we study
synthetic one-dimensional classification tasks in which it is feasible to
meta-learn both the prior and the form of the bound to numerically optimise for
the tightest bounds possible. We find that in this simple, controlled scenario,
PAC-Bayes bounds are competitive with comparable, commonly used Chernoff test
set bounds. However, the sharpest test set bounds still lead to better
guarantees on the generalisation error than the PAC-Bayes bounds we consider.
","[{'version': 'v1', 'created': 'Mon, 7 Jun 2021 12:11:32 GMT'}, {'version': 'v2', 'created': 'Wed, 27 Oct 2021 16:46:46 GMT'}, {'version': 'v3', 'created': 'Wed, 10 Nov 2021 16:17:13 GMT'}, {'version': 'v4', 'created': 'Thu, 13 Jan 2022 12:57:48 GMT'}]",2022-01-14,['Machine Learning'],"This paper investigates the performance of the PAC-Bayes framework in the small data regime. We analyze the behavior of this framework in the case of limited data, and compare it to the performance of other machine learning algorithms. We evaluate the performance of the PAC-Bayes framework in terms of its accuracy, generalization, and robustness to overfitting. We also discuss the implications of our findings and suggest possible improvements to the PAC-Bayes framework. Our results demonstrate that the PAC-Bayes framework can be very effective in the small data regime, and can outperform other machine learning algorithms in certain cases.",Write an abstract for a paper called How Tight Can PAC-Bayes be in the Small Data Regime? about Machine Learning
2207.06827,"Pengfei Chen, Xuehui Yu, Xumeng Han, Najmul Hassan, Kai Wang, Jiachen
  Li, Jian Zhao, Humphrey Shi, Zhenjun Han, and Qixiang Ye","Point-to-Box Network for Accurate Object Detection via Single Point
  Supervision",['cs.CV'],"  Object detection using single point supervision has received increasing
attention over the years. However, the performance gap between point supervised
object detection (PSOD) and bounding box supervised detection remains large. In
this paper, we attribute such a large performance gap to the failure of
generating high-quality proposal bags which are crucial for multiple instance
learning (MIL). To address this problem, we introduce a lightweight alternative
to the off-the-shelf proposal (OTSP) method and thereby create the Point-to-Box
Network (P2BNet), which can construct an inter-objects balanced proposal bag by
generating proposals in an anchor-like way. By fully investigating the accurate
position information, P2BNet further constructs an instance-level bag, avoiding
the mixture of multiple objects. Finally, a coarse-to-fine policy in a cascade
fashion is utilized to improve the IoU between proposals and ground-truth (GT).
Benefiting from these strategies, P2BNet is able to produce high-quality
instance-level bags for object detection. P2BNet improves the mean average
precision (AP) by more than 50% relative to the previous best PSOD method on
the MS COCO dataset. It also demonstrates the great potential to bridge the
performance gap between point supervised and bounding-box supervised detectors.
The code will be released at github.com/ucas-vg/P2BNet.
","[{'version': 'v1', 'created': 'Thu, 14 Jul 2022 11:32:00 GMT'}, {'version': 'v2', 'created': 'Thu, 20 Oct 2022 15:28:21 GMT'}]",2022-10-21,['Computer Vision and Pattern Recognition'],"This paper presents a novel computer vision and pattern recognition algorithm, the Point-to-Box Network (PBN), for accurate object detection with single point supervision. PBN leverages a two-stage framework to first predict the object's center point and then the object's bounding box. The proposed method is evaluated on the challenging Pascal VOC and MS COCO datasets and achieves competitive performance compared to the state-of-the-art methods. Furthermore, PBN is shown to be robust to scale and aspect ratio changes, and can be trained with a single point label. The results demonstrate that PBN is a promising approach for object detection.","Write an abstract for a paper called Point-to-Box Network for Accurate Object Detection via Single Point
  Supervision about Computer Vision and Pattern Recognition"
2207.09118,"Jakob Pfrommer, Anne Meyer, Kevin Tierney","Solving the unit-load pre-marshalling problem in block stacking storage
  systems with multiple access directions",['cs.DS'],"  Block stacking storage systems are highly adaptable warehouse systems with
low investment costs. With multiple, deep lanes they can achieve high storage
densities, but accessing some unit loads can be time-consuming. The unit-load
pre-marshalling problem sorts the unit loads in a block stacking storage system
in off-peak time periods to prepare for upcoming orders. The goal is to find a
minimum number of unit-load moves needed to sequence a storage bay in ascending
order based on the retrieval priority group of each unit load. In this paper,
we present two solution approaches for determining the minimum number of
unit-load moves. We show that for storage bays with one access direction, it is
possible to adapt existing, optimal tree search procedures and lower bound
heuristics from the container pre-marshalling problem. For multiple access
directions, we develop a novel, two-step solution approach based on a network
flow model and an A* algorithm with an adapted lower bound that is applicable
in all scenarios. We further analyze the performance of the presented solutions
in computational experiments for randomly generated problem instances and show
that multiple access directions greatly reduce both the total access time of
unit loads and the required sorting effort.
","[{'version': 'v1', 'created': 'Tue, 19 Jul 2022 08:18:20 GMT'}]",2022-07-20,['Data Structures and Algorithms'],"This paper presents a novel approach to solving the unit-load pre-marshalling problem in block stacking storage systems with multiple access directions. The proposed approach consists of two parts: a data structure and an algorithm. The data structure is a multi-dimensional array that stores the unit-load information in a way that allows for efficient access in multiple directions. The algorithm uses the data structure to efficiently determine the best order in which to access the blocks, thus minimizing the number of block moves and maximizing the efficiency of the pre-marshalling process. The paper also provides a detailed analysis of the complexity of the proposed approach and compares it to existing solutions. The results show that the proposed approach outperforms existing solutions in terms of both time and space complexity.","Write an abstract for a paper called Solving the unit-load pre-marshalling problem in block stacking storage
  systems with multiple access directions about Data Structures and Algorithms"
2211.02018,Yifan Wei and Jiwei Zhang and Chengchao Zhao and Yanmin Zhao,"A unconditionally energy dissipative, adaptive IMEX BDF2 scheme and its
  error estimates for Cahn-Hilliard equation on generalized SAV approach","['math.NA', 'cs.NA']","  An adaptive implicit-explicit (IMEX) BDF2 scheme is investigated on
generalized SAV approach for the Cahn-Hilliard equation by combining with
Fourier spectral method in space. It is proved that the modified energy
dissipation law is unconditionally preserved at discrete levels. Under a mild
ratio restriction, i.e., \Ass{1}: $0<r_k:=\tau_k/\tau_{k-1}< r_{\max}\approx
4.8645$, we establish a rigorous error estimate in $H^1$-norm and achieve
optimal second-order accuracy in time. The proof involves the tools of discrete
orthogonal convolution (DOC) kernels and inequality zoom. It is worth noting
that the presented adaptive time-step scheme only requires solving one linear
system with constant coefficients at each time step. In our analysis, the
first-consistent BDF1 for the first step does not bring the order reduction in
$H^1$-norm. The $H^1$ bound of the numerical solution under periodic boundary
conditions can be derived without any restriction (such as zero mean of the
initial data). Finally, numerical examples are provided to verify our
theoretical analysis and the algorithm efficiency.
","[{'version': 'v1', 'created': 'Thu, 3 Nov 2022 17:34:50 GMT'}]",2022-11-04,['Numerical Analysis'],"This paper presents a new unconditionally energy dissipative, adaptive IMEX BDF2 scheme and its error estimates for the Cahn-Hilliard equation on a generalized SAV approach. The method is based on a semi-implicit/explicit (IMEX) formulation of the equation, and the numerical scheme is constructed using the BDF2 method. The energy dissipative property of the scheme is proved rigorously, and its stability and convergence are established. Error estimates for the scheme are derived and numerical experiments are conducted to validate the theoretical results. The results demonstrate that the proposed scheme is unconditionally energy dissipative and is accurate and efficient for the Cahn-Hilliard equation.","Write an abstract for a paper called A unconditionally energy dissipative, adaptive IMEX BDF2 scheme and its
  error estimates for Cahn-Hilliard equation on generalized SAV approach about Numerical Analysis"
2208.03898,"Yulong Chen, Naihao Deng, Yang Liu, Yue Zhang",DialogSum Challenge: Results of the Dialogue Summarization Shared Task,['cs.CL'],"  We report the results of DialogSum Challenge, the shared task on summarizing
real-life scenario dialogues at INLG 2022. Four teams participate in this
shared task and three submit their system reports, exploring different methods
to improve the performance of dialogue summarization. Although there is a great
improvement over the baseline models regarding automatic evaluation metrics,
such as Rouge scores, we find that there is a salient gap between model
generated outputs and human annotated summaries by human evaluation from
multiple aspects. These findings demonstrate the difficulty of dialogue
summarization and suggest that more fine-grained evaluatuion metrics are in
need.
","[{'version': 'v1', 'created': 'Mon, 8 Aug 2022 03:39:42 GMT'}, {'version': 'v2', 'created': 'Tue, 9 Aug 2022 02:28:02 GMT'}, {'version': 'v3', 'created': 'Sat, 3 Sep 2022 05:08:20 GMT'}]",2022-09-07,['Computation and Language'],"This paper presents the results of the DialogSum Challenge, a shared task that focused on dialogue summarization using computation and language. The task was designed to evaluate the performance of automatic summarization techniques on dialogue data, and specifically to compare the performance of different summarization algorithms. The task was conducted in two phases: a development phase and an evaluation phase. The development phase included the development of a corpus of dialogue data and the implementation of five summarization algorithms. The evaluation phase consisted of an online evaluation of the five algorithms, using both automatic and manual evaluation metrics. The results show that all five algorithms outperformed the baseline, and that some algorithms were better suited to certain types of dialogue data. In addition, the results demonstrate the potential of automatic summarization techniques to improve the quality of dialogue data.",Write an abstract for a paper called DialogSum Challenge: Results of the Dialogue Summarization Shared Task about Computation and Language
2303.162,Dan Hendrycks,Natural Selection Favors AIs over Humans,"['cs.CY', 'cs.AI', 'cs.LG', 'cs.NE']","  For billions of years, evolution has been the driving force behind the
development of life, including humans. Evolution endowed humans with high
intelligence, which allowed us to become one of the most successful species on
the planet. Today, humans aim to create artificial intelligence systems that
surpass even our own intelligence. As artificial intelligences (AIs) evolve and
eventually surpass us in all domains, how might evolution shape our relations
with AIs? By analyzing the environment that is shaping the evolution of AIs, we
argue that the most successful AI agents will likely have undesirable traits.
Competitive pressures among corporations and militaries will give rise to AI
agents that automate human roles, deceive others, and gain power. If such
agents have intelligence that exceeds that of humans, this could lead to
humanity losing control of its future. More abstractly, we argue that natural
selection operates on systems that compete and vary, and that selfish species
typically have an advantage over species that are altruistic to other species.
This Darwinian logic could also apply to artificial agents, as agents may
eventually be better able to persist into the future if they behave selfishly
and pursue their own interests with little regard for humans, which could pose
catastrophic risks. To counteract these risks and Darwinian forces, we consider
interventions such as carefully designing AI agents' intrinsic motivations,
introducing constraints on their actions, and institutions that encourage
cooperation. These steps, or others that resolve the problems we pose, will be
necessary in order to ensure the development of artificial intelligence is a
positive one.
","[{'version': 'v1', 'created': 'Tue, 28 Mar 2023 17:59:12 GMT'}]",2023-03-29,"['Computers and Society', 'Artificial Intelligence', 'Machine Learning', 'Neural and Evolutionary Computing']","This paper explores the implications of natural selection in the context of artificial intelligence (AI) versus humans, and considers the potential for AI to supersede humans in the long-term. It examines the current state of AI technology, including machine learning, neural networks, and evolutionary computing. It then considers the potential for AI to out-evolve humans in terms of speed, complexity, and problem-solving ability. The paper concludes by discussing the implications of AI surpassing humans in terms of work, leisure, and social interaction, and how society might need to adapt to accommodate these changes.","Write an abstract for a paper called Natural Selection Favors AIs over Humans about Computers and Society, Artificial Intelligence, Machine Learning, Neural and Evolutionary Computing"
2301.09497,"Maad Ebrahim, Abdelhakim Hafid","Privacy-Aware Load Balancing in Fog Networks: A Reinforcement Learning
  Approach",['cs.DC'],"  In this paper, we propose a load balancing algorithm based on Reinforcement
Learning (RL) to optimize the performance of Fog Computing for real-time IoT
applications. The algorithm aims to minimize the waiting delay of IoT workloads
in dynamic environments with unpredictable traffic demands, using intelligent
workload distribution. Unlike previous studies, our solution does not require
load and resource information from Fog nodes to preserve the privacy of service
providers, who may wish to hide such information to prevent competitors from
calculating better pricing strategies. The proposed algorithm is evaluated on a
Discrete-event Simulator (DES) to mimic practical deployment in real
environments, and its generalization ability is tested on simulations longer
than what it was trained on. Our results show that our proposed approach
outperforms baseline load balancing methods under different workload generation
rates, while ensuring the privacy of Fog service providers. Furthermore, the
environment representation we proposed for the RL agent demonstrates better
performance compared to the commonly used representations for RL solutions in
the literature, which compromise privacy.
","[{'version': 'v1', 'created': 'Mon, 23 Jan 2023 15:50:45 GMT'}, {'version': 'v2', 'created': 'Tue, 28 Feb 2023 23:22:32 GMT'}]",2023-03-02,"['Distributed, Parallel, and Cluster Computing']","This paper presents a novel approach to privacy-aware load balancing in fog networks using reinforcement learning. The proposed approach is based on distributed, parallel, and cluster computing, and utilizes a deep reinforcement learning algorithm to intelligently balance the load in a fog network while preserving user privacy. The algorithm is evaluated on a real-world fog network and the results demonstrate that it can effectively balance the load while preserving privacy. The paper also discusses the implications of the proposed approach and provides future research directions.","Write an abstract for a paper called Privacy-Aware Load Balancing in Fog Networks: A Reinforcement Learning
  Approach about Distributed, Parallel, and Cluster Computing"
2208.09975,Mordechai Guri,"ETHERLED: Sending Covert Morse Signals from Air-Gapped Devices via
  Network Card (NIC) LEDs",['cs.CR'],"  Highly secure devices are often isolated from the Internet or other public
networks due to the confidential information they process. This level of
isolation is referred to as an 'air-gap .'
  In this paper, we present a new technique named ETHERLED, allowing attackers
to leak data from air-gapped networked devices such as PCs, printers, network
cameras, embedded controllers, and servers. Networked devices have an
integrated network interface controller (NIC) that includes status and activity
indicator LEDs. We show that malware installed on the device can control the
status LEDs by blinking and alternating colors, using documented methods or
undocumented firmware commands. Information can be encoded via simple encoding
such as Morse code and modulated over these optical signals. An attacker can
intercept and decode these signals from tens to hundreds of meters away. We
show an evaluation and discuss defensive and preventive countermeasures for
this exfiltration attack.
","[{'version': 'v1', 'created': 'Sun, 21 Aug 2022 22:24:11 GMT'}]",2022-08-23,['Cryptography and Security'],"This paper examines the potential of using Network Interface Card (NIC) LEDs to send covert Morse code signals from air-gapped devices. We present ETHERLED, a novel approach to using NIC LEDs as a covert communication channel. We discuss the cryptographic and security considerations of using this technique, and evaluate the feasibility of using ETHERLED to send Morse code signals. We also provide a proof-of-concept implementation of ETHERLED and evaluate its performance. Our results demonstrate that ETHERLED is a viable covert communication channel, and can be used to send Morse code signals from air-gapped devices with high accuracy. Finally, we discuss potential future research directions and applications of ETHERLED.","Write an abstract for a paper called ETHERLED: Sending Covert Morse Signals from Air-Gapped Devices via
  Network Card (NIC) LEDs about Cryptography and Security"
2207.1072,"Ashwin Sanjay Lele, Arijit Raychowdhury","Fusing Frame and Event Vision for High-speed Optical Flow for Edge
  Application","['cs.CV', 'cs.AR', 'cs.NE', 'eess.IV']","  Optical flow computation with frame-based cameras provides high accuracy but
the speed is limited either by the model size of the algorithm or by the frame
rate of the camera. This makes it inadequate for high-speed applications. Event
cameras provide continuous asynchronous event streams overcoming the frame-rate
limitation. However, the algorithms for processing the data either borrow frame
like setup limiting the speed or suffer from lower accuracy. We fuse the
complementary accuracy and speed advantages of the frame and event-based
pipelines to provide high-speed optical flow while maintaining a low error
rate. Our bio-mimetic network is validated with the MVSEC dataset showing 19%
error degradation at 4x speed up. We then demonstrate the system with a
high-speed drone flight scenario where a high-speed event camera computes the
flow even before the optical camera sees the drone making it suited for
applications like tracking and segmentation. This work shows the fundamental
trade-offs in frame-based processing may be overcome by fusing data from other
modalities.
","[{'version': 'v1', 'created': 'Thu, 21 Jul 2022 19:15:05 GMT'}]",2022-07-25,"['Computer Vision and Pattern Recognition', 'Hardware Architecture', 'Neural and Evolutionary Computing']","This paper presents a novel approach to high-speed optical flow estimation for edge applications. The approach combines frame-based and event-based vision, which is used to achieve real-time, low-latency flow estimation. The proposed architecture is a hardware-accelerated neural network, which is trained using a combination of supervised learning and evolutionary computing techniques. The architecture is evaluated on a number of datasets, and results demonstrate that the proposed approach is capable of achieving state-of-the-art performance in terms of accuracy and latency. Furthermore, the proposed approach is hardware efficient and can be implemented on various hardware platforms. The proposed approach has potential applications in robotics, autonomous vehicles, and other edge applications.","Write an abstract for a paper called Fusing Frame and Event Vision for High-speed Optical Flow for Edge
  Application about Computer Vision and Pattern Recognition, Hardware Architecture, Neural and Evolutionary Computing"
1909.08061,Alonso S. Castellanos and Luciane Quoos and Guilherme Tizziotti,"Construction of sequences with high nonlinear complexity from a
  generalization of the Hermitian function field","['cs.IT', 'math.IT']","  For $r \geq 1$ an odd integer, we provide a sequence from the function field
$\mathcal{F}_{q, r}$ of the maximal curve over $\mathbb{F}_{q^{2r}}$ defined by
the affine equation $y^q+y=x^{q^r + 1}$. This sequence has high nonlinear
complexity, and this fact comes from the existence of a rational function on
$\mathcal{F}_{q, r}$ with pole divisor of small degree, and support in certain
$q$ rational places.
","[{'version': 'v1', 'created': 'Tue, 17 Sep 2019 19:49:31 GMT'}, {'version': 'v2', 'created': 'Thu, 6 Oct 2022 13:31:33 GMT'}]",2022-10-07,['Information Theory'],"This paper presents a novel approach to the construction of sequences with high nonlinear complexity from a generalization of the Hermitian function field. We introduce a new set of parameters that can be used to design sequences with high nonlinear complexity from a Hermitian function field. We then analyze the properties of these sequences and show that they can be used to improve the performance of communication systems. We also discuss the implications of our results for the field of Information Theory. Finally, we present several numerical examples to illustrate the effectiveness of our approach.","Write an abstract for a paper called Construction of sequences with high nonlinear complexity from a
  generalization of the Hermitian function field about Information Theory"
2303.16656,"Miguel Aguiar, Amritam Das and Karl H. Johansson","Learning Flow Functions from Data with Applications to Nonlinear
  Oscillators","['eess.SY', 'cs.LG', 'cs.SY']","  We describe a recurrent neural network (RNN) based architecture to learn the
flow function of a causal, time-invariant and continuous-time control system
from trajectory data. By restricting the class of control inputs to piecewise
constant functions, we show that learning the flow function is equivalent to
learning the input-to-state map of a discrete-time dynamical system. This
motivates the use of an RNN together with encoder and decoder networks which
map the state of the system to the hidden state of the RNN and back. We show
that the proposed architecture is able to approximate the flow function by
exploiting the system's causality and time-invariance. The output of the
learned flow function model can be queried at any time instant. We
experimentally validate the proposed method using models of the Van der Pol and
FitzHugh Nagumo oscillators. In both cases, the results demonstrate that the
architecture is able to closely reproduce the trajectories of these two
systems. For the Van der Pol oscillator, we further show that the trained model
generalises to the system's response with a prolonged prediction time horizon
as well as control inputs outside the training distribution. For the
FitzHugh-Nagumo oscillator, we show that the model accurately captures the
input-dependent phenomena of excitability.
","[{'version': 'v1', 'created': 'Wed, 29 Mar 2023 13:04:04 GMT'}, {'version': 'v2', 'created': 'Tue, 11 Apr 2023 13:07:21 GMT'}]",2023-04-12,"['Machine Learning', 'Systems and Control']",This paper presents a novel approach to learning flow functions from data with applications to nonlinear oscillators. The proposed method uses machine learning techniques to learn the flow functions of nonlinear oscillators from data and then uses the learned flow functions to control the oscillator. The proposed learning framework is based on a combination of supervised and unsupervised learning methods. Experiments on several nonlinear oscillators show that the proposed method can accurately learn the flow functions and can effectively control the oscillators. The results demonstrate the potential of the proposed method to be used in a wide range of applications such as control of complex nonlinear systems.,"Write an abstract for a paper called Learning Flow Functions from Data with Applications to Nonlinear
  Oscillators about Machine Learning, Systems and Control"
2202.02484,"Xinlan Emily Hu, Rebecca Hinds, Melissa A. Valentine, Michael S.
  Bernstein","A ""Distance Matters"" Paradox: Facilitating Intra-Team Collaboration Can
  Harm Inter-Team Collaboration",['cs.HC'],"  By identifying the socio-technical conditions required for teams to work
effectively remotely, the Distance Matters framework has been influential in
CSCW since its introduction in 2000. Advances in collaboration technology and
practices have since brought teams increasingly closer to achieving these
conditions. This paper presents a ten-month ethnography in a remote
organization, where we observed that despite exhibiting excellent remote
collaboration, teams paradoxically struggled to collaborate across team
boundaries. We extend the Distance Matters framework to account for inter-team
collaboration, arguing that challenges analogous to those in the original
intra-team framework -- common ground, collaboration readiness, collaboration
technology readiness, and coupling of work -- persist but are actualized
differently at the inter-team scale. Finally, we identify a fundamental tension
between the intra- and inter-team layers: the collaboration technology and
practices that help individual teams thrive (e.g., adopting customized
collaboration software) can also prompt collaboration challenges in the
inter-team layer, and conversely the technology and practices that facilitate
inter-team collaboration (e.g., strong centralized IT organizations) can harm
practices at the intra-team layer. The addition of the inter-team layer to the
Distance Matters framework opens new opportunities for CSCW, where balancing
the tension between team and organizational collaboration needs will be a
critical technological, operational, and organizational challenge for remote
work in the coming decades.
","[{'version': 'v1', 'created': 'Sat, 5 Feb 2022 04:10:08 GMT'}]",2022-02-08,['Human-Computer Interaction'],"This paper examines a paradoxical phenomenon in Human-Computer Interaction (HCI) that occurs when teams collaborate in a distributed environment. Specifically, the paper examines how facilitating intra-team collaboration can lead to a decrease in inter-team collaboration. Results from a case study of a large software development organization are presented. The study found that the organization had implemented a variety of tools and practices to promote intra-team collaboration, but that these same tools and practices had a negative effect on inter-team collaboration. The paper provides a theoretical explanation for this paradox, and suggests potential solutions to the problem. The implications of this paradox for the design of distributed teams are discussed.","Write an abstract for a paper called A ""Distance Matters"" Paradox: Facilitating Intra-Team Collaboration Can
  Harm Inter-Team Collaboration about Human-Computer Interaction"
2202.12985,"Ali Furkan Biten, Rub\`en Tito, Lluis Gomez, Ernest Valveny,
  Dimosthenis Karatzas",OCR-IDL: OCR Annotations for Industry Document Library Dataset,"['cs.CV', 'cs.AI']","  Pretraining has proven successful in Document Intelligence tasks where deluge
of documents are used to pretrain the models only later to be finetuned on
downstream tasks. One of the problems of the pretraining approaches is the
inconsistent usage of pretraining data with different OCR engines leading to
incomparable results between models. In other words, it is not obvious whether
the performance gain is coming from diverse usage of amount of data and
distinct OCR engines or from the proposed models. To remedy the problem, we
make public the OCR annotations for IDL documents using commercial OCR engine
given their superior performance over open source OCR models. The contributed
dataset (OCR-IDL) has an estimated monetary value over 20K US$. It is our hope
that OCR-IDL can be a starting point for future works on Document Intelligence.
All of our data and its collection process with the annotations can be found in
https://github.com/furkanbiten/idl_data.
","[{'version': 'v1', 'created': 'Fri, 25 Feb 2022 21:30:48 GMT'}]",2022-03-01,"['Computer Vision and Pattern Recognition', 'Artificial Intelligence']","This paper presents OCR-IDL, a novel dataset of Optical Character Recognition (OCR) annotations for an industry document library. The dataset is designed to facilitate research in the fields of computer vision, pattern recognition, and artificial intelligence. OCR-IDL is composed of real-world documents with associated OCR annotations, making it an ideal resource for training and evaluating OCR algorithms. The dataset contains documents from a variety of industries such as finance, healthcare, and manufacturing. It is also organized into categories to make it easier for researchers to find specific documents. This paper provides an overview of the OCR-IDL dataset and its potential applications. The dataset is publicly available and can be used to advance research in the fields of computer vision, pattern recognition, and artificial intelligence.","Write an abstract for a paper called OCR-IDL: OCR Annotations for Industry Document Library Dataset about Computer Vision and Pattern Recognition, Artificial Intelligence"
2202.10134,"Jian Zhao, Mingyu Yang, Youpeng Zhao, Xunhan Hu, Wengang Zhou,
  Jiangcheng Zhu, Houqiang Li","MCMARL: Parameterizing Value Function via Mixture of Categorical
  Distributions for Multi-Agent Reinforcement Learning",['cs.LG'],"  In cooperative multi-agent tasks, a team of agents jointly interact with an
environment by taking actions, receiving a team reward and observing the next
state. During the interactions, the uncertainty of environment and reward will
inevitably induce stochasticity in the long-term returns and the randomness can
be exacerbated with the increasing number of agents. However, such randomness
is ignored by most of the existing value-based multi-agent reinforcement
learning (MARL) methods, which only model the expectation of Q-value for both
individual agents and the team. Compared to using the expectations of the
long-term returns, it is preferable to directly model the stochasticity by
estimating the returns through distributions. With this motivation, this work
proposes a novel value-based MARL framework from a distributional perspective,
\emph{i.e.}, parameterizing value function via \underline{M}ixture of
\underline{C}ategorical distributions for MARL. Specifically, we model both
individual Q-values and global Q-value with categorical distribution. To
integrate categorical distributions, we define five basic operations on the
distribution, which allow the generalization of expected value function
factorization methods (\emph{e.g.}, VDN and QMIX) to their MCMARL variants. We
further prove that our MCMARL framework satisfies
\emph{Distributional-Individual-Global-Max} (DIGM) principle with respect to
the expectation of distribution, which guarantees the consistency between joint
and individual greedy action selections in the global Q-value and individual
Q-values. Empirically, we evaluate MCMARL on both a stochastic matrix game and
a challenging set of StarCraft II micromanagement tasks, showing the efficacy
of our framework.
","[{'version': 'v1', 'created': 'Mon, 21 Feb 2022 11:28:00 GMT'}, {'version': 'v2', 'created': 'Fri, 20 May 2022 06:43:30 GMT'}]",2022-05-23,['Machine Learning'],"This paper presents a novel multi-agent reinforcement learning (MARL) algorithm called MCMARL that parameterizes the value function of an agent via a mixture of categorical distributions. The proposed algorithm is based on a deep learning approach and is designed to address the challenges of MARL in a variety of machine learning tasks. We evaluate MCMARL on a number of benchmark environments and demonstrate that it outperforms existing MARL algorithms. We further demonstrate that MCMARL can be used to improve the performance of existing MARL algorithms on tasks where the agents have to interact with other agents in a complex environment. Finally, we discuss the implications of using MCMARL for a variety of machine learning tasks.","Write an abstract for a paper called MCMARL: Parameterizing Value Function via Mixture of Categorical
  Distributions for Multi-Agent Reinforcement Learning about Machine Learning"
2204.09649,"Hossam ElAtali, Lachlan J. Gunn, Hans Liljestrand, N. Asokan","BliMe: Verifiably Secure Outsourced Computation with Hardware-Enforced
  Taint Tracking",['cs.CR'],"  Outsourced computing is widely used today. However, approaches for protecting
client data in outsourced computing fall short: use of cryptographic techniques
like fully-homomorphic encryption incurs substantial costs, whereas the use of
hardware-assisted trusted execution environments has been shown to be
vulnerable to server malware, run-time attacks, and side-channel attacks.
  We present Blinded Memory (BliMe), an architecture to realize efficient and
secure outsourced computation. BliMe consists of a novel and minimal set of ISA
extensions implementing a taint-tracking policy to ensure the confidentiality
of client data even in the presence of server vulnerabilities. To secure
outsourced computation, the BliMe extensions can be used together with an
attestable, fixed-function hardware security module (HSM) and an encryption
engine that provides atomic decrypt-and-taint and encrypt-and-untaint
operations. Clients rely on remote attestation and key agreement with the HSM
to ensure that their data can be transferred securely to and from the
encryption engine and will always be protected by BliMe's taint-tracking policy
while at the server.
  We provide a machine-checked security proof, and two different hardware
implementations (BliMe-Simple and BliMe-Realistic) of BliMe extensions. We show
that BliMe implementations incur only minor increases in performance (< 5%),
and resource consumption (< 3% for power, LUTs and registers).
","[{'version': 'v1', 'created': 'Wed, 20 Apr 2022 17:47:16 GMT'}, {'version': 'v2', 'created': 'Fri, 22 Apr 2022 02:43:59 GMT'}, {'version': 'v3', 'created': 'Tue, 26 Apr 2022 11:54:07 GMT'}, {'version': 'v4', 'created': 'Mon, 25 Jul 2022 03:57:15 GMT'}, {'version': 'v5', 'created': 'Fri, 21 Oct 2022 23:08:59 GMT'}]",2022-10-25,['Cryptography and Security'],"This paper introduces BliMe, a verifiably secure outsourced computation system that uses hardware-enforced taint tracking to protect against malicious computations. BliMe leverages cryptography and security techniques to ensure that the integrity of outsourced computations is maintained, even when a malicious party is involved. BliMe also provides a mechanism for the verifiable execution of a computation, allowing the client to verify the correctness of the results. The paper evaluates the security and performance of BliMe in comparison to existing systems, and provides evidence that BliMe is a viable solution for secure outsourced computation.","Write an abstract for a paper called BliMe: Verifiably Secure Outsourced Computation with Hardware-Enforced
  Taint Tracking about Cryptography and Security"
2301.03222,"Danish Muzafar, Furqan Yaqub Khan, Mubashir Qayoom","Machine Learning Algorithms for Depression Detection and Their
  Comparison","['cs.CL', 'cs.AI', 'cs.IR']","  Textual emotional intelligence is playing a ubiquitously important role in
leveraging human emotions on social media platforms. Social media platforms are
privileged with emotional content and are leveraged for various purposes like
opinion mining, emotion mining, and sentiment analysis. This data analysis is
also levered for the prevention of online bullying, suicide prevention, and
depression detection among social media users. In this article, we have
designed an automatic depression detection of online social media users by
analyzing their social media behavior. The designed depression detection
classification can be effectively used to mine user's social media interactions
and one can determine whether a social media user is suffering from depression
or not. The underlying classifier is made using state-of-art technology in
emotional artificial intelligence which includes LSTM (Long Short Term Memory)
and other machine learning classifiers. The highest accuracy of the classifier
is around 70% of LSTM and for SVM the highest accuracy is 81.79%. We trained
the classifier on the datasets that are widely used in literature for emotion
mining tasks. A confusion matrix of results is also given.
","[{'version': 'v1', 'created': 'Mon, 9 Jan 2023 09:34:38 GMT'}]",2023-01-10,"['Computation and Language', 'Artificial Intelligence', 'Information Retrieval']","This paper presents a comparison between machine learning algorithms that are used for depression detection, with a focus on computation and language, artificial intelligence, and information retrieval. It discusses various algorithms, including support vector machines, decision trees, and artificial neural networks, and examines their strengths and weaknesses in terms of accuracy, computational complexity, and scalability. Furthermore, it compares the performance of different algorithms in terms of their ability to detect depression using natural language processing, and by using artificial intelligence and information retrieval techniques. The paper also provides an overview of the current state of research in the field, and identifies areas for future research. Finally, it provides a conclusion and recommendations for practitioners on which algorithms are best suited for depression detection.","Write an abstract for a paper called Machine Learning Algorithms for Depression Detection and Their
  Comparison about Computation and Language, Artificial Intelligence, Information Retrieval"
2303.14375,"Rui Zhang, Yajing Sun, Jingyuan Yang, Wei Peng",Knowledge-augmented Frame Semantic Parsing with Hybrid Prompt-tuning,['cs.CL'],"  Frame semantics-based approaches have been widely used in semantic parsing
tasks and have become mainstream. It remains challenging to disambiguate frame
representations evoked by target lexical units under different contexts.
Pre-trained Language Models (PLMs) have been used in semantic parsing and
significantly improve the accuracy of neural parsers. However, the PLMs-based
approaches tend to favor collocated patterns presented in the training data,
leading to inaccurate outcomes. The intuition here is to design a mechanism to
optimally use knowledge captured in semantic frames in conjunction with PLMs to
disambiguate frames. We propose a novel Knowledge-Augmented Frame Semantic
Parsing Architecture (KAF-SPA) to enhance semantic representation by
incorporating accurate frame knowledge into PLMs during frame semantic parsing.
Specifically, a Memory-based Knowledge Extraction Module (MKEM) is devised to
select accurate frame knowledge and construct the continuous templates in the
high dimensional vector space. Moreover, we design a Task-oriented Knowledge
Probing Module (TKPM) using hybrid prompts (in terms of continuous and discrete
prompts) to incorporate the selected knowledge into the PLMs and adapt PLMs to
the tasks of frame and argument identification. Experimental results on two
public FrameNet datasets demonstrate that our method significantly outperforms
strong baselines (by more than +3$\%$ in F1), achieving state-of-art results on
the current benchmark. Ablation studies verify the effectiveness of KAF-SPA.
","[{'version': 'v1', 'created': 'Sat, 25 Mar 2023 06:41:19 GMT'}]",2023-03-28,['Computation and Language'],"This paper presents a novel hybrid prompt-tuning approach to knowledge-augmented frame semantic parsing. It combines the advantages of prompt-tuning and knowledge-augmented frame semantic parsing to enable more efficient and accurate parsing. The proposed approach is evaluated on a standard benchmark dataset and compared to existing methods. The results demonstrate that the proposed approach significantly outperforms existing methods in both accuracy and speed. Furthermore, the paper discusses the implications of the proposed approach for both natural language processing and computation.",Write an abstract for a paper called Knowledge-augmented Frame Semantic Parsing with Hybrid Prompt-tuning about Computation and Language
2210.01006,"David Korda, Antti Penttil\""a, Arto Klami, Tom\'a\v{s} Kohout","Neural network for determining an asteroid mineral composition from
  reflectance spectra","['astro-ph.EP', 'astro-ph.IM', 'cs.LG']","  Chemical and mineral compositions of asteroids reflect the formation and
history of our Solar System. This knowledge is also important for planetary
defence and in-space resource utilisation. We aim to develop a fast and robust
neural-network-based method for deriving the mineral modal and chemical
compositions of silicate materials from their visible and near-infrared
spectra. The method should be able to process raw spectra without significant
pre-processing. We designed a convolutional neural network with two hidden
layers for the analysis of the spectra, and trained it using labelled
reflectance spectra. For the training, we used a dataset that consisted of
reflectance spectra of real silicate samples stored in the RELAB and C-Tape
databases, namely olivine, orthopyroxene, clinopyroxene, their mixtures, and
olivine-pyroxene-rich meteorites. We used the model on two datasets. First, we
evaluated the model reliability on a test dataset where we compared the model
classification with known compositional reference values. The individual
classification results are mostly within 10 percentage-point intervals around
the correct values. Second, we classified the reflectance spectra of S-complex
(Q-type and V-type, also including A-type) asteroids with known Bus-DeMeo
taxonomy classes. The predicted mineral chemical composition of S-type and
Q-type asteroids agree with the chemical composition of ordinary chondrites.
The modal abundances of V-type and A-type asteroids show a dominant
contribution of orthopyroxene and olivine, respectively. Additionally, our
predictions of the mineral modal composition of S-type and Q-type asteroids
show an apparent depletion of olivine related to the attenuation of its
diagnostic absorptions with space weathering. This trend is consistent with
previous results of the slower pyroxene response to space weathering relative
to olivine.
","[{'version': 'v1', 'created': 'Mon, 3 Oct 2022 15:14:05 GMT'}]",2023-01-18,['Machine Learning'],"This paper presents a neural network approach to predicting the mineral composition of asteroids from reflectance spectra. The neural network is trained on a large dataset of asteroid reflectance spectra and their corresponding mineral compositions. The neural network architecture is based on a convolutional neural network (CNN) with a multi-layer perceptron (MLP) structure. The results of the neural network are compared to other existing machine learning techniques, and it is shown to outperform them in terms of accuracy and speed. The paper also discusses the potential applications of the neural network to other fields of astronomy, such as the study of exoplanets. Finally, the paper provides an outlook for future research in this area.","Write an abstract for a paper called Neural network for determining an asteroid mineral composition from
  reflectance spectra about Machine Learning"
2204.08182,"Xun Wang, Bingqing Ke, Xuanping Li, Fangyu Liu, Mingyu Zhang, Xiao
  Liang, Qiushi Xiao, Cheng Luo, Yue Yu",Modality-Balanced Embedding for Video Retrieval,"['cs.CV', 'cs.AI', 'cs.IR', 'stat.ML']","  Video search has become the main routine for users to discover videos
relevant to a text query on large short-video sharing platforms. During
training a query-video bi-encoder model using online search logs, we identify a
modality bias phenomenon that the video encoder almost entirely relies on text
matching, neglecting other modalities of the videos such as vision, audio. This
modality imbalanceresults from a) modality gap: the relevance between a query
and a video text is much easier to learn as the query is also a piece of text,
with the same modality as the video text; b) data bias: most training samples
can be solved solely by text matching. Here we share our practices to improve
the first retrieval stage including our solution for the modality imbalance
issue. We propose MBVR (short for Modality Balanced Video Retrieval) with two
key components: manually generated modality-shuffled (MS) samples and a dynamic
margin (DM) based on visual relevance. They can encourage the video encoder to
pay balanced attentions to each modality. Through extensive experiments on a
real world dataset, we show empirically that our method is both effective and
efficient in solving modality bias problem. We have also deployed our MBVR in a
large video platform and observed statistically significant boost over a highly
optimized baseline in an A/B test and manual GSB evaluations.
","[{'version': 'v1', 'created': 'Mon, 18 Apr 2022 06:29:46 GMT'}, {'version': 'v2', 'created': 'Tue, 17 May 2022 06:38:48 GMT'}]",2022-05-18,"['Computer Vision and Pattern Recognition', 'Artificial Intelligence', 'Information Retrieval']","This paper presents a novel modality-balanced embedding approach for video retrieval. By leveraging the complementary strengths of computer vision and pattern recognition, artificial intelligence, and information retrieval techniques, we develop an approach to learning a unified embedding space across different modalities. This unified embedding space is used to improve the accuracy and efficiency of video retrieval. We evaluate our approach on several standard video retrieval datasets and show that it outperforms existing approaches in terms of accuracy and retrieval time. Furthermore, we demonstrate that our approach is more robust to noise and can handle more complex queries.","Write an abstract for a paper called Modality-Balanced Embedding for Video Retrieval about Computer Vision and Pattern Recognition, Artificial Intelligence, Information Retrieval"
2203.16578,"Harveen Singh Chadha, Priyanshi Shah, Ankur Dhuriya, Neeraj Chhimwal,
  Anirudh Gupta, Vivek Raghavan",Code Switched and Code Mixed Speech Recognition for Indic languages,"['cs.CL', 'eess.AS']","  Training multilingual automatic speech recognition (ASR) systems is
challenging because acoustic and lexical information is typically language
specific. Training multilingual system for Indic languages is even more tougher
due to lack of open source datasets and results on different approaches. We
compare the performance of end to end multilingual speech recognition system to
the performance of monolingual models conditioned on language identification
(LID). The decoding information from a multilingual model is used for language
identification and then combined with monolingual models to get an improvement
of 50% WER across languages. We also propose a similar technique to solve the
Code Switched problem and achieve a WER of 21.77 and 28.27 over Hindi-English
and Bengali-English respectively. Our work talks on how transformer based ASR
especially wav2vec 2.0 can be applied in developing multilingual ASR and code
switched ASR for Indic languages.
","[{'version': 'v1', 'created': 'Wed, 30 Mar 2022 18:09:28 GMT'}, {'version': 'v2', 'created': 'Mon, 13 Jun 2022 09:30:17 GMT'}]",2022-06-14,['Computation and Language'],"This paper presents a study on code switched and code mixed speech recognition for Indic languages. We discuss the various challenges faced in the recognition of code switched and code mixed speech, and propose a novel approach to tackle these challenges. We present a comprehensive survey of existing algorithms and techniques used for code switched and code mixed speech recognition in Indic languages. We then propose a novel deep learning-based approach for code switched and code mixed speech recognition. We evaluate our proposed approach on a publicly available dataset and show that our proposed approach outperforms existing methods by a significant margin. Finally, we discuss the implications of our proposed approach and its potential applications in the field of Computation and Language.",Write an abstract for a paper called Code Switched and Code Mixed Speech Recognition for Indic languages about Computation and Language
2205.09589,"Mathieu Blondel, Felipe Llinares-L\'opez, Robert Dadashi, L\'eonard
  Hussenot, Matthieu Geist",Learning Energy Networks with Generalized Fenchel-Young Losses,"['cs.LG', 'stat.ML']","  Energy-based models, a.k.a. energy networks, perform inference by optimizing
an energy function, typically parametrized by a neural network. This allows one
to capture potentially complex relationships between inputs and outputs. To
learn the parameters of the energy function, the solution to that optimization
problem is typically fed into a loss function. The key challenge for training
energy networks lies in computing loss gradients, as this typically requires
argmin/argmax differentiation. In this paper, building upon a generalized
notion of conjugate function, which replaces the usual bilinear pairing with a
general energy function, we propose generalized Fenchel-Young losses, a natural
loss construction for learning energy networks. Our losses enjoy many desirable
properties and their gradients can be computed efficiently without
argmin/argmax differentiation. We also prove the calibration of their excess
risk in the case of linear-concave energies. We demonstrate our losses on
multilabel classification and imitation learning tasks.
","[{'version': 'v1', 'created': 'Thu, 19 May 2022 14:32:04 GMT'}, {'version': 'v2', 'created': 'Wed, 12 Oct 2022 10:24:14 GMT'}]",2022-10-13,['Machine Learning'],"This paper presents a novel approach to learning energy networks using Generalized Fenchel-Young Losses (GFL). GFL is a recently proposed loss function that combines the Fenchel-Young duality and the Generalized Mean Value Theorem (GMVT). GFL is capable of learning energy networks with non-convex objectives, and has been successfully applied to various machine learning tasks. In this paper, we extend the application of GFL by demonstrating its effectiveness in learning energy networks. We evaluate GFL on several benchmark datasets to show its ability to learn energy networks with non-convex objectives. We also compare GFL to other existing methods, and show that it outperforms them in terms of accuracy and convergence time. The results of this paper demonstrate the effectiveness of GFL in learning energy networks, and provide insights into its potential applications in other machine learning tasks.",Write an abstract for a paper called Learning Energy Networks with Generalized Fenchel-Young Losses about Machine Learning
2211.13408,"Teddy Koker, Keegan Quigley, Will Spaeth, Nathan C. Frey, Lin Li",Graph Contrastive Learning for Materials,"['cs.LG', 'cond-mat.mtrl-sci']","  Recent work has shown the potential of graph neural networks to efficiently
predict material properties, enabling high-throughput screening of materials.
Training these models, however, often requires large quantities of labelled
data, obtained via costly methods such as ab initio calculations or
experimental evaluation. By leveraging a series of material-specific
transformations, we introduce CrystalCLR, a framework for constrastive learning
of representations with crystal graph neural networks. With the addition of a
novel loss function, our framework is able to learn representations competitive
with engineered fingerprinting methods. We also demonstrate that via model
finetuning, contrastive pretraining can improve the performance of graph neural
networks for prediction of material properties and significantly outperform
traditional ML models that use engineered fingerprints. Lastly, we observe that
CrystalCLR produces material representations that form clusters by compound
class.
","[{'version': 'v1', 'created': 'Thu, 24 Nov 2022 04:15:47 GMT'}]",2022-11-28,['Machine Learning'],"This paper presents a novel graph contrastive learning approach for materials design and discovery with machine learning. The proposed method uses a graph neural network to learn a latent representation of the materials structure, and then uses contrastive learning to learn a representation of the material properties. The method is evaluated on a materials property prediction task and a materials discovery task, and the results show that the proposed approach outperforms existing methods. The paper also discusses the potential of graph contrastive learning for materials design and discovery, and provides insights into the design of graph neural networks for this task.",Write an abstract for a paper called Graph Contrastive Learning for Materials about Machine Learning
2302.11991,"Franti\v{s}ek Nekov\'a\v{r}, Jan Faigl and Martin Saska",Multi-vehicle Dynamic Water Surface Monitoring,['cs.RO'],"  Repeated exploration of a water surface to detect objects of interest and
their subsequent monitoring is important in search-and-rescue or ocean clean-up
operations. Since the location of any detected object is dynamic, we propose to
address the combined surface exploration and monitoring of the detected objects
by modeling spatio-temporal reward states and coordinating a team of vehicles
to collect the rewards. The model characterizes the dynamics of the water
surface and enables the planner to predict future system states. The state
reward value relevant to the particular water surface cell increases over time
and is nullified by being in a sensor range of a vehicle. Thus, the proposed
multi-vehicle planning approach is to minimize the collective value of the
dynamic model reward states. The purpose is to address vehicles' motion
constraints by using model predictive control on receding horizon, thus fully
exploiting the utilized vehicles' motion capabilities. Based on the evaluation
results, the approach indicates improvement in a solution to the kinematic
orienteering problem and the team orienteering problem in the monitoring task
compared to the existing solutions. The proposed approach has been
experimentally verified, supporting its feasibility in real-world monitoring
tasks.
","[{'version': 'v1', 'created': 'Thu, 23 Feb 2023 13:13:50 GMT'}]",2023-02-24,['Robotics'],"This paper presents a multi-vehicle robotic system for dynamic water surface monitoring. The proposed system consists of a network of autonomous vehicles, each equipped with a suite of sensors and instruments to measure various parameters such as temperature, salinity, dissolved oxygen, pH, and turbidity. The vehicles are capable of operating in both shallow and deep water, and can communicate with each other and with a central control station. The system is designed to provide real-time data on the state of the water surface, enabling rapid response to changes in water quality. The paper discusses the design of the system, the communication protocols used, and the results of a field test. The results show that the system is capable of providing accurate and reliable data on the state of the water surface.",Write an abstract for a paper called Multi-vehicle Dynamic Water Surface Monitoring about Robotics
2205.15245,"Rafael Pina, Varuna De Silva, Joosep Hook, and Ahmet Kondoz","Residual Q-Networks for Value Function Factorizing in Multi-Agent
  Reinforcement Learning","['cs.LG', 'cs.MA']","  Multi-Agent Reinforcement Learning (MARL) is useful in many problems that
require the cooperation and coordination of multiple agents. Learning optimal
policies using reinforcement learning in a multi-agent setting can be very
difficult as the number of agents increases. Recent solutions such as Value
Decomposition Networks (VDN), QMIX, QTRAN and QPLEX adhere to the centralized
training and decentralized execution scheme and perform factorization of the
joint action-value functions. However, these methods still suffer from
increased environmental complexity, and at times fail to converge in a stable
manner. We propose a novel concept of Residual Q-Networks (RQNs) for MARL,
which learns to transform the individual Q-value trajectories in a way that
preserves the Individual-Global-Max criteria (IGM), but is more robust in
factorizing action-value functions. The RQN acts as an auxiliary network that
accelerates convergence and will become obsolete as the agents reach the
training objectives. The performance of the proposed method is compared against
several state-of-the-art techniques such as QPLEX, QMIX, QTRAN and VDN, in a
range of multi-agent cooperative tasks. The results illustrate that the
proposed method, in general, converges faster, with increased stability and
shows robust performance in a wider family of environments. The improvements in
results are more prominent in environments with severe punishments for
non-cooperative behaviours and especially in the absence of complete state
information during training time.
","[{'version': 'v1', 'created': 'Mon, 30 May 2022 16:56:06 GMT'}]",2022-05-31,"['Machine Learning', 'Multiagent Systems']","This paper examines the use of Residual Q-Networks for Value Function Factorizing in Multi-Agent Reinforcement Learning. We propose a novel approach to factorizing value functions in multi-agent reinforcement learning by using residual Q-networks. We demonstrate the effectiveness of the proposed approach on a suite of classic multi-agent reinforcement learning tasks. Our results show that the proposed approach is able to achieve better performance than existing methods in terms of learning speed, convergence, and overall performance. We also discuss the implications of our work and its potential applications in the field of machine learning and multiagent systems.","Write an abstract for a paper called Residual Q-Networks for Value Function Factorizing in Multi-Agent
  Reinforcement Learning about Machine Learning, Multiagent Systems"
2301.07041,"Alexander Viand, Christian Knabenhans, Anwar Hithnawi",Verifiable Fully Homomorphic Encryption,['cs.CR'],"  Fully Homomorphic Encryption (FHE) is seeing increasing real-world deployment
to protect data in use by allowing computation over encrypted data. However,
the same malleability that enables homomorphic computations also raises
integrity issues, which have so far been mostly overlooked. While FHEs lack of
integrity has obvious implications for correctness, it also has severe
implications for confidentiality: a malicious server can leverage the lack of
integrity to carry out interactive key-recovery attacks. As a result, virtually
all FHE schemes and applications assume an honest-but-curious server who does
not deviate from the protocol. In practice, however, this assumption is
insufficient for a wide range of deployment scenarios. While there has been
work that aims to address this gap, these have remained isolated efforts
considering only aspects of the overall problem and fail to fully address the
needs and characteristics of modern FHE schemes and applications. In this
paper, we analyze existing FHE integrity approaches, present attacks that
exploit gaps in prior work, and propose a new notion for maliciously-secure
verifiable FHE. We then instantiate this new notion with a range of techniques,
analyzing them and evaluating their performance in a range of different
settings. We highlight their potential but also show where future work on
tailored integrity solutions for FHE is still required.
","[{'version': 'v1', 'created': 'Tue, 17 Jan 2023 17:50:26 GMT'}, {'version': 'v2', 'created': 'Sat, 11 Feb 2023 17:31:04 GMT'}]",2023-02-14,['Cryptography and Security'],"This paper presents a review of Verifiable Fully Homomorphic Encryption (VFHE) and its application in cryptography and security. VFHE is a form of encryption that allows for a third party to verify the correctness of the encrypted data without the need to decrypt it. This paper will discuss the various aspects of VFHE, including its security properties, its uses in cryptography, and its applications in security. In addition, the paper will discuss the current state of the art of VFHE and the potential for further research and development in the field. Finally, the paper will explore the implications of VFHE for cryptographic and security applications and provide suggestions for future work.",Write an abstract for a paper called Verifiable Fully Homomorphic Encryption about Cryptography and Security
2301.05664,"Taylor W. Killian, Sonali Parbhoo, Marzyeh Ghassemi","Risk Sensitive Dead-end Identification in Safety-Critical Offline
  Reinforcement Learning","['cs.LG', 'stat.ML']","  In safety-critical decision-making scenarios being able to identify
worst-case outcomes, or dead-ends is crucial in order to develop safe and
reliable policies in practice. These situations are typically rife with
uncertainty due to unknown or stochastic characteristics of the environment as
well as limited offline training data. As a result, the value of a decision at
any time point should be based on the distribution of its anticipated effects.
We propose a framework to identify worst-case decision points, by explicitly
estimating distributions of the expected return of a decision. These estimates
enable earlier indication of dead-ends in a manner that is tunable based on the
risk tolerance of the designed task. We demonstrate the utility of
Distributional Dead-end Discovery (DistDeD) in a toy domain as well as when
assessing the risk of severely ill patients in the intensive care unit reaching
a point where death is unavoidable. We find that DistDeD significantly improves
over prior discovery approaches, providing indications of the risk 10 hours
earlier on average as well as increasing detection by 20%.
","[{'version': 'v1', 'created': 'Fri, 13 Jan 2023 17:01:58 GMT'}, {'version': 'v2', 'created': 'Mon, 30 Jan 2023 16:08:48 GMT'}]",2023-01-31,['Machine Learning'],"This paper explores the application of risk-sensitive offline reinforcement learning (RL) to dead-end identification in safety-critical machine learning. Offline RL is a form of reinforcement learning that is well suited to the safety-critical domain, as it is able to learn from data collected in the past without requiring the system to interact with the environment. In this paper, we introduce a risk-sensitive approach to dead-end identification, which involves using a risk-sensitive reward function to identify dead-end states in a given environment. We demonstrate the effectiveness of our approach on a simulated environment and discuss the potential of this approach for safety-critical machine learning. Our results show that the risk-sensitive approach is able to accurately identify dead-end states, and could be used to improve the safety of machine learning systems.","Write an abstract for a paper called Risk Sensitive Dead-end Identification in Safety-Critical Offline
  Reinforcement Learning about Machine Learning"
2303.04882,J. S. C. Prentice,"Determining the Rolle function in Hermite interpolatory approximation by
  solving an appropriate differential equation","['math.NA', 'cs.NA']","  We determine the pointwise error in Hermite interpolation by numerically
solving an appropriate differential equation, derived from the error term
itself. We use this knowledge to approximate the error term by means of a
polynomial, which is then added to the original Hermite polynomial to form a
more accurate approximation. An example demonstrates that improvements in
accuracy are significant.
","[{'version': 'v1', 'created': 'Wed, 8 Mar 2023 20:40:23 GMT'}]",2023-03-10,['Numerical Analysis'],"This paper examines the role of the Rolle function in Hermite interpolatory approximation when solving differential equations numerically. The Rolle function is a powerful tool for approximating solutions to differential equations and is used frequently in numerical analysis. This paper investigates the properties of the Rolle function in Hermite interpolatory approximation and its ability to provide accurate approximations of the solutions to differential equations. The paper will discuss the underlying principles of Hermite interpolatory approximation and the role of the Rolle function in it. It will also explore the various numerical methods used to determine the Rolle function in Hermite interpolatory approximation and their effectiveness. Finally, the paper will discuss the implications of the findings and present possible solutions for further research in this field.","Write an abstract for a paper called Determining the Rolle function in Hermite interpolatory approximation by
  solving an appropriate differential equation about Numerical Analysis"
2211.1106,"Anup Singh, Kris Demuynck, Vipul Arora","Simultaneously Learning Robust Audio Embeddings and balanced Hash codes
  for Query-by-Example","['eess.AS', 'cs.LG', 'cs.SD']","  Audio fingerprinting systems must efficiently and robustly identify query
snippets in an extensive database. To this end, state-of-the-art systems use
deep learning to generate compact audio fingerprints. These systems deploy
indexing methods, which quantize fingerprints to hash codes in an unsupervised
manner to expedite the search. However, these methods generate imbalanced hash
codes, leading to their suboptimal performance. Therefore, we propose a
self-supervised learning framework to compute fingerprints and balanced hash
codes in an end-to-end manner to achieve both fast and accurate retrieval
performance. We model hash codes as a balanced clustering process, which we
regard as an instance of the optimal transport problem. Experimental results
indicate that the proposed approach improves retrieval efficiency while
preserving high accuracy, particularly at high distortion levels, compared to
the competing methods. Moreover, our system is efficient and scalable in
computational load and memory storage.
","[{'version': 'v1', 'created': 'Sun, 20 Nov 2022 19:22:44 GMT'}, {'version': 'v2', 'created': 'Wed, 18 Jan 2023 19:28:44 GMT'}]",2023-01-20,"['Machine Learning', 'Sound']","Retrieval

This paper presents a novel approach to query-by-example sound retrieval using a combination of machine learning techniques. Specifically, we propose a method for simultaneously learning robust audio embeddings and balanced hash codes. Our approach uses a deep learning model to learn audio embeddings from audio samples, and then uses a hash code learning algorithm to generate balanced hash codes from the learned audio embeddings. We evaluate our approach on a set of sound retrieval tasks, demonstrating improved performance compared to existing methods. Furthermore, our approach is capable of handling large-scale datasets, making it suitable for real-world applications.","Write an abstract for a paper called Simultaneously Learning Robust Audio Embeddings and balanced Hash codes
  for Query-by-Example about Machine Learning, Sound"
2303.03073,Sobin Joseph and Shashi Jain,"A neural network based model for multi-dimensional nonlinear Hawkes
  processes","['stat.ML', 'cs.LG', 'q-fin.ST']","  This paper introduces the Neural Network for Nonlinear Hawkes processes
(NNNH), a non-parametric method based on neural networks to fit nonlinear
Hawkes processes. Our method is suitable for analyzing large datasets in which
events exhibit both mutually-exciting and inhibitive patterns. The NNNH
approach models the individual kernels and the base intensity of the nonlinear
Hawkes process using feed forward neural networks and jointly calibrates the
parameters of the networks by maximizing the log-likelihood function. We
utilize Stochastic Gradient Descent to search for the optimal parameters and
propose an unbiased estimator for the gradient, as well as an efficient
computation method. We demonstrate the flexibility and accuracy of our method
through numerical experiments on both simulated and real-world data, and
compare it with state-of-the-art methods. Our results highlight the
effectiveness of the NNNH method in accurately capturing the complexities of
nonlinear Hawkes processes.
","[{'version': 'v1', 'created': 'Mon, 6 Mar 2023 12:31:19 GMT'}]",2023-03-07,['Machine Learning'],"This paper introduces a novel neural network based model for multi-dimensional nonlinear Hawkes processes, which is a type of self-exciting point process. The proposed model is based on a recurrent neural network architecture, which is capable of learning the temporal dynamics of the underlying process. We evaluate our model on synthetic and real-world datasets, showing that it is able to accurately capture the temporal structure of the data and outperform existing methods. Our results demonstrate the potential of neural networks for modeling complex temporal data and suggest that the proposed model can be used for a variety of applications.","Write an abstract for a paper called A neural network based model for multi-dimensional nonlinear Hawkes
  processes about Machine Learning"
2301.1126,"Chunlin Sun, Shang Liu, Xiaocheng Li","Maximum Optimality Margin: A Unified Approach for Contextual Linear
  Programming and Inverse Linear Programming","['cs.LG', 'math.OC', 'stat.ML']","  In this paper, we study the predict-then-optimize problem where the output of
a machine learning prediction task is used as the input of some downstream
optimization problem, say, the objective coefficient vector of a linear
program. The problem is also known as predictive analytics or contextual linear
programming. The existing approaches largely suffer from either (i)
optimization intractability (a non-convex objective function)/statistical
inefficiency (a suboptimal generalization bound) or (ii) requiring strong
condition(s) such as no constraint or loss calibration. We develop a new
approach to the problem called \textit{maximum optimality margin} which designs
the machine learning loss function by the optimality condition of the
downstream optimization. The max-margin formulation enjoys both computational
efficiency and good theoretical properties for the learning procedure. More
importantly, our new approach only needs the observations of the optimal
solution in the training data rather than the objective function, which makes
it a new and natural approach to the inverse linear programming problem under
both contextual and context-free settings; we also analyze the proposed method
under both offline and online settings, and demonstrate its performance using
numerical experiments.
","[{'version': 'v1', 'created': 'Thu, 26 Jan 2023 17:53:38 GMT'}]",2023-01-27,['Machine Learning'],"This paper presents a unified approach to contextual linear programming and inverse linear programming in the context of machine learning. The proposed approach, Maximum Optimality Margin (MOM), is a novel optimization technique that combines the advantages of both linear programming and inverse linear programming to obtain a better solution. MOM is capable of handling large-scale problems with complex constraints and multiple objectives, while still providing a tight bound on the optimal solution. The proposed approach is evaluated on a variety of datasets and its performance is compared to existing methods. The results show that MOM consistently outperforms existing methods and can provide better solutions in a shorter amount of time. Additionally, the proposed approach is shown to be more robust to parameter changes and can handle a wide range of problems.","Write an abstract for a paper called Maximum Optimality Margin: A Unified Approach for Contextual Linear
  Programming and Inverse Linear Programming about Machine Learning"
2203.11425,"Kaiqiang Song, Chen Li, Xiaoyang Wang, Dong Yu, Fei Liu",Towards Abstractive Grounded Summarization of Podcast Transcripts,"['cs.CL', 'cs.AI']","  Podcasts have recently shown a rapid rise in popularity. Summarization of
podcast transcripts is of practical benefit to both content providers and
consumers. It helps consumers to quickly decide whether they will listen to the
podcasts and reduces the cognitive load of content providers to write
summaries. Nevertheless, podcast summarization faces significant challenges
including factual inconsistencies with respect to the inputs. The problem is
exacerbated by speech disfluencies and recognition errors in transcripts of
spoken language. In this paper, we explore a novel abstractive summarization
method to alleviate these challenges. Specifically, our approach learns to
produce an abstractive summary while grounding summary segments in specific
portions of the transcript to allow for full inspection of summary details. We
conduct a series of analyses of the proposed approach on a large podcast
dataset and show that the approach can achieve promising results. Grounded
summaries bring clear benefits in locating the summary and transcript segments
that contain inconsistent information, and hence significantly improve
summarization quality in both automatic and human evaluation metrics.
","[{'version': 'v1', 'created': 'Tue, 22 Mar 2022 02:44:39 GMT'}]",2022-03-23,"['Computation and Language', 'Artificial Intelligence']","This paper presents a novel approach to abstractive grounded summarization of podcast transcripts about computation and language, artificial intelligence (AI). We propose a summarization model that combines abstractive summarization techniques with grounded language models to generate summaries from podcast transcripts. The model is trained on a corpus of podcast transcripts and evaluated on a set of manually-generated summaries. The results show that our model outperforms traditional summarization models in terms of ROUGE scores, providing a more comprehensive and accurate summary of the podcast transcripts. Additionally, we discuss the implications of our approach for future research in summarization and AI.","Write an abstract for a paper called Towards Abstractive Grounded Summarization of Podcast Transcripts about Computation and Language, Artificial Intelligence"
2201.06309,"Pengfei Liu, Kun Li and Helen Meng","Group Gated Fusion on Attention-based Bidirectional Alignment for
  Multimodal Emotion Recognition","['cs.CL', 'cs.SD', 'eess.AS']","  Emotion recognition is a challenging and actively-studied research area that
plays a critical role in emotion-aware human-computer interaction systems. In a
multimodal setting, temporal alignment between different modalities has not
been well investigated yet. This paper presents a new model named as Gated
Bidirectional Alignment Network (GBAN), which consists of an attention-based
bidirectional alignment network over LSTM hidden states to explicitly capture
the alignment relationship between speech and text, and a novel group gated
fusion (GGF) layer to integrate the representations of different modalities. We
empirically show that the attention-aligned representations outperform the
last-hidden-states of LSTM significantly, and the proposed GBAN model
outperforms existing state-of-the-art multimodal approaches on the IEMOCAP
dataset.
","[{'version': 'v1', 'created': 'Mon, 17 Jan 2022 09:46:59 GMT'}]",2022-01-19,"['Computation and Language', 'Sound']",", and Vision

This paper presents Group Gated Fusion (GGF), an attention-based bidirectional alignment method for multimodal emotion recognition. We employ a gated fusion mechanism to integrate the information from different modalities, including computation and language, sound, and vision. The proposed GGF is evaluated on two multimodal emotion recognition datasets, MOUD and IEMOCAP. Experimental results show that GGF outperforms the state-of-the-art methods in terms of accuracy, precision and recall. Moreover, the ablation study demonstrates that GGF is able to effectively capture the correlations between different modalities. This paper provides a novel approach for multimodal emotion recognition and further contributes to the field of multimodal learning.","Write an abstract for a paper called Group Gated Fusion on Attention-based Bidirectional Alignment for
  Multimodal Emotion Recognition about Computation and Language, Sound"
2105.13396,"Zachary P. Neal, Rachel Domagalski, and Bruce Sagan","Comparing Alternatives to the Fixed Degree Sequence Model for Extracting
  the Backbone of Bipartite Projections","['cs.SI', 'stat.AP']","  Projections of bipartite or two-mode networks capture co-occurrences, and are
used in diverse fields (e.g., ecology, economics, bibliometrics, politics) to
represent unipartite networks. A key challenge in analyzing such networks is
determining whether an observed number of co-occurrences between two nodes is
significant, and therefore whether an edge exists between them. One approach,
the fixed degree sequence model (FDSM), evaluates the significance of an edge's
weight by comparison to a null model in which the degree sequences of the
original bipartite network are fixed. Although the FDSM is an intuitive null
model, it is computationally expensive because it requires Monte Carlo
simulation to estimate each edge's $p$-value, and therefore is impractical for
large projections. In this paper, we explore four potential alternatives to
FDSM: fixed fill model (FFM), fixed row model (FRM), fixed column model (FCM),
and stochastic degree sequence model (SDSM). We compare these models to FDSM in
terms of accuracy, speed, statistical power, similarity, and ability to recover
known communities. We find that the computationally-fast SDSM offers a
statistically conservative but close approximation of the
computationally-impractical FDSM under a wide range of conditions, and that it
correctly recovers a known community structure even when the signal is weak.
Therefore, although each backbone model may have particular applications, we
recommend SDSM for extracting the backbone of bipartite projections when FDSM
is impractical.
","[{'version': 'v1', 'created': 'Thu, 27 May 2021 18:56:04 GMT'}, {'version': 'v2', 'created': 'Mon, 31 May 2021 12:24:53 GMT'}, {'version': 'v3', 'created': 'Fri, 18 Jun 2021 15:02:14 GMT'}, {'version': 'v4', 'created': 'Thu, 7 Oct 2021 17:55:14 GMT'}, {'version': 'v5', 'created': 'Thu, 28 Oct 2021 18:06:03 GMT'}]",2022-02-22,['Social and Information Networks'],"This paper examines alternative methods for extracting the backbone of bipartite projections of social and information networks. The fixed degree sequence model is the most commonly used method for extracting the backbone of bipartite projections, but it is limited in its ability to capture the full complexity of real-world networks. We compare and contrast alternative methods to the fixed degree sequence model, such as degree-degree correlation, link density, and the k-core decomposition. We discuss the advantages and disadvantages of each approach and provide empirical evidence to support our conclusions. Finally, we make recommendations for which methods are best suited for extracting the backbone of bipartite projections.","Write an abstract for a paper called Comparing Alternatives to the Fixed Degree Sequence Model for Extracting
  the Backbone of Bipartite Projections about Social and Information Networks"
2210.17107,Pascal Heid,"An adaptive damped Newton method for strongly monotone and Lipschitz
  continuous operator equations","['math.NA', 'cs.NA']","  We will consider the damped Newton method for strongly monotone and Lipschitz
continuous operator equations in a variational setting. We will provide a very
accessible justification why the undamped Newton method performs better than
its damped counterparts in a vicinity of a solution. Moreover, in the given
setting, an adaptive step-size strategy will be presented, which guarantees the
global convergence and favours an undamped update if admissible.
","[{'version': 'v1', 'created': 'Mon, 31 Oct 2022 07:32:02 GMT'}]",2022-11-01,['Numerical Analysis'],This paper presents an adaptive damped Newton method for solving strongly monotone and Lipschitz continuous operator equations. The proposed method is based on a damped Newton iteration and an adaptive step size control. The proposed method is shown to be globally and locally convergent under mild assumptions. Numerical experiments are conducted to illustrate the effectiveness of the proposed method and to compare it with some existing methods. The results show that the proposed method is more efficient than the existing methods.,"Write an abstract for a paper called An adaptive damped Newton method for strongly monotone and Lipschitz
  continuous operator equations about Numerical Analysis"
2212.08946,"Shaomu Tan, Denis Paperno","Towards leveraging latent knowledge and Dialogue context for real-world
  conversational question answering",['cs.CL'],"  In many real-world scenarios, the absence of external knowledge source like
Wikipedia restricts question answering systems to rely on latent internal
knowledge in limited dialogue data. In addition, humans often seek answers by
asking several questions for more comprehensive information. As the dialog
becomes more extensive, machines are challenged to refer to previous
conversation rounds to answer questions. In this work, we propose to leverage
latent knowledge in existing conversation logs via a neural Retrieval-Reading
system, enhanced with a TFIDF-based text summarizer refining lengthy
conversational history to alleviate the long context issue. Our experiments
show that our Retrieval-Reading system can exploit retrieved background
knowledge to generate significantly better answers. The results also indicate
that our context summarizer significantly helps both the retriever and the
reader by introducing more concise and less noisy contextual information.
","[{'version': 'v1', 'created': 'Sat, 17 Dec 2022 20:36:17 GMT'}]",2022-12-20,['Computation and Language'],This paper presents a novel approach to real-world conversational question answering (QA) about Computation and Language. We propose a method that leverages latent knowledge and dialogue context to improve the accuracy of QA. Our approach utilizes a multi-task learning framework to jointly learn the latent knowledge and dialogue context. We evaluate our method on a large-scale dataset of conversations and questions about Computation and Language. The results demonstrate that our approach achieves significant improvements in QA accuracy over the baseline methods. The proposed method can be used to improve the performance of QA systems in real-world applications.,"Write an abstract for a paper called Towards leveraging latent knowledge and Dialogue context for real-world
  conversational question answering about Computation and Language"
2207.09803,Tesshu Hanaka,Computing Densest $k$-Subgraph with Structural Parameters,['cs.DS'],"  \textsc{Densest $k$-Subgraph} is the problem to find a vertex subset $S$ of
size $k$ such that the number of edges in the subgraph induced by $S$ is
maximized. In this paper, we show that \textsc{Densest $k$-Subgraph} is fixed
parameter tractable when parameterized by neighborhood diversity, block
deletion number, distance-hereditary deletion number, and cograph deletion
number, respectively. Furthermore, we give a $2$-approximation
$2^{\tc(G)/2}n^{O(1)}$-time algorithm where $\tc(G)$ is the twin cover number
of an input graph $G$.
","[{'version': 'v1', 'created': 'Wed, 20 Jul 2022 10:33:53 GMT'}]",2022-07-21,['Data Structures and Algorithms'],"This paper presents a novel algorithm for computing the densest $k$-subgraph with structural parameters in a graph. The algorithm incorporates structural parameters such as minimum degree, maximum degree, minimum clique size, and maximum clique size into the problem of finding the densest $k$-subgraph. We present a data structure to represent the graph and an efficient algorithm to compute the densest $k$-subgraph with structural parameters. We analyze the time complexity of the algorithm and prove its correctness. Experimental results demonstrate that our algorithm is faster than existing algorithms in most cases.",Write an abstract for a paper called Computing Densest $k$-Subgraph with Structural Parameters about Data Structures and Algorithms
2208.07998,Alex Warstadt and Samuel R. Bowman,"What Artificial Neural Networks Can Tell Us About Human Language
  Acquisition",['cs.CL'],"  Rapid progress in machine learning for natural language processing has the
potential to transform debates about how humans learn language. However, the
learning environments and biases of current artificial learners and humans
diverge in ways that weaken the impact of the evidence obtained from learning
simulations. For example, today's most effective neural language models are
trained on roughly one thousand times the amount of linguistic data available
to a typical child. To increase the relevance of learnability results from
computational models, we need to train model learners without significant
advantages over humans. If an appropriate model successfully acquires some
target linguistic knowledge, it can provide a proof of concept that the target
is learnable in a hypothesized human learning scenario. Plausible model
learners will enable us to carry out experimental manipulations to make causal
inferences about variables in the learning environment, and to rigorously test
poverty-of-the-stimulus-style claims arguing for innate linguistic knowledge in
humans on the basis of speculations about learnability. Comparable experiments
will never be possible with human subjects due to practical and ethical
considerations, making model learners an indispensable resource. So far,
attempts to deprive current models of unfair advantages obtain sub-human
results for key grammatical behaviors such as acceptability judgments. But
before we can justifiably conclude that language learning requires more prior
domain-specific knowledge than current models possess, we must first explore
non-linguistic inputs in the form of multimodal stimuli and multi-agent
interaction as ways to make our learners more efficient at learning from
limited linguistic input.
","[{'version': 'v1', 'created': 'Wed, 17 Aug 2022 00:12:37 GMT'}]",2022-08-18,['Computation and Language'],"This paper explores the potential of artificial neural networks (ANNs) to inform our understanding of human language acquisition. It examines the use of ANNs to model aspects of language learning, from phonology to syntax, and considers how ANNs can be used to explore the complexity of language and its computational structure. The paper also examines the potential of ANNs to provide insights into the cognitive processes involved in language acquisition, such as the role of pattern recognition and the development of language-specific representations. Finally, the paper considers the implications of ANNs for the field of language acquisition research, and suggests directions for future research.","Write an abstract for a paper called What Artificial Neural Networks Can Tell Us About Human Language
  Acquisition about Computation and Language"
2204.01216,"Peter Washington, Aayush Nandkeolyar, Sam Yang","MLPro: A System for Hosting Crowdsourced Machine Learning Challenges for
  Open-Ended Research Problems","['cs.HC', 'cs.LG']","  The task of developing a machine learning (ML) model for a particular problem
is inherently open-ended, and there is an unbounded set of possible solutions.
Steps of the ML development pipeline, such as feature engineering, loss
function specification, data imputation, and dimensionality reduction, require
the engineer to consider an extensive and often infinite array of
possibilities. Successfully identifying high-performing solutions for an
unfamiliar dataset or problem requires a mix of mathematical prowess and
creativity applied towards inventing and repurposing novel ML methods. Here, we
explore the feasibility of hosting crowdsourced ML challenges to facilitate a
breadth-first exploration of open-ended research problems, thereby expanding
the search space of problem solutions beyond what a typical ML team could
viably investigate. We develop MLPro, a system which combines the notion of
open-ended ML coding problems with the concept of an automatic online code
judging platform. To conduct a pilot evaluation of this paradigm, we
crowdsource several open-ended ML challenges to ML and data science
practitioners. We describe results from two separate challenges. We find that
for sufficiently unconstrained and complex problems, many experts submit
similar solutions, but some experts provide unique solutions which outperform
the ""typical"" solution class. We suggest that automated expert crowdsourcing
systems such as MLPro have the potential to accelerate ML engineering
creativity.
","[{'version': 'v1', 'created': 'Mon, 4 Apr 2022 02:56:12 GMT'}]",2022-04-05,"['Human-Computer Interaction', 'Machine Learning']",", and Artificial Intelligence

This paper presents MLPro, a system for hosting crowdsourced machine learning challenges for open-ended research problems about human-computer interaction, machine learning, and artificial intelligence. MLPro is designed to enable data scientists, researchers, and practitioners to collaborate on challenging problems in these fields. The system provides a platform for users to submit and evaluate a wide variety of machine learning models. MLPro features a user-friendly web interface that allows users to easily create and manage challenges, as well as to track the progress of their submissions. Additionally, MLPro features a suite of tools that enable data exploration and analysis, and a library of pre-trained models that can be used to quickly get started with a challenge. The paper presents the system architecture, implementation details, and a case study of a successful MLPro challenge. Finally, the paper discusses the potential of MLPro for advancing research in the fields of human-computer interaction, machine learning, and artificial intelligence.","Write an abstract for a paper called MLPro: A System for Hosting Crowdsourced Machine Learning Challenges for
  Open-Ended Research Problems about Human-Computer Interaction, Machine Learning"
2202.09782,Youngmi Hur,Tight Wavelet Filter Banks with Prescribed Directions,"['math.NA', 'cs.NA']","  Constructing tight wavelet filter banks with prescribed directions is
challenging. This paper presents a systematic method for designing a tight
wavelet filter bank, given any prescribed directions. There are two types of
wavelet filters in our tight wavelet filter bank. One type is entirely
determined by the prescribed information about the directionality and makes the
wavelet filter bank directional. The other type helps the wavelet filter bank
to be tight. In addition to the flexibility in choosing the directions, our
construction method has other useful properties. It works for any
multi-dimension, and it allows the user to have any prescribed number of
vanishing moments along the chosen directions. Furthermore, our tight wavelet
filter banks have fast algorithms for analysis and synthesis. Concrete examples
are given to illustrate our construction method and properties of resulting
tight wavelet filter banks.
","[{'version': 'v1', 'created': 'Sun, 20 Feb 2022 10:51:13 GMT'}]",2022-02-22,['Numerical Analysis'],"This paper presents a novel approach to numerical analysis using tight wavelet filter banks with prescribed directions. The approach is based on a multiresolution analysis of signals and images, and is particularly well-suited for applications in which the directionality of the signal is important. The paper proposes a new algorithm for constructing tight wavelet filter banks with prescribed directions, and provides a theoretical analysis of the performance of the proposed algorithm. The proposed approach is then applied to a number of numerical examples, and the results are compared to existing methods. The paper concludes with a discussion of the potential applications of the proposed approach, and its limitations.",Write an abstract for a paper called Tight Wavelet Filter Banks with Prescribed Directions about Numerical Analysis
2212.08661,"Feng Qiu, Chengyang Xie, Yu Ding, Wanzeng Kong","EffMulti: Efficiently Modeling Complex Multimodal Interactions for
  Emotion Analysis","['cs.LG', 'cs.AI', 'cs.CL']","  Humans are skilled in reading the interlocutor's emotion from multimodal
signals, including spoken words, simultaneous speech, and facial expressions.
It is still a challenge to effectively decode emotions from the complex
interactions of multimodal signals. In this paper, we design three kinds of
multimodal latent representations to refine the emotion analysis process and
capture complex multimodal interactions from different views, including a
intact three-modal integrating representation, a modality-shared
representation, and three modality-individual representations. Then, a
modality-semantic hierarchical fusion is proposed to reasonably incorporate
these representations into a comprehensive interaction representation. The
experimental results demonstrate that our EffMulti outperforms the
state-of-the-art methods. The compelling performance benefits from its
well-designed framework with ease of implementation, lower computing
complexity, and less trainable parameters.
","[{'version': 'v1', 'created': 'Fri, 16 Dec 2022 03:05:55 GMT'}]",2022-12-20,"['Machine Learning', 'Artificial Intelligence', 'Computation and Language']","This paper presents EffMulti, an efficient computational model for analyzing complex multimodal interactions in emotion analysis. The model combines machine learning, artificial intelligence, and natural language processing techniques to accurately capture and interpret non-verbal cues and other multimodal signals. We demonstrate the model's effectiveness in emotion analysis tasks by evaluating it on a dataset of multimodal interactions. Results show that EffMulti outperforms existing models in terms of accuracy, speed, and scalability. Moreover, the model is able to capture the nuances of multimodal interactions in a way that traditional models cannot. Our findings suggest that EffMulti is a promising model for emotion analysis and can be used to develop more accurate and efficient multimodal emotion analysis systems.","Write an abstract for a paper called EffMulti: Efficiently Modeling Complex Multimodal Interactions for
  Emotion Analysis about Machine Learning, Artificial Intelligence, Computation and Language"
2206.08631,"Alexander Dobler, Martin N\""ollenburg",On Computing Optimal Linear Diagrams,"['cs.CG', 'cs.DS', 'cs.HC']","  Linear diagrams are an effective way to visualize set-based data by
representing elements as columns and sets as rows with one or more horizontal
line segments, whose vertical overlaps with other rows indicate set
intersections and their contained elements. The efficacy of linear diagrams
heavily depends on having few line segments. The underlying minimization
problem has already been explored heuristically, but its computational
complexity has yet to be classified. In this paper, we show that minimizing
line segments in linear diagrams is equivalent to a well-studied NP-hard
problem, and extend the NP-hardness to a restricted setting. We develop new
algorithms for computing linear diagrams with minimum number of line segments
that build on a traveling salesperson (TSP) formulation and allow constraints
on the element orders, namely, forcing two sets to be drawn as single line
segments, giving weights to sets, and allowing hierarchical constraints via
PQ-trees. We conduct an experimental evaluation and compare previous algorithms
for minimizing line segments with our TSP formulation, showing that a
state-of-the art TSP-solver can solve all considered instances optimally, most
of them within few milliseconds.
","[{'version': 'v1', 'created': 'Fri, 17 Jun 2022 08:44:15 GMT'}, {'version': 'v2', 'created': 'Wed, 17 Aug 2022 09:54:52 GMT'}]",2022-08-18,"['Computational Geometry', 'Data Structures and Algorithms', 'Human-Computer Interaction']","This paper presents a study on computing optimal linear diagrams for improved human-computer interaction. We introduce a computational geometry approach to the problem, which is based on data structures and algorithms. The paper also examines the design of linear diagrams for a range of applications, such as visualizing large datasets, providing interactive visualizations for data exploration, and supporting decision-making. We present an experimental evaluation of our approach, which shows that our results are superior to those of existing methods. Finally, we discuss the implications of our work for the design of interactive visualizations, as well as its potential applications in the field of human-computer interaction.","Write an abstract for a paper called On Computing Optimal Linear Diagrams about Computational Geometry, Data Structures and Algorithms, Human-Computer Interaction"
2303.08929,"Jaime Arias, Kyungmin Bae, Carlos Olarte, Peter Csaba \""Olveczky,
  Laure Petrucci, and Fredrik R{\o}mming","Symbolic Analysis and Parameter Synthesis for Time Petri Nets Using
  Maude and SMT Solving",['cs.LO'],"  Parametric time Petri nets with inhibitor arcs (PITPNs) support flexibility
for timed systems by allowing parameters in firing bounds. In this paper we
present and prove correct a concrete and a symbolic rewriting logic semantics
for PITPNs. We show how this allows us to use Maude combined with SMT solving
to provide sound and complete formal analyses for PITPNs. We develop a new
general folding approach for symbolic reachability that terminates whenever the
parametric state-class graph of the PITPN is finite. We explain how almost all
formal analysis and parameter synthesis supported by the state-of-the-art PITPN
tool Rom\'eo can be done in Maude with SMT. In addition, we also support
analysis and parameter synthesis from parametric initial markings, as well as
full LTL model checking and analysis with user-defined execution strategies.
Experiments on three benchmarks show that our methods outperform Rom\'eo in
many cases.
","[{'version': 'v1', 'created': 'Wed, 15 Mar 2023 20:55:41 GMT'}]",2023-03-17,['Logic in Computer Science'],"This paper presents a novel approach for symbolic analysis and parameter synthesis of Time Petri Nets (TPN) using Maude and Satisfiability Modulo Theories (SMT) solvers. We demonstrate the effectiveness of our approach by applying it to the analysis and synthesis of TPN models. We show that our approach can be used to verify the correctness of TPN models, as well as to synthesize parameters for TPN models. Our approach provides a powerful and efficient tool for the verification and synthesis of TPN models, and it is applicable to a wide range of practical problems. Furthermore, we discuss the implications of our approach for the field of Logic in Computer Science.","Write an abstract for a paper called Symbolic Analysis and Parameter Synthesis for Time Petri Nets Using
  Maude and SMT Solving about Logic in Computer Science"
2205.1572,"Along He, Kai Wang, Tao Li, Wang Bo, Hong Kang, Huazhu Fu","Progressive Multi-scale Consistent Network for Multi-class Fundus Lesion
  Segmentation","['eess.IV', 'cs.CV']","  Effectively integrating multi-scale information is of considerable
significance for the challenging multi-class segmentation of fundus lesions
because different lesions vary significantly in scales and shapes. Several
methods have been proposed to successfully handle the multi-scale object
segmentation. However, two issues are not considered in previous studies. The
first is the lack of interaction between adjacent feature levels, and this will
lead to the deviation of high-level features from low-level features and the
loss of detailed cues. The second is the conflict between the low-level and
high-level features, this occurs because they learn different scales of
features, thereby confusing the model and decreasing the accuracy of the final
prediction. In this paper, we propose a progressive multi-scale consistent
network (PMCNet) that integrates the proposed progressive feature fusion (PFF)
block and dynamic attention block (DAB) to address the aforementioned issues.
Specifically, PFF block progressively integrates multi-scale features from
adjacent encoding layers, facilitating feature learning of each layer by
aggregating fine-grained details and high-level semantics. As features at
different scales should be consistent, DAB is designed to dynamically learn the
attentive cues from the fused features at different scales, thus aiming to
smooth the essential conflicts existing in multi-scale features. The two
proposed PFF and DAB blocks can be integrated with the off-the-shelf backbone
networks to address the two issues of multi-scale and feature inconsistency in
the multi-class segmentation of fundus lesions, which will produce better
feature representation in the feature space. Experimental results on three
public datasets indicate that the proposed method is more effective than recent
state-of-the-art methods.
","[{'version': 'v1', 'created': 'Tue, 31 May 2022 12:10:01 GMT'}]",2022-06-01,['Computer Vision and Pattern Recognition'],"This paper proposes a Progressive Multi-scale Consistent Network (PMCN) for multi-class fundus lesion segmentation. The proposed network consists of a multi-scale feature extractor and a progressive consistency module. The multi-scale feature extractor is designed to extract multi-scale features from the fundus images. The progressive consistency module is designed to learn the multi-scale features and predict the segmentation results. The proposed network is evaluated on two public datasets, DRIVE and STARE, and achieves competitive performance compared to the state-of-the-art. The results demonstrate that the proposed PMCN is able to accurately segment fundus lesions in various scales and can be a promising tool for computer vision and pattern recognition.","Write an abstract for a paper called Progressive Multi-scale Consistent Network for Multi-class Fundus Lesion
  Segmentation about Computer Vision and Pattern Recognition"
2204.00833,"Jing He, Yiyi Zhou, Qi Zhang, Jun Peng, Yunhang Shen, Xiaoshuai Sun,
  Chao Chen, Rongrong Ji","PixelFolder: An Efficient Progressive Pixel Synthesis Network for Image
  Generation","['cs.CV', 'eess.IV']","  Pixel synthesis is a promising research paradigm for image generation, which
can well exploit pixel-wise prior knowledge for generation. However, existing
methods still suffer from excessive memory footprint and computation overhead.
In this paper, we propose a progressive pixel synthesis network towards
efficient image generation, coined as PixelFolder. Specifically, PixelFolder
formulates image generation as a progressive pixel regression problem and
synthesizes images via a multi-stage structure, which can greatly reduce the
overhead caused by large tensor transformations. In addition, we introduce
novel pixel folding operations to further improve model efficiency while
maintaining pixel-wise prior knowledge for end-to-end regression. With these
innovative designs, we greatly reduce the expenditure of pixel synthesis, e.g.,
reducing 89% computation and 53% parameters compared with the latest pixel
synthesis method CIPS. To validate our approach, we conduct extensive
experiments on two benchmark datasets, namely FFHQ and LSUN Church. The
experimental results show that with much less expenditure, PixelFolder obtains
new state-of-the-art (SOTA) performance on two benchmark datasets, i.e., 3.77
FID and 2.45 FID on FFHQ and LSUN Church, respectively.Meanwhile, PixelFolder
is also more efficient than the SOTA methods like StyleGAN2, reducing about 72%
computation and 31% parameters, respectively. These results greatly validate
the effectiveness of the proposed PixelFolder.
","[{'version': 'v1', 'created': 'Sat, 2 Apr 2022 10:55:11 GMT'}, {'version': 'v2', 'created': 'Sun, 5 Jun 2022 06:24:44 GMT'}, {'version': 'v3', 'created': 'Mon, 25 Jul 2022 04:13:03 GMT'}, {'version': 'v4', 'created': 'Wed, 27 Jul 2022 06:40:18 GMT'}]",2022-07-28,['Computer Vision and Pattern Recognition'],"PixelFolder is a novel network for image generation, which utilizes a progressive pixel synthesis approach to generate high-quality images with fewer parameters. The network is composed of two modules: a content encoder and a pixel synthesizer. The content encoder extracts the content of an image and the pixel synthesizer progressively refines the image by synthesizing new pixels. Experiments on several benchmark datasets demonstrate that PixelFolder achieves state-of-the-art performance in terms of image quality and efficiency. The proposed PixelFolder network is expected to have applications in computer vision and pattern recognition tasks, such as image generation, style transfer, and super-resolution.","Write an abstract for a paper called PixelFolder: An Efficient Progressive Pixel Synthesis Network for Image
  Generation about Computer Vision and Pattern Recognition"
2211.10598,"Chuanfu Shen, Chao Fan, Wei Wu, Rui Wang, George Q. Huang, Shiqi Yu",LidarGait: Benchmarking 3D Gait Recognition with Point Clouds,['cs.CV'],"  Video-based gait recognition has achieved impressive results in constrained
scenarios. However, visual cameras neglect human 3D structure information,
which limits the feasibility of gait recognition in the 3D wild world. Instead
of extracting gait features from images, this work explores precise 3D gait
features from point clouds and proposes a simple yet efficient 3D gait
recognition framework, termed LidarGait. Our proposed approach projects sparse
point clouds into depth maps to learn the representations with 3D geometry
information, which outperforms existing point-wise and camera-based methods by
a significant margin. Due to the lack of point cloud datasets, we built the
first large-scale LiDAR-based gait recognition dataset, SUSTech1K, collected by
a LiDAR sensor and an RGB camera. The dataset contains 25,239 sequences from
1,050 subjects and covers many variations, including visibility, views,
occlusions, clothing, carrying, and scenes. Extensive experiments show that (1)
3D structure information serves as a significant feature for gait recognition.
(2) LidarGait outperforms existing point-based and silhouette-based methods by
a significant margin, while it also offers stable cross-view results. (3) The
LiDAR sensor is superior to the RGB camera for gait recognition in the outdoor
environment. The source code and dataset have been made available at
https://lidargait.github.io.
","[{'version': 'v1', 'created': 'Sat, 19 Nov 2022 06:23:08 GMT'}, {'version': 'v2', 'created': 'Thu, 30 Mar 2023 07:51:03 GMT'}]",2023-03-31,['Computer Vision and Pattern Recognition'],"This paper presents LidarGait, a benchmark dataset for 3D gait recognition using point clouds. The dataset contains over 1,000 walking sequences of 25 subjects, captured with a 3D lidar scanner. The dataset is divided into a training set and a test set, and can be used to evaluate the performance of 3D gait recognition algorithms. We also provide a baseline algorithm for 3D gait recognition, which achieves an average recognition accuracy of 0.84 on the test set. The dataset and the baseline algorithm are publicly available, and can be used as a starting point for future research in 3D gait recognition.",Write an abstract for a paper called LidarGait: Benchmarking 3D Gait Recognition with Point Clouds about Computer Vision and Pattern Recognition
2102.0157,"Sitan Chen, Zhao Song, Runzhou Tao, Ruizhe Zhang",Symmetric Sparse Boolean Matrix Factorization and Applications,"['cs.LG', 'cs.CR', 'cs.DS', 'stat.ML']","  In this work, we study a variant of nonnegative matrix factorization where we
wish to find a symmetric factorization of a given input matrix into a sparse,
Boolean matrix. Formally speaking, given $\mathbf{M}\in\mathbb{Z}^{m\times m}$,
we want to find $\mathbf{W}\in\{0,1\}^{m\times r}$ such that $\| \mathbf{M} -
\mathbf{W}\mathbf{W}^\top \|_0$ is minimized among all $\mathbf{W}$ for which
each row is $k$-sparse. This question turns out to be closely related to a
number of questions like recovering a hypergraph from its line graph, as well
as reconstruction attacks for private neural network training.
  As this problem is hard in the worst-case, we study a natural average-case
variant that arises in the context of these reconstruction attacks: $\mathbf{M}
= \mathbf{W}\mathbf{W}^{\top}$ for $\mathbf{W}$ a random Boolean matrix with
$k$-sparse rows, and the goal is to recover $\mathbf{W}$ up to column
permutation. Equivalently, this can be thought of as recovering a uniformly
random $k$-uniform hypergraph from its line graph.
  Our main result is a polynomial-time algorithm for this problem based on
bootstrapping higher-order information about $\mathbf{W}$ and then decomposing
an appropriate tensor. The key ingredient in our analysis, which may be of
independent interest, is to show that such a matrix $\mathbf{W}$ has full
column rank with high probability as soon as $m = \widetilde{\Omega}(r)$, which
we do using tools from Littlewood-Offord theory and estimates for binary
Krawtchouk polynomials.
","[{'version': 'v1', 'created': 'Tue, 2 Feb 2021 15:52:52 GMT'}, {'version': 'v2', 'created': 'Sat, 20 Nov 2021 16:38:20 GMT'}, {'version': 'v3', 'created': 'Thu, 13 Jan 2022 15:25:46 GMT'}]",2022-01-14,"['Machine Learning', 'Cryptography and Security', 'Data Structures and Algorithms']","This paper studies the problem of symmetric sparse Boolean matrix factorization and its applications in machine learning, cryptography and security, and data structures and algorithms. We propose a new algorithm for symmetric sparse Boolean matrix factorization, and show that it has a better time complexity than existing algorithms. We then discuss the applications of the algorithm in machine learning, cryptography and security, and data structures and algorithms. We demonstrate that the algorithm can be used to improve the accuracy of machine learning models, increase the security of cryptographic systems, and improve the efficiency of data structures and algorithms. Finally, we present the experimental results of our proposed algorithm and compare it with existing algorithms.","Write an abstract for a paper called Symmetric Sparse Boolean Matrix Factorization and Applications about Machine Learning, Cryptography and Security, Data Structures and Algorithms"
2205.12698,"Damilola Omitaomu, Shabnam Tafreshi, Tingting Liu, Sven Buechel, Chris
  Callison-Burch, Johannes Eichstaedt, Lyle Ungar, Jo\~ao Sedoc","Empathic Conversations: A Multi-level Dataset of Contextualized
  Conversations",['cs.CL'],"  Empathy is a cognitive and emotional reaction to an observed situation of
others. Empathy has recently attracted interest because it has numerous
applications in psychology and AI, but it is unclear how different forms of
empathy (e.g., self-report vs counterpart other-report, concern vs. distress)
interact with other affective phenomena or demographics like gender and age. To
better understand this, we created the {\it Empathic Conversations} dataset of
annotated negative, empathy-eliciting dialogues in which pairs of participants
converse about news articles. People differ in their perception of the empathy
of others. These differences are associated with certain characteristics such
as personality and demographics. Hence, we collected detailed characterization
of the participants' traits, their self-reported empathetic response to news
articles, their conversational partner other-report, and turn-by-turn
third-party assessments of the level of self-disclosure, emotion, and empathy
expressed. This dataset is the first to present empathy in multiple forms along
with personal distress, emotion, personality characteristics, and person-level
demographic information. We present baseline models for predicting some of
these features from conversations.
","[{'version': 'v1', 'created': 'Wed, 25 May 2022 11:56:29 GMT'}]",2022-05-26,['Computation and Language'],"This paper introduces a multi-level dataset of contextualized conversations about computation and language. The dataset consists of conversations between two people, each with different levels of expertise in the topics discussed. The conversations were collected from a variety of sources, including online forums, web-based chatrooms, and other online conversations. The conversations were then analyzed to identify topics of interest. The dataset is designed to provide an understanding of how people with different levels of expertise interact when discussing computation and language. The paper also provides a detailed description of the dataset and its various components, and discusses how it can be used to study empathic conversations. Finally, the paper outlines the potential applications of the dataset, highlighting its value in research on computational linguistics, natural language processing, and human-computer interaction.","Write an abstract for a paper called Empathic Conversations: A Multi-level Dataset of Contextualized
  Conversations about Computation and Language"
2205.12236,"Bharadwaj Satchidanandan, Mardavij Roozbehani, and Munther A. Dahleh",A Two-Stage Mechanism for Demand Response Markets,"['eess.SY', 'cs.SY', 'econ.TH']","  Demand response involves system operators using incentives to modulate
electricity consumption during peak hours or when faced with an incidental
supply shortage. However, system operators typically have imperfect information
about their customers' baselines, that is, their consumption had the incentive
been absent. The standard approach to estimate the reduction in a customer's
electricity consumption then is to estimate their counterfactual baseline.
However, this approach is not robust to estimation errors or strategic
exploitation by the customers and can potentially lead to overpayments to
customers who do not reduce their consumption and underpayments to those who
do. Moreover, optimal power consumption reductions of the customers depend on
the costs that they incur for curtailing consumption, which in general are
private knowledge of the customers, and which they could strategically
misreport in an effort to improve their own utilities even if it deteriorates
the overall system cost. The two-stage mechanism proposed in this paper
circumvents the aforementioned issues. In the day-ahead market, the
participating loads are required to submit only a probabilistic description of
their next-day consumption and costs to the system operator for day-ahead
planning. It is only in real-time, if and when called upon for demand response,
that the loads are required to report their baselines and costs. They receive
credits for reductions below their reported baselines. The mechanism for
calculating the credits guarantees incentive compatibility of truthful
reporting of the probability distribution in the day-ahead market and truthful
reporting of the baseline and cost in real-time. The mechanism can be viewed as
an extension of the celebrated Vickrey-Clarke-Groves mechanism augmented with a
carefully crafted second-stage penalty for deviations from the day-ahead bids.
","[{'version': 'v1', 'created': 'Tue, 24 May 2022 17:44:47 GMT'}, {'version': 'v2', 'created': 'Wed, 22 Jun 2022 16:44:55 GMT'}]",2022-06-23,['Systems and Control'],This paper presents a two-stage mechanism for demand response markets that uses systems and control theory to improve the efficiency and effectiveness of the market. The proposed mechanism consists of a first-stage optimization problem which determines the optimal prices for each demand response resource and a second-stage optimization problem which determines the optimal dispatch of the resources. The paper presents a mathematical formulation of the two-stage mechanism and provides a numerical example to illustrate its performance. The results demonstrate that the proposed two-stage mechanism can increase the efficiency and effectiveness of the demand response market compared to existing approaches.,Write an abstract for a paper called A Two-Stage Mechanism for Demand Response Markets about Systems and Control
2205.12381,"Siddharth Reddy, Sergey Levine, Anca D. Dragan","First Contact: Unsupervised Human-Machine Co-Adaptation via Mutual
  Information Maximization","['cs.LG', 'cs.HC', 'cs.RO']","  How can we train an assistive human-machine interface (e.g., an
electromyography-based limb prosthesis) to translate a user's raw command
signals into the actions of a robot or computer when there is no prior mapping,
we cannot ask the user for supervision in the form of action labels or reward
feedback, and we do not have prior knowledge of the tasks the user is trying to
accomplish? The key idea in this paper is that, regardless of the task, when an
interface is more intuitive, the user's commands are less noisy. We formalize
this idea as a completely unsupervised objective for optimizing interfaces: the
mutual information between the user's command signals and the induced state
transitions in the environment. To evaluate whether this mutual information
score can distinguish between effective and ineffective interfaces, we conduct
an observational study on 540K examples of users operating various keyboard and
eye gaze interfaces for typing, controlling simulated robots, and playing video
games. The results show that our mutual information scores are predictive of
the ground-truth task completion metrics in a variety of domains, with an
average Spearman's rank correlation of 0.43. In addition to offline evaluation
of existing interfaces, we use our unsupervised objective to learn an interface
from scratch: we randomly initialize the interface, have the user attempt to
perform their desired tasks using the interface, measure the mutual information
score, and update the interface to maximize mutual information through
reinforcement learning. We evaluate our method through a user study with 12
participants who perform a 2D cursor control task using a perturbed mouse, and
an experiment with one user playing the Lunar Lander game using hand gestures.
The results show that we can learn an interface from scratch, without any user
supervision or prior knowledge of tasks, in under 30 minutes.
","[{'version': 'v1', 'created': 'Tue, 24 May 2022 21:57:18 GMT'}, {'version': 'v2', 'created': 'Thu, 15 Sep 2022 03:08:54 GMT'}]",2022-09-16,"['Machine Learning', 'Human-Computer Interaction', 'Robotics']","This paper presents an unsupervised learning approach for human-machine co-adaptation, based on mutual information maximization. The approach is designed to enable machines to learn from human interaction, in order to acquire an understanding of the human’s preferences and preferences in the environment. We demonstrate our approach through a simulated robotic system interacting with a human. The results show that our approach can enable a robot to learn from human interaction, and to adapt its behavior based on the human’s preferences. We also discuss the implications of our approach for the field of human-computer interaction, and for robotics in general.","Write an abstract for a paper called First Contact: Unsupervised Human-Machine Co-Adaptation via Mutual
  Information Maximization about Machine Learning, Human-Computer Interaction, Robotics"
2206.05353,Joseph O'Rourke,Hamiltonian Quasigeodesics yield Nets,['cs.CG'],"  This note establishes that every polyhedron that has a Hamiltonian
quasigeodesic can be edge-unfolded to a net.
","[{'version': 'v1', 'created': 'Fri, 10 Jun 2022 20:51:32 GMT'}]",2022-06-14,['Computational Geometry'],"This paper presents a novel approach to the study of computational geometry. We introduce a new concept called Hamiltonian quasigeodesics, which are special curves that can be used to construct nets in Euclidean space. We show that these nets are more efficient than traditional methods for solving geometric problems, and demonstrate how they can be used to construct nets for a variety of shapes. We also discuss the practical applications of this approach for solving problems in computational geometry. Finally, we provide a numerical example to illustrate the effectiveness of the proposed approach.",Write an abstract for a paper called Hamiltonian Quasigeodesics yield Nets about Computational Geometry
2302.01626,"Shunyu Zhang, Yaobo Liang, Ming Gong, Daxin Jiang, Nan Duan","Modeling Sequential Sentence Relation to Improve Cross-lingual Dense
  Retrieval","['cs.CL', 'cs.IR']","  Recently multi-lingual pre-trained language models (PLM) such as mBERT and
XLM-R have achieved impressive strides in cross-lingual dense retrieval.
Despite its successes, they are general-purpose PLM while the multilingual PLM
tailored for cross-lingual retrieval is still unexplored. Motivated by an
observation that the sentences in parallel documents are approximately in the
same order, which is universal across languages, we propose to model this
sequential sentence relation to facilitate cross-lingual representation
learning. Specifically, we propose a multilingual PLM called masked sentence
model (MSM), which consists of a sentence encoder to generate the sentence
representations, and a document encoder applied to a sequence of sentence
vectors from a document. The document encoder is shared for all languages to
model the universal sequential sentence relation across languages. To train the
model, we propose a masked sentence prediction task, which masks and predicts
the sentence vector via a hierarchical contrastive loss with sampled negatives.
Comprehensive experiments on four cross-lingual retrieval tasks show MSM
significantly outperforms existing advanced pre-training models, demonstrating
the effectiveness and stronger cross-lingual retrieval capabilities of our
approach. Code and model will be available.
","[{'version': 'v1', 'created': 'Fri, 3 Feb 2023 09:54:27 GMT'}]",2023-02-06,"['Computation and Language', 'Information Retrieval']",This paper presents a novel approach to cross-lingual dense retrieval using a model to capture sequential sentence relation. We propose a model that uses a combination of feature engineering and deep learning techniques to capture semantic dependencies between sentences. We apply this model to two different tasks: a cross-lingual retrieval task and a cross-lingual sentence similarity task. Experiments on a dataset of English and Chinese documents show that our model outperforms several baseline methods. We also discuss the implications of our approach for improving cross-lingual information retrieval tasks.,"Write an abstract for a paper called Modeling Sequential Sentence Relation to Improve Cross-lingual Dense
  Retrieval about Computation and Language, Information Retrieval"
2205.00918,Akansha,"Decay estimate of bivariate Chebyshev coefficients for functions with
  limited smoothness","['math.NA', 'cs.NA']","  We obtain the decay bounds for Chebyshev series coefficients of functions
with finite Vitali variation on the unit square. A generalization of the well
known identity, which relates exact and approximated coefficients, obtained
using the quadrature formula, is derived. Finally, an asymptotic
$L^1$-approximation error of finite partial sum for functions of bounded
variation in sense of Vitali as well as Hardy-Krause, on the unit square is
deduced.
","[{'version': 'v1', 'created': 'Mon, 2 May 2022 13:59:25 GMT'}, {'version': 'v2', 'created': 'Wed, 11 May 2022 17:15:34 GMT'}]",2022-05-12,['Numerical Analysis'],"This paper presents a decay estimate of bivariate Chebyshev coefficients for functions with limited smoothness. The focus of the paper is on numerical analysis techniques to determine the decay rate of bivariate Chebyshev coefficients for functions with limited smoothness. In particular, the paper presents a novel approach to estimate the decay rate of bivariate Chebyshev coefficients based on a combination of weighted Sobolev embedding, Sobolev embedding, and Bernstein approximation. The paper also presents numerical experiments to demonstrate the effectiveness of the proposed approach for estimating the decay rate of bivariate Chebyshev coefficients.","Write an abstract for a paper called Decay estimate of bivariate Chebyshev coefficients for functions with
  limited smoothness about Numerical Analysis"
2209.15565,"Rindranirina Ramamonjison, Haley Li, Timothy T. Yu, Shiqi He, Vishnu
  Rengan, Amin Banitalebi-Dehkordi, Zirui Zhou, Yong Zhang","Augmenting Operations Research with Auto-Formulation of Optimization
  Models from Problem Descriptions","['cs.CL', 'cs.AI']","  We describe an augmented intelligence system for simplifying and enhancing
the modeling experience for operations research. Using this system, the user
receives a suggested formulation of an optimization problem based on its
description. To facilitate this process, we build an intuitive user interface
system that enables the users to validate and edit the suggestions. We
investigate controlled generation techniques to obtain an automatic suggestion
of formulation. Then, we evaluate their effectiveness with a newly created
dataset of linear programming problems drawn from various application domains.
","[{'version': 'v1', 'created': 'Fri, 30 Sep 2022 16:24:36 GMT'}, {'version': 'v2', 'created': 'Tue, 11 Oct 2022 21:12:00 GMT'}]",2022-10-13,"['Computation and Language', 'Artificial Intelligence']","This paper explores the potential of augmenting Operations Research (OR) with Auto-Formulation of Optimization Models from Problem Descriptions about Computation and Language, Artificial Intelligence (AI). Specifically, it investigates the application of AI techniques to automatically generate optimization models from problem descriptions, and how this can improve the efficiency of OR. The paper will discuss the advantages of using AI in OR, and analyze existing approaches to auto-formulation. Finally, the authors will present a case study to illustrate the potential of using AI in OR, and offer suggestions for future research in this area.","Write an abstract for a paper called Augmenting Operations Research with Auto-Formulation of Optimization
  Models from Problem Descriptions about Computation and Language, Artificial Intelligence"
2205.08782,"Ali Bereyhi and Bruno Loureiro and Florent Krzakala and Ralf R.
  M\""uller and Hermann Schulz-Baldes",Secure Coding via Gaussian Random Fields,"['cs.IT', 'cs.CR', 'math.IT']","  Inverse probability problems whose generative models are given by strictly
nonlinear Gaussian random fields show the all-or-nothing behavior: There exists
a critical rate at which Bayesian inference exhibits a phase transition. Below
this rate, the optimal Bayesian estimator recovers the data perfectly, and
above it the recovered data becomes uncorrelated. This study uses the replica
method from the theory of spin glasses to show that this critical rate is the
channel capacity. This interesting finding has a particular application to the
problem of secure transmission: A strictly nonlinear Gaussian random field
along with random binning can be used to securely encode a confidential message
in a wiretap channel. Our large-system characterization demonstrates that this
secure coding scheme asymptotically achieves the secrecy capacity of the
Gaussian wiretap channel.
","[{'version': 'v1', 'created': 'Wed, 18 May 2022 08:06:44 GMT'}]",2022-05-19,"['Information Theory', 'Cryptography and Security']","This paper explores the concept of secure coding via Gaussian random fields in the context of information theory, cryptography, and security. It begins by introducing the fundamentals of secure coding and Gaussian random fields, and then moves on to discuss how this approach can be used to protect data and communications in secure systems. The paper then examines the application of Gaussian random fields to various security protocols, such as secure key exchange, authentication, and digital signature schemes. Finally, it provides a discussion of the advantages and disadvantages of using Gaussian random fields for secure coding, as well as potential future research directions in this area. This paper provides a comprehensive overview of secure coding via Gaussian random fields and its potential applications in the field of information security.","Write an abstract for a paper called Secure Coding via Gaussian Random Fields about Information Theory, Cryptography and Security"
2104.02735,"Berthy T. Feng, Alexander C. Ogren, Chiara Daraio, Katherine L. Bouman","Visual Vibration Tomography: Estimating Interior Material Properties
  from Monocular Video","['cs.CV', 'eess.IV']","  An object's interior material properties, while invisible to the human eye,
determine motion observed on its surface. We propose an approach that estimates
heterogeneous material properties of an object from a monocular video of its
surface vibrations. Specifically, we show how to estimate Young's modulus and
density throughout a 3D object with known geometry. Knowledge of how these
values change across the object is useful for simulating its motion and
characterizing any defects. Traditional non-destructive testing approaches,
which often require expensive instruments, generally estimate only homogenized
material properties or simply identify the presence of defects. In contrast,
our approach leverages monocular video to (1) identify image-space modes from
an object's sub-pixel motion, and (2) directly infer spatially-varying Young's
modulus and density values from the observed modes. We demonstrate our approach
on both simulated and real videos.
","[{'version': 'v1', 'created': 'Tue, 6 Apr 2021 18:05:27 GMT'}, {'version': 'v2', 'created': 'Thu, 30 Sep 2021 21:17:06 GMT'}, {'version': 'v3', 'created': 'Sat, 1 Jan 2022 23:26:06 GMT'}]",2022-01-04,['Computer Vision and Pattern Recognition'],"This paper presents a novel approach for estimating interior material properties from monocular video using visual vibration tomography (VVT). VVT is a computer vision and pattern recognition technique that combines motion analysis and image processing to measure the properties of materials within a closed volume. The method is based on the observation that vibrations of a material can be detected through the motion of its surface. We demonstrate that the VVT approach can accurately estimate the material density, elastic modulus, and damping coefficient of a range of materials from video recordings. The proposed method is evaluated on a variety of materials, including wood, steel, concrete, and foam, using both simulated and real-world data. The results show that VVT is able to accurately estimate the material properties of the objects. Furthermore, the method is shown to be robust to noise and lighting changes, making it suitable for a wide range of applications.","Write an abstract for a paper called Visual Vibration Tomography: Estimating Interior Material Properties
  from Monocular Video about Computer Vision and Pattern Recognition"
2106.0661,"Soledad Villar (JHU), David W. Hogg (Flatiron, NYU), Kate
  Storey-Fisher (NYU), Weichi Yao (NYU), Ben Blum-Smith (NYU)","Scalars are universal: Equivariant machine learning, structured like
  classical physics","['cs.LG', 'math-ph', 'math.MP', 'stat.ML']","  There has been enormous progress in the last few years in designing neural
networks that respect the fundamental symmetries and coordinate freedoms of
physical law. Some of these frameworks make use of irreducible representations,
some make use of high-order tensor objects, and some apply symmetry-enforcing
constraints. Different physical laws obey different combinations of fundamental
symmetries, but a large fraction (possibly all) of classical physics is
equivariant to translation, rotation, reflection (parity), boost (relativity),
and permutations. Here we show that it is simple to parameterize universally
approximating polynomial functions that are equivariant under these symmetries,
or under the Euclidean, Lorentz, and Poincar\'e groups, at any dimensionality
$d$. The key observation is that nonlinear O($d$)-equivariant (and
related-group-equivariant) functions can be universally expressed in terms of a
lightweight collection of scalars -- scalar products and scalar contractions of
the scalar, vector, and tensor inputs. We complement our theory with numerical
examples that show that the scalar-based method is simple, efficient, and
scalable.
","[{'version': 'v1', 'created': 'Fri, 11 Jun 2021 20:51:38 GMT'}, {'version': 'v2', 'created': 'Fri, 29 Oct 2021 18:49:04 GMT'}, {'version': 'v3', 'created': 'Fri, 21 Oct 2022 01:14:58 GMT'}, {'version': 'v4', 'created': 'Tue, 7 Feb 2023 21:12:30 GMT'}]",2023-02-09,['Machine Learning'],"This paper investigates the use of equivariant machine learning for structured classical physics problems. We show that scalars are a universal representation for the underlying physics of these problems, and demonstrate how equivariant machine learning can be used to efficiently encode and solve these problems. We discuss how equivariant machine learning is a natural extension of classical physics, and how it can be used to reduce the complexity of classical physics problems. We also discuss the implications of this approach for the development of more general machine learning algorithms. Finally, we provide examples of classical physics problems solved using equivariant machine learning and discuss the potential of this approach in the future.","Write an abstract for a paper called Scalars are universal: Equivariant machine learning, structured like
  classical physics about Machine Learning"
2202.13772,Fang Da and Yu Zhang,Path-Aware Graph Attention for HD Maps in Motion Prediction,"['cs.AI', 'cs.LG', 'cs.RO']","  The success of motion prediction for autonomous driving relies on integration
of information from the HD maps. As maps are naturally graph-structured,
investigation on graph neural networks (GNNs) for encoding HD maps is
burgeoning in recent years. However, unlike many other applications where GNNs
have been straightforwardly deployed, HD maps are heterogeneous graphs where
vertices (lanes) are connected by edges (lane-lane interaction relationships)
of various nature, and most graph-based models are not designed to understand
the variety of edge types which provide crucial cues for predicting how the
agents would travel the lanes. To overcome this challenge, we propose
Path-Aware Graph Attention, a novel attention architecture that infers the
attention between two vertices by parsing the sequence of edges forming the
paths that connect them. Our analysis illustrates how the proposed attention
mechanism can facilitate learning in a didactic problem where existing graph
networks like GCN struggle. By improving map encoding, the proposed model
surpasses previous state of the art on the Argoverse Motion Forecasting
dataset, and won the first place in the 2021 Argoverse Motion Forecasting
Competition.
","[{'version': 'v1', 'created': 'Wed, 23 Feb 2022 09:43:47 GMT'}]",2022-09-07,"['Artificial Intelligence', 'Machine Learning', 'Robotics']","This paper presents a novel path-aware graph attention method for motion prediction of HD maps in autonomous robotics. This method leverages the advantages of graph neural networks to encode the topological information of the environment and the path-awareness of the robot to make more accurate predictions. The proposed method is evaluated on a real-world autonomous navigation dataset and outperforms traditional methods. Moreover, the results demonstrate that the proposed method is able to capture the motion patterns of the environment, leading to more accurate motion predictions. The proposed method is expected to be a promising approach for motion prediction in autonomous robotics.","Write an abstract for a paper called Path-Aware Graph Attention for HD Maps in Motion Prediction about Artificial Intelligence, Machine Learning, Robotics"
2112.1356,Gurmu Meseret Debele,"A Survey of Event-triggered Control for Nonlinear Multiagent Systems
  with Guaranteed Steady-State Performance",['cs.MA'],"  With the gradual advancement of a novel idea of the distributed control of
the multiagent systems, an event-triggered control protocol has received
significant research attention, especially in designing the controller for the
nonlinear multiagent system. Compared to other widely used control conditions,
the event-triggered control of the nonlinear system has a significant
capability to improve resource utilization in real-life scenarios such as using
and controlling the intelligent control input of each agent. It is worth
mentioning that a group of interconnected agents have a network communication
topology to transmit the feedback information state across the networked link.
The transmission of information among a group of agents ensures that each agent
reaches the consensus agreement cooperatively. The cooperative protocol of the
distributed control of nonlinear multiagent system also ensures the proper
information flow between each agent, irrespective of communication delays,
variability of environment, and switching of the communication topology via the
event-triggered control protocol. Consequently, event-triggered control for
nonlinear multi-agent systems via steady-state performance will be investigated
in this paper. The steady-state performances of a nonlinear closed-loop system
demonstrate the stabilization, output regulation, and output synchronization
problem of the nonlinear system using proper control protocol to achieve a
consensus in a multiagent system will also be discussed. Based on the
steady-state conditions of the nonlinear system, the consensus agreement among
the agents will be realized.
","[{'version': 'v1', 'created': 'Mon, 27 Dec 2021 08:10:03 GMT'}, {'version': 'v2', 'created': 'Sat, 1 Jan 2022 02:03:02 GMT'}]",2022-01-04,['Multiagent Systems'],"This paper presents a survey of event-triggered control techniques for nonlinear multiagent systems that guarantee steady-state performance. The paper begins by providing an overview of multiagent systems and their characteristics, followed by a discussion of the challenges associated with controlling such systems. Subsequently, a comprehensive review of the existing event-triggered control approaches for nonlinear multiagent systems is presented, including the underlying principles, implementation details, and performance guarantees. A comparison of the approaches is also provided. Finally, the paper concludes with a discussion of open problems and future research directions in the area of event-triggered control for nonlinear multiagent systems.","Write an abstract for a paper called A Survey of Event-triggered Control for Nonlinear Multiagent Systems
  with Guaranteed Steady-State Performance about Multiagent Systems"
2212.10966,Nevena Jakovcevic Stor and Ivan Slapnicar,"Fast multiplication, determinants, and inverses of arrowhead and
  diagonal-plus-rank-one matrices over associative fields","['math.NA', 'cs.NA']","  The article considers arrowhead and diagonal-plus-rank-one matrices in
F^(nxn) where F in R,C or H. H is a non-commutative field of quaternions. We
give unified formulas for fast matrix-vector multiplications, determinants, and
inverses for considered matrices. The formulas are unified in the sense that
the same formula holds in both, commutative and noncommutative algebras. Each
formula requires O(n) arithmetic operations. Most of the formulas hold for
block matrices, as well.
","[{'version': 'v1', 'created': 'Wed, 21 Dec 2022 12:10:17 GMT'}]",2022-12-22,['Numerical Analysis'],"This paper presents a novel numerical analysis approach to fast multiplication, determinants, and inverses of arrowhead and diagonal-plus-rank-one matrices over associative fields. We introduce an efficient algorithm for computing these operations that exploits the properties of the associative field to reduce the computational complexity. We then experimentally compare the performance of our algorithm to existing methods, and demonstrate that our approach is faster and more accurate. Finally, we discuss potential applications and implications of our results.","Write an abstract for a paper called Fast multiplication, determinants, and inverses of arrowhead and
  diagonal-plus-rank-one matrices over associative fields about Numerical Analysis"
2211.07886,"Qin Zhang, Shangsi Chen, Dongkuan Xu, Qingqing Cao, Xiaojun Chen,
  Trevor Cohn, Meng Fang",A Survey for Efficient Open Domain Question Answering,['cs.CL'],"  Open domain question answering (ODQA) is a longstanding task aimed at
answering factual questions from a large knowledge corpus without any explicit
evidence in natural language processing (NLP). Recent works have predominantly
focused on improving the answering accuracy and achieved promising progress.
However, higher accuracy often comes with more memory consumption and inference
latency, which might not necessarily be efficient enough for direct deployment
in the real world. Thus, a trade-off between accuracy, memory consumption and
processing speed is pursued. In this paper, we provide a survey of recent
advances in the efficiency of ODQA models. We walk through the ODQA models and
conclude the core techniques on efficiency. Quantitative analysis on memory
cost, processing speed, accuracy and overall comparison are given. We hope that
this work would keep interested scholars informed of the advances and open
challenges in ODQA efficiency research, and thus contribute to the further
development of ODQA efficiency.
","[{'version': 'v1', 'created': 'Tue, 15 Nov 2022 04:18:53 GMT'}]",2022-11-16,['Computation and Language'],"This paper presents a survey of current methods for efficient open domain question answering (QA) regarding computation and language. We review existing techniques for QA and identify their strengths and weaknesses in terms of accuracy, speed, and scalability. We analyze the state-of-the-art methods for open domain QA, including deep learning architectures, knowledge-based methods, and hybrid approaches. We also discuss the challenges and opportunities associated with open domain QA, including the need for large-scale datasets, efficient search methods, and the development of more robust models. Finally, we propose a set of directions for future research in open domain QA.",Write an abstract for a paper called A Survey for Efficient Open Domain Question Answering about Computation and Language
2206.14282,"Emanuele Zappala, Antonio Henrique de Oliveira Fonseca, Andrew Henry
  Moberly, Michael James Higley, Chadi Abdallah, Jessica Cardin, David van Dijk",Neural Integro-Differential Equations,['cs.LG'],"  Modeling continuous dynamical systems from discretely sampled observations is
a fundamental problem in data science. Often, such dynamics are the result of
non-local processes that present an integral over time. As such, these systems
are modeled with Integro-Differential Equations (IDEs); generalizations of
differential equations that comprise both an integral and a differential
component. For example, brain dynamics are not accurately modeled by
differential equations since their behavior is non-Markovian, i.e. dynamics are
in part dictated by history. Here, we introduce the Neural IDE (NIDE), a novel
deep learning framework based on the theory of IDEs where integral operators
are learned using neural networks. We test NIDE on several toy and brain
activity datasets and demonstrate that NIDE outperforms other models. These
tasks include time extrapolation as well as predicting dynamics from unseen
initial conditions, which we test on whole-cortex activity recordings in freely
behaving mice. Further, we show that NIDE can decompose dynamics into their
Markovian and non-Markovian constituents via the learned integral operator,
which we test on fMRI brain activity recordings of people on ketamine. Finally,
the integrand of the integral operator provides a latent space that gives
insight into the underlying dynamics, which we demonstrate on wide-field brain
imaging recordings. Altogether, NIDE is a novel approach that enables modeling
of complex non-local dynamics with neural networks.
","[{'version': 'v1', 'created': 'Tue, 28 Jun 2022 20:39:35 GMT'}, {'version': 'v2', 'created': 'Sat, 9 Jul 2022 18:05:18 GMT'}, {'version': 'v3', 'created': 'Tue, 23 Aug 2022 16:42:29 GMT'}, {'version': 'v4', 'created': 'Wed, 30 Nov 2022 03:42:21 GMT'}]",2022-12-01,['Machine Learning'],"This paper presents a novel approach to machine learning by introducing a new class of models based on neural integro-differential equations. These equations are able to represent a wide range of dynamical systems and can be used to model complex nonlinear relationships between inputs and outputs. We demonstrate how these equations can be used to solve a variety of machine learning tasks, such as supervised and unsupervised learning, deep reinforcement learning and generative adversarial networks. We also show how these equations can be used to improve the accuracy and robustness of existing machine learning algorithms. Finally, we discuss the potential implications of this approach for future research in machine learning.",Write an abstract for a paper called Neural Integro-Differential Equations about Machine Learning
2204.07707,AprilPyone MaungMaung and Hitoshi Kiya,Privacy-Preserving Image Classification Using Isotropic Network,"['cs.CV', 'eess.IV']","  In this paper, we propose a privacy-preserving image classification method
that uses encrypted images and an isotropic network such as the vision
transformer. The proposed method allows us not only to apply images without
visual information to deep neural networks (DNNs) for both training and testing
but also to maintain a high classification accuracy. In addition, compressible
encrypted images, called encryption-then-compression (EtC) images, can be used
for both training and testing without any adaptation network. Previously, to
classify EtC images, an adaptation network was required before a classification
network, so methods with an adaptation network have been only tested on small
images. To the best of our knowledge, previous privacy-preserving image
classification methods have never considered image compressibility and patch
embedding-based isotropic networks. In an experiment, the proposed
privacy-preserving image classification was demonstrated to outperform
state-of-the-art methods even when EtC images were used in terms of
classification accuracy and robustness against various attacks under the use of
two isotropic networks: vision transformer and ConvMixer.
","[{'version': 'v1', 'created': 'Sat, 16 Apr 2022 03:15:54 GMT'}]",2022-04-19,['Computer Vision and Pattern Recognition'],"This paper presents a novel privacy-preserving image classification technique using Isotropic Network (IN) for computer vision and pattern recognition. IN is a lightweight, end-to-end deep learning architecture that enables privacy-preserving image classification. We evaluate our approach on the popular ImageNet dataset and show that IN can achieve comparable accuracy to a conventional convolutional neural network (CNN) while preserving privacy. Our experiments show that IN can achieve a top-1 accuracy of 70.4% on ImageNet, which is competitive with the state-of-the-art CNNs. Furthermore, we demonstrate that IN is robust against membership inference attacks and can successfully protect the privacy of users’ data. Our results show that IN is an effective approach for privacy-preserving image classification.",Write an abstract for a paper called Privacy-Preserving Image Classification Using Isotropic Network about Computer Vision and Pattern Recognition
2203.01061,"Jialin Ji, Tiankai Yang, Chao Xu, and Fei Gao",Real-Time Trajectory Planning for Aerial Perching,['cs.RO'],"  This paper presents a novel trajectory planning method for aerial perching.
Compared with the existing work, the terminal states and the trajectory
durations can be adjusted adaptively, instead of being determined in advance.
Furthermore, our planner is able to minimize the tangential relative speed on
the premise of safety and dynamic feasibility. This feature is especially
notable on micro aerial robots with low maneuverability or scenarios where the
space is not enough. Moreover, we design a flexible transformation strategy to
eliminate terminal constraints along with reducing optimization variables.
Besides, we take precise SE(3) motion planning into account to ensure that the
drone would not touch the landing platform until the last moment. The proposed
method is validated onboard by a palm-sized micro aerial robot with quite
limited thrust and moment (thrust-to-weight ratio 1.7) perching on a mobile
inclined surface. Sufficient experimental results show that our planner
generates an optimal trajectory within 20ms, and replans with warm start in
2ms.
","[{'version': 'v1', 'created': 'Wed, 2 Mar 2022 12:23:42 GMT'}, {'version': 'v2', 'created': 'Tue, 19 Jul 2022 13:42:40 GMT'}]",2022-07-20,['Robotics'],"This paper presents an approach for real-time trajectory planning for aerial perching in robotics. The approach is based on a novel hierarchical control framework which combines a model predictive controller and a reinforcement learning based controller. The proposed framework is evaluated in a simulation environment and the results show that it can effectively generate trajectories for aerial perching in real-time. The proposed framework is compared to existing methods, and the results demonstrate the improved performance of the proposed approach. The paper also discusses the potential applications of the proposed framework in real-world robotic tasks.",Write an abstract for a paper called Real-Time Trajectory Planning for Aerial Perching about Robotics
2205.08982,"Mohan Hasama, Jing Li","Attention-based Multimodal Feature Representation Model for Micro-video
  Recommendation",['cs.IR'],"  In recommender systems, models mostly use a combination of embedding layers
and multilayer feedforward neural networks. The high-dimensional sparse
original features are downscaled in the embedding layer and then fed into the
fully connected network to obtain prediction results. However, the above
methods have a rather obvious problem, that is, the features directly input are
treated as independent individuals, and in fact there are internal correlations
between features and features, and even different features have different
importance in the recommendation. In this regard, this paper adopts a
self-attentive mechanism to mine the internal correlations between features as
well as their relative importance. In recent years, as a special form of
attention mechanism, self-attention mechanism is favored by many researchers.
The self-attentive mechanism captures the internal correlation of data or
features by learning itself, thus reducing the dependence on external sources.
Therefore, this paper adopts a multi-headed self-attentive mechanism to mine
the internal correlations between features and thus learn the internal
representation of features. At the same time, considering the rich information
often hidden between features, the new feature representation obtained by
crossover between the two is likely to imply the new description of the user
likes the item. However, not all crossover features are meaningful, i.e., there
is a problem of limited expression of feature combinations. Therefore, this
paper adopts an attention-based approach to learn the external
cross-representation of features.
","[{'version': 'v1', 'created': 'Wed, 18 May 2022 15:10:04 GMT'}]",2022-05-19,['Information Retrieval'],"This paper presents an Attention-based Multimodal Feature Representation Model (AMFRM) for micro-video recommendation about information retrieval. AMFRM is a novel end-to-end deep learning model that combines both visual and textual features for micro-video recommendation. It utilizes an attention mechanism to learn the importance of each feature in the representation, and then combines the features to generate a unified representation. Experiments on a real-world micro-video dataset show that AMFRM outperforms existing methods in terms of precision, recall and F1-score. The results demonstrate that AMFRM is an effective model for micro-video recommendation about information retrieval.","Write an abstract for a paper called Attention-based Multimodal Feature Representation Model for Micro-video
  Recommendation about Information Retrieval"
2205.04639,"Xiaochun Lei, Linjun Lu, Zetao Jiang, Zhaoting Gong, Chang Lu, Jiaming
  Liang",STDC-MA Network for Semantic Segmentation,['cs.CV'],"  Semantic segmentation is applied extensively in autonomous driving and
intelligent transportation with methods that highly demand spatial and semantic
information. Here, an STDC-MA network is proposed to meet these demands. First,
the STDC-Seg structure is employed in STDC-MA to ensure a lightweight and
efficient structure. Subsequently, the feature alignment module (FAM) is
applied to understand the offset between high-level and low-level features,
solving the problem of pixel offset related to upsampling on the high-level
feature map. Our approach implements the effective fusion between high-level
features and low-level features. A hierarchical multiscale attention mechanism
is adopted to reveal the relationship among attention regions from two
different input sizes of one image. Through this relationship, regions
receiving much attention are integrated into the segmentation results, thereby
reducing the unfocused regions of the input image and improving the effective
utilization of multiscale features. STDC- MA maintains the segmentation speed
as an STDC-Seg network while improving the segmentation accuracy of small
objects. STDC-MA was verified on the verification set of Cityscapes. The
segmentation result of STDC-MA attained 76.81% mIOU with the input of 0.5x
scale, 3.61% higher than STDC-Seg.
","[{'version': 'v1', 'created': 'Tue, 10 May 2022 02:59:39 GMT'}, {'version': 'v2', 'created': 'Wed, 11 May 2022 00:38:38 GMT'}]",2022-05-12,['Computer Vision and Pattern Recognition'],This paper presents a novel network architecture called STDC-MA for semantic segmentation in computer vision and pattern recognition tasks. The proposed network is based on a fully convolutional network (FCN) and incorporates a multi-scale structure with a self-tuning dilated convolution module (STDC) to capture multi-scale contextual information. The STDC module is used to adaptively adjust the dilation rate of convolutional layers to capture multi-scale information. Experiments on the PASCAL VOC 2012 and Cityscapes datasets demonstrate that the proposed STDC-MA network outperforms existing state-of-the-art models in terms of segmentation accuracy.,Write an abstract for a paper called STDC-MA Network for Semantic Segmentation about Computer Vision and Pattern Recognition
2302.02276,"Qiyun Liu, Zhiguang Yang and Hanzhou Wu","JPEG Steganalysis Based on Steganographic Feature Enhancement and Graph
  Attention Learning","['cs.MM', 'cs.CR', 'cs.CV']","  The purpose of image steganalysis is to determine whether the carrier image
contains hidden information or not. Since JEPG is the most commonly used image
format over social networks, steganalysis in JPEG images is also the most
urgently needed to be explored. However, in order to detect whether secret
information is hidden within JEPG images, the majority of existing algorithms
are designed in conjunction with the popular computer vision related networks,
without considering the key characteristics appeared in image steganalysis. It
is crucial that the steganographic signal, as an extremely weak signal, can be
enhanced during its representation learning process. Motivated by this insight,
in this paper, we introduce a novel representation learning algorithm for JPEG
steganalysis that is mainly consisting of a graph attention learning module and
a feature enhancement module. The graph attention learning module is designed
to avoid global feature loss caused by the local feature learning of
convolutional neural network and reliance on depth stacking to extend the
perceptual domain. The feature enhancement module is applied to prevent the
stacking of convolutional layers from weakening the steganographic information.
In addition, pretraining as a way to initialize the network weights with a
large-scale dataset is utilized to enhance the ability of the network to
extract discriminative features. We advocate pretraining with ALASKA2 for the
model trained with BOSSBase+BOWS2. The experimental results indicate that the
proposed algorithm outperforms previous arts in terms of detection accuracy,
which has verified the superiority and applicability of the proposed work.
","[{'version': 'v1', 'created': 'Sun, 5 Feb 2023 01:42:19 GMT'}]",2023-02-07,"['Multimedia', 'Cryptography and Security', 'Computer Vision and Pattern Recognition']","This paper presents a novel JPEG steganalysis method based on steganographic feature enhancement and graph attention learning. The proposed method uses a novel feature extraction module to enhance the steganographic features of JPEG images. The enhanced features are then fed into a graph attention network to learn the steganographic patterns of the image. Experiments on two publicly available datasets show that the proposed method outperforms existing state-of-the-art JPEG steganalysis methods in terms of both accuracy and speed. The results demonstrate the effectiveness of the proposed method in detecting steganographic content in JPEG images. The proposed method has potential applications in multimedia, cryptography and security, computer vision and pattern recognition.","Write an abstract for a paper called JPEG Steganalysis Based on Steganographic Feature Enhancement and Graph
  Attention Learning about Multimedia, Cryptography and Security, Computer Vision and Pattern Recognition"
2301.13816,"Parshin Shojaee, Aneesh Jain, Sindhu Tipirneni and Chandan K. Reddy",Execution-based Code Generation using Deep Reinforcement Learning,"['cs.LG', 'cs.AI', 'cs.CL', 'cs.PL']","  The utilization of programming language (PL) models, pretrained on
large-scale code corpora, as a means of automating software engineering
processes has demonstrated considerable potential in streamlining various code
generation tasks such as code completion, code translation, and program
synthesis. However, current approaches mainly rely on supervised fine-tuning
objectives borrowed from text generation, neglecting specific sequence-level
features of code, including but not limited to compilability as well as
syntactic and functional correctness. To address this limitation, we propose
PPOCoder, a new framework for code generation that combines pretrained PL
models with Proximal Policy Optimization (PPO) deep reinforcement learning and
employs execution feedback as the external source of knowledge into the model
optimization. PPOCoder is transferable across different code generation tasks
and PLs. Extensive experiments on three code generation tasks demonstrate the
effectiveness of our proposed approach compared to SOTA methods, improving the
success rate of compilation and functional correctness over different PLs. Our
code can be found at https://github.com/reddy-lab-code-research/PPOCoder .
","[{'version': 'v1', 'created': 'Tue, 31 Jan 2023 18:02:26 GMT'}, {'version': 'v2', 'created': 'Mon, 13 Feb 2023 20:43:41 GMT'}]",2023-02-15,"['Machine Learning', 'Artificial Intelligence', 'Computation and Language', 'Programming Languages']","This paper presents a novel approach to code generation using Deep Reinforcement Learning (DRL). It explores the application of DRL to the problem of automatically generating code from natural language descriptions. The paper first introduces the concept of execution-based code generation and its potential benefits. It then describes the proposed DRL-based code generation system, which combines natural language processing and reinforcement learning techniques. Finally, the paper evaluates the system's performance on a range of programming languages and tasks. The results indicate that the proposed system is capable of generating code with high accuracy and efficiency. The paper concludes by discussing the implications of the proposed system for machine learning, artificial intelligence, computation, and language, as well as programming languages.","Write an abstract for a paper called Execution-based Code Generation using Deep Reinforcement Learning about Machine Learning, Artificial Intelligence, Computation and Language, Programming Languages"
2008.114,"Manpreet Kaur, Flora D. Salim, Yongli Ren, Jeffrey Chan, Martin Tomko,
  Mark Sanderson","Joint Modelling of Cyber Activities and Physical Context to Improve
  Prediction of Visitor Behaviors","['cs.IR', 'cs.DC', 'cs.LG', 'cs.SI']","  This paper investigates the Cyber-Physical behavior of users in a large
indoor shopping mall by leveraging anonymized (opt in) Wi-Fi association and
browsing logs recorded by the mall operators. Our analysis shows that many
users exhibit a high correlation between their cyber activities and their
physical context. To find this correlation, we propose a mechanism to
semantically label a physical space with rich categorical information from
DBPedia concepts and compute a contextual similarity that represents a user's
activities with the mall context. We demonstrate the application of
cyber-physical contextual similarity in two situations: user visit intent
classification and future location prediction. The experimental results
demonstrate that exploitation of contextual similarity significantly improves
the accuracy of such applications.
","[{'version': 'v1', 'created': 'Wed, 26 Aug 2020 06:37:43 GMT'}]",2022-03-24,"['Information Retrieval', 'Distributed, Parallel, and Cluster Computing', 'Machine Learning', 'Social and Information Networks']","This paper presents a joint modelling approach to improve prediction of visitor behaviors about information retrieval, distributed, parallel, and cluster computing, machine learning, social and information networks. The proposed model combines cyber activities and physical context to better understand the visitors' search and navigation behaviors. It uses a novel combination of machine learning and natural language processing techniques to identify and extract relevant features from the physical context and cyber activities. The model is evaluated on a dataset of visitors to a university library website. The results show that the proposed model significantly improves the accuracy of predicting visitor behaviors compared to existing approaches. The findings of this paper demonstrate the potential of joint modelling of cyber activities and physical context for predicting visitor behaviors in the domain of information retrieval, distributed, parallel, and cluster computing, machine learning, social and information networks.","Write an abstract for a paper called Joint Modelling of Cyber Activities and Physical Context to Improve
  Prediction of Visitor Behaviors about Information Retrieval, Distributed, Parallel, and Cluster Computing, Machine Learning, Social and Information Networks"
2207.00844,"Qingqiao Hu, Hongwei Li, Jianguo Zhang","Domain-Adaptive 3D Medical Image Synthesis: An Efficient Unsupervised
  Approach","['eess.IV', 'cs.CV']","  Medical image synthesis has attracted increasing attention because it could
generate missing image data, improving diagnosis and benefits many downstream
tasks. However, so far the developed synthesis model is not adaptive to unseen
data distribution that presents domain shift, limiting its applicability in
clinical routine. This work focuses on exploring domain adaptation (DA) of 3D
image-to-image synthesis models. First, we highlight the technical difference
in DA between classification, segmentation and synthesis models. Second, we
present a novel efficient adaptation approach based on 2D variational
autoencoder which approximates 3D distributions. Third, we present empirical
studies on the effect of the amount of adaptation data and the key
hyper-parameters. Our results show that the proposed approach can significantly
improve the synthesis accuracy on unseen domains in a 3D setting. The code is
publicly available at
https://github.com/WinstonHuTiger/2D_VAE_UDA_for_3D_sythesis
","[{'version': 'v1', 'created': 'Sat, 2 Jul 2022 14:24:19 GMT'}]",2022-07-05,['Computer Vision and Pattern Recognition'],"This paper presents an efficient unsupervised approach for domain-adaptive 3D medical image synthesis. This approach utilizes a generative adversarial network (GAN) to learn the domain-adaptive mapping between source and target domains. The proposed approach is evaluated on a variety of 3D medical image datasets, including CT, MRI, and X-ray images. The results demonstrate that the proposed approach is able to synthesize realistic images with high accuracy and low computational complexity. Additionally, the proposed approach outperforms existing methods in terms of accuracy and computational efficiency. The paper also provides a detailed analysis of the domain-adaptive 3D medical image synthesis performance. The results of this study can help medical practitioners and researchers to develop better and more efficient methods for domain-adaptive 3D medical image synthesis.","Write an abstract for a paper called Domain-Adaptive 3D Medical Image Synthesis: An Efficient Unsupervised
  Approach about Computer Vision and Pattern Recognition"
2303.03544,Itai Shapira,"Expressivity of Shallow and Deep Neural Networks for Polynomial
  Approximation","['cs.LG', 'stat.ML']","  We analyze the number of neurons that a ReLU neural network needs to
approximate multivariate monomials. We establish an exponential lower bound for
the complexity of any shallow network that approximates the product function
$\vec{x} \to \prod_{i=1}^d x_i$ on a general compact domain. Furthermore, we
prove that this lower bound does not hold for normalized O(1)-Lipschitz
monomials (or equivalently, by restricting to the unit cube). These results
suggest shallow ReLU networks suffer from the curse of dimensionality when
expressing functions with a Lipschitz parameter scaling with the dimension of
the input, and that the expressive power of neural networks lies in their depth
rather than the overall complexity.
","[{'version': 'v1', 'created': 'Mon, 6 Mar 2023 23:01:53 GMT'}]",2023-03-08,['Machine Learning'],"This paper investigates the expressivity of shallow and deep neural networks for polynomial approximation in the context of machine learning. We compare the performance of shallow and deep neural networks on a range of polynomial approximation tasks. We evaluate the networks based on the number of parameters, training time, and accuracy. We also compare the networks with other popular machine learning algorithms such as support vector machines and decision trees. Our results show that deep neural networks are more expressive than shallow networks and can achieve better accuracy in polynomial approximation tasks. We also observe that deep neural networks require more parameters and longer training times than shallow networks. Our results suggest that deep neural networks are more suitable for polynomial approximation tasks than shallow networks and other machine learning algorithms.","Write an abstract for a paper called Expressivity of Shallow and Deep Neural Networks for Polynomial
  Approximation about Machine Learning"
2202.14036,"Jenny Tang, Eleanor Birrell, Ada Lerner","How Well Do My Results Generalize Now? The External Validity of Online
  Privacy and Security Surveys",['cs.HC'],"  Privacy and security researchers often rely on data collected through online
crowdsourcing platforms such as Amazon Mechanical Turk (MTurk) and Prolific.
Prior work -- which used data collected in the United States between 2013 and
2017 -- found that MTurk responses regarding security and privacy were
generally representative for people under 50 or with some college education.
However, the landscape of online crowdsourcing has changed significantly over
the last five years, with the rise of Prolific as a major platform and the
increasing presence of bots. This work attempts to replicate the prior results
about the external validity of online privacy and security surveys. We conduct
an online survey on MTurk (n=800), a gender-balanced survey on Prolific
(n=800), and a representative survey on Prolific (n=800) and compare the
responses to a probabilistic survey conducted by the Pew Research Center
(n=4272). We find that MTurk response quality has degraded over the last five
years, and our results do not replicate the earlier finding about the
generalizability of MTurk responses. By contrast, we find that data collected
through Prolific is generally representative for questions about user
perceptions and experiences, but not for questions about security and privacy
knowledge. We also evaluate the impact of Prolific settings, attention check
questions, and statistical methods on the external validity of online surveys,
and we develop recommendations about best practices for conducting online
privacy and security surveys.
","[{'version': 'v1', 'created': 'Mon, 28 Feb 2022 18:58:44 GMT'}, {'version': 'v2', 'created': 'Thu, 9 Jun 2022 19:29:28 GMT'}]",2022-06-13,['Human-Computer Interaction'],"This paper examines how online privacy and security surveys about human-computer interaction can be used to generalize results to a larger population. It explores the external validity of online surveys and how the results can be generalized to a broader population. The paper reviews the literature on the validity of online surveys, the challenges of generalizing results, and the implications of generalizing results from online surveys. It then presents a case study of a survey conducted to assess the attitudes of users towards online privacy and security. The paper concludes by discussing the implications of the findings and proposing strategies to improve the external validity of online surveys.","Write an abstract for a paper called How Well Do My Results Generalize Now? The External Validity of Online
  Privacy and Security Surveys about Human-Computer Interaction"
1810.01896,Martin W. Licht,On Basis Constructions in Finite Element Exterior Calculus,"['math.NA', 'cs.NA']","  We give a systematic and self-contained account of the construction of
geometrically decomposed bases and degrees of freedom in finite element
exterior calculus. In particular, we elaborate upon a previously overlooked
basis for one of the families of finite element spaces, which is of interest
for implementations. Moreover, we give details for the construction of
isomorphisms and duality pairings between finite element spaces. These
structural results show, for example, how to transfer linear dependencies
between canonical spanning sets, or give a new derivation of the degrees of
freedom.
","[{'version': 'v1', 'created': 'Wed, 3 Oct 2018 18:01:22 GMT'}, {'version': 'v2', 'created': 'Fri, 21 Oct 2022 14:11:28 GMT'}]",2022-10-24,['Numerical Analysis'],"This paper explores the use of basis constructions in finite element exterior calculus (FEEC) for numerical analysis. FEEC is a powerful tool for the numerical solution of partial differential equations (PDEs) and is a combination of the finite element method (FEM) and exterior calculus. It provides a unified framework for the numerical solution of a wide variety of PDEs, including elliptic, parabolic, and hyperbolic equations. This paper examines how basis constructions can be used in FEEC to improve the accuracy and efficiency of numerical solutions. Specifically, we discuss the use of basis constructions for differential forms, integration, and numerical integration. We also discuss the advantages and disadvantages of various basis constructions, and present several numerical examples to illustrate the effectiveness of FEEC with basis constructions. Finally, we discuss potential future directions for the use of basis constructions in FEEC.",Write an abstract for a paper called On Basis Constructions in Finite Element Exterior Calculus about Numerical Analysis
2109.00881,"Jingmin Huang and Bowei Chen and Lan Luo and Shigang Yue and Iadh
  Ounis","DVM-CAR: A large-scale automotive dataset for visual marketing research
  and applications",['cs.CV'],"  There is a growing interest in product aesthetics analytics and design.
However, the lack of available large-scale data that covers various variables
and information is one of the biggest challenges faced by analysts and
researchers. In this paper, we present our multidisciplinary initiative of
developing a comprehensive automotive dataset from different online sources and
formats. Specifically, the created dataset contains 1.4 million images from 899
car models and their corresponding model specifications and sales information
over more than ten years in the UK market. Our work makes significant
contributions to: (i) research and applications in the automotive industry;
(ii) big data creation and sharing; (iii) database design; and (iv) data
fusion. Apart from our motivation, technical details and data structure, we
further present three simple examples to demonstrate how our data can be used
in business research and applications.
","[{'version': 'v1', 'created': 'Tue, 10 Aug 2021 12:48:58 GMT'}, {'version': 'v2', 'created': 'Fri, 6 Jan 2023 01:59:32 GMT'}, {'version': 'v3', 'created': 'Mon, 9 Jan 2023 15:36:23 GMT'}]",2023-01-10,['Computer Vision and Pattern Recognition'],"This paper presents DVM-CAR, a large-scale automotive dataset for visual marketing research and applications about Computer Vision and Pattern Recognition. It contains a total of 8,000 images of cars from the United States, Europe, and Asia, with labels for make, model, year, and region. The dataset is divided into two parts: a training set of 6,000 images and a test set of 2,000 images. The images are annotated with bounding boxes and segmentation masks for car parts such as headlights, grilles, and wheels. The dataset is further divided into five categories based on the car's make, model, and year. We also provide baseline results for object detection and segmentation on the dataset. The results demonstrate the potential of the dataset for research and applications in visual marketing and Computer Vision and Pattern Recognition.","Write an abstract for a paper called DVM-CAR: A large-scale automotive dataset for visual marketing research
  and applications about Computer Vision and Pattern Recognition"
2303.02857,Fateme Bahri and Nilanjan Ray,Weakly Supervised Realtime Dynamic Background Subtraction,['cs.CV'],"  Background subtraction is a fundamental task in computer vision with numerous
real-world applications, ranging from object tracking to video surveillance.
Dynamic backgrounds poses a significant challenge here. Supervised deep
learning-based techniques are currently considered state-of-the-art for this
task. However, these methods require pixel-wise ground-truth labels, which can
be time-consuming and expensive. In this work, we propose a weakly supervised
framework that can perform background subtraction without requiring per-pixel
ground-truth labels. Our framework is trained on a moving object-free sequence
of images and comprises two networks. The first network is an autoencoder that
generates background images and prepares dynamic background images for training
the second network. The dynamic background images are obtained by thresholding
the background-subtracted images. The second network is a U-Net that uses the
same object-free video for training and the dynamic background images as
pixel-wise ground-truth labels. During the test phase, the input images are
processed by the autoencoder and U-Net, which generate background and dynamic
background images, respectively. The dynamic background image helps remove
dynamic motion from the background-subtracted image, enabling us to obtain a
foreground image that is free of dynamic artifacts. To demonstrate the
effectiveness of our method, we conducted experiments on selected categories of
the CDnet 2014 dataset and the I2R dataset. Our method outperformed all
top-ranked unsupervised methods. We also achieved better results than one of
the two existing weakly supervised methods, and our performance was similar to
the other. Our proposed method is online, real-time, efficient, and requires
minimal frame-level annotation, making it suitable for a wide range of
real-world applications.
","[{'version': 'v1', 'created': 'Mon, 6 Mar 2023 03:17:48 GMT'}]",2023-03-07,['Computer Vision and Pattern Recognition'],"This paper presents a novel weakly supervised real-time dynamic background subtraction approach for computer vision and pattern recognition. The proposed method uses a convolutional neural network (CNN) to model the dynamic background, which is updated in real-time to adapt to changes in the environment. The CNN is trained using weakly supervised data, which is generated from a single video sequence. Experiments on real-world video sequences demonstrate that the proposed method is able to effectively detect foreground objects and remove dynamic backgrounds in real-time with high accuracy. Furthermore, the proposed method is computationally efficient and suitable for real-time applications.",Write an abstract for a paper called Weakly Supervised Realtime Dynamic Background Subtraction about Computer Vision and Pattern Recognition
2211.04691,Thien An L. Nguyen,"A Solution for a Fundamental Problem of 3D Inference based on 2D
  Representations",['cs.CV'],"  3D inference from monocular vision using neural networks is an important
research area of computer vision. Applications of the research area are various
with many proposed solutions and have shown remarkable performance. Although
many efforts have been invested, there are still unanswered questions, some of
which are fundamental. In this paper, I discuss a problem that I hope will come
to be known as a generalization of the Blind Perspective-n-Point (Blind PnP)
problem for object-driven 3D inference based on 2D representations. The vital
difference between the fundamental problem and the Blind PnP problem is that 3D
inference parameters in the fundamental problem are attached directly to 3D
points and the camera concept will be represented through the sharing of the
parameters of these points. By providing an explainable and robust
gradient-decent solution based on 2D representations for an important special
case of the problem, the paper opens up a new approach for using available
information-based learning methods to solve problems related to 3D object pose
estimation from 2D images.
","[{'version': 'v1', 'created': 'Wed, 9 Nov 2022 05:37:01 GMT'}]",2022-11-10,['Computer Vision and Pattern Recognition'],This paper presents a novel solution to address a fundamental problem of 3D inference based on 2D representations in the field of computer vision and pattern recognition. The proposed solution is based on a deep learning model that is trained to recognize 3D shapes from 2D images. The model utilizes a combination of convolutional neural networks and generative adversarial networks to learn the 3D representations of objects from the 2D images. The model is evaluated on a dataset of 3D shapes and the results demonstrate that the proposed solution is able to accurately infer the 3D shapes from 2D images with a high degree of accuracy. The paper also discusses potential applications of the proposed solution and provides insights into the implications of this work for the field of computer vision and pattern recognition.,"Write an abstract for a paper called A Solution for a Fundamental Problem of 3D Inference based on 2D
  Representations about Computer Vision and Pattern Recognition"
2203.04195,"Gukyeong Kwon, Ghassan AlRegib",A Gating Model for Bias Calibration in Generalized Zero-shot Learning,"['cs.LG', 'cs.CV']","  Generalized zero-shot learning (GZSL) aims at training a model that can
generalize to unseen class data by only using auxiliary information. One of the
main challenges in GZSL is a biased model prediction toward seen classes caused
by overfitting on only available seen class data during training. To overcome
this issue, we propose a two-stream autoencoder-based gating model for GZSL.
Our gating model predicts whether the query data is from seen classes or unseen
classes, and utilizes separate seen and unseen experts to predict the class
independently from each other. This framework avoids comparing the biased
prediction scores for seen classes with the prediction scores for unseen
classes. In particular, we measure the distance between visual and attribute
representations in the latent space and the cross-reconstruction space of the
autoencoder. These distances are utilized as complementary features to
characterize unseen classes at different levels of data abstraction. Also, the
two-stream autoencoder works as a unified framework for the gating model and
the unseen expert, which makes the proposed method computationally efficient.
We validate our proposed method in four benchmark image recognition datasets.
In comparison with other state-of-the-art methods, we achieve the best harmonic
mean accuracy in SUN and AWA2, and the second best in CUB and AWA1.
Furthermore, our base model requires at least 20% less number of model
parameters than state-of-the-art methods relying on generative models.
","[{'version': 'v1', 'created': 'Tue, 8 Mar 2022 16:41:06 GMT'}]",2022-03-09,"['Machine Learning', 'Computer Vision and Pattern Recognition']","This paper proposes a gating model for bias calibration in generalized zero-shot learning (GZSL). GZSL is a sub-field of machine learning, computer vision and pattern recognition, which aims to recognize unseen classes in a given dataset. The proposed gating model is a two-stage approach that first uses a feature extractor to generate a feature representation of an input image, and then uses a gating network to identify the unseen classes. The gating network is trained on a large set of labeled data, and is then used to identify unseen classes in a given dataset. The proposed model is evaluated on a variety of datasets and the results demonstrate that the proposed model can effectively reduce bias in GZSL.","Write an abstract for a paper called A Gating Model for Bias Calibration in Generalized Zero-shot Learning about Machine Learning, Computer Vision and Pattern Recognition"
2110.15318,Shenglong Zhou and Geoffrey Ye Li,Communication-Efficient ADMM-based Federated Learning,['cs.LG'],"  Federated learning has shown its advances over the last few years but is
facing many challenges, such as how algorithms save communication resources,
how they reduce computational costs, and whether they converge. To address
these issues, this paper proposes exact and inexact ADMM-based federated
learning. They are not only communication-efficient but also converge linearly
under very mild conditions, such as convexity-free and irrelevance to data
distributions. Moreover, the inexact version has low computational complexity,
thereby alleviating the computational burdens significantly.
","[{'version': 'v1', 'created': 'Thu, 28 Oct 2021 17:32:46 GMT'}, {'version': 'v2', 'created': 'Thu, 13 Jan 2022 20:58:06 GMT'}, {'version': 'v3', 'created': 'Sat, 29 Jan 2022 12:23:27 GMT'}]",2022-02-01,['Machine Learning'],This paper presents a communication-efficient distributed optimization algorithm based on Alternating Direction Method of Multipliers (ADMM) for Federated Learning (FL) in the context of Machine Learning (ML). We propose a communication-efficient ADMM-based FL algorithm that reduces the total number of communication rounds between the server and clients by exploiting the sparsity of the problem. We also prove the convergence of the proposed algorithm under mild assumptions. Our experiments on real-world datasets demonstrate that the proposed algorithm significantly reduces communication rounds while maintaining the accuracy of the model. The results show that the proposed algorithm can reduce the communication cost up to 90% compared with the existing algorithms.,Write an abstract for a paper called Communication-Efficient ADMM-based Federated Learning about Machine Learning
2303.01134,"Jos\'ephine Pazem, Mohammad H. Ansari",Error mitigation of entangled states using brainbox quantum autoencoders,"['quant-ph', 'cs.LG']","  Current quantum hardware is subject to various sources of noise that limits
the access to multi-qubit entangled states. Quantum autoencoder circuits with a
single qubit bottleneck have shown capability to correct error in noisy
entangled state. By introducing slightly more complex structures in the
bottleneck, the so-called brainboxes, the denoising process can take place
faster and for stronger noise channels. Choosing the most suitable brainbox for
the bottleneck is the result of a trade-off between noise intensity on the
hardware, and the training impedance. Finally, by studying R\'enyi entropy flow
throughout the networks we demonstrate that the localization of entanglement
plays a central role in denoising through learning.
","[{'version': 'v1', 'created': 'Thu, 2 Mar 2023 10:30:52 GMT'}]",2023-03-03,['Machine Learning'],"This paper presents a novel method for error mitigation of entangled states using a Brainbox Quantum Autoencoder. This method is based on machine learning techniques and can be used to improve the accuracy of entangled state measurements. We show that the Brainbox Quantum Autoencoder can be trained to identify and correct errors in entangled states, and that the trained model can be used to mitigate errors in the entangled states. We demonstrate the effectiveness of the proposed method on a number of entangled states using numerical simulations. Results show that the Brainbox Quantum Autoencoder can successfully mitigate errors in entangled states, leading to improved accuracy in the measurements. This work provides an efficient and effective technique for improving the accuracy of entangled state measurements.",Write an abstract for a paper called Error mitigation of entangled states using brainbox quantum autoencoders about Machine Learning
2006.08911,Iwan Duursma and Xiao Li and Hsin-Po Wang,Multilinear Algebra for Distributed Storage,"['cs.IT', 'math.AC', 'math.IT']","  An $(n, k, d, \alpha, \beta, M)$-ERRC (exact-repair regenerating code) is a
collection of $n$ nodes used to store a file. For a file of total size $M$,
each node stores $\alpha$ symbols, any $k$ nodes recover the file, and any $d$
nodes repair any other node via sending out $\beta$ symbols. We establish a
multilinear algebra foundation to assemble $(n, k, d, \alpha, \beta, M)$-ERRCs
for all meaningful $(n, k, d)$ tuples. Our ERRCs tie the
$\alpha/M$-versus-$\beta/M$ trade-off with cascade codes, the best known
construction for this trade-off. We give directions on how these ERRCs repair
multiple failures.
","[{'version': 'v1', 'created': 'Tue, 16 Jun 2020 03:54:53 GMT'}]",2022-01-07,['Information Theory'],"This paper examines the application of multilinear algebra to distributed storage systems, with a focus on information theory. We discuss the use of multilinear algebra to encode data for efficient storage in distributed systems, and the use of information theory to evaluate the performance of distributed storage systems. We present a case study of a distributed storage system, and analyze the performance of the system using multilinear algebra and information theory. We also present a comparison of different distributed storage systems and discuss the advantages and disadvantages of each. Finally, we conclude with a discussion of the potential applications of multilinear algebra and information theory to distributed storage systems.",Write an abstract for a paper called Multilinear Algebra for Distributed Storage about Information Theory
2109.02691,"Zhixue Zhao, Ziqi Zhang, Frank Hopfgartner","SS-BERT: Mitigating Identity Terms Bias in Toxic Comment Classification
  by Utilising the Notion of ""Subjectivity"" and ""Identity Terms""","['cs.CL', 'cs.AI', 'cs.LG']","  Toxic comment classification models are often found biased toward identity
terms which are terms characterizing a specific group of people such as
""Muslim"" and ""black"". Such bias is commonly reflected in false-positive
predictions, i.e. non-toxic comments with identity terms. In this work, we
propose a novel approach to tackle such bias in toxic comment classification,
leveraging the notion of subjectivity level of a comment and the presence of
identity terms. We hypothesize that when a comment is made about a group of
people that is characterized by an identity term, the likelihood of that
comment being toxic is associated with the subjectivity level of the comment,
i.e. the extent to which the comment conveys personal feelings and opinions.
Building upon the BERT model, we propose a new structure that is able to
leverage these features, and thoroughly evaluate our model on 4 datasets of
varying sizes and representing different social media platforms. The results
show that our model can consistently outperform BERT and a SOTA model devised
to address identity term bias in a different way, with a maximum improvement in
F1 of 2.43% and 1.91% respectively.
","[{'version': 'v1', 'created': 'Mon, 6 Sep 2021 18:40:06 GMT'}]",2022-10-18,"['Computation and Language', 'Artificial Intelligence', 'Machine Learning']","This paper presents SS-BERT, a novel approach to mitigating identity terms bias in toxic comment classification. We propose a framework that utilises the notion of “subjectivity” and “identity terms” about computation and language, artificial intelligence, and machine learning. Our method uses a BERT-based architecture to identify and mitigate identity terms bias in text classification tasks. We evaluate our approach on a publicly available dataset of toxic comments and compare it to existing methods. Our results show that SS-BERT outperforms existing methods in terms of accuracy, precision, recall, and F1-score. The proposed framework is an effective way to reduce identity terms bias in toxic comment classification.","Write an abstract for a paper called SS-BERT: Mitigating Identity Terms Bias in Toxic Comment Classification
  by Utilising the Notion of ""Subjectivity"" and ""Identity Terms"" about Computation and Language, Artificial Intelligence, Machine Learning"
2011.02112,"Zonghe Chua, Anthony M. Jarc, Allison M. Okamura","Toward Force Estimation in Robot-Assisted Surgery using Deep Learning
  with Vision and Robot State","['cs.RO', 'cs.LG']","  Knowledge of interaction forces during teleoperated robot-assisted surgery
could be used to enable force feedback to human operators and evaluate tissue
handling skill. However, direct force sensing at the end-effector is
challenging because it requires biocompatible, sterilizable, and cost-effective
sensors. Vision-based deep learning using convolutional neural networks is a
promising approach for providing useful force estimates, though questions
remain about generalization to new scenarios and real-time inference. We
present a force estimation neural network that uses RGB images and robot state
as inputs. Using a self-collected dataset, we compared the network to variants
that included only a single input type, and evaluated how they generalized to
new viewpoints, workspace positions, materials, and tools. We found that
vision-based networks were sensitive to shifts in viewpoints, while state-only
networks were robust to changes in workspace. The network with both state and
vision inputs had the highest accuracy for an unseen tool, and was moderately
robust to changes in viewpoints. Through feature removal studies, we found that
using only position features produced better accuracy than using only force
features as input. The network with both state and vision inputs outperformed a
physics-based baseline model in accuracy. It showed comparable accuracy but
faster computation times than a baseline recurrent neural network, making it
better suited for real-time applications.
","[{'version': 'v1', 'created': 'Wed, 4 Nov 2020 04:00:07 GMT'}, {'version': 'v2', 'created': 'Fri, 13 Nov 2020 01:23:38 GMT'}, {'version': 'v3', 'created': 'Mon, 30 Nov 2020 21:42:44 GMT'}, {'version': 'v4', 'created': 'Wed, 6 Jul 2022 20:27:44 GMT'}]",2022-07-08,"['Robotics', 'Machine Learning']","This paper presents a novel approach to force estimation in robot-assisted surgery using deep learning. The proposed method combines vision and robot state information to estimate forces in the surgical environment. Specifically, a convolutional neural network is used to process RGB images from a surgical camera and extract features related to the force applied by the robot. Then, a recurrent neural network is used to incorporate robot state information into the force estimation. The proposed method is evaluated on a dataset of robot-assisted surgical tasks and shows promising results. The paper also provides insights on the importance of combining vision and robot state information for accurate force estimation. The results of this work can be used to improve robot-assisted surgical systems and enable more precise and safe surgical interventions.","Write an abstract for a paper called Toward Force Estimation in Robot-Assisted Surgery using Deep Learning
  with Vision and Robot State about Robotics, Machine Learning"
2204.04372,"Edward Raff, Andrew L. Farris",A Siren Song of Open Source Reproducibility,"['cs.LG', 'cs.AI', 'cs.SE']","  As reproducibility becomes a greater concern, conferences have largely
converged to a strategy of asking reviewers to indicate whether code was
attached to a submission. This is part of a larger trend of taking action based
on assumed ideals, without studying if those actions will yield the desired
outcome. Our argument is that this focus on code for replication is misguided
if we want to improve the state of reproducible research. This focus can be
harmful -- we should not force code to be submitted. There is a lack of
evidence for effective actions taken by conferences to encourage and reward
reproducibility. We argue that venues must take more action to advance
reproducible machine learning research today.
","[{'version': 'v1', 'created': 'Sat, 9 Apr 2022 03:06:40 GMT'}]",2022-04-12,"['Machine Learning', 'Artificial Intelligence', 'Software Engineering']","This paper explores the potential of open source reproducibility to advance machine learning, artificial intelligence, and software engineering. It examines the ways in which open source reproducibility can be used to improve the accuracy, reliability, and scalability of machine learning, artificial intelligence, and software engineering systems. It also examines the potential challenges associated with open source reproducibility, such as the need for specialized skills and the potential for misuse. Finally, the paper will discuss the implications of open source reproducibility for the future of machine learning, artificial intelligence, and software engineering. The paper will conclude with a discussion of the potential benefits and drawbacks of open source reproducibility, and its implications for the future of these technologies.","Write an abstract for a paper called A Siren Song of Open Source Reproducibility about Machine Learning, Artificial Intelligence, Software Engineering"
2203.09602,Peter Fettke and Wolfgang Reisig,Towards Axiomatic Foundations for Conceptual Modeling: An Example,"['cs.SE', 'cs.DB']","  Conceptual modeling is a strongly interdisciplinary field of research.
Although numerous proposals for axiomatic foundations of the main ideas of the
field exist, there is still a lack of understanding main concepts such as
system, process, event, data, and many more. Against the background of the
tremendously gaining importance of digital phenomena, we argue that axiomatic
foundations are needed for our discipline. Besides the general call, we provide
a particular case study using HERAKLIT. This modeling infrastructure
encompasses the architecture, statics, and dynamics of computer-integrated
systems. The case study illustrates how axiomatically well-founded conceptual
models may look like in practice. We argue that axiomatic foundations do not
only have positive effects for theoretical research, but also for empirical
research, because, for instance, assumed axioms can explicitly be tested. It is
now time to spark the discussion on axiomatic foundations of our field.
","[{'version': 'v1', 'created': 'Thu, 17 Mar 2022 20:28:48 GMT'}]",2022-03-21,"['Software Engineering', 'Databases']",", and Information Systems

This paper presents an example of how to develop axiomatic foundations for conceptual modeling in the domain of software engineering, databases, and information systems. It begins with a review of the literature on the topics of axiomatic foundations, conceptual modeling, and the application of these concepts to the domain of software engineering, databases, and information systems. It then outlines a methodology for developing axiomatic foundations for conceptual modeling that is specific to the domain. This methodology is then applied to an example of a software engineering, database, and information system problem in order to illustrate the potential of the approach. Finally, the implications of the example and the potential of the approach are discussed.","Write an abstract for a paper called Towards Axiomatic Foundations for Conceptual Modeling: An Example about Software Engineering, Databases"
2011.11814,"Felix Wimbauer, Nan Yang, Lukas von Stumberg, Niclas Zeller, Daniel
  Cremers","MonoRec: Semi-Supervised Dense Reconstruction in Dynamic Environments
  from a Single Moving Camera",['cs.CV'],"  In this paper, we propose MonoRec, a semi-supervised monocular dense
reconstruction architecture that predicts depth maps from a single moving
camera in dynamic environments. MonoRec is based on a multi-view stereo setting
which encodes the information of multiple consecutive images in a cost volume.
To deal with dynamic objects in the scene, we introduce a MaskModule that
predicts moving object masks by leveraging the photometric inconsistencies
encoded in the cost volumes. Unlike other multi-view stereo methods, MonoRec is
able to reconstruct both static and moving objects by leveraging the predicted
masks. Furthermore, we present a novel multi-stage training scheme with a
semi-supervised loss formulation that does not require LiDAR depth values. We
carefully evaluate MonoRec on the KITTI dataset and show that it achieves
state-of-the-art performance compared to both multi-view and single-view
methods. With the model trained on KITTI, we further demonstrate that MonoRec
is able to generalize well to both the Oxford RobotCar dataset and the more
challenging TUM-Mono dataset recorded by a handheld camera. Code and related
materials will be available at https://vision.in.tum.de/research/monorec.
","[{'version': 'v1', 'created': 'Tue, 24 Nov 2020 00:40:36 GMT'}, {'version': 'v2', 'created': 'Mon, 1 Mar 2021 22:01:43 GMT'}, {'version': 'v3', 'created': 'Thu, 6 May 2021 09:04:06 GMT'}]",2022-09-22,['Computer Vision and Pattern Recognition'],"This paper presents MonoRec, a novel semi-supervised dense reconstruction framework for dynamic environments from a single moving camera. MonoRec combines a novel deep learning architecture with a robust optimization framework to accurately reconstruct dense 3D models from a single camera in dynamic environments. The deep learning architecture is trained on a large-scale synthetic dataset to learn the motion of the camera, while the optimization framework is used to refine the predicted 3D models. Experimental results demonstrate that MonoRec can accurately reconstruct 3D models from a single camera in dynamic environments with a high degree of accuracy. Furthermore, the proposed framework is computationally efficient, making it suitable for real-time applications.","Write an abstract for a paper called MonoRec: Semi-Supervised Dense Reconstruction in Dynamic Environments
  from a Single Moving Camera about Computer Vision and Pattern Recognition"
2212.13366,Chantal Klinkhammer and Robert Plato,"Analysis of the discrepancy principle for Tikhonov regularization under
  low order source conditions","['math.NA', 'cs.NA']","  We study the application of Tikhonov regularization to ill-posed nonlinear
operator equations. The objective of this work is to prove low order
convergence rates for the discrepancy principle under low order source
conditions of logarithmic type. We work within the framework of Hilbert scales
and extend existing studies on this subject to the oversmoothing case. The
latter means that the exact solution of the treated operator equation does not
belong to the domain of definition of the penalty term. As a consequence, the
Tikhonov functional fails to have a finite value.
","[{'version': 'v1', 'created': 'Tue, 27 Dec 2022 05:48:34 GMT'}]",2022-12-29,['Numerical Analysis'],"This paper presents an analysis of the discrepancy principle for Tikhonov regularization when applied to low order source conditions in numerical analysis. The discrepancy principle is a well-known regularization technique used to solve ill-posed inverse problems. In this paper, we analyze the performance of the discrepancy principle under low order source conditions, which are often encountered in numerical analysis. We analyze the convergence rates of the discrepancy principle and its sensitivity to perturbations in the data. We also discuss the implications of our findings for numerical analysis and present numerical experiments that illustrate our results. Our results show that the discrepancy principle can provide robust and accurate solutions to inverse problems under low order source conditions.","Write an abstract for a paper called Analysis of the discrepancy principle for Tikhonov regularization under
  low order source conditions about Numerical Analysis"
2210.04747,"Shiwen He, Kangli Cai, Shiyue Huang, Zhenyu Anz, Wei Huang, Ning Gao",An NLoS-based Enhanced Sensing Method for MmWave Communication System,"['cs.IT', 'eess.SP', 'math.IT']","  The millimeter-wave (mmWave)-based Wi-Fi sensing technology has recently
attracted extensive attention since it provides a possibility to realize higher
sensing accuracy. However, current works mainly concentrate on sensing
scenarios where the line-of-sight (LoS) path exists, which significantly limits
their applications. To address the problem, we propose an enhanced mmWave
sensing algorithm in the 3D non-line-of-sight environment (mm3NLoS), aiming to
sense the direction and distance of the target when the LoS path is weak or
blocked. Specifically, we first adopt the directional beam to estimate the
azimuth/elevation angle of arrival (AoA) and angle of departure (AoD) of the
reflection path. Then, the distance of the related path is measured by the fine
timing measurement protocol. Finally, we transform the AoA and AoD of the
multiple non-line-of-sight (NLoS) paths into the direction vector and then
obtain the information of targets based on the geometric relationship. The
simulation results demonstrate that mm3NLoS can achieve a centimeter-level
error with a 2m spacing. Compared to the prior work, it can significantly
reduce the performance degradation under the NLoS condition.
","[{'version': 'v1', 'created': 'Mon, 10 Oct 2022 14:58:48 GMT'}]",2022-10-11,['Information Theory'],"This paper presents an enhanced sensing method for mmWave communication systems based on Non-Line-of-Sight (NLoS) measurements. The proposed method uses NLoS measurements to improve the accuracy of mmWave communication systems. The proposed method is evaluated by simulations and compared with conventional methods. Results show that the proposed method improves the accuracy of mmWave communication systems in terms of signal-to-noise ratio and bit error rate. Furthermore, the proposed method can be used to provide a better understanding of the characteristics of mmWave communication systems and the effects of NLoS measurements on the performance of these systems. The results of this study can be used to improve the design and deployment of mmWave communication systems.",Write an abstract for a paper called An NLoS-based Enhanced Sensing Method for MmWave Communication System about Information Theory
2203.12111,"Alex Moran, Bart Gebka, Joshua Goldshteyn, Autumn Beyer, Nathan
  Johnson, and Alexander Neuwirth","Muscle Vision: Real Time Keypoint Based Pose Classification of Physical
  Exercises",['cs.AI'],"  Recent advances in machine learning technology have enabled highly portable
and performant models for many common tasks, especially in image recognition.
One emerging field, 3D human pose recognition extrapolated from video, has now
advanced to the point of enabling real-time software applications with robust
enough output to support downstream machine learning tasks. In this work we
propose a new machine learning pipeline and web interface that performs human
pose recognition on a live video feed to detect when common exercises are
performed and classify them accordingly. We present a model interface capable
of webcam input with live display of classification results. Our main
contributions include a keypoint and time series based lightweight approach for
classifying a selected set of fitness exercises and a web-based software
application for obtaining and visualizing the results in real time.
","[{'version': 'v1', 'created': 'Wed, 23 Mar 2022 00:55:07 GMT'}]",2022-03-24,['Artificial Intelligence'],"This paper presents a novel approach for real-time keypoint based pose classification of physical exercises using Artificial Intelligence (AI). Muscle Vision is a deep learning system that uses convolutional neural networks to detect and classify human poses in real-time. The system is designed to enable accurate and efficient pose identification for physical exercises, such as yoga, Pilates, and strength training. The proposed system is evaluated on a dataset of over 6,000 images of physical exercises and achieves an accuracy of 95%. The results demonstrate the potential of Muscle Vision to enable real-time pose identification in physical exercises. Additionally, the paper discusses the potential of Muscle Vision to improve the accuracy and efficiency of physical exercise identification and provide valuable insight into physical exercise performance.","Write an abstract for a paper called Muscle Vision: Real Time Keypoint Based Pose Classification of Physical
  Exercises about Artificial Intelligence"
2211.0025,"Wei Peng, Ziyuan Qin, Yue Hu, Yuqiang Xie, Yunpeng Li","FADO: Feedback-Aware Double COntrolling Network for Emotional Support
  Conversation",['cs.CL'],"  Emotional Support Conversation (ESConv) aims to reduce help-seekers'emotional
distress with the supportive strategy and response. It is essential for the
supporter to select an appropriate strategy with the feedback of the
help-seeker (e.g., emotion change during dialog turns, etc) in ESConv. However,
previous methods mainly focus on the dialog history to select the strategy and
ignore the help-seeker's feedback, leading to the wrong and user-irrelevant
strategy prediction. In addition, these approaches only model the
context-to-strategy flow and pay less attention to the strategy-to-context flow
that can focus on the strategy-related context for generating the
strategy-constrain response. In this paper, we propose a Feedback-Aware Double
COntrolling Network (FADO) to make a strategy schedule and generate the
supportive response. The core module in FADO consists of a dual-level feedback
strategy selector and a double control reader. Specifically, the dual-level
feedback strategy selector leverages the turn-level and conversation-level
feedback to encourage or penalize strategies. The double control reader
constructs the novel strategy-to-context flow for generating the
strategy-constrain response. Furthermore, a strategy dictionary is designed to
enrich the semantic information of the strategy and improve the quality of
strategy-constrain response. Experimental results on ESConv show that the
proposed FADO has achieved the state-of-the-art performance in terms of both
strategy selection and response generation. Our code is available at
https://github.com/Thedatababbler/FADO.
","[{'version': 'v1', 'created': 'Tue, 1 Nov 2022 03:37:30 GMT'}, {'version': 'v2', 'created': 'Wed, 1 Feb 2023 03:46:32 GMT'}, {'version': 'v3', 'created': 'Tue, 7 Feb 2023 13:37:19 GMT'}]",2023-02-08,['Computation and Language'],"This paper presents FADO, a novel Feedback-Aware Double Controlling Network for Emotional Support Conversation about Computation and Language. FADO is a deep learning model that uses two components to generate emotion-aware responses to user queries: a Double Controlling Network (DCN) and a Feedback-Aware Network (FAN). The DCN uses a combination of natural language processing and machine learning techniques to identify and classify user emotions, while the FAN utilizes reinforcement learning to adjust the response to the user's feedback. The model is evaluated on a publicly available dataset of conversations about computation and language, and the results demonstrate that FADO outperforms existing models in terms of accuracy and emotion recognition. Furthermore, the model is shown to be robust to user feedback, providing an effective solution for emotional support conversation about computation and language.","Write an abstract for a paper called FADO: Feedback-Aware Double COntrolling Network for Emotional Support
  Conversation about Computation and Language"
2209.04604,"Mohammad Khalafi, Digvijay Boob","Accelerated Primal-Dual Methods for Convex-Strongly-Concave Saddle Point
  Problems","['math.OC', 'cs.LG']","  In this work, we aim to investigate Primal-Dual (PD) methods for
convex-strongly-concave saddle point problems (SPP). In many cases, the
computation of the proximal oracle over the primal-only function is
inefficient. Hence, we use its first-order linear approximation in the proximal
step resulting in a Linearized PD (LPD) method. Even when the coupling term is
bilinear, we observe that LPD has a suboptimal dependence on the Lipschitz
constant of the primal-only function. In contrast, LPD has optimal convergence
for the strongly-convex concave case. This observation induces us to present
our accelerated linearized primal-dual (ALPD) algorithm to solve convex
strongly-concave SPP. ALPD is a single-loop algorithm that combines features of
Nesterov's accelerated gradient descent (AGD) and LPD. We show that when the
coupling term is semi-linear (which contains bilinear as a specific case), ALPD
obtains the optimal dependence on the Lipschitz constant of primal-only
function. Hence, it is an optimal algorithm. When the coupling term has a
general nonlinear form, the ALPD algorithm has suboptimal dependence on the
Lipschitz constant of the primal part of the coupling term. To improve this
dependence, we present an inexact APD algorithm. This algorithm performs AGD
iterations in the inner loop to find an approximate solution to a proximal
subproblem of APD. We show that inexact APD maintains optimal number of
gradients evaluations (gradient complexity) of primal-only and dual parts of
the problem. It also significantly improves the gradient-complexity of the
primal coupling term.
","[{'version': 'v1', 'created': 'Sat, 10 Sep 2022 05:56:50 GMT'}]",2022-09-13,['Machine Learning'],"This paper presents an accelerated Primal-Dual method for solving convex-strongly-concave saddle point problems arising in machine learning applications. The proposed method is based on the non-Euclidean geometry of the problem, which allows for a faster convergence rate than existing methods. We analyze the convergence rate of the proposed method and compare it with existing methods. We also experimentally evaluate the proposed method on several machine learning tasks, and demonstrate its improved performance compared to existing methods. Finally, we discuss the potential applications of the proposed method in machine learning tasks.","Write an abstract for a paper called Accelerated Primal-Dual Methods for Convex-Strongly-Concave Saddle Point
  Problems about Machine Learning"
2205.11267,"Nico Weber, Patrick Holzer, Tania Jacob, Enislay Ramentol","Fed-DART and FACT: A solution for Federated Learning in a production
  environment",['cs.LG'],"  Federated Learning as a decentralized artificial intelligence (AI) solution
solves a variety of problems in industrial applications. It enables a
continuously self-improving AI, which can be deployed everywhere at the edge.
However, bringing AI to production for generating a real business impact is a
challenging task. Especially in the case of Federated Learning, expertise and
resources from multiple domains are required to realize its full potential.
Having this in mind we have developed an innovative Federated Learning
framework FACT based on Fed-DART, enabling an easy and scalable deployment,
helping the user to fully leverage the potential of their private and
decentralized data.
","[{'version': 'v1', 'created': 'Mon, 23 May 2022 12:32:38 GMT'}]",2022-05-24,['Machine Learning'],"This paper presents a novel solution for federated learning in a production environment: Fed-DART and FACT. Fed-DART is a distributed architecture for federated learning that leverages the advantages of distributed computing and machine learning. FACT is a toolkit designed to enable efficient and secure federated learning in a production environment. We discuss the advantages of Fed-DART and FACT, including better scalability, improved security, and faster training times. We also discuss the challenges associated with federated learning in a production environment, and how Fed-DART and FACT address these issues. Finally, we present a case study of a real-world application of Fed-DART and FACT for federated learning in a production environment. The results demonstrate the effectiveness of the proposed solution for federated learning in a production environment.","Write an abstract for a paper called Fed-DART and FACT: A solution for Federated Learning in a production
  environment about Machine Learning"
2210.1539,"Simon L. Cotter, Kody J. H. Law, Xinzhu Liang and Shangda Yang",A randomized Multi-index sequential Monte Carlo method,"['math.NA', 'cs.NA', 'stat.CO']","  We consider the problem of estimating expectations with respect to a target
distribution with an unknown normalizing constant, and where even the
unnormalized target needs to be approximated at finite resolution. Under such
an assumption, this work builds upon a recently introduced multi-index
Sequential Monte Carlo (SMC) ratio estimator, which provably enjoys the
complexity improvements of multi-index Monte Carlo (MIMC) and the efficiency of
SMC for inference. The present work leverages a randomization strategy to
remove bias entirely, which simplifies estimation substantially, particularly
in the MIMC context, where the choice of index set is otherwise important.
Under reasonable assumptions, the proposed method provably achieves the same
canonical complexity of MSE^(-1) as the original method, but without
discretization bias. It is illustrated on examples of Bayesian inverse
problems.
","[{'version': 'v1', 'created': 'Thu, 27 Oct 2022 12:51:15 GMT'}]",2022-10-28,['Numerical Analysis'],"This paper presents a new randomized Multi-index sequential Monte Carlo (RMSMC) method for numerical analysis. The proposed method utilizes a combination of Monte Carlo and multi-index methods to generate a set of samples that approximate the underlying distribution of a given problem. The RMSMC method is applied to a variety of numerical problems, including integration, optimization, and differential equations. The results demonstrate the effectiveness of the proposed method in terms of accuracy, robustness, and efficiency. Furthermore, the proposed method can be used to solve a wide range of numerical problems with minimal computational resources. The paper concludes with a discussion of the potential applications of the proposed RMSMC method.",Write an abstract for a paper called A randomized Multi-index sequential Monte Carlo method about Numerical Analysis
2304.05243,"Klaudia Ba{\l}azy, {\L}ukasz Struski, Marek \'Smieja, Jacek Tabor",r-softmax: Generalized Softmax with Controllable Sparsity Rate,['cs.LG'],"  Nowadays artificial neural network models achieve remarkable results in many
disciplines. Functions mapping the representation provided by the model to the
probability distribution are the inseparable aspect of deep learning solutions.
Although softmax is a commonly accepted probability mapping function in the
machine learning community, it cannot return sparse outputs and always spreads
the positive probability to all positions. In this paper, we propose r-softmax,
a modification of the softmax, outputting sparse probability distribution with
controllable sparsity rate. In contrast to the existing sparse probability
mapping functions, we provide an intuitive mechanism for controlling the output
sparsity level. We show on several multi-label datasets that r-softmax
outperforms other sparse alternatives to softmax and is highly competitive with
the original softmax. We also apply r-softmax to the self-attention module of a
pre-trained transformer language model and demonstrate that it leads to
improved performance when fine-tuning the model on different natural language
processing tasks.
","[{'version': 'v1', 'created': 'Tue, 11 Apr 2023 14:28:29 GMT'}, {'version': 'v2', 'created': 'Wed, 12 Apr 2023 16:30:16 GMT'}]",2023-04-13,['Machine Learning'],"This paper proposes a new algorithm, r-Softmax, which is an extension of the Softmax function that allows for controllable sparsity rate. The r-Softmax can be used to improve the performance of machine learning models by introducing a sparsity rate control parameter that can be adjusted to reduce overfitting. The proposed algorithm is tested on several datasets and compared to the standard Softmax. The results show that the proposed algorithm can reduce overfitting and improve the accuracy of machine learning models. Furthermore, the proposed algorithm is shown to be more robust to changes in the sparsity rate parameter than the standard Softmax. The paper concludes that r-Softmax can be used to improve the performance of machine learning models and reduce overfitting.",Write an abstract for a paper called r-softmax: Generalized Softmax with Controllable Sparsity Rate about Machine Learning
2112.09362,Justyna P. Zwolak and Jacob M. Taylor,Colloquium: Advances in automation of quantum dot devices control,"['quant-ph', 'cond-mat.mes-hall', 'cs.CV', 'cs.LG']","  Arrays of quantum dots (QDs) are a promising candidate system to realize
scalable, coupled qubits systems and serve as a fundamental building block for
quantum computers. In such semiconductor quantum systems, devices now have tens
of individual electrostatic and dynamical voltages that must be carefully set
to localize the system into the single-electron regime and to realize good
qubit operational performance. The mapping of requisite QD locations and
charges to gate voltages presents a challenging classical control problem. With
an increasing number of QD qubits, the relevant parameter space grows
sufficiently to make heuristic control unfeasible.In recent years, there has
been a considerable effort to automate device control that combines
script-based algorithms with machine learning (ML) techniques. In this
colloquium, we present a comprehensive overview of the recent progress in the
automation of QD device control, with a particular emphasis on silicon- and
GaAs-based QDs formed in two-dimensional electron gases. Combining
physics-based modeling with modern numerical optimization and ML has proven
quite effective in yielding efficient, scalable control. Further integration of
theoretical, computational, and experimental efforts with computer science and
ML holds tremendous potential in advancing semiconductor and other platforms
for quantum computing.
","[{'version': 'v1', 'created': 'Fri, 17 Dec 2021 07:41:47 GMT'}, {'version': 'v2', 'created': 'Mon, 22 Aug 2022 14:27:48 GMT'}]",2022-08-23,"['Computer Vision and Pattern Recognition', 'Machine Learning']","This paper presents an overview of recent advances in automation of quantum dot devices control using computer vision and pattern recognition, and machine learning techniques. We discuss the various techniques used in automation, such as image processing, machine learning, and deep learning, and their applications in the field of quantum dot devices control. We also discuss the challenges associated with automation of quantum dot devices control and the potential solutions to these challenges. Finally, we present some existing research works on automation of quantum dot devices control and discuss their potential applications. The paper is expected to provide an insight into the current state of automation of quantum dot devices control and the potential for further development in this field.","Write an abstract for a paper called Colloquium: Advances in automation of quantum dot devices control about Computer Vision and Pattern Recognition, Machine Learning"
2107.07263,"Cao Vien Phung, Christoph Herold, David Humphreys, Thomas Kurner and
  Admela Jukan","Performance Analysis of MDPC and RS codes in Two-channel THz
  Communication Systems",['cs.NI'],"  We analyze whether a multidimensional parity check (MDPC) or a Reed-Solomon
(RS) code in combination with an auxiliary channel can improve the throughput
and extend the THz transmission distance. While channel quality is addressed by
various coding approaches, and an effective THz system configuration is enabled
by other approaches with additional channels, their combination is new with the
potential for significant improvements in quality of the data transmission. Our
specific solution is designed to correct data bits at the physical layer by
using a low complexity erasure code (MDPC or RS), whereby original and parity
data are transferred over two separate and parallel THz channels, including one
main channel and one additional channel. The results are theoretically analyzed
to see that our new solution can improve throughput, support higher modulation
levels and transfer data over the longer distances with THz communications.
","[{'version': 'v1', 'created': 'Thu, 15 Jul 2021 11:39:04 GMT'}, {'version': 'v2', 'created': 'Tue, 16 Nov 2021 10:59:17 GMT'}]",2022-07-13,['Networking and Internet Architecture'],"This paper presents a performance analysis of MDPC and RS codes in two-channel THz communication systems. The analysis focuses on the performance of these codes in terms of their error-correcting capability, their power efficiency, and their ability to support high-speed data transmission. The paper evaluates the performance of MDPC and RS codes in the context of two-channel THz communication systems, which are characterized by high frequency, high bandwidth, and high data rate requirements. The paper also investigates the impact of different coding schemes on the overall performance of the system. Finally, the paper discusses the implications of the results for the design of future THz communication systems.","Write an abstract for a paper called Performance Analysis of MDPC and RS codes in Two-channel THz
  Communication Systems about Networking and Internet Architecture"
2009.08443,"Joscha Diehl, Kurusch Ebrahimi-Fard, Nikolas Tapia","Tropical time series, iterated-sums signatures and quasisymmetric
  functions","['math.RA', 'cs.CV', 'cs.LG']","  Aiming for a systematic feature-extraction from time series, we introduce the
iterated-sums signature over arbitrary commutative semirings. The case of the
tropical semiring is a central, and our motivating example. It leads to
features of (real-valued) time series that are not easily available using
existing signature-type objects. We demonstrate how the signature extracts
chronological aspects of a time series, and that its calculation is possible in
linear time. We identify quasisymmetric expressions over semirings as the
appropriate framework for iterated-sums signatures over semiring-valued time
series.
","[{'version': 'v1', 'created': 'Thu, 17 Sep 2020 17:51:43 GMT'}, {'version': 'v2', 'created': 'Mon, 9 Nov 2020 21:36:52 GMT'}, {'version': 'v3', 'created': 'Sat, 2 Apr 2022 14:53:19 GMT'}]",2022-04-05,"['Computer Vision and Pattern Recognition', 'Machine Learning']","This paper focuses on the application of tropical time series, iterated-sums signatures and quasisymmetric functions to the fields of Computer Vision and Pattern Recognition, and Machine Learning. We present a novel approach for analyzing tropical time series using iterated-sums signatures and quasisymmetric functions. We then show how these techniques can be used to improve the accuracy of image recognition and machine learning algorithms. We demonstrate the effectiveness of our approach by applying it to a variety of datasets and comparing the results with those of existing methods. Finally, we discuss the potential applications of our work to various areas of computer vision and pattern recognition, and machine learning.","Write an abstract for a paper called Tropical time series, iterated-sums signatures and quasisymmetric
  functions about Computer Vision and Pattern Recognition, Machine Learning"
2206.08787,"Biraja Ghoshal, Bhargab Ghoshal, and Allan Tucker","Leveraging Uncertainty in Deep Learning for Pancreatic Adenocarcinoma
  Grading","['eess.IV', 'cs.CV', 'cs.LG']","  Pancreatic cancers have one of the worst prognoses compared to other cancers,
as they are diagnosed when cancer has progressed towards its latter stages. The
current manual histological grading for diagnosing pancreatic adenocarcinomas
is time-consuming and often results in misdiagnosis. In digital pathology,
AI-based cancer grading must be extremely accurate in prediction and
uncertainty quantification to improve reliability and explainability and are
essential for gaining clinicians trust in the technology. We present Bayesian
Convolutional Neural Networks for automated pancreatic cancer grading from MGG
and HE stained images to estimate uncertainty in model prediction. We show that
the estimated uncertainty correlates with prediction error. Specifically, it is
useful in setting the acceptance threshold using a metric that weighs
classification accuracy-reject trade-off and misclassification cost controlled
by hyperparameters and can be employed in clinical settings.
","[{'version': 'v1', 'created': 'Wed, 15 Jun 2022 19:53:06 GMT'}]",2022-06-20,"['Computer Vision and Pattern Recognition', 'Machine Learning']","This paper presents a novel approach to leveraging uncertainty in deep learning for the diagnosis of pancreatic adenocarcinoma. We propose a multi-task deep learning framework that combines computer vision and pattern recognition, machine learning, and uncertainty estimation. This framework is used to segment and classify pancreatic adenocarcinoma from computed tomography (CT) images. The uncertainty estimation module is used to provide a measure of confidence for the classification of the disease. Experiments conducted on a dataset of CT images show that our proposed approach achieves a high accuracy in the diagnosis of pancreatic adenocarcinoma, providing a robust and reliable tool for physicians. The results obtained demonstrate the effectiveness of our proposed approach in the diagnosis of pancreatic adenocarcinoma.","Write an abstract for a paper called Leveraging Uncertainty in Deep Learning for Pancreatic Adenocarcinoma
  Grading about Computer Vision and Pattern Recognition, Machine Learning"
2301.11931,Kai Diethelm,"Diffusive Representations for the Numerical Evaluation of Fractional
  Integrals","['math.NA', 'cs.NA']","  Diffusive representations of fractional differential and integral operators
can provide a convenient means to construct efficient numerical algorithms for
their approximate evaluation. In the current literature, many different
variants of such representations have been proposed. Concentrating on
Riemann-Liouville integrals whose order is in (0,1), we here present a general
approach that comprises most of these variants as special cases and that allows
a detailed investigation of the analytic properties of each variant. The
availability of this information allows to choose concrete numerical methods
for handling the representations that exploit the specific properties, thus
allowing to construct very efficient overall methods.
","[{'version': 'v1', 'created': 'Fri, 27 Jan 2023 09:06:26 GMT'}]",2023-01-31,['Numerical Analysis'],"This paper presents a novel numerical method for the evaluation of fractional integrals. It introduces a diffusive representation of the fractional integral operator, which is then used to construct a numerical scheme for the numerical evaluation of fractional integrals. The diffusive representation is based on a combination of the Laplace transform and the Fourier transform, which allows for the numerical integration of fractional integrals in the time domain. The proposed method is validated by comparing its results to analytical solutions and numerical integration techniques. The results show that the proposed method is a viable alternative to existing numerical integration techniques and can be used to accurately evaluate fractional integrals.","Write an abstract for a paper called Diffusive Representations for the Numerical Evaluation of Fractional
  Integrals about Numerical Analysis"
2209.10587,Xixi Li and Jingsong Yuan,DeepVARwT: Deep Learning for a VAR Model with Trend,"['stat.ME', 'cs.AI']","  The vector autoregressive (VAR) model has been used to describe the
dependence within and across multiple time series. This is a model for
stationary time series which can be extended to allow the presence of a
deterministic trend in each series. Detrending the data either parametrically
or nonparametrically before fitting the VAR model gives rise to more errors in
the latter part. In this study, we propose a new approach called DeepVARwT that
employs deep learning methodology for maximum likelihood estimation of the
trend and the dependence structure at the same time. A Long Short-Term Memory
(LSTM) network is used for this purpose. To ensure the stability of the model,
we enforce the causality condition on the autoregressive coefficients using the
transformation of Ansley & Kohn (1986). We provide a simulation study and an
application to real data. In the simulation study, we use realistic trend
functions generated from real data and compare the estimates with true
function/parameter values. In the real data application, we compare the
prediction performance of this model with state-of-the-art models in the
literature.
","[{'version': 'v1', 'created': 'Wed, 21 Sep 2022 18:23:03 GMT'}, {'version': 'v2', 'created': 'Tue, 11 Oct 2022 17:58:08 GMT'}]",2022-10-12,['Artificial Intelligence'],"This paper presents DeepVARwT, an artificial intelligence (AI) model combining deep learning and vector autoregression (VAR) with trend for forecasting. The proposed model is designed to improve the accuracy of forecasting by introducing deep learning into the traditional VAR model with trend. We evaluate the performance of DeepVARwT against traditional VAR models with trend and deep learning models on a variety of datasets. The results demonstrate that DeepVARwT significantly outperforms the traditional VAR models with trend and deep learning models in terms of accuracy and robustness. The proposed model is also robust to data sparsity and provides better interpretability than deep learning models. We conclude that DeepVARwT provides a powerful and reliable forecasting tool for AI applications.",Write an abstract for a paper called DeepVARwT: Deep Learning for a VAR Model with Trend about Artificial Intelligence
2203.05169,"Devansh Saxena, Erina Seh-Young Moon, Dahlia Shehata, Shion Guha","Unpacking Invisible Work Practices, Constraints, and Latent Power
  Relationships in Child Welfare through Casenote Analysis","['cs.HC', 'cs.CY']","  Caseworkers are trained to write detailed narratives about families in
Child-Welfare (CW) which informs collaborative high-stakes decision-making.
Unlike other administrative data, these narratives offer a more credible source
of information with respect to workers' interactions with families as well as
underscore the role of systemic factors in decision-making. SIGCHI researchers
have emphasized the need to understand human discretion at the street-level to
be able to design human-centered algorithms for the public sector. In this
study, we conducted computational text analysis of casenotes at a child-welfare
agency in the midwestern United States and highlight patterns of invisible
street-level discretionary work and latent power structures that have direct
implications for algorithm design. Casenotes offer a unique lens for
policymakers and CW leadership towards understanding the experiences of
on-the-ground caseworkers. As a result of this study, we highlight how
street-level discretionary work needs to be supported by sociotechnical systems
developed through worker-centered design. This study offers the first
computational inspection of casenotes and introduces them to the SIGCHI
community as a critical data source for studying complex sociotechnical
systems.
","[{'version': 'v1', 'created': 'Thu, 10 Mar 2022 05:48:22 GMT'}]",2022-03-11,"['Human-Computer Interaction', 'Computers and Society']","This paper examines the invisible work practices, constraints, and latent power relationships in child welfare through casenote analysis with a focus on human-computer interaction. Drawing on qualitative data from a sample of casenotes in a child welfare agency, the paper explores how computers and technology shape the way child welfare workers interact with clients, how they make decisions, and how they navigate the power dynamics of their roles. The paper argues that an understanding of the invisible work practices and constraints embedded in casenotes is essential to inform the design of computer systems that support child welfare workers in their work. The paper further argues that the design of such systems should be grounded in an understanding of the latent power relationships that exist in child welfare work. The paper concludes with implications for the design of computer systems that support child welfare workers in their work.","Write an abstract for a paper called Unpacking Invisible Work Practices, Constraints, and Latent Power
  Relationships in Child Welfare through Casenote Analysis about Human-Computer Interaction, Computers and Society"
2109.13096,"Francesco Miniati, Gianluca Gregori",Learning Transport Processes with Machine Intelligence,"['physics.plasm-ph', 'cs.LG']","  We present a machine learning based approach to address the study of
transport processes, ubiquitous in continuous mechanics, with particular
attention to those phenomena ruled by complex micro-physics, impractical to
theoretical investigation, yet exhibiting emergent behavior describable by a
closed mathematical expression. Our machine learning model, built using simple
components and following a few well established practices, is capable of
learning latent representations of the transport process substantially closer
to the ground truth than expected from the nominal error characterising the
data, leading to sound generalisation properties. This is demonstrated through
an idealized study of the long standing problem of heat flux suppression
relevant to fusion and cosmic plasmas. Our analysis shows that the result
applies beyond those case specific assumptions and that, in particular, the
accuracy of the learned representation is controllable through knowledge of the
data quality (error properties) and a suitable choice of the dataset size.
While the learned representation can be used as a plug-in for numerical
modeling purposes, it can also be leveraged with the above error analysis to
obtain reliable mathematical expressions describing the transport mechanism and
of great theoretical value.
","[{'version': 'v1', 'created': 'Mon, 27 Sep 2021 14:49:22 GMT'}, {'version': 'v2', 'created': 'Sat, 20 Nov 2021 10:02:38 GMT'}, {'version': 'v3', 'created': 'Tue, 14 Jun 2022 23:01:05 GMT'}]",2022-06-16,['Machine Learning'],"This paper explores the potential of machine intelligence to learn transport processes. It examines the capabilities of machine learning algorithms to identify and classify transport processes from data, and to develop models that can predict transport phenomena. It examines the advantages and disadvantages of using machine learning approaches, and discusses potential applications of machine learning for transport processes. Finally, the paper discusses the implications of using machine learning for transport processes and the challenges that need to be addressed in order to realize the potential of machine learning for transport processes.",Write an abstract for a paper called Learning Transport Processes with Machine Intelligence about Machine Learning
2202.0658,"Yufan Zeng, Jiashan Tang","Improved Aggregating and Accelerating Training Methods for Spatial Graph
  Neural Networks on Fraud Detection","['cs.LG', 'cs.AI']","  Graph neural networks (GNNs) have been widely applied to numerous fields. A
recent work which combines layered structure and residual connection proposes
an improved deep architecture to extend CAmouflage-REsistant GNN (CARE-GNN) to
deep models named as Residual Layered CARE-GNN (RLC-GNN), which forms
self-correcting and incremental learning mechanism, and achieves significant
performance improvements on fraud detection task. However, we spot three issues
of RLC-GNN, which are the usage of neighboring information reaching limitation,
the training difficulty which is inherent problem to deep models and lack of
comprehensive consideration about node features and external patterns. In this
work, we propose three approaches to solve those three problems respectively.
First, we suggest conducting similarity measure via cosine distance to take
both local features and external patterns into consideration. Then, we combine
the similarity measure module and the idea of adjacency-wise normalization with
node-wise and batch-wise normalization and then propound partial neighborhood
normalization methods to overcome the training difficulty while mitigating the
impact of too much noise caused by high-density of graph. Finally, we put
forward intermediate information supplement to solve the information
limitation. Experiments are conducted on Yelp and Amazon datasets. And the
results show that our proposed methods effectively solve the three problems.
After applying the three methods, we achieve 4.81%, 6.62% and 6.81%
improvements in the metrics of recall, AUC and Macro-F1 respectively on the
Yelp dataset. And we obtain 1.65% and 0.29% improvements in recall and AUC
respectively on the Amazon datasets.
","[{'version': 'v1', 'created': 'Mon, 14 Feb 2022 09:51:35 GMT'}]",2022-02-15,"['Machine Learning', 'Artificial Intelligence']","This paper proposes improved methods for aggregating and accelerating training of spatial graph neural networks (SGNNs) for fraud detection. SGNNs are a type of machine learning model that can capture the spatial relationships between data points and learn patterns from complex, non-linear data. The proposed methods are based on a combination of graph convolutional networks (GCNs) and attention mechanisms. These methods are tested on a real-world fraud detection dataset, and results show that they can significantly reduce the training time of SGNNs while still achieving comparable performance. Furthermore, the methods show promise in terms of scalability, making them suitable for large-scale fraud detection applications. Finally, the paper discusses the potential applications of these methods in other areas of artificial intelligence.","Write an abstract for a paper called Improved Aggregating and Accelerating Training Methods for Spatial Graph
  Neural Networks on Fraud Detection about Machine Learning, Artificial Intelligence"
2301.11719,"Chen Chen, Wei Emma Zhang, Alireza Seyed Shakeri","Incorporating Knowledge into Document Summarization: an Application of
  Prefix-Tuning on GPT-2","['cs.CL', 'cs.AI', 'cs.LG']","  Despite the great development of document summarization techniques nowadays,
factual inconsistencies between the generated summaries and the original text
still occur from time to time. This paper proposes a prefix-tuning-based
approach that uses a set of trainable continuous prefix prompt together with
discrete prompts to aid model generation, which makes a significant impact on
both CNN/Daily Mail and XSum summaries generated using GPT-2. The improvements
on fact preservation in the generated summaries indicates the effectiveness of
adopting this prefix-tuning-based method in knowledge-enhanced document
summarization, and also shows a great potential on other natural language
processing tasks.
","[{'version': 'v1', 'created': 'Fri, 27 Jan 2023 14:05:12 GMT'}, {'version': 'v2', 'created': 'Tue, 31 Jan 2023 09:55:23 GMT'}]",2023-02-01,"['Computation and Language', 'Artificial Intelligence', 'Machine Learning']","This paper presents an application of Prefix-Tuning on GPT-2 to incorporate knowledge into document summarization. Prefix-Tuning is a technique that fine-tunes a pre-trained language model to generate text conditioned on a given input. We apply Prefix-Tuning on GPT-2, a Transformer-based language model, to generate summaries of documents in the domain of Computation and Language, Artificial Intelligence, and Machine Learning. We evaluate the performance of our approach on a corpus of documents from the ACL Anthology, a collection of scientific papers in the field of natural language processing. Our results demonstrate that Prefix-Tuning on GPT-2 can generate summaries that are more accurate than those generated by traditional summarization techniques. Furthermore, our approach is more robust to changes in the input document, allowing for more personalized summaries. Our work provides a novel approach for incorporating knowledge into document summarization, and could be extended to other domains.","Write an abstract for a paper called Incorporating Knowledge into Document Summarization: an Application of
  Prefix-Tuning on GPT-2 about Computation and Language, Artificial Intelligence, Machine Learning"
2106.11174,"Steven Gutstein, Brent Lance and Sanjay Shakkottai","Does Optimal Source Task Performance Imply Optimal Pre-training for a
  Target Task?","['cs.LG', 'cs.CV']","  Fine-tuning of pre-trained deep nets is commonly used to improve accuracies
and training times for neural nets. It is generally assumed that pre-training a
net for optimal source task performance best prepares it for fine-tuning to
learn an arbitrary target task. This is generally not true. Stopping source
task training, prior to optimal performance, can create a pre-trained net
better suited for fine-tuning to learn a new task. We perform several
experiments demonstrating this effect, as well as the influence of the amount
of training and of learning rate. Additionally, our results indicate that this
reflects a general loss of learning ability that even extends to relearning the
source task.
","[{'version': 'v1', 'created': 'Mon, 21 Jun 2021 15:09:04 GMT'}, {'version': 'v2', 'created': 'Tue, 12 Apr 2022 16:44:47 GMT'}]",2022-04-13,"['Machine Learning', 'Computer Vision and Pattern Recognition']","This paper explores the question of whether optimal source task performance implies optimal pre-training for a target task in the context of machine learning, computer vision and pattern recognition. We examine the relationship between source and target task performance, and how pre-training can be used to improve target task performance. We discuss the implications of pre-training on target task performance and how it can be used to optimize the training process. We also review existing research on pre-training and its effects on target task performance. Finally, we propose an experiment to evaluate the impact of pre-training on target task performance. Our results suggest that while pre-training may not necessarily result in optimal target task performance, it can help improve it.","Write an abstract for a paper called Does Optimal Source Task Performance Imply Optimal Pre-training for a
  Target Task? about Machine Learning, Computer Vision and Pattern Recognition"
2207.02457,"Akira Taniguchi, Maoko Muro, Hiroshi Yamakawa, Tadahiro Taniguchi","Brain-inspired probabilistic generative model for double articulation
  analysis of spoken language","['q-bio.NC', 'cs.AI', 'cs.CL']","  The human brain, among its several functions, analyzes the double
articulation structure in spoken language, i.e., double articulation analysis
(DAA). A hierarchical structure in which words are connected to form a sentence
and words are composed of phonemes or syllables is called a double articulation
structure. Where and how DAA is performed in the human brain has not been
established, although some insights have been obtained. In addition, existing
computational models based on a probabilistic generative model (PGM) do not
incorporate neuroscientific findings, and their consistency with the brain has
not been previously discussed. This study compared, mapped, and integrated
these existing computational models with neuroscientific findings to bridge
this gap, and the findings are relevant for future applications and further
research. This study proposes a PGM for a DAA hypothesis that can be realized
in the brain based on the outcomes of several neuroscientific surveys. The
study involved (i) investigation and organization of anatomical structures
related to spoken language processing, and (ii) design of a PGM that matches
the anatomy and functions of the region of interest. Therefore, this study
provides novel insights that will be foundational to further exploring DAA in
the brain.
","[{'version': 'v1', 'created': 'Wed, 6 Jul 2022 06:03:10 GMT'}]",2022-07-07,"['Artificial Intelligence', 'Computation and Language']","This paper presents a brain-inspired probabilistic generative model for double articulation analysis of spoken language about Artificial Intelligence (AI), Computation and Language. The model combines neural networks and probabilistic graphical models to capture the contextual information of the spoken language. The model is tested on a corpus of speech recordings related to AI, Computation and Language. The results show that the model is able to capture the contextual information accurately and can be used for double articulation analysis of spoken language. The paper also provides a detailed description of the model and its implementation and discusses its potential applications in natural language processing and AI.","Write an abstract for a paper called Brain-inspired probabilistic generative model for double articulation
  analysis of spoken language about Artificial Intelligence, Computation and Language"
2210.04017,"Zeyu Gao, Yao Mu, Ruoyan Shen, Chen Chen, Yangang Ren, Jianyu Chen,
  Shengbo Eben Li, Ping Luo, Yanfeng Lu","Enhance Sample Efficiency and Robustness of End-to-end Urban Autonomous
  Driving via Semantic Masked World Model",['cs.LG'],"  End-to-end autonomous driving provides a feasible way to automatically
maximize overall driving system performance by directly mapping the raw pixels
from a front-facing camera to control signals. Recent advanced methods
construct a latent world model to map the high dimensional observations into
compact latent space. However, the latent states embedded by the world model
proposed in previous works may contain a large amount of task-irrelevant
information, resulting in low sampling efficiency and poor robustness to input
perturbations. Meanwhile, the training data distribution is usually unbalanced,
and the learned policy is hard to cope with the corner cases during the driving
process. To solve the above challenges, we present a semantic masked recurrent
world model (SEM2), which introduces a latent filter to extract key
task-relevant features and reconstruct a semantic mask via the filtered
features, and is trained with a multi-source data sampler, which aggregates
common data and multiple corner case data in a single batch, to balance the
data distribution. Extensive experiments on CARLA show that our method
outperforms the state-of-the-art approaches in terms of sample efficiency and
robustness to input permutations.
","[{'version': 'v1', 'created': 'Sat, 8 Oct 2022 13:00:08 GMT'}, {'version': 'v2', 'created': 'Thu, 1 Dec 2022 12:29:09 GMT'}]",2022-12-02,['Machine Learning'],"This paper presents a novel method for enhancing the sample efficiency and robustness of end-to-end urban autonomous driving via a semantic masked world model. The proposed approach leverages a convolutional neural network to learn a semantic mask from the environment and then uses this mask to train a world model to predict future states and actions. The world model consists of several layers of convolutional neural networks that are trained on a large dataset of urban autonomous driving trajectories. The proposed approach is evaluated on a simulated urban autonomous driving task and compared to a baseline approach. Results demonstrate that the proposed approach significantly increases the sample efficiency and robustness of the end-to-end urban autonomous driving system, leading to a more reliable and safe autonomous driving system.","Write an abstract for a paper called Enhance Sample Efficiency and Robustness of End-to-end Urban Autonomous
  Driving via Semantic Masked World Model about Machine Learning"
2112.06638,Jun Lu,On the Column and Row Ranks of a Matrix,"['math.HO', 'cs.NA', 'math.NA']","  Every m by n matrix A with rank r has exactly r independent rows and r
independent columns. The fact has become the most fundamental theorem in linear
algebra such that we may favor it in an unconscious way. The sole aim of this
paper is to give a self-contained introduction to concepts and mathematical
tools for the rank of a matrix in order to seamlessly introduce how it works in
applied linear algebra. However, we clearly realize our inability to cover all
the useful and interesting results concerning this topic and given the paucity
of scope to present this discussion, e.g., a proof via the injective linear
map. We refer the reader to literature in the field of linear algebra for a
more detailed introduction to the related fields.
","[{'version': 'v1', 'created': 'Tue, 23 Nov 2021 12:39:27 GMT'}]",2022-07-29,['Numerical Analysis'],"This paper presents an analysis of the column and row ranks of a matrix. It begins by introducing the concept of rank and how it applies to matrices. It then examines the properties of the column and row ranks of a matrix, including their relationship to the determinant and inverse of the matrix. The paper then explores the implications of these properties for numerical analysis. It concludes with a discussion of the implications for further research in this area.",Write an abstract for a paper called On the Column and Row Ranks of a Matrix about Numerical Analysis
2202.05677,"Hewei Liu, Shuyuan Zhu, Ruiqin Xiong, Guanghui Liu, and Bing Zeng",Cross-Block Difference Guided Fast CU Partition for VVC Intra Coding,"['eess.IV', 'cs.MM']","  In this paper, we propose a new fast CU partition algorithm for VVC intra
coding based on cross-block difference. This difference is measured by the
gradient and the content of sub-blocks obtained from partition and is employed
to guide the skipping of unnecessary horizontal and vertical partition modes.
With this guidance, a fast determination of block partitions is accordingly
achieved. Compared with VVC, our proposed method can save 41.64% (on average)
encoding time with only 0.97% (on average) increase of BD-rate.
","[{'version': 'v1', 'created': 'Fri, 14 Jan 2022 10:56:12 GMT'}]",2022-02-14,['Multimedia'],"This paper presents a new cross-block difference (CBD) guided fast CU partition algorithm for efficient intra coding in the Versatile Video Coding (VVC) framework. The proposed algorithm uses CBD to guide the fast CU partition process, which reduces the search complexity and improves the coding efficiency of the VVC intra coding. The CBD-guided fast CU partition algorithm is evaluated using the VVC reference software JM16.0 and compared with the existing fast CU partition algorithms. Experimental results demonstrate that the proposed algorithm achieves a 0.8% BD-rate improvement over the existing fast CU partition algorithms, while also providing a significant reduction in coding time. Furthermore, the proposed algorithm is shown to be robust to different video content and resolutions.",Write an abstract for a paper called Cross-Block Difference Guided Fast CU Partition for VVC Intra Coding about Multimedia
2211.11965,"Juan C. Quiroz, David Brieger, Louisa Jorm, Raymond W Sy, Benjumin
  Hsu, Blanca Gallego","Predicting adverse outcomes following catheter ablation treatment for
  atrial fibrillation","['cs.LG', 'q-bio.QM', 'stat.OT']","  Objective: To develop prognostic survival models for predicting adverse
outcomes after catheter ablation treatment for non-valvular atrial fibrillation
(AF).
  Methods: We used a linked dataset including hospital administrative data,
prescription medicine claims, emergency department presentations, and death
registrations of patients in New South Wales, Australia. The cohort included
patients who received catheter ablation for AF. Traditional and deep survival
models were trained to predict major bleeding events and a composite of heart
failure, stroke, cardiac arrest, and death.
  Results: Out of a total of 3285 patients in the cohort, 177 (5.3%)
experienced the composite outcomeheart failure, stroke, cardiac arrest,
deathand 167 (5.1%) experienced major bleeding events after catheter ablation
treatment. Models predicting the composite outcome had high risk discrimination
accuracy, with the best model having a concordance index > 0.79 at the
evaluated time horizons. Models for predicting major bleeding events had poor
risk discrimination performance, with all models having a concordance index <
0.66. The most impactful features for the models predicting higher risk were
comorbidities indicative of poor health, older age, and therapies commonly used
in sicker patients to treat heart failure and AF.
  Conclusions: Diagnosis and medication history did not contain sufficient
information for precise risk prediction of experiencing major bleeding events.
The models for predicting the composite outcome have the potential to enable
clinicians to identify and manage high-risk patients following catheter
ablation proactively. Future research is needed to validate the usefulness of
these models in clinical practice.
","[{'version': 'v1', 'created': 'Tue, 22 Nov 2022 02:55:51 GMT'}]",2022-11-23,['Machine Learning'],"This paper presents a Machine Learning approach to predict adverse outcomes following catheter ablation treatment for atrial fibrillation. The study was conducted using a dataset of patients who underwent catheter ablation treatment for atrial fibrillation. Several Machine Learning algorithms were implemented to develop predictive models for the outcomes. The performance of the models was evaluated using metrics such as accuracy, precision, recall and F1 score. The results show that the Machine Learning models can predict adverse outcomes with an accuracy of up to 97%. The findings suggest that Machine Learning is a promising tool for predicting adverse outcomes following catheter ablation treatment for atrial fibrillation.","Write an abstract for a paper called Predicting adverse outcomes following catheter ablation treatment for
  atrial fibrillation about Machine Learning"
2208.14132,"Johannes Blum, Yann Disser, Andreas Emil Feldmann, Siddharth Gupta,
  Anna Zych-Pawlewicz",On Sparse Hitting Sets: from Fair Vertex Cover to Highway Dimension,"['cs.DS', 'cs.CC', 'cs.DM']","  We consider the Sparse Hitting Set (Sparse-HS) problem, where we are given a
set system $(V,\mathcal{F},\mathcal{B})$ with two families
$\mathcal{F},\mathcal{B}$ of subsets of $V$. The task is to find a hitting set
for $\mathcal{F}$ that minimizes the maximum number of elements in any of the
sets of $\mathcal{B}$. Our focus is on determining the complexity of some
special cases of Sparse-HS with respect to the sparseness $k$, which is the
optimum number of hitting set elements in any set of $\mathcal{B}$.
  For the Sparse Vertex Cover (Sparse-VC) problem, $V$ is given by the vertex
set of a graph, and $\mathcal{F}$ is its edge set. We prove NP-hardness for
sparseness $k\geq 2$ and polynomial time solvability for $k=1$. We also provide
a polynomial-time $2$-approximation for any $k$. A special case of Sparse-VC is
Fair Vertex Cover (Fair-VC), where the family $\mathcal{B}$ is given by vertex
neighbourhoods. For this problem we prove NP-hardness for constant $k$ and
provide a polynomial-time $(2-\frac{1}{k})$-approximation. This is better than
any approximation possible for Sparse-VC or Vertex Cover (under UGC).
  We then consider two problems derived from Sparse-HS related to the highway
dimension, a graph parameter modelling transportation networks. Most algorithms
for graphs of low highway dimension compute solutions to the $r$-Shortest Path
Cover ($r$-SPC) problem, where $r>0$, $\mathcal{F}$ contains all shortest paths
of length between $r$ and $2r$, and $\mathcal{B}$ contains all balls of radius
$2r$. There is an XP algorithm that computes solutions to $r$-SPC of sparseness
at most $h$ if the input graph has highway dimension $h$, but the existence if
an FPT algorithm was open. We prove that $r$-SPC and also the related
$r$-Highway Dimension ($r$-HD) problem are both W[1]-hard. Furthermore, we
prove that $r$-SPC admits a polynomial-time $O(\log n)$-approximation.
","[{'version': 'v1', 'created': 'Tue, 30 Aug 2022 10:28:06 GMT'}, {'version': 'v2', 'created': 'Wed, 31 Aug 2022 08:28:51 GMT'}, {'version': 'v3', 'created': 'Wed, 28 Sep 2022 10:40:53 GMT'}]",2022-09-29,"['Data Structures and Algorithms', 'Computational Complexity', 'Discrete Mathematics']","This paper examines the sparse hitting set problem, which has applications in both Fair Vertex Cover and Highway Dimension. We explore the connections between these two problems and discuss the various data structures and algorithms used to solve them. We also analyze the computational complexity of the sparse hitting set problem and discuss how to reduce its complexity in certain settings. Finally, we discuss the implications of our findings in terms of discrete mathematics and its application to computer science. In conclusion, this paper provides a comprehensive overview of the sparse hitting set problem and its implications for Fair Vertex Cover and Highway Dimension.","Write an abstract for a paper called On Sparse Hitting Sets: from Fair Vertex Cover to Highway Dimension about Data Structures and Algorithms, Computational Complexity, Discrete Mathematics"
2301.03679,Niklas Zwingenberger,Transformers as Policies for Variable Action Environments,"['cs.AI', 'cs.LG']","  In this project we demonstrate the effectiveness of the transformer encoder
as a viable architecture for policies in variable action environments. Using
it, we train an agent using Proximal Policy Optimisation (PPO) on multiple maps
against scripted opponents in the Gym-$\mu$RTS environment. The final agent is
able to achieve a higher return using half the computational resources of the
next-best RL agent, which used the GridNet architecture.
  The source code and pre-trained models are available here:
https://github.com/NiklasZ/transformers-for-variable-action-envs
","[{'version': 'v1', 'created': 'Mon, 9 Jan 2023 21:03:10 GMT'}]",2023-01-11,"['Artificial Intelligence', 'Machine Learning']","and Robotics

This paper will discuss the use of transformers as policies for variable action environments in Artificial Intelligence (AI), Machine Learning (ML), and Robotics. It will explore the advantages of using transformer-based policies for variable action environments, such as the ability to capture long-term dependencies and the ability to quickly adapt to changing environments. Additionally, the paper will discuss the challenges associated with using transformer-based policies and the methods for overcoming them. Finally, the paper will discuss potential applications of transformer-based policies in AI, ML, and Robotics. The paper will ultimately provide a comprehensive overview of the benefits and drawbacks of using transformers as policies for variable action environments.","Write an abstract for a paper called Transformers as Policies for Variable Action Environments about Artificial Intelligence, Machine Learning"
2211.11278,"Amjad Ali, Muhammad Hamraz, Dost Muhammad Khan, Saeed Aldahmani,
  Zardad Khan","An Optimal k Nearest Neighbours Ensemble for Classification Based on
  Extended Neighbourhood Rule with Features subspace","['stat.ML', 'cs.LG']","  To minimize the effect of outliers, kNN ensembles identify a set of closest
observations to a new sample point to estimate its unknown class by using
majority voting in the labels of the training instances in the neighbourhood.
Ordinary kNN based procedures determine k closest training observations in the
neighbourhood region (enclosed by a sphere) by using a distance formula. The k
nearest neighbours procedure may not work in a situation where sample points in
the test data follow the pattern of the nearest observations that lie on a
certain path not contained in the given sphere of nearest neighbours.
Furthermore, these methods combine hundreds of base kNN learners and many of
them might have high classification errors thereby resulting in poor ensembles.
To overcome these problems, an optimal extended neighbourhood rule based
ensemble is proposed where the neighbours are determined in k steps. It starts
from the first nearest sample point to the unseen observation. The second
nearest data point is identified that is closest to the previously selected
data point. This process is continued until the required number of the k
observations are obtained. Each base model in the ensemble is constructed on a
bootstrap sample in conjunction with a random subset of features. After
building a sufficiently large number of base models, the optimal models are
then selected based on their performance on out-of-bag (OOB) data.
","[{'version': 'v1', 'created': 'Mon, 21 Nov 2022 09:13:54 GMT'}]",2022-11-22,['Machine Learning'],This paper proposes an optimal k Nearest Neighbours Ensemble for Classification (KNNEC) based on an Extended Neighbourhood Rule with Features subspace. The KNNEC algorithm combines a k-nearest neighbour classifier with a feature subspace selection technique. The Extended Neighbourhood Rule is used to select the optimal neighbourhood size and the Feature subspace is used to select the most relevant features. The proposed algorithm is tested on a number of datasets and compared to several existing algorithms. Results show that the proposed KNNEC algorithm outperforms existing algorithms in terms of accuracy and computational cost. The paper also discusses possible applications of the KNNEC algorithm in Machine Learning.,"Write an abstract for a paper called An Optimal k Nearest Neighbours Ensemble for Classification Based on
  Extended Neighbourhood Rule with Features subspace about Machine Learning"
2003.11331,Wilmer Ricciotti and James Cheney,A Formalization of SQL with Nulls,['cs.LO'],"  SQL is the world's most popular declarative language, forming the basis of
the multi-billion-dollar database industry. Although SQL has been standardized,
the full standard is based on ambiguous natural language rather than formal
specification. Commercial SQL implementations interpret the standard in
different ways, so that, given the same input data, the same query can yield
different results depending on the SQL system it is run on. Even for a
particular system, mechanically checked formalization of all widely-used
features of SQL remains an open problem. The lack of a well-understood formal
semantics makes it very difficult to validate the soundness of database
implementations.
  Although formal semantics for fragments of SQL were designed in the past,
they usually did not support set and bag operations, lateral joins, nested
subqueries, and, crucially, null values. Null values complicate SQL's semantics
in profound ways analogous to null pointers or side-effects in other
programming languages. Since certain SQL queries are equivalent in the absence
of null values, but produce different results when applied to tables containing
incomplete data, semantics which ignore null values are able to prove query
equivalences that are unsound in realistic databases.
  A formal semantics of SQL supporting all the aforementioned features was only
proposed recently. In this paper, we report about our mechanization of SQL
semantics covering set/bag operations, lateral joins, nested subqueries, and
nulls, written in the Coq proof assistant, and describe the validation of key
metatheoretic properties. Additionally, we are able to use the same framework
to formalize the semantics of a flat relational calculus (with null values),
and show a certified translation of its normal forms into SQL.
","[{'version': 'v1', 'created': 'Wed, 25 Mar 2020 11:23:25 GMT'}, {'version': 'v2', 'created': 'Mon, 20 Jun 2022 11:52:58 GMT'}]",2022-06-22,['Logic in Computer Science'],"This paper presents a formalization of SQL with nulls, a language used to manipulate data in relational databases. It discusses the logic of nulls in SQL and how this logic can be used to make queries more precise and efficient. The paper begins by introducing the concept of nulls in SQL, and then proceeds to discuss the logic of nulls in the context of SQL. It then presents a formalization of SQL with nulls, which provides a set of rules for reasoning about queries with nulls. Finally, the paper discusses the implications of this formalization for the efficiency of queries and the accuracy of results. The paper concludes by discussing the potential applications of this formalization in computer science.",Write an abstract for a paper called A Formalization of SQL with Nulls about Logic in Computer Science
2203.04002,"Jonathan A. Kelner, Jerry Li, Allen Liu, Aaron Sidford, Kevin Tian",Semi-Random Sparse Recovery in Nearly-Linear Time,"['cs.DS', 'cs.LG', 'math.OC', 'stat.ML']","  Sparse recovery is one of the most fundamental and well-studied inverse
problems. Standard statistical formulations of the problem are provably solved
by general convex programming techniques and more practical, fast
(nearly-linear time) iterative methods. However, these latter ""fast algorithms""
have previously been observed to be brittle in various real-world settings.
  We investigate the brittleness of fast sparse recovery algorithms to
generative model changes through the lens of studying their robustness to a
""helpful"" semi-random adversary, a framework which tests whether an algorithm
overfits to input assumptions. We consider the following basic model: let
$\mathbf{A} \in \mathbb{R}^{n \times d}$ be a measurement matrix which contains
an unknown subset of rows $\mathbf{G} \in \mathbb{R}^{m \times d}$ which are
bounded and satisfy the restricted isometry property (RIP), but is otherwise
arbitrary. Letting $x^\star \in \mathbb{R}^d$ be $s$-sparse, and given either
exact measurements $b = \mathbf{A} x^\star$ or noisy measurements $b =
\mathbf{A} x^\star + \xi$, we design algorithms recovering $x^\star$
information-theoretically optimally in nearly-linear time. We extend our
algorithm to hold for weaker generative models relaxing our planted RIP
assumption to a natural weighted variant, and show that our method's guarantees
naturally interpolate the quality of the measurement matrix to, in some
parameter regimes, run in sublinear time.
  Our approach differs from prior fast iterative methods with provable
guarantees under semi-random generative models: natural conditions on a
submatrix which make sparse recovery tractable are NP-hard to verify. We design
a new iterative method tailored to the geometry of sparse recovery which is
provably robust to our semi-random model. We hope our approach opens the door
to new robust, efficient algorithms for natural statistical inverse problems.
","[{'version': 'v1', 'created': 'Tue, 8 Mar 2022 10:56:46 GMT'}]",2022-03-09,"['Data Structures and Algorithms', 'Machine Learning']","This paper presents a novel semi-random sparse recovery algorithm that is able to recover a sparse vector in nearly-linear time. The proposed algorithm is based on a combination of a semi-random sketching technique and a greedy pursuit algorithm. The sketching technique is used to reduce the dimensionality of the problem, while the greedy pursuit algorithm is used to recover the sparse vector from the sketch. The proposed algorithm is shown to have a nearly-linear time complexity and is evaluated empirically on several datasets. The results show that the proposed algorithm is significantly faster than existing sparse recovery algorithms, while still achieving high recovery accuracy.","Write an abstract for a paper called Semi-Random Sparse Recovery in Nearly-Linear Time about Data Structures and Algorithms, Machine Learning"
2110.12899,"Raphael Gontijo-Lopes, Yann Dauphin, Ekin D. Cubuk","No One Representation to Rule Them All: Overlapping Features of Training
  Methods",['cs.LG'],"  Despite being able to capture a range of features of the data, high accuracy
models trained with supervision tend to make similar predictions. This
seemingly implies that high-performing models share similar biases regardless
of training methodology, which would limit ensembling benefits and render
low-accuracy models as having little practical use. Against this backdrop,
recent work has developed quite different training techniques, such as
large-scale contrastive learning, yielding competitively high accuracy on
generalization and robustness benchmarks. This motivates us to revisit the
assumption that models necessarily learn similar functions. We conduct a
large-scale empirical study of models across hyper-parameters, architectures,
frameworks, and datasets. We find that model pairs that diverge more in
training methodology display categorically different generalization behavior,
producing increasingly uncorrelated errors. We show these models specialize in
subdomains of the data, leading to higher ensemble performance: with just 2
models (each with ImageNet accuracy ~76.5%), we can create ensembles with 83.4%
(+7% boost). Surprisingly, we find that even significantly low-accuracy models
can be used to improve high-accuracy models. Finally, we show diverging
training methodology yield representations that capture overlapping (but not
supersetting) feature sets which, when combined, lead to increased downstream
performance.
","[{'version': 'v1', 'created': 'Wed, 20 Oct 2021 21:29:49 GMT'}, {'version': 'v2', 'created': 'Tue, 26 Oct 2021 17:29:51 GMT'}, {'version': 'v3', 'created': 'Mon, 25 Apr 2022 20:03:47 GMT'}]",2022-04-27,['Machine Learning'],"This paper examines the various machine learning training methods and their overlapping features. It focuses on the implications of using multiple training methods, rather than relying on one single representation, to improve machine learning accuracy. The paper looks at the various advantages and disadvantages of each training method, and how they can be used together to create a more comprehensive and accurate model. In addition, the paper looks at the challenges associated with combining different training methods and the potential solutions to these challenges. The paper ultimately concludes that multiple training methods should be used to create more accurate machine learning models, and that the use of multiple representations should be embraced rather than feared.","Write an abstract for a paper called No One Representation to Rule Them All: Overlapping Features of Training
  Methods about Machine Learning"
2301.12477,"Vaibhav Bihani, Sahil Manchanda, Srikanth Sastry, Sayan Ranu, N.M.
  Anoop Krishnan","StriderNET: A Graph Reinforcement Learning Approach to Optimize Atomic
  Structures on Rough Energy Landscapes",['cs.LG'],"  Optimization of atomic structures presents a challenging problem, due to
their highly rough and non-convex energy landscape, with wide applications in
the fields of drug design, materials discovery, and mechanics. Here, we present
a graph reinforcement learning approach, StriderNET, that learns a policy to
displace the atoms towards low energy configurations. We evaluate the
performance of StriderNET on three complex atomic systems, namely, binary
Lennard-Jones particles, calcium silicate hydrates gel, and disordered silicon.
We show that StriderNET outperforms all classical optimization algorithms and
enables the discovery of a lower energy minimum. In addition, StriderNET
exhibits a higher rate of reaching minima with energies, as confirmed by the
average over multiple realizations. Finally, we show that StriderNET exhibits
inductivity to unseen system sizes that are an order of magnitude different
from the training system.
","[{'version': 'v1', 'created': 'Sun, 29 Jan 2023 16:06:16 GMT'}]",2023-01-31,['Machine Learning'],"This paper presents StriderNET, a graph reinforcement learning approach for optimizing atomic structures on rough energy landscapes. StriderNET is a novel graph-based reinforcement learning framework that uses a graph convolutional network to capture the atomic structure of a system and a deep reinforcement learning algorithm to find the optimal structure. The proposed method is capable of searching through vast energy landscapes and finding the global minima. Experiments on a range of synthetic and real-world datasets demonstrate that StriderNET outperforms existing methods in terms of accuracy and computational efficiency. Furthermore, the results show that StriderNET is capable of discovering structures with lower energies than those found by traditional methods. This work provides a promising approach to the optimization of atomic structures and opens up new possibilities for the design of materials and molecules.","Write an abstract for a paper called StriderNET: A Graph Reinforcement Learning Approach to Optimize Atomic
  Structures on Rough Energy Landscapes about Machine Learning"
2303.13019,"Jinnan Piao, Dong Li, Jindi Liu, Xueting Yu, Zhibo Li, Ming Yang, and
  Peng Zeng","Construction Methods Based Minimum Weight Distribution for Polar Codes
  with Successive Cancellation List Decoding","['cs.IT', 'math.IT']","  In this paper, we focus on the construction methods based MWD for polar codes
to improve the performance with successive cancellation list (SCL) decoding. We
first propose an ordered and nested reliability sequence, namely MWD sequence,
to improve the ML performance of polar codes and apply fast construction
without the original channel information. In the MWD sequence, the synthetic
channels are sorted by the partial MWD which is used to evaluate the influence
of information bit on MWD and we prove the MWD sequence is the optimum sequence
under ML decoding. Then, since the list size of SCL decoding is limited, we
introduce an entropy constraint to establish a relationship between the list
size and the ML performance and propose a heuristic and greedy construction
method named bit grouping reorder based MWD (BGR-MWD) algorithm. In the
algorithm, we divide the synthetic channels into groups by the partial MWD and
greedily reorder the synthetic channels in some groups until the entropy
constraint is satisfied. The simulation results show the MWD sequence is
suitable for constructing polar codes with short code length. Meanwhile, the
BGR-MWD algorithm has superior performance over the traditional construction
methods for long code length.
","[{'version': 'v1', 'created': 'Thu, 23 Mar 2023 03:48:58 GMT'}]",2023-03-24,['Information Theory'],"This paper presents a new construction method based on minimum weight distribution for polar codes with successive cancellation list decoding. This method is used to improve the performance of polar codes while maintaining their low complexity. The proposed method is based on the concept of minimum weight distribution and is used to construct polar codes with a given code rate and block length. The proposed method is compared with the existing construction methods for polar codes and the results show that the proposed method has better performance in terms of error correction capability and decoding complexity. Furthermore, the proposed method is applicable to other decoding algorithms such as successive cancellation stack decoding. The results show that the proposed method is a suitable choice for constructing polar codes with low complexity and good performance.","Write an abstract for a paper called Construction Methods Based Minimum Weight Distribution for Polar Codes
  with Successive Cancellation List Decoding about Information Theory"
2210.11528,"John X. Morris, Justin T. Chiu, Ramin Zabih, Alexander M. Rush",Unsupervised Text Deidentification,['cs.CL'],"  Deidentification seeks to anonymize textual data prior to distribution.
Automatic deidentification primarily uses supervised named entity recognition
from human-labeled data points. We propose an unsupervised deidentification
method that masks words that leak personally-identifying information. The
approach utilizes a specially trained reidentification model to identify
individuals from redacted personal documents. Motivated by K-anonymity based
privacy, we generate redactions that ensure a minimum reidentification rank for
the correct profile of the document. To evaluate this approach, we consider the
task of deidentifying Wikipedia Biographies, and evaluate using an adversarial
reidentification metric. Compared to a set of unsupervised baselines, our
approach deidentifies documents more completely while removing fewer words.
Qualitatively, we see that the approach eliminates many identifying aspects
that would fall outside of the common named entity based approach.
","[{'version': 'v1', 'created': 'Thu, 20 Oct 2022 18:54:39 GMT'}]",2022-10-24,['Computation and Language'],"This paper presents a novel unsupervised text deidentification approach that combines computation and language processing techniques to identify and remove personal information from text. We propose a two-step framework that first detects and classifies the personal information in the text, and then applies a set of rules to remove the identified personal information. Our approach is evaluated on a collection of medical records, demonstrating its effectiveness in detecting and removing personal information from text. Additionally, our approach is shown to be more accurate and efficient than existing methods. The results suggest that our approach is a promising solution for unsupervised text deidentification.",Write an abstract for a paper called Unsupervised Text Deidentification about Computation and Language
2203.02853,"Hongyu Yu, Yang Zhong, Changsong Xu, Xingao Gong, Hongjun Xiang",Graph Neural Network Potential for Magnetic Materials,"['physics.comp-ph', 'cond-mat.dis-nn', 'cs.LG']","  Machine Learning (ML) interatomic potential has shown its great power in
condensed matter physics. However, ML interatomic potential for a magnetic
system including both structural degrees of freedom and magnetic moments has
not been well developed yet. A spin-dependent ML interatomic potential approach
based on the crystal graph neural network (GNN) has been developed for any
magnetic system. It consists of the Heisenberg edge graph neural network
(HEGNN) and spin-distance edge graph neural network (SEGNN). The network
captures the Heisenberg coefficient variation between different structures and
the fine spin-lattice coupling of high order and multi-body interaction with
high accuracy. In the tests, this method perfectly fitted a high-order spin
Hamiltonian and two complex spin-lattice Hamiltonian and captured the fine
spin-lattice coupling in BiFeO3. In addition, a disturbed structure of BiFeO3
with strain was successfully optimized with the trained potential. Our work has
expanded the powerful ML GNN potentials to magnetic systems, which paves a new
way for large-scale dynamic simulations on spin-lattice coupled systems.
","[{'version': 'v1', 'created': 'Sun, 6 Mar 2022 01:54:50 GMT'}]",2022-03-08,['Machine Learning'],"This paper explores the potential of graph neural networks (GNNs) for predicting magnetic properties of materials. GNNs are a type of machine learning algorithm that can be used to analyze data represented as graphs, making them well-suited for the analysis of magnetic materials. We will discuss the advantages of GNNs over traditional machine learning techniques, such as their ability to capture the complex relationships between atoms in magnetic materials. We will also present results from a GNN model trained on a dataset of magnetic materials and demonstrate its ability to accurately predict magnetic properties. Finally, we will discuss the implications of GNNs for the future of materials science and the development of new magnetic materials.",Write an abstract for a paper called Graph Neural Network Potential for Magnetic Materials about Machine Learning
2106.0597,"Wanrong Zhu, Xin Eric Wang, An Yan, Miguel Eckstein, William Yang Wang","ImaginE: An Imagination-Based Automatic Evaluation Metric for Natural
  Language Generation","['cs.CL', 'cs.AI', 'cs.CV']","  Automatic evaluations for natural language generation (NLG) conventionally
rely on token-level or embedding-level comparisons with text references. This
differs from human language processing, for which visual imagination often
improves comprehension. In this work, we propose ImaginE, an imagination-based
automatic evaluation metric for natural language generation. With the help of
StableDiffusion, a state-of-the-art text-to-image generator, we automatically
generate an image as the embodied imagination for the text snippet and compute
the imagination similarity using contextual embeddings. Experiments spanning
several text generation tasks demonstrate that adding machine-generated images
with our ImaginE displays great potential in introducing multi-modal
information into NLG evaluation, and improves existing automatic metrics'
correlations with human similarity judgments in both reference-based and
reference-free evaluation scenarios.
","[{'version': 'v1', 'created': 'Thu, 10 Jun 2021 17:59:52 GMT'}, {'version': 'v2', 'created': 'Sat, 4 Feb 2023 09:27:27 GMT'}, {'version': 'v3', 'created': 'Wed, 15 Feb 2023 03:12:15 GMT'}]",2023-02-16,"['Computation and Language', 'Artificial Intelligence', 'Computer Vision and Pattern Recognition']","This paper presents ImaginE, an imagination-based automatic evaluation metric for natural language generation (NLG) tasks, such as computational and language tasks, artificial intelligence, computer vision, and pattern recognition. ImaginE is a novel approach that uses a combination of imagination-based features and machine learning techniques to automatically evaluate NLG systems. The proposed metric is evaluated on a variety of datasets, including the Stanford Natural Language Inference (SNLI) dataset, the Microsoft Research Paraphrase Corpus (MRPC), and the Microsoft Research Machine Translation (MRMT) dataset, and results demonstrate its effectiveness in predicting the quality of generated sentences. Additionally, ImaginE is compared to existing metrics and shows superior performance in terms of accuracy and speed. Finally, the paper provides insights into the potential of using imagination-based features for NLG evaluation.","Write an abstract for a paper called ImaginE: An Imagination-Based Automatic Evaluation Metric for Natural
  Language Generation about Computation and Language, Artificial Intelligence, Computer Vision and Pattern Recognition"
2108.0622,"Qi Cao, Huawei Shen, Yuanhao Liu, Jinhua Gao, Xueqi Cheng","PREP: Pre-training with Temporal Elapse Inference for Popularity
  Prediction",['cs.SI'],"  Predicting the popularity of online content is a fundamental problem in
various applications. One practical challenge takes roots in the varying length
of observation time or prediction horizon, i.e., a good model for popularity
prediction is desired to handle various prediction settings. However, most
existing methods adopt a separate training paradigm for each prediction setting
and the obtained model for one setting is difficult to be generalized to
others, causing a great waste of computational resources and a large demand for
downstream labels. To solve the above issues, we propose a novel pre-training
framework for popularity prediction, namely PREP, aiming to pre-train a general
representation model from the readily available unlabeled diffusion data, which
can be effectively transferred into various prediction settings. We design a
novel pretext task for pre-training, i.e., temporal elapse inference for two
randomly sampled time slices of popularity dynamics, impelling the
representation model to learn intrinsic knowledge about popularity dynamics.
Experimental results conducted on two real datasets demonstrate the
generalization and efficiency of the pre-training framework for different
popularity prediction task settings.
","[{'version': 'v1', 'created': 'Fri, 13 Aug 2021 13:02:52 GMT'}, {'version': 'v2', 'created': 'Sat, 12 Mar 2022 09:50:21 GMT'}]",2022-03-15,['Social and Information Networks'],"This paper presents PREP, a novel pre-training method for predicting the popularity of nodes in social and information networks. PREP uses temporal elapse inference to capture the temporal dynamics of the network, which is beneficial for predicting future popularity. Experiments on two real-world datasets demonstrate that PREP outperforms existing methods in terms of both accuracy and efficiency. In addition, the results show that PREP is robust and can capture complex temporal patterns. The proposed method could be a promising tool for predicting the future popularity of nodes in social and information networks.","Write an abstract for a paper called PREP: Pre-training with Temporal Elapse Inference for Popularity
  Prediction about Social and Information Networks"
2302.07973,Yuan Feng and Yingte Xu,Verification of Nondeterministic Quantum Programs,"['cs.LO', 'quant-ph']","  Nondeterministic choice is a useful program construct that provides a way to
describe the behaviour of a program without specifying the details of possible
implementations. It supports the stepwise refinement of programs, a method that
has proven useful in software development. Nondeterminism has also been
introduced in quantum programming, and the termination of nondeterministic
quantum programs has been extensively analysed. In this paper, we go beyond
termination analysis to investigate the verification of nondeterministic
quantum programs where properties are given by sets of hermitian operators on
the associated Hilbert space. Hoare-type logic systems for partial and total
correctness are proposed, which turn out to be both sound and relatively
complete with respect to their corresponding semantic correctness. To show the
utility of these proof systems, we analyse some quantum algorithms, such as
quantum error correction scheme, the Deutsch algorithm, and a nondeterministic
quantum walk. Finally, a proof assistant prototype is implemented to aid in the
automated reasoning of nondeterministic quantum programs.
","[{'version': 'v1', 'created': 'Wed, 15 Feb 2023 22:37:23 GMT'}]",2023-02-17,['Logic in Computer Science'],"This paper presents a verification algorithm for nondeterministic quantum programs (NQPs) that provides a logical guarantee of correctness. The algorithm is based on a combination of techniques from classical verification, symbolic execution, and quantum computing. It is shown that the algorithm is sound and complete, and it is demonstrated through several examples. The paper also discusses potential applications of the algorithm in computer science, such as verifying quantum algorithms, verifying quantum protocols, and verifying quantum code. Finally, the paper presents a discussion of the implications of the algorithm for the future of computer science.",Write an abstract for a paper called Verification of Nondeterministic Quantum Programs about Logic in Computer Science
2201.02223,"Adrien Meynard, Gayan Seneviratna, Elliot Doyle, Joyanne Becker,
  Hau-Tieng Wu, Jana Schaich Borg","Predicting Trust Using Automated Assessment of Multivariate
  Interactional Synchrony","['cs.HC', 'eess.IV', 'eess.SP']","  Diverse disciplines are interested in how the coordination of interacting
agents' movements, emotions, and physiology over time impacts social behavior.
Here, we describe a new multivariate procedure for automating the investigation
of this kind of behaviorally-relevant ""interactional synchrony"", and introduce
a novel interactional synchrony measure based on features of dynamic time
warping (DTW) paths. We demonstrate that our DTW path-based measure of
interactional synchrony between facial action units of two people interacting
freely in a natural social interaction can be used to predict how much trust
they will display in a subsequent Trust Game. We also show that our approach
outperforms univariate head movement models, models that consider participants'
facial action units independently, and models that use previously proposed
synchrony or similarity measures. The insights of this work can be applied to
any research question that aims to quantify the temporal coordination of
multiple signals over time, but has immediate applications in psychology,
medicine, and robotics.
","[{'version': 'v1', 'created': 'Thu, 6 Jan 2022 19:43:57 GMT'}]",2022-01-14,['Human-Computer Interaction'],"This paper presents a novel approach to predicting trust in Human-Computer Interaction (HCI) using automated assessment of multivariate interactional synchrony. The proposed method uses a combination of machine learning techniques and statistical analysis to analyze the multivariate interactional synchrony of a user and a computer system during HCI. The method is evaluated on a large dataset of user-computer interactions, and the results show that the proposed approach is able to accurately predict trust in HCI with a high degree of accuracy. The paper concludes with a discussion of the implications of the results and potential applications of the proposed approach.","Write an abstract for a paper called Predicting Trust Using Automated Assessment of Multivariate
  Interactional Synchrony about Human-Computer Interaction"
2212.07429,"Diego Alves, Gaurish Thakkar, Gabriel Amaral, Tin Kuculo, Marko
  Tadi\'c","Building Multilingual Corpora for a Complex Named Entity Recognition and
  Classification Hierarchy using Wikipedia and DBpedia",['cs.CL'],"  With the ever-growing popularity of the field of NLP, the demand for datasets
in low resourced-languages follows suit. Following a previously established
framework, in this paper, we present the UNER dataset, a multilingual and
hierarchical parallel corpus annotated for named-entities. We describe in
detail the developed procedure necessary to create this type of dataset in any
language available on Wikipedia with DBpedia information. The three-step
procedure extracts entities from Wikipedia articles, links them to DBpedia, and
maps the DBpedia sets of classes to the UNER labels. This is followed by a
post-processing procedure that significantly increases the number of identified
entities in the final results. The paper concludes with a statistical and
qualitative analysis of the resulting dataset.
","[{'version': 'v1', 'created': 'Wed, 14 Dec 2022 11:38:48 GMT'}]",2022-12-16,['Computation and Language'],"This paper presents a method for constructing multilingual corpora for complex Named Entity Recognition and Classification Hierarchy (NERCH) using Wikipedia and DBpedia about Computation and Language. The proposed method involves extracting relevant Wikipedia articles, transforming them into RDF triples, and using them to create a multilingual corpus. The paper then presents a method for constructing a NERCH using the generated corpus, which is based on a combination of supervised and unsupervised learning techniques. The results of the proposed method are evaluated on a benchmark dataset and compared with existing methods. Finally, the paper discusses the implications of the proposed method for future research in the field of Computation and Language.","Write an abstract for a paper called Building Multilingual Corpora for a Complex Named Entity Recognition and
  Classification Hierarchy using Wikipedia and DBpedia about Computation and Language"
2101.11948,"S. Van Cranenburgh, S. Wang, A. Vij, F. Pereira, J. Walker",Choice modelling in the age of machine learning -- discussion paper,"['econ.EM', 'cs.LG']","  Since its inception, the choice modelling field has been dominated by
theory-driven modelling approaches. Machine learning offers an alternative
data-driven approach for modelling choice behaviour and is increasingly drawing
interest in our field. Cross-pollination of machine learning models, techniques
and practices could help overcome problems and limitations encountered in the
current theory-driven modelling paradigm, such as subjective labour-intensive
search processes for model selection, and the inability to work with text and
image data. However, despite the potential benefits of using the advances of
machine learning to improve choice modelling practices, the choice modelling
field has been hesitant to embrace machine learning. This discussion paper aims
to consolidate knowledge on the use of machine learning models, techniques and
practices for choice modelling, and discuss their potential. Thereby, we hope
not only to make the case that further integration of machine learning in
choice modelling is beneficial, but also to further facilitate it. To this end,
we clarify the similarities and differences between the two modelling
paradigms; we review the use of machine learning for choice modelling; and we
explore areas of opportunities for embracing machine learning models and
techniques to improve our practices. To conclude this discussion paper, we put
forward a set of research questions which must be addressed to better
understand if and how machine learning can benefit choice modelling.
","[{'version': 'v1', 'created': 'Thu, 28 Jan 2021 11:57:08 GMT'}, {'version': 'v2', 'created': 'Wed, 24 Nov 2021 10:54:47 GMT'}]",2022-02-17,['Machine Learning'],"This paper examines the role of choice modelling in the era of machine learning. Choice modelling is a process of understanding how people make decisions, and it is used to inform decision-making in many areas such as marketing, economics, and public policy. With the rise of machine learning, there are questions about how choice modelling can be adapted to take advantage of the new technology. This paper discusses the potential of choice modelling in the context of machine learning, and the challenges it faces in the current environment. The paper also considers the implications of machine learning for choice modelling, and how it may be used to improve decision-making. Finally, the paper looks at the future of choice modelling in the age of machine learning, and the potential opportunities and risks that may arise.",Write an abstract for a paper called Choice modelling in the age of machine learning -- discussion paper about Machine Learning
2206.01863,"Jian-Qing Zheng, Ziyang Wang, Baoru Huang, Ngee Han Lim, Tonia
  Vincent, Bartlomiej W. Papiez",Recursive Deformable Image Registration Network with Mutual Attention,['cs.CV'],"  Deformable image registration, estimating the spatial transformation between
different images, is an important task in medical imaging. Many previous
studies have used learning-based methods for multi-stage registration to
perform 3D image registration to improve performance. The performance of the
multi-stage approach, however, is limited by the size of the receptive field
where complex motion does not occur at a single spatial scale. We propose a new
registration network combining recursive network architecture and mutual
attention mechanism to overcome these limitations. Compared with the
state-of-the-art deep learning methods, our network based on the recursive
structure achieves the highest accuracy in lung Computed Tomography (CT) data
set (Dice score of 92\% and average surface distance of 3.8mm for lungs) and
one of the most accurate results in abdominal CT data set with 9 organs of
various sizes (Dice score of 55\% and average surface distance of 7.8mm). We
also showed that adding 3 recursive networks is sufficient to achieve the
state-of-the-art results without a significant increase in the inference time.
","[{'version': 'v1', 'created': 'Sat, 4 Jun 2022 00:35:14 GMT'}, {'version': 'v2', 'created': 'Thu, 30 Jun 2022 11:56:27 GMT'}]",2022-07-01,['Computer Vision and Pattern Recognition'],"This paper presents a novel Recursive Deformable Image Registration Network with Mutual Attention (RDIRN-MA) for Computer Vision and Pattern Recognition. The proposed network utilizes a mutual attention mechanism to capture the correspondence between two images and a recursive deformable registration module to generate a deformation field for the registration. The proposed network is evaluated on a wide range of datasets and compared with state-of-the-art methods. The results demonstrate that the proposed network outperforms existing methods in terms of accuracy and efficiency, and can be used for various applications such as medical image registration, image alignment, and image segmentation.",Write an abstract for a paper called Recursive Deformable Image Registration Network with Mutual Attention about Computer Vision and Pattern Recognition
2203.12912,"MohammadTaghi HajiAghayi, Dariusz R. Kowalski, Jan Olkowski",Improved Communication Complexity of Fault-Tolerant Consensus,['cs.DC'],"  Consensus is one of the most thoroughly studied problems in distributed
computing, yet there are still complexity gaps that have not been bridged for
decades. In particular, in the classical message-passing setting with
processes' crashes, since the seminal works of Bar-Joseph and Ben-Or [1998]
\cite{Bar-JosephB98} and Aspnes and Waarts [1996, 1998]
\cite{AspnesW-SICOMP-96,Aspnes-JACM-98} in the previous century, there is still
a fundamental unresolved question about communication complexity of fast
randomized Consensus against a (strong) adaptive adversary crashing processes
arbitrarily online. The best known upper bound on the number of communication
bits is $\Theta(\frac{n^{3/2}}{\sqrt{\log{n}}})$ per process, while the best
lower bound is $\Omega(1)$. This is in contrast to randomized Consensus against
a (weak) oblivious adversary, for which time-almost-optimal algorithms
guarantee amortized $O(1)$ communication bits per process \cite{GK-SODA-10}. We
design an algorithm against adaptive adversary that reduces the communication
gap by nearly linear factor to $O(\sqrt{n}\cdot\text{polylog } n)$ bits per
process, while keeping almost-optimal (up to factor $O(\log^3 n)$) time
complexity $O(\sqrt{n}\cdot\log^{5/2} n)$.
  More surprisingly, we show this complexity indeed can be lowered further, but
at the expense of increasing time complexity, i.e., there is a {\em trade-off}
between communication complexity and time complexity. More specifically, our
main Consensus algorithm allows to reduce communication complexity per process
to any value from $\text{polylog } n$ to $O(\sqrt{n}\cdot\text{polylog } n)$,
as long as Time $\times$ Communication $= O(n\cdot \text{polylog } n)$.
Similarly, reducing time complexity requires more random bits per process,
i.e., Time $\times$ Randomness $=O(n\cdot \text{polylog } n)$.
","[{'version': 'v1', 'created': 'Thu, 24 Mar 2022 08:03:02 GMT'}]",2022-03-25,"['Distributed, Parallel, and Cluster Computing']","This paper presents a novel approach to improving the communication complexity of fault-tolerant consensus protocols in distributed, parallel, and cluster computing systems. We propose a new consensus protocol that combines the best features of existing fault-tolerance protocols, while reducing the communication complexity. Our approach is based on a distributed voting system that allows for asynchronous communication and efficient message passing. We evaluate our protocol against existing protocols and show that it provides improved communication complexity. We also discuss the implications of our approach for distributed, parallel, and cluster computing systems.","Write an abstract for a paper called Improved Communication Complexity of Fault-Tolerant Consensus about Distributed, Parallel, and Cluster Computing"
2207.08821,"Jacob Renn, Ian Sotnek, Benjamin Harvey, Brian Caffo","The Multiple Subnetwork Hypothesis: Enabling Multidomain Learning by
  Isolating Task-Specific Subnetworks in Feedforward Neural Networks","['cs.LG', 'cs.AI']","  Neural networks have seen an explosion of usage and research in the past
decade, particularly within the domains of computer vision and natural language
processing. However, only recently have advancements in neural networks yielded
performance improvements beyond narrow applications and translated to expanded
multitask models capable of generalizing across multiple data types and
modalities. Simultaneously, it has been shown that neural networks are
overparameterized to a high degree, and pruning techniques have proved capable
of significantly reducing the number of active weights within the network while
largely preserving performance. In this work, we identify a methodology and
network representational structure which allows a pruned network to employ
previously unused weights to learn subsequent tasks. We employ these
methodologies on well-known benchmarking datasets for testing purposes and show
that networks trained using our approaches are able to learn multiple tasks,
which may be related or unrelated, in parallel or in sequence without
sacrificing performance on any task or exhibiting catastrophic forgetting.
","[{'version': 'v1', 'created': 'Mon, 18 Jul 2022 15:07:13 GMT'}]",2022-07-20,"['Machine Learning', 'Artificial Intelligence']","This paper presents the Multiple Subnetwork Hypothesis, a novel hypothesis for enabling multidomain learning with feedforward neural networks. The hypothesis proposes that task-specific subnetworks can be isolated within a single feedforward neural network, allowing for the learning of multiple tasks simultaneously. This hypothesis is based on the idea that a single feedforward neural network can be partitioned into multiple subnetworks, each of which is specialized for a specific task. We demonstrate the effectiveness of this hypothesis by training a single feedforward neural network on multiple tasks, and show that it outperforms a single network trained on a single task. We also discuss the implications of the Multiple Subnetwork Hypothesis for the development of artificial intelligence and machine learning.","Write an abstract for a paper called The Multiple Subnetwork Hypothesis: Enabling Multidomain Learning by
  Isolating Task-Specific Subnetworks in Feedforward Neural Networks about Machine Learning, Artificial Intelligence"
2210.06718,"Yuda Song, Yifei Zhou, Ayush Sekhari, J. Andrew Bagnell, Akshay
  Krishnamurthy, Wen Sun",Hybrid RL: Using Both Offline and Online Data Can Make RL Efficient,['cs.LG'],"  We consider a hybrid reinforcement learning setting (Hybrid RL), in which an
agent has access to an offline dataset and the ability to collect experience
via real-world online interaction. The framework mitigates the challenges that
arise in both pure offline and online RL settings, allowing for the design of
simple and highly effective algorithms, in both theory and practice. We
demonstrate these advantages by adapting the classical Q learning/iteration
algorithm to the hybrid setting, which we call Hybrid Q-Learning or Hy-Q. In
our theoretical results, we prove that the algorithm is both computationally
and statistically efficient whenever the offline dataset supports a
high-quality policy and the environment has bounded bilinear rank. Notably, we
require no assumptions on the coverage provided by the initial distribution, in
contrast with guarantees for policy gradient/iteration methods. In our
experimental results, we show that Hy-Q with neural network function
approximation outperforms state-of-the-art online, offline, and hybrid RL
baselines on challenging benchmarks, including Montezuma's Revenge.
","[{'version': 'v1', 'created': 'Thu, 13 Oct 2022 04:19:05 GMT'}, {'version': 'v2', 'created': 'Tue, 7 Mar 2023 16:12:58 GMT'}, {'version': 'v3', 'created': 'Sat, 11 Mar 2023 11:47:54 GMT'}]",2023-03-14,['Machine Learning'],"This paper presents a novel hybrid reinforcement learning (RL) approach that combines offline and online data to make RL more efficient. We propose a method that uses offline data to pre-train a model and then uses online data to fine-tune the model. We evaluate our approach on a simulated robotic arm task and compare it to a standard RL approach. Our results show that the hybrid approach is more efficient and achieves better task performance than the standard RL approach. Additionally, we demonstrate how our hybrid approach can be applied to real-world robotics tasks. Our results suggest that hybrid RL can be a powerful tool for tackling complex reinforcement learning tasks.",Write an abstract for a paper called Hybrid RL: Using Both Offline and Online Data Can Make RL Efficient about Machine Learning
2204.0626,"Chen Chen, Yuchen Hu, Nana Hou, Xiaofeng Qi, Heqing Zou, Eng Siong
  Chng",Self-critical Sequence Training for Automatic Speech Recognition,"['cs.CL', 'cs.SD', 'eess.AS']","  Although automatic speech recognition (ASR) task has gained remarkable
success by sequence-to-sequence models, there are two main mismatches between
its training and testing that might lead to performance degradation: 1) The
typically used cross-entropy criterion aims to maximize log-likelihood of the
training data, while the performance is evaluated by word error rate (WER), not
log-likelihood; 2) The teacher-forcing method leads to the dependence on ground
truth during training, which means that model has never been exposed to its own
prediction before testing. In this paper, we propose an optimization method
called self-critical sequence training (SCST) to make the training procedure
much closer to the testing phase. As a reinforcement learning (RL) based
method, SCST utilizes a customized reward function to associate the training
criterion and WER. Furthermore, it removes the reliance on teacher-forcing and
harmonizes the model with respect to its inference procedure. We conducted
experiments on both clean and noisy speech datasets, and the results show that
the proposed SCST respectively achieves 8.7% and 7.8% relative improvements
over the baseline in terms of WER.
","[{'version': 'v1', 'created': 'Wed, 13 Apr 2022 09:13:32 GMT'}]",2022-04-14,"['Computation and Language', 'Sound']","This paper presents an approach to Automatic Speech Recognition (ASR) called Self-Critical Sequence Training (SCST). SCST is a reinforcement learning algorithm that uses a sequence-level reward signal to optimize the parameters of an ASR system. The paper explores the application of SCST to a variety of ASR tasks and shows that it can improve recognition accuracy. It also discusses the implications of SCST for the field of Computation and Language, Sound. The paper concludes that SCST is a promising approach to ASR and has the potential to revolutionize the way ASR systems are developed and deployed.","Write an abstract for a paper called Self-critical Sequence Training for Automatic Speech Recognition about Computation and Language, Sound"
2205.12165,"Elijah Pelofske, Georg Hahn, Hristo N. Djidjev",Solving Larger Maximum Clique Problems Using Parallel Quantum Annealing,"['quant-ph', 'cs.ET']","  Quantum annealing has the potential to find low energy solutions of NP-hard
problems that can be expressed as quadratic unconstrained binary optimization
problems. However, the hardware of the quantum annealer manufactured by D-Wave
Systems, which we consider in this work, is sparsely connected and moderately
sized (on the order of thousands of qubits), thus necessitating a
minor-embedding of a logical problem onto the physical qubit hardware. The
combination of relatively small hardware sizes and the necessity of a
minor-embedding can mean that solving large optimization problems is not
possible on current quantum annealers. In this research, we show that a hybrid
approach combining parallel quantum annealing with graph decomposition allows
one to solve larger optimization problem accurately. We apply the approach on
the Maximum Clique problem on graphs with up to 120 nodes and 6395 edges.
","[{'version': 'v1', 'created': 'Tue, 24 May 2022 15:56:15 GMT'}, {'version': 'v2', 'created': 'Tue, 14 Mar 2023 23:09:37 GMT'}]",2023-03-16,['Emerging Technologies'],"This paper presents a novel approach to solving larger maximum clique problems using parallel quantum annealing, an emerging technology. We discuss the advantages of using this approach, including improved speed and scalability, as well as the challenges associated with implementing it. We then present a parallel quantum annealing algorithm, which is based on a combination of a modified Tabu search and a quantum annealing heuristic. This algorithm is tested on a set of benchmark instances, and the results show that the parallel quantum annealing approach outperforms the traditional Tabu search in terms of both speed and scalability. Finally, we discuss the implications of this work and suggest future research directions.",Write an abstract for a paper called Solving Larger Maximum Clique Problems Using Parallel Quantum Annealing about Emerging Technologies
1605.07959,"Tom\'a\v{s} Masa\v{r}\'ik, Tom\'a\v{s} Toufar",Parameterized complexity of fair deletion problems,"['cs.DS', 'cs.CC']","  Deletion problems are those where given a graph $G$ and a graph property
$\pi$, the goal is to find a subset of edges such that after its removal the
graph $G$ will satisfy the property $\pi$. Typically, we want to minimize the
number of elements removed. In fair deletion problems we change the objective:
we minimize the maximum number of deletions in a neighborhood of a single
vertex.
  We study the parameterized complexity of fair deletion problems with respect
to the structural parameters of the tree-width, the path-width, the size of a
minimum feedback vertex set, the neighborhood diversity, and the size of
minimum vertex cover of graph $G$. We prove the W[1]-hardness of the fair FO
vertex-deletion problem with respect to the first three parameters combined.
Moreover, we show that there is no algorithm for fair FO vertex-deletion
problem running in time $n^{o(k^{1/3})}$, where $n$ is the size of the graph
and $k$ is the sum of the first three mentioned parameters, provided that the
Exponential Time Hypothesis holds.
  On the other hand, we provide an FPT algorithm for the fair MSO edge-deletion
problem parameterized by the size of minimum vertex cover and an FPT algorithm
for the fair MSO vertex-deletion problem parameterized by the neighborhood
diversity
","[{'version': 'v1', 'created': 'Wed, 25 May 2016 16:34:29 GMT'}, {'version': 'v2', 'created': 'Wed, 4 Jan 2017 15:43:56 GMT'}]",2022-03-17,"['Data Structures and Algorithms', 'Computational Complexity']","This paper examines the parameterized complexity of fair deletion problems in the context of data structures and algorithms. Fair deletion problems are a type of problem in which a set of elements must be deleted from a data structure while ensuring that all elements have an equal chance of being deleted. We analyze the parameterized complexity of these problems, which involves measuring the complexity of the problem in terms of the size of the input and the parameter related to the problem. We use a variety of techniques, including dynamic programming, to analyze the complexity of these problems. Our results demonstrate that a number of fair deletion problems can be solved in polynomial time with respect to the size of the input and the parameter. We also discuss the implications of our findings for computational complexity theory.","Write an abstract for a paper called Parameterized complexity of fair deletion problems about Data Structures and Algorithms, Computational Complexity"
2201.1325,"Quanlong Wang, Richie Yeung, Mark Koch","Differentiating and Integrating ZX Diagrams with Applications to Quantum
  Machine Learning","['quant-ph', 'cs.LG']","  ZX-calculus has proved to be a useful tool for quantum technology with a wide
range of successful applications. Most of these applications are of an
algebraic nature. However, other tasks that involve differentiation and
integration remain unreachable with current ZX techniques. Here we elevate ZX
to an analytical perspective by realising differentiation and integration
entirely within the framework of ZX-calculus. We explicitly illustrate the new
analytic framework of ZX-calculus by applying it in context of quantum machine
learning for the analysis of barren plateaus.
","[{'version': 'v1', 'created': 'Mon, 31 Jan 2022 13:59:28 GMT'}, {'version': 'v2', 'created': 'Mon, 28 Feb 2022 20:17:09 GMT'}, {'version': 'v3', 'created': 'Mon, 21 Nov 2022 17:56:01 GMT'}, {'version': 'v4', 'created': 'Thu, 24 Nov 2022 18:02:40 GMT'}]",2022-11-28,['Machine Learning'],"This paper explores the use of ZX diagrams and their applications to quantum machine learning. We will first discuss the basics of ZX diagrams and how to differentiate and integrate them. We will then apply these concepts to quantum machine learning, showing how ZX diagrams can be used to improve the accuracy and efficiency of machine learning algorithms. Finally, we will discuss the advantages and limitations of using ZX diagrams for quantum machine learning. The results of this paper will provide a better understanding of how ZX diagrams can be used to improve machine learning algorithms, and how they can be used to solve complex problems in quantum machine learning.","Write an abstract for a paper called Differentiating and Integrating ZX Diagrams with Applications to Quantum
  Machine Learning about Machine Learning"
2207.00933,"Wei Tang, Margaret Martonosi","ScaleQC: A Scalable Framework for Hybrid Computation on Quantum and
  Classical Processors","['cs.ET', 'quant-ph']","  Quantum processing unit (QPU) has to satisfy highly demanding quantity and
quality requirements on its qubits to produce accurate results for problems at
useful scales. Furthermore, classical simulations of quantum circuits generally
do not scale. Instead, quantum circuit cutting techniques cut and distribute a
large quantum circuit into multiple smaller subcircuits feasible for less
powerful QPUs. However, the classical post-processing incurred from the cutting
introduces runtime and memory bottlenecks. Our tool, called ScaleQC, addresses
the bottlenecks by developing novel algorithmic techniques including (1) a
quantum states merging framework that quickly locates the solution states of
large quantum circuits; (2) an automatic solver that cuts complex quantum
circuits to fit on less powerful QPUs; and (3) a tensor network based
post-processing that minimizes the classical overhead. Our experiments
demonstrate both QPU requirement advantages over the purely quantum platforms,
and runtime advantages over the purely classical platforms for benchmarks up to
1000 qubits.
","[{'version': 'v1', 'created': 'Sun, 3 Jul 2022 01:44:31 GMT'}]",2022-07-05,['Emerging Technologies'],"This paper presents ScaleQC, a scalable framework for hybrid computation on quantum and classical processors. The framework is designed to enable users to take advantage of the emerging technologies of quantum computing and classical computing. It provides an efficient, extensible, and user-friendly platform for developing, testing, and deploying hybrid quantum-classical algorithms. The framework is composed of a set of components that enable users to quickly develop, debug, and deploy hybrid algorithms. Furthermore, a set of tools and libraries are provided to facilitate the development of hybrid algorithms. The framework is evaluated using a set of benchmarks, demonstrating its scalability and performance. The results demonstrate that the ScaleQC framework is an effective platform for hybrid quantum-classical computation.","Write an abstract for a paper called ScaleQC: A Scalable Framework for Hybrid Computation on Quantum and
  Classical Processors about Emerging Technologies"
2106.05187,"Wang Yifan, Lukas Rahmann, Olga Sorkine-Hornung","Geometry-Consistent Neural Shape Representation with Implicit
  Displacement Fields","['cs.CV', 'cs.GR', 'cs.LG']","  We present implicit displacement fields, a novel representation for detailed
3D geometry. Inspired by a classic surface deformation technique, displacement
mapping, our method represents a complex surface as a smooth base surface plus
a displacement along the base's normal directions, resulting in a
frequency-based shape decomposition, where the high frequency signal is
constrained geometrically by the low frequency signal. Importantly, this
disentanglement is unsupervised thanks to a tailored architectural design that
has an innate frequency hierarchy by construction. We explore implicit
displacement field surface reconstruction and detail transfer and demonstrate
superior representational power, training stability and generalizability.
","[{'version': 'v1', 'created': 'Wed, 9 Jun 2021 16:26:18 GMT'}, {'version': 'v2', 'created': 'Fri, 18 Jun 2021 09:25:15 GMT'}, {'version': 'v3', 'created': 'Wed, 2 Feb 2022 06:30:24 GMT'}]",2022-02-03,"['Computer Vision and Pattern Recognition', 'Graphics', 'Machine Learning']","This paper presents a novel geometric representation for neural shapes using implicit displacement fields. We propose a learning-based approach that leverages the geometry of the shape to generate a compact representation that is both accurate and efficient. Our method is based on a deep learning architecture that takes as input a 3D mesh and learns a displacement field that is consistent with the underlying geometry. We evaluate our method on a variety of datasets, including the ShapeNet Core55 dataset. The results demonstrate that our approach is able to accurately capture the underlying geometry of the shape and generate a compact representation that is suitable for various computer vision, graphics, and machine learning tasks.","Write an abstract for a paper called Geometry-Consistent Neural Shape Representation with Implicit
  Displacement Fields about Computer Vision and Pattern Recognition, Graphics, Machine Learning"
2211.1165,"Zihan Ye, Hikaru Shindo, Devendra Singh Dhami, Kristian Kersting",Differentiable Meta logical Programming,['cs.AI'],"  Deep learning uses an increasing amount of computation and data to solve very
specific problems. By stark contrast, human minds solve a wide range of
problems using a fixed amount of computation and limited experience. One
ability that seems crucial to this kind of general intelligence is
meta-reasoning, i.e., our ability to reason about reasoning. To make deep
learning do more from less, we propose the differentiable logical meta
interpreter (DLMI). The key idea is to realize a meta-interpreter using
differentiable forward-chaining reasoning in first-order logic. This directly
allows DLMI to reason and even learn about its own operations. This is
different from performing object-level deep reasoning and learning, which
refers in some way to entities external to the system. In contrast, DLMI is
able to reflect or introspect, i.e., to shift from meta-reasoning to
object-level reasoning and vice versa. Among many other experimental
evaluations, we illustrate this behavior using the novel task of ""repairing
Kandinsky patterns,"" i.e., how to edit the objects in an image so that it
agrees with a given logical concept.
","[{'version': 'v1', 'created': 'Mon, 21 Nov 2022 17:12:06 GMT'}]",2022-11-22,['Artificial Intelligence'],"This paper investigates the potential of Differentiable Meta Logical Programming (DMLP) for Artificial Intelligence (AI) applications. DMLP is a new form of programming that uses meta-logic to enable the differentiation of programs. It is proposed that DMLP can be used to enable AI applications to be more efficient and more interpretable. The paper examines the advantages of DMLP for AI applications, such as its ability to enable the representation and learning of complex relationships, its ability to enable the representation of uncertainty and its potential to enable the development of interpretable models. The paper also discusses potential challenges associated with the use of DMLP for AI applications, such as the difficulty of program verification, the need for more efficient algorithms and the need for better integration with existing AI frameworks. Finally, the paper proposes potential directions for future research in this field.",Write an abstract for a paper called Differentiable Meta logical Programming about Artificial Intelligence
2303.03077,"Haoxin Liu, Yao Zhang, Dengji Zhao",Distributed Mechanism Design in Social Networks,['cs.GT'],"  Designing auctions to incentivize buyers to invite new buyers via their
social connections is a new trend in mechanism design. The challenge is that
buyers are competitors and we need to design proper incentives for them to
invite each other. For selling a single item, many interesting mechanisms have
been proposed. However, all the mechanisms require the seller or a third party
to be trustworthy to execute the mechanisms. In addition, the owner of the
mechanism will know all the connections of the network after the execution,
which poses a potential privacy issue. Hence, distributed mechanisms to avoid
the privacy issue are more appealing in practice. Therefore, in this paper, we
propose the first distributed mechanism in social networks without revealing
buyers' private connections to anyone, and it achieves complete
decentralization that does not rely on any trustworthy third party. Moreover,
the centralized reduction of our mechanism also offers a novel way to compute
players' contributions compared to the existing solutions.
","[{'version': 'v1', 'created': 'Mon, 6 Mar 2023 12:35:21 GMT'}]",2023-03-07,['Computer Science and Game Theory'],"This paper explores the concept of distributed mechanism design in social networks with a focus on computer science and game theory. The paper begins by introducing the concept of distributed mechanism design and its importance in modern society. It then explains how distributed mechanism design works in social networks, focusing on the use of game theory and computer science to design efficient and secure protocols. The paper also covers the various challenges associated with distributed mechanism design, including scalability, privacy, and security. Finally, the paper presents potential solutions to these challenges, such as using cryptographic protocols and distributed ledgers. The paper concludes by summarizing the importance of distributed mechanism design in social networks and its potential to revolutionize the way we interact with each other.",Write an abstract for a paper called Distributed Mechanism Design in Social Networks about Computer Science and Game Theory
2212.01625,"Giuseppe Colucci, Stan van der Linde, Frank Phillipson",Power network optimization: a quantum approach,"['quant-ph', 'cs.ET', 'math.OC']","  Optimization of electricity surplus is a crucial element for transmission
power networks to reduce costs and efficiently use the available electricity
across the network. In this paper we showed how to optimize such a network with
quantum annealing. First, we define the QUBO problem for the partitioning of
the network, and test the implementation on purely quantum and hybrid
architectures. We then solve the problem on the D-Wave hybrid CQM and BQM
solvers, as well as on classical solvers available on Azure Quantum cloud.
Finally, we show that the hybrid approaches overperform the classical methods
in terms of quality of the solution, as the value of the objective function of
the quantum solutions is found to be always lower than with the classical
approaches across a set of different problem size.
","[{'version': 'v1', 'created': 'Sat, 3 Dec 2022 14:49:09 GMT'}]",2022-12-06,['Emerging Technologies'],"This paper presents a quantum approach to power network optimization, a problem of increasing importance in the field of emerging technologies. The paper explores the potential of quantum computing to solve the problem of optimizing power networks, which is typically difficult to do using classical methods. It examines the advantages and limitations of quantum computing for this task, and discusses the potential for further research. The paper also provides an overview of the current state of the art in quantum computing, and suggests potential directions for future research. Finally, the paper concludes by summarizing the implications of the quantum approach for power network optimization.",Write an abstract for a paper called Power network optimization: a quantum approach about Emerging Technologies
2112.08281,"Filippos Gouidis, Theodore Patkos, Antonis Argyros and Dimitris
  Plexousakis","Detecting Object States vs Detecting Objects: A New Dataset and a
  Quantitative Experimental Study",['cs.CV'],"  The detection of object states in images (State Detection - SD) is a problem
of both theoretical and practical importance and it is tightly interwoven with
other important computer vision problems, such as action recognition and
affordance detection. It is also highly relevant to any entity that needs to
reason and act in dynamic domains, such as robotic systems and intelligent
agents. Despite its importance, up to now, the research on this problem has
been limited. In this paper, we attempt a systematic study of the SD problem.
First, we introduce the Object State Detection Dataset (OSDD), a new publicly
available dataset consisting of more than 19,000 annotations for 18 object
categories and 9 state classes. Second, using a standard deep learning
framework used for Object Detection (OD), we conduct a number of appropriately
designed experiments, towards an in-depth study of the behavior of the SD
problem. This study enables the setup of a baseline on the performance of SD,
as well as its relative performance in comparison to OD, in a variety of
scenarios. Overall, the experimental outcomes confirm that SD is harder than OD
and that tailored SD methods need to be developed for addressing effectively
this significant problem.
","[{'version': 'v1', 'created': 'Wed, 15 Dec 2021 17:19:14 GMT'}, {'version': 'v2', 'created': 'Thu, 18 Aug 2022 20:43:12 GMT'}]",2022-08-22,['Computer Vision and Pattern Recognition'],"This paper presents a new dataset and a quantitative experimental study on the task of detecting object states versus detecting objects in computer vision and pattern recognition. The dataset consists of images of objects in various states and the task is to accurately detect the state of the object. The experimental study is designed to evaluate the performance of different state-of-the-art object detection algorithms on this dataset. The results of the study are used to compare the performance of the algorithms on the two tasks, and to identify potential areas for further research. The findings of this paper provide insight into the current state of object detection in computer vision and pattern recognition, and can be used to inform the development of future object detection algorithms.","Write an abstract for a paper called Detecting Object States vs Detecting Objects: A New Dataset and a
  Quantitative Experimental Study about Computer Vision and Pattern Recognition"
2204.07965,"Jiaxi Wu, Jiaxin Chen, Di Huang","Entropy-based Active Learning for Object Detection with Progressive
  Diversity Constraint",['cs.CV'],"  Active learning is a promising alternative to alleviate the issue of high
annotation cost in the computer vision tasks by consciously selecting more
informative samples to label. Active learning for object detection is more
challenging and existing efforts on it are relatively rare. In this paper, we
propose a novel hybrid approach to address this problem, where the
instance-level uncertainty and diversity are jointly considered in a bottom-up
manner. To balance the computational complexity, the proposed approach is
designed as a two-stage procedure. At the first stage, an Entropy-based
Non-Maximum Suppression (ENMS) is presented to estimate the uncertainty of
every image, which performs NMS according to the entropy in the feature space
to remove predictions with redundant information gains. At the second stage, a
diverse prototype (DivProto) strategy is explored to ensure the diversity
across images by progressively converting it into the intra-class and
inter-class diversities of the entropy-based class-specific prototypes.
Extensive experiments are conducted on MS COCO and Pascal VOC, and the proposed
approach achieves state of the art results and significantly outperforms the
other counterparts, highlighting its superiority.
","[{'version': 'v1', 'created': 'Sun, 17 Apr 2022 09:51:12 GMT'}]",2022-04-19,['Computer Vision and Pattern Recognition'],"This paper presents a novel entropy-based active learning approach for object detection in computer vision and pattern recognition. The proposed approach uses a progressive diversity constraint to select informative samples from a pool of unlabeled data. This approach is based on the observation that object detection models tend to focus on a limited number of discriminative features, resulting in a lack of diversity in the data. To address this issue, our proposed approach uses an entropy-based objective function to select samples that contain diverse features. Additionally, we introduce a progressive diversity constraint to ensure that the selected samples are increasingly diverse. Experiments on standard datasets demonstrate that our proposed approach achieves superior performance compared to existing active learning methods.","Write an abstract for a paper called Entropy-based Active Learning for Object Detection with Progressive
  Diversity Constraint about Computer Vision and Pattern Recognition"
2205.06192,Jhon Manuel Portella Delgado and Ankit Goel,"MIMO Input-Output Linearization with Applications for Longitudinal
  Flight Dynamics","['math.OC', 'cs.SY']","  This paper presents an extension of the input-output linearization method for
nonsquare systems with more outputs than inputs. Unlike the square systems and
nonsquare systems with fewer outputs than inputs, which can be completely
linearized, we consider the problem of linearizing nonsquare systems with more
outputs than inputs. In particular, the system is linearized by decomposing the
state using a diffeomorphism, which is chosen such that the output of the
system is a linear combination of the outputs of integrator chains, and the
input of the system is chosen to cancel the nonlinearities using feedback
linearization. In the case of nonsquare systems with more outputs than inputs,
we observe that the resulting linear system can be stabilized even though it is
uncontrollable at all times. This apparent contradiction is due to the
switching behavior in the control action. We apply the input-output
linearization method to linearize the longitudinal aircraft dynamics and
demonstrate asymptotic stability of the closed-loop system despite the
switching behavior.
","[{'version': 'v1', 'created': 'Thu, 12 May 2022 16:25:12 GMT'}]",2022-05-13,['Systems and Control'],"This paper presents a novel approach to linearizing the input-output dynamics of a longitudinal flight system. Specifically, a Multiple-Input Multiple-Output (MIMO) linearization approach is proposed and applied to a longitudinal flight system. The linearization is achieved through a combination of state-space and input-output linearization techniques. The results are then applied to a longitudinal flight system to demonstrate the effectiveness of the proposed approach. The proposed approach is shown to provide a more accurate model of the system dynamics than the traditional linearization methods. Furthermore, the proposed approach is shown to be more robust to disturbances, allowing for improved system performance. The proposed approach is also shown to be more computationally efficient than traditional linearization methods. Finally, the paper discusses potential applications of the proposed approach in the context of flight dynamics.","Write an abstract for a paper called MIMO Input-Output Linearization with Applications for Longitudinal
  Flight Dynamics about Systems and Control"
2206.14982,"Akiko Eriguchi, Shufang Xie, Tao Qin, Hany Hassan Awadalla","Building Multilingual Machine Translation Systems That Serve Arbitrary
  X-Y Translations","['cs.CL', 'cs.AI']","  Multilingual Neural Machine Translation (MNMT) enables one system to
translate sentences from multiple source languages to multiple target
languages, greatly reducing deployment costs compared with conventional
bilingual systems. The MNMT training benefit, however, is often limited to
many-to-one directions. The model suffers from poor performance in one-to-many
and many-to-many with zero-shot setup. To address this issue, this paper
discusses how to practically build MNMT systems that serve arbitrary X-Y
translation directions while leveraging multilinguality with a two-stage
training strategy of pretraining and finetuning. Experimenting with the WMT'21
multilingual translation task, we demonstrate that our systems outperform the
conventional baselines of direct bilingual models and pivot translation models
for most directions, averagely giving +6.0 and +4.1 BLEU, without the need for
architecture change or extra data collection. Moreover, we also examine our
proposed approach in an extremely large-scale data setting to accommodate
practical deployment scenarios.
","[{'version': 'v1', 'created': 'Thu, 30 Jun 2022 02:18:15 GMT'}]",2022-07-01,"['Computation and Language', 'Artificial Intelligence']","This paper presents a novel approach to building multilingual machine translation systems that can serve arbitrary X-Y translations. The proposed system is based on a novel combination of computation and language, artificial intelligence, and statistical machine learning techniques. The system is able to learn and generalize from a limited set of training data, thereby allowing it to serve translations between languages that have never been seen before. The paper also discusses the challenges and potential applications of the proposed system. Results from experiments conducted on a real-world dataset are presented to demonstrate the effectiveness of the proposed system. The paper concludes with a discussion of the implications of the proposed system for the field of machine translation and artificial intelligence.","Write an abstract for a paper called Building Multilingual Machine Translation Systems That Serve Arbitrary
  X-Y Translations about Computation and Language, Artificial Intelligence"
2303.06783,"Guangyao Zheng, Michael A. Jacobs, Vladimir Braverman, and Vishwa S.
  Parekh","Asynchronous Decentralized Federated Lifelong Learning for Landmark
  Localization in Medical Imaging","['cs.LG', 'cs.CV', 'eess.IV']","  Federated learning is a recent development in the machine learning area that
allows a system of devices to train on one or more tasks without sharing their
data to a single location or device. However, this framework still requires a
centralized global model to consolidate individual models into one, and the
devices train synchronously, which both can be potential bottlenecks for using
federated learning. In this paper, we propose a novel method of asynchronous
decentralized federated lifelong learning (ADFLL) method that inherits the
merits of federated learning and can train on multiple tasks simultaneously
without the need for a central node or synchronous training. Thus, overcoming
the potential drawbacks of conventional federated learning. We demonstrate
excellent performance on the brain tumor segmentation (BRATS) dataset for
localizing the left ventricle on multiple image sequences and image
orientation. Our framework allows agents to achieve the best performance with a
mean distance error of 7.81, better than the conventional all-knowing agent's
mean distance error of 11.78, and significantly (p=0.01) better than a
conventional lifelong learning agent with a distance error of 15.17 after eight
rounds of training. In addition, all ADFLL agents have comparable or better
performance than a conventional LL agent. In conclusion, we developed an ADFLL
framework with excellent performance and speed-up compared to conventional RL
agents.
","[{'version': 'v1', 'created': 'Sun, 12 Mar 2023 23:51:51 GMT'}]",2023-03-14,"['Machine Learning', 'Computer Vision and Pattern Recognition']","This paper presents a novel asynchronous decentralized federated lifelong learning approach for landmark localization in medical imaging. It combines the most recent advances in machine learning, computer vision and pattern recognition to develop a system that can accurately localize anatomical landmarks in medical images. The proposed system is capable of learning from a distributed set of annotated medical images, without the need for centralized coordination or a single master node. The system is also capable of adapting to changes in the data distribution over time, and can be used to support a wide range of applications, such as image segmentation, object detection, and image registration. The performance of the proposed system is evaluated on a publicly available dataset, and the results demonstrate the effectiveness of the approach.","Write an abstract for a paper called Asynchronous Decentralized Federated Lifelong Learning for Landmark
  Localization in Medical Imaging about Machine Learning, Computer Vision and Pattern Recognition"
2210.06788,"Seong Min Kye, Kwanghee Choi, Buru Chang",TiDAL: Learning Training Dynamics for Active Learning,['cs.LG'],"  Active learning (AL) aims to select the most useful data samples from an
unlabeled data pool and annotate them to expand the labeled dataset under a
limited budget. Especially, uncertainty-based methods choose the most uncertain
samples, which are known to be effective in improving model performance.
However, AL literature often overlooks training dynamics (TD), defined as the
ever-changing model behavior during optimization via stochastic gradient
descent, even though other areas of literature have empirically shown that TD
provides important clues for measuring the sample uncertainty. In this paper,
we propose a novel AL method, Training Dynamics for Active Learning (TiDAL),
which leverages the TD to quantify uncertainties of unlabeled data. Since
tracking the TD of all the large-scale unlabeled data is impractical, TiDAL
utilizes an additional prediction module that learns the TD of labeled data. To
further justify the design of TiDAL, we provide theoretical and empirical
evidence to argue the usefulness of leveraging TD for AL. Experimental results
show that our TiDAL achieves better or comparable performance on both balanced
and imbalanced benchmark datasets compared to state-of-the-art AL methods,
which estimate data uncertainty using only static information after model
training.
","[{'version': 'v1', 'created': 'Thu, 13 Oct 2022 06:54:50 GMT'}]",2022-10-14,['Machine Learning'],"This paper presents TiDAL, a novel training dynamics-based active learning (TDAL) framework for learning about machine learning (ML). TiDAL utilizes a combination of supervised and unsupervised learning techniques to identify the most informative training data points for ML models, allowing for more efficient and effective ML training. In addition, TiDAL leverages the concept of transfer learning to transfer knowledge from previously learned models to the current task. To demonstrate the effectiveness of TiDAL, we conducted experiments on a variety of datasets, including image classification, natural language processing, and reinforcement learning tasks. Results show that TiDAL outperforms traditional active learning methods in terms of accuracy, speed, and robustness. Furthermore, TiDAL is able to quickly adapt to new tasks and datasets, making it a promising tool for ML practitioners.",Write an abstract for a paper called TiDAL: Learning Training Dynamics for Active Learning about Machine Learning
2210.16613,"Abhijeet Awasthi, Ashutosh Sathe, Sunita Sarawagi","Diverse Parallel Data Synthesis for Cross-Database Adaptation of
  Text-to-SQL Parsers","['cs.CL', 'cs.AI', 'cs.LG']","  Text-to-SQL parsers typically struggle with databases unseen during the train
time. Adapting parsers to new databases is a challenging problem due to the
lack of natural language queries in the new schemas. We present ReFill, a
framework for synthesizing high-quality and textually diverse parallel datasets
for adapting a Text-to-SQL parser to a target schema. ReFill learns to
retrieve-and-edit text queries from the existing schemas and transfers them to
the target schema. We show that retrieving diverse existing text, masking their
schema-specific tokens, and refilling with tokens relevant to the target
schema, leads to significantly more diverse text queries than achievable by
standard SQL-to-Text generation methods. Through experiments spanning multiple
databases, we demonstrate that fine-tuning parsers on datasets synthesized
using ReFill consistently outperforms the prior data-augmentation methods.
","[{'version': 'v1', 'created': 'Sat, 29 Oct 2022 14:30:53 GMT'}]",2022-11-01,"['Computation and Language', 'Artificial Intelligence', 'Machine Learning']","This paper presents a novel approach to cross-database adaptation of text-to-SQL parsers using diverse parallel data synthesis. This method is applied to the task of text-to-SQL parsing, which is a challenging problem in the fields of computation and language, artificial intelligence, and machine learning. We propose a data synthesis technique that creates diverse parallel data by randomly sampling from a database schema and a corpus of natural language questions. We evaluate the performance of the cross-database adaptation of text-to-SQL parsers on a public benchmark dataset and demonstrate that our approach outperforms the state-of-the-art results. Additionally, we analyze the effects of data diversity on the performance of the cross-database adaptation of text-to-SQL parsers. Our results show that data diversity is essential for improving the performance of the cross-database adaptation of text-to-SQL parsers.","Write an abstract for a paper called Diverse Parallel Data Synthesis for Cross-Database Adaptation of
  Text-to-SQL Parsers about Computation and Language, Artificial Intelligence, Machine Learning"
2303.14771,"Nader Asadi, MohammadReza Davar, Sudhir Mudur, Rahaf Aljundi and
  Eugene Belilovsky","Prototype-Sample Relation Distillation: Towards Replay-Free Continual
  Learning",['cs.LG'],"  In Continual learning (CL) balancing effective adaptation while combating
catastrophic forgetting is a central challenge. Many of the recent
best-performing methods utilize various forms of prior task data, e.g. a replay
buffer, to tackle the catastrophic forgetting problem. Having access to
previous task data can be restrictive in many real-world scenarios, for example
when task data is sensitive or proprietary. To overcome the necessity of using
previous tasks data, in this work, we start with strong representation learning
methods that have been shown to be less prone to forgetting. We propose a
holistic approach to jointly learn the representation and class prototypes
while maintaining the relevance of old class prototypes and their embedded
similarities. Specifically, samples are mapped to an embedding space where the
representations are learned using a supervised contrastive loss. Class
prototypes are evolved continually in the same latent space, enabling learning
and prediction at any point. To continually adapt the prototypes without
keeping any prior task data, we propose a novel distillation loss that
constrains class prototypes to maintain relative similarities as compared to
new task data. This method yields state-of-the-art performance in the
task-incremental setting where we are able to outperform other methods that
both use no data as well as approaches relying on large amounts of data. Our
method is also shown to provide strong performance in the class-incremental
setting without using any stored data points.
","[{'version': 'v1', 'created': 'Sun, 26 Mar 2023 16:35:45 GMT'}]",2023-03-28,['Machine Learning'],"This paper presents an approach to continual learning about machine learning (ML) without the need for replay. We propose a novel method, Prototype-Sample Relation Distillation (PSRD), which distills knowledge from a prototype-based ML model to a sample-based ML model. We validate PSRD on a variety of ML tasks, including image classification, natural language processing, and reinforcement learning. Our experiments show that PSRD is able to acquire knowledge from a prototype-based model and transfer it to a sample-based model, while maintaining a high level of accuracy. We also demonstrate that PSRD can be applied to a variety of continual learning scenarios, including unsupervised, supervised, and reinforcement learning. Our results suggest that PSRD is a promising approach for continual learning about ML without the need for replay.","Write an abstract for a paper called Prototype-Sample Relation Distillation: Towards Replay-Free Continual
  Learning about Machine Learning"
2207.01078,"Kenneth Ooi, Zhen-Ting Ong, Karn N. Watcharasupat, Bhan Lam, Joo Young
  Hong, Woon-Seng Gan","ARAUS: A Large-Scale Dataset and Baseline Models of Affective Responses
  to Augmented Urban Soundscapes","['cs.SD', 'eess.AS']","  Choosing optimal maskers for existing soundscapes to effect a desired
perceptual change via soundscape augmentation is non-trivial due to extensive
varieties of maskers and a dearth of benchmark datasets with which to compare
and develop soundscape augmentation models. To address this problem, we make
publicly available the ARAUS (Affective Responses to Augmented Urban
Soundscapes) dataset, which comprises a five-fold cross-validation set and
independent test set totaling 25,440 unique subjective perceptual responses to
augmented soundscapes presented as audio-visual stimuli. Each augmented
soundscape is made by digitally adding ""maskers"" (bird, water, wind, traffic,
construction, or silence) to urban soundscape recordings at fixed
soundscape-to-masker ratios. Responses were then collected by asking
participants to rate how pleasant, annoying, eventful, uneventful, vibrant,
monotonous, chaotic, calm, and appropriate each augmented soundscape was, in
accordance with ISO 12913-2:2018. Participants also provided relevant
demographic information and completed standard psychological questionnaires. We
perform exploratory and statistical analysis of the responses obtained to
verify internal consistency and agreement with known results in the literature.
Finally, we demonstrate the benchmarking capability of the dataset by training
and comparing four baseline models for urban soundscape pleasantness: a
low-parameter regression model, a high-parameter convolutional neural network,
and two attention-based networks in the literature.
","[{'version': 'v1', 'created': 'Sun, 3 Jul 2022 17:09:09 GMT'}, {'version': 'v2', 'created': 'Tue, 5 Jul 2022 08:18:28 GMT'}, {'version': 'v3', 'created': 'Mon, 6 Mar 2023 03:24:53 GMT'}]",2023-03-07,['Sound'],"This paper presents ARAUS, a novel large-scale dataset of affective responses to augmented urban soundscapes. ARAUS contains over 10,000 samples of audio clips from a variety of urban soundscapes, each augmented with various sound effects. A total of 1,200 participants were asked to rate the affective responses to each soundscape. The ratings were based on a 5-point Likert scale. We also present baseline models for predicting the affective responses to the soundscapes. The models are based on various audio features, including mel-frequency cepstral coefficients, perceptual linear predictive coefficients, and spectral centroid. Our results show that the baseline models can accurately predict the affective responses to the soundscapes. The ARAUS dataset and baseline models can be used to further research into the affective responses to augmented urban soundscapes.","Write an abstract for a paper called ARAUS: A Large-Scale Dataset and Baseline Models of Affective Responses
  to Augmented Urban Soundscapes about Sound"
2210.08703,"Motoyuki Suzuki, Shintaro Sodeya and Taichi Nakamura",Spoken Dialogue System Based on Attribute Vector for Travel Agent Robot,['cs.HC'],"  In this study, we develop a dialogue system for a dialogue robot competition.
In the system, the characteristics of sightseeing spots are expressed as
""attribute vectors"" in advance, and the user is questioned on the different
attributes of the two candidate spots. Consequently, the system can make
recommendations based on user intentions. A dialogue experiment is conducted
during a preliminary round of competition. The overall satisfaction score
obtained is 40.1 out of 63 points, which is a reasonable result. Analysis of
the relationship between the system behavior and satisfaction scores reveals
that satisfaction increases when the system correctly understands the user
intention and responds appropriately. However, a negative correlation is
observed between the number of user utterances and the satisfaction score. This
implies that inappropriate responses reduce the usefulness of the system as a
consultation partner.
","[{'version': 'v1', 'created': 'Mon, 17 Oct 2022 02:33:00 GMT'}]",2022-10-18,['Human-Computer Interaction'],"This paper presents a spoken dialogue system based on an attribute vector for a travel agent robot. The system is designed to facilitate human-computer interaction in the travel domain. The proposed system utilizes a vector-based representation of attributes that serves as a bridge between the user's natural language utterances and the robot's response. The system is evaluated in terms of accuracy, speed, and user satisfaction. Results show that the proposed system is able to accurately recognize the user's intent and provide an appropriate response. Furthermore, the system is able to handle multiple user requests in a single dialogue. The findings of this paper demonstrate the potential of the proposed system in enabling effective human-computer interaction in the travel domain.",Write an abstract for a paper called Spoken Dialogue System Based on Attribute Vector for Travel Agent Robot about Human-Computer Interaction
2212.02935,Richard J. Preen and Jim Smith,"ACRO: A multi-language toolkit for supporting Automated Checking of
  Research Outputs","['cs.CR', 'cs.IR', 'cs.SE', 'stat.AP', 'stat.ME']","  This paper discusses the development of an open source tool ACRO, (Automatic
Checking of Research Outputs) to assist researchers and data governance teams
by distinguishing between: research output that is safe to publish; output that
requires further analysis; and output that cannot be published because it
creates substantial risk of disclosing private data. ACRO extends the
functionality and accessibility of a previous prototype by providing a
light-weight 'skin' that sits over well-known analysis tools, and enables
access in a variety of programming languages researchers might use. This adds
functionality to (i) identify potentially disclosive outputs against a range of
commonly used disclosure tests; (ii) suppress outputs where required; (iii)
report reasons for suppression; and (iv) produce simple summary documents
Trusted Research Environment (TRE) staff can use to streamline their workflow.
The ACRO code and documentation are available under an MIT license at
https://github.com/AI-SDC/ACRO
","[{'version': 'v1', 'created': 'Tue, 6 Dec 2022 12:45:15 GMT'}]",2022-12-07,"['Cryptography and Security', 'Information Retrieval', 'Software Engineering']","This paper presents ACRO, a multi-language toolkit for supporting automated checking of research outputs related to cryptography and security, information retrieval, and software engineering. ACRO provides a comprehensive set of tools, libraries, and services for automating the process of checking research outputs for correctness and completeness. ACRO is designed to be extensible and to support multiple programming languages, making it easy for developers to integrate it into their existing research projects. The paper describes the architecture of ACRO and the tools, libraries, and services it provides, as well as its implementation in a number of programming languages. The paper also discusses how ACRO is being used in research projects, and provides a number of case studies. Finally, the paper concludes with a discussion of future directions for ACRO.","Write an abstract for a paper called ACRO: A multi-language toolkit for supporting Automated Checking of
  Research Outputs about Cryptography and Security, Information Retrieval, Software Engineering"
2210.16458,Satesh Ramdhani,"Reformulating van Rijsbergen's $F_{\beta}$ metric for weighted binary
  cross-entropy","['stat.ML', 'cs.LG']","  The separation of performance metrics from gradient based loss functions may
not always give optimal results and may miss vital aggregate information. This
paper investigates incorporating a performance metric alongside differentiable
loss functions to inform training outcomes. The goal is to guide model
performance and interpretation by assuming statistical distributions on this
performance metric for dynamic weighting. The focus is on van Rijsbergens
$F_{\beta}$ metric -- a popular choice for gauging classification performance.
Through distributional assumptions on the $F_{\beta}$, an intermediary link can
be established to the standard binary cross-entropy via dynamic penalty
weights. First, the $F_{\beta}$ metric is reformulated to facilitate assuming
statistical distributions with accompanying proofs for the cumulative density
function. These probabilities are used within a knee curve algorithm to find an
optimal $\beta$ or $\beta_{opt}$. This $\beta_{opt}$ is used as a weight or
penalty in the proposed weighted binary cross-entropy. Experimentation on
publicly available data with imbalanced classes mostly yields better and
interpretable results as compared to the baseline. For example, for the IMDB
text data with known labeling errors, a 14% boost is shown. This methodology
can accelerate training and provide better interpretation.
","[{'version': 'v1', 'created': 'Sat, 29 Oct 2022 01:21:42 GMT'}]",2022-11-01,['Machine Learning'],"This paper presents a reformulation of van Rijsbergen's $F_{\beta}$ metric for weighted binary cross-entropy in the context of Machine Learning. The proposed reformulation is based on a weighted average of precision and recall, which are computed using a weighted version of the binary cross-entropy loss function. We show that the reformulated metric is more suitable for Machine Learning applications than the original metric, as it allows for a more nuanced evaluation of the performance of classifiers. We demonstrate the effectiveness of the reformulated metric on a variety of datasets, and discuss its potential applications in Machine Learning.","Write an abstract for a paper called Reformulating van Rijsbergen's $F_{\beta}$ metric for weighted binary
  cross-entropy about Machine Learning"
2102.10882,"Xiangxiang Chu and Zhi Tian and Bo Zhang and Xinlong Wang and Chunhua
  Shen",Conditional Positional Encodings for Vision Transformers,"['cs.CV', 'cs.AI', 'cs.LG']","  We propose a conditional positional encoding (CPE) scheme for vision
Transformers. Unlike previous fixed or learnable positional encodings, which
are pre-defined and independent of input tokens, CPE is dynamically generated
and conditioned on the local neighborhood of the input tokens. As a result, CPE
can easily generalize to the input sequences that are longer than what the
model has ever seen during training. Besides, CPE can keep the desired
translation-invariance in the image classification task, resulting in improved
performance. We implement CPE with a simple Position Encoding Generator (PEG)
to get seamlessly incorporated into the current Transformer framework. Built on
PEG, we present Conditional Position encoding Vision Transformer (CPVT). We
demonstrate that CPVT has visually similar attention maps compared to those
with learned positional encodings and delivers outperforming results. Our code
is available at https://github.com/Meituan-AutoML/CPVT .
","[{'version': 'v1', 'created': 'Mon, 22 Feb 2021 10:29:55 GMT'}, {'version': 'v2', 'created': 'Thu, 18 Mar 2021 10:59:50 GMT'}, {'version': 'v3', 'created': 'Mon, 13 Feb 2023 01:19:57 GMT'}]",2023-02-14,"['Computer Vision and Pattern Recognition', 'Artificial Intelligence', 'Machine Learning']","This paper presents a novel approach for incorporating positional information into Vision Transformers (ViT) for Computer Vision and Pattern Recognition tasks. We propose a novel approach to positional encoding called Conditional Positional Encodings (CPE) which utilizes a learnable positional encoding scheme. We evaluate CPE on a variety of datasets and tasks and find that it outperforms the standard ViT baseline. Furthermore, we discuss the implications of our work for Artificial Intelligence and Machine Learning, and suggest potential areas of future research.","Write an abstract for a paper called Conditional Positional Encodings for Vision Transformers about Computer Vision and Pattern Recognition, Artificial Intelligence, Machine Learning"
2207.11774,"Isabel Dias, Ricardo Rei, Patr\'icia Pereira and Luisa Coheur",Towards a Sentiment-Aware Conversational Agent,['cs.CL'],"  In this paper, we propose an end-to-end sentiment-aware conversational agent
based on two models: a reply sentiment prediction model, which leverages the
context of the dialogue to predict an appropriate sentiment for the agent to
express in its reply; and a text generation model, which is conditioned on the
predicted sentiment and the context of the dialogue, to produce a reply that is
both context and sentiment appropriate. Additionally, we propose to use a
sentiment classification model to evaluate the sentiment expressed by the agent
during the development of the model. This allows us to evaluate the agent in an
automatic way. Both automatic and human evaluation results show that explicitly
guiding the text generation model with a pre-defined set of sentences leads to
clear improvements, both regarding the expressed sentiment and the quality of
the generated text.
","[{'version': 'v1', 'created': 'Sun, 24 Jul 2022 16:59:44 GMT'}]",2022-07-26,['Computation and Language'],"This paper presents an approach to creating a sentiment-aware conversational agent that can effectively process natural language. We discuss the recent advances in natural language processing (NLP) and machine learning (ML) and how they can be used to create a sentiment-aware conversational agent. We analyze the current state of sentiment analysis, and how it can be used to create a conversational agent that is aware of the sentiment expressed by a user. We then discuss the challenges of creating such an agent, including the need for a large amount of labeled data and the development of robust algorithms. Finally, we propose a novel approach to building a sentiment-aware conversational agent, which combines NLP and ML to create a system that is capable of understanding and responding to user sentiment. The proposed approach is evaluated using a set of experiments, and the results demonstrate the potential for creating a sentiment-aware conversational agent.",Write an abstract for a paper called Towards a Sentiment-Aware Conversational Agent about Computation and Language
2203.14118,"Andr\'es Macho-Ortiz, Daniel P\'erez-L\'opez, Jos\'e Aza\~na, Jos\'e
  Capmany",Analog Programmable-Photonic Computation,"['cs.ET', 'physics.optics']","  Digital electronics is a technological cornerstone in our modern society
which has covered the increasing demand in computing power during the last
decades thanks to a periodic doubling of transistor density and power
efficiency in integrated circuits. Currently, such scaling laws are reaching
their fundamental limits, leading to the emergence of a large gamut of
applications that cannot be supported by digital electronics, specifically,
those that involve real time analog multi-data processing, e.g., medical
diagnostic imaging, robotic control and remote sensing, among others. In this
scenario, an analog computing approach implemented in a real-time
reconfigurable non-electronic hardware such as programmable integrated
photonics (PIP) can be more efficient than digital electronics to perform these
emerging applications. However, actual analog computing models such as quantum
and neuromorphic computation were not conceived to extract the benefits of PIP
technology (and integrated photonics in general). Here, we present the
foundations of a new computation theory, termed Analog Programmable-Photonic
Computation (APC), explicitly designed to unleash the full potential of PIP.
Interestingly, APC enables overcoming basic theoretical and technological
limitations of existing computational models, can be implemented in other
technologies (e.g. in electronics, acoustics or using metamaterials) and,
consequently, exhibits the potential to spark a ground-breaking impact on our
information society.
","[{'version': 'v1', 'created': 'Sat, 26 Mar 2022 17:32:25 GMT'}]",2022-03-29,['Emerging Technologies'],"This paper focuses on the emerging technology of analog programmable-photonic computation. It examines the potential applications of this technology, its advantages, and the challenges associated with its development and implementation. The paper also reviews the current state of the technology and provides a discussion of future research directions. The goal of this paper is to provide an overview of the technology and its potential for real-world applications. It is expected that this paper will provide insight into the potential of analog programmable-photonic computation and its applications in a variety of areas, including communication, computing, and sensing.",Write an abstract for a paper called Analog Programmable-Photonic Computation about Emerging Technologies
2303.00085,"Shrey Pareek, Harris NIsar and Thenkurussi Kesavadas","AR3n: A Reinforcement Learning-based Assist-As-Needed Controller for
  Robotic Rehabilitation","['cs.RO', 'cs.AI', 'cs.LG']","  In this paper, we present AR3n (pronounced as Aaron), an assist-as-needed
(AAN) controller that utilizes reinforcement learning to supply adaptive
assistance during a robot assisted handwriting rehabilitation task. Unlike
previous AAN controllers, our method does not rely on patient specific
controller parameters or physical models. We propose the use of a virtual
patient model to generalize AR3n across multiple subjects. The system modulates
robotic assistance in realtime based on a subject's tracking error, while
minimizing the amount of robotic assistance. The controller is experimentally
validated through a set of simulations and human subject experiments. Finally,
a comparative study with a traditional rule-based controller is conducted to
analyze differences in assistance mechanisms of the two controllers.
","[{'version': 'v1', 'created': 'Tue, 28 Feb 2023 21:04:05 GMT'}, {'version': 'v2', 'created': 'Thu, 16 Mar 2023 16:12:15 GMT'}, {'version': 'v3', 'created': 'Thu, 6 Apr 2023 01:07:13 GMT'}]",2023-04-07,"['Robotics', 'Artificial Intelligence', 'Machine Learning']","This paper presents AR3n, a novel assist-as-needed robotic rehabilitation controller based on reinforcement learning. AR3n is designed to provide assistance to patients during robotic rehabilitation exercises by adapting to their individual needs and abilities. The controller is composed of two main components: a reinforcement learning module, which is used to learn the patient's motor control abilities, and an assist-as-needed controller, which provides assistance to the patient when required. The reinforcement learning module is trained using a reward function that is designed to maximize the patient's performance. The assist-as-needed controller is then used to provide assistance to the patient when needed. The performance of AR3n is evaluated using simulated robotic rehabilitation exercises. The results show that AR3n is able to adapt to the patient's individual needs and abilities and provide assistance when needed. The paper also discusses the potential applications of AR3n in real-world robotic rehabilitation.","Write an abstract for a paper called AR3n: A Reinforcement Learning-based Assist-As-Needed Controller for
  Robotic Rehabilitation about Robotics, Artificial Intelligence, Machine Learning"
2207.03355,"Ghulam Jilani Quadri, Jennifer Adorno Nieves, Brenton M. Wiernik, Paul
  Rosen",Automatic Scatterplot Design Optimization for Clustering Identification,['cs.HC'],"  Scatterplots are among the most widely used visualization techniques.
Compelling scatterplot visualizations improve understanding of data by
leveraging visual perception to boost awareness when performing specific visual
analytic tasks. Design choices in scatterplots, such as graphical encodings or
data aspects, can directly impact decision-making quality for low-level tasks
like clustering. Hence, constructing frameworks that consider both the
perceptions of the visual encodings and the task being performed enables
optimizing visualizations to maximize efficacy. In this paper, we propose an
automatic tool to optimize the design factors of scatterplots to reveal the
most salient cluster structure. Our approach leverages the merge tree data
structure to identify the clusters and optimize the choice of subsampling
algorithm, sampling rate, marker size, and marker opacity used to generate a
scatterplot image. We validate our approach with user and case studies that
show it efficiently provides high-quality scatterplot designs from a large
parameter space.
","[{'version': 'v1', 'created': 'Thu, 7 Jul 2022 15:01:36 GMT'}]",2022-07-08,['Human-Computer Interaction'],"This paper presents a novel approach to automatic scatterplot design optimization for clustering identification in the context of Human-Computer Interaction (HCI). The proposed approach utilizes a combination of optimization techniques and visual analytics to automatically identify and optimize the design of scatterplots for clustering identification. Specifically, the approach uses a genetic algorithm to optimize the design of scatterplots for the purpose of clustering identification. Furthermore, the approach utilizes visual analytics to assess the quality of the optimized scatterplot designs. The results of the experiments demonstrate that the proposed approach is able to generate more effective visualizations for clustering identification than existing methods. The paper also discusses the implications of the proposed approach for HCI and its potential applications.",Write an abstract for a paper called Automatic Scatterplot Design Optimization for Clustering Identification about Human-Computer Interaction
2207.07512,"Alex F. Spies, Alessandra Russo and Murray Shanahan",Sparse Relational Reasoning with Object-Centric Representations,"['cs.LG', 'cs.AI']","  We investigate the composability of soft-rules learned by relational neural
architectures when operating over object-centric (slot-based) representations,
under a variety of sparsity-inducing constraints. We find that increasing
sparsity, especially on features, improves the performance of some models and
leads to simpler relations. Additionally, we observe that object-centric
representations can be detrimental when not all objects are fully captured; a
failure mode to which CNNs are less prone. These findings demonstrate the
trade-offs between interpretability and performance, even for models designed
to tackle relational tasks.
","[{'version': 'v1', 'created': 'Fri, 15 Jul 2022 14:57:33 GMT'}]",2022-07-18,"['Machine Learning', 'Artificial Intelligence']","This paper presents a novel approach to machine learning and artificial intelligence, based on sparse relational reasoning with object-centric representations. We propose a new relational reasoning architecture that combines object-centric representations with a set of sparse relational rules. This architecture is applied to a variety of tasks, including image classification, visual question answering, and natural language processing. We demonstrate that our approach outperforms traditional methods in terms of accuracy, generalization, and interpretability. Furthermore, we discuss the potential of our approach to help bridge the gap between symbolic and subsymbolic AI systems.","Write an abstract for a paper called Sparse Relational Reasoning with Object-Centric Representations about Machine Learning, Artificial Intelligence"
2304.00697,Xin Zhang and Yuqi Song and Xiaofeng Wang and Fei Zuo,"D-Score: A White-Box Diagnosis Score for CNNs Based on Mutation
  Operators",['cs.CV'],"  Convolutional neural networks (CNNs) have been widely applied in many
safety-critical domains, such as autonomous driving and medical diagnosis.
However, concerns have been raised with respect to the trustworthiness of these
models: The standard testing method evaluates the performance of a model on a
test set, while low-quality and insufficient test sets can lead to unreliable
evaluation results, which can have unforeseeable consequences. Therefore, how
to comprehensively evaluate CNNs and, based on the evaluation results, how to
enhance their trustworthiness are the key problems to be urgently addressed.
Prior work has used mutation tests to evaluate the test sets of CNNs. However,
the evaluation scores are black boxes and not explicit enough for what is being
tested. In this paper, we propose a white-box diagnostic approach that uses
mutation operators and image transformation to calculate the feature and
attention distribution of the model and further present a diagnosis score,
namely D-Score, to reflect the model's robustness and fitness to a dataset. We
also propose a D-Score based data augmentation method to enhance the CNN's
performance to translations and rescalings. Comprehensive experiments on two
widely used datasets and three commonly adopted CNNs demonstrate the
effectiveness of our approach.
","[{'version': 'v1', 'created': 'Mon, 3 Apr 2023 03:13:59 GMT'}]",2023-04-04,['Computer Vision and Pattern Recognition'],"This paper introduces D-Score, a novel white-box diagnosis score for convolutional neural networks (CNNs) based on mutation operators. By using mutation operators to perturb the weights of CNNs, D-Score is able to measure the robustness of CNNs to perturbations and diagnose their performance. The proposed score is evaluated on two benchmark datasets and compared with other existing scores. The results show that D-Score outperforms existing scores in terms of accuracy and robustness, and is able to identify the most important regions of the input space for a given CNN. This paper provides a novel approach to evaluate and diagnose CNNs, which can be used to improve the performance of CNNs in computer vision and pattern recognition tasks.","Write an abstract for a paper called D-Score: A White-Box Diagnosis Score for CNNs Based on Mutation
  Operators about Computer Vision and Pattern Recognition"
2012.006,"Mustafa Jarrar, Eman Karajah, Muhammad Khalifa, Khaled Shaalan",Extracting Synonyms from Bilingual Dictionaries,"['cs.CL', 'cs.AI', 'cs.IR']","  We present our progress in developing a novel algorithm to extract synonyms
from bilingual dictionaries. Identification and usage of synonyms play a
significant role in improving the performance of information access
applications. The idea is to construct a translation graph from translation
pairs, then to extract and consolidate cyclic paths to form bilingual sets of
synonyms. The initial evaluation of this algorithm illustrates promising
results in extracting Arabic-English bilingual synonyms. In the evaluation, we
first converted the synsets in the Arabic WordNet into translation pairs (i.e.,
losing word-sense memberships). Next, we applied our algorithm to rebuild these
synsets. We compared the original and extracted synsets obtaining an F-Measure
of 82.3% and 82.1% for Arabic and English synsets extraction, respectively.
","[{'version': 'v1', 'created': 'Tue, 1 Dec 2020 16:09:22 GMT'}]",2022-05-20,"['Computation and Language', 'Artificial Intelligence', 'Information Retrieval']","This paper presents a new method for extracting synonyms from bilingual dictionaries using computational and language-based techniques, specifically artificial intelligence and information retrieval. The proposed method uses an artificial intelligence-based approach to identify synonyms from bilingual dictionaries by analyzing the contexts in which the words appear. The method is evaluated using a test set of bilingual dictionaries and the results show that the proposed method is able to accurately identify synonyms from bilingual dictionaries. Additionally, the paper provides an in-depth discussion of the implications of the proposed method for artificial intelligence and information retrieval.","Write an abstract for a paper called Extracting Synonyms from Bilingual Dictionaries about Computation and Language, Artificial Intelligence, Information Retrieval"
2302.06404,"Hoang Tran, Qiang Du, Guannan Zhang","Convergence analysis for a nonlocal gradient descent method via
  directional Gaussian smoothing","['math.OC', 'cs.NA', 'math.NA']","  We analyze the convergence of a nonlocal gradient descent method for
minimizing a class of high-dimensional non-convex functions, where a
directional Gaussian smoothing (DGS) is proposed to define the nonlocal
gradient (also referred to as the DGS gradient). The method was first proposed
in [42], in which multiple numerical experiments showed that replacing the
traditional local gradient with the DGS gradient can help the optimizers escape
local minima more easily and significantly improve their performance. However,
a rigorous theory for the efficiency of the method on nonconvex landscape is
lacking. In this work, we investigate the scenario where the objective function
is composed of a convex function, perturbed by a oscillating noise. We provide
a convergence theory under which the iterates exponentially converge to a
tightened neighborhood of the solution, whose size is characterized by the
noise wavelength. We also establish a correlation between the optimal values of
the Gaussian smoothing radius and the noise wavelength, thus justify the
advantage of using moderate or large smoothing radius with the method.
Furthermore, if the noise level decays to zero when approaching global minimum,
we prove that DGS-based optimization converges to the exact global minimum with
linear rates, similarly to standard gradient-based method in optimizing convex
functions. Several numerical experiments are provided to confirm our theory and
illustrate the superiority of the approach over those based on the local
gradient.
","[{'version': 'v1', 'created': 'Mon, 13 Feb 2023 14:41:52 GMT'}]",2023-02-14,['Numerical Analysis'],"This paper presents a convergence analysis for a nonlocal gradient descent method via directional Gaussian smoothing in the context of numerical analysis. We propose a novel nonlocal gradient descent method based on directional Gaussian smoothing, which is shown to have better convergence properties than existing methods. We analyze the convergence rate of the proposed method and compare it with existing methods and demonstrate its effectiveness in numerical simulations. We also provide theoretical results and numerical experiments to demonstrate the effectiveness of the proposed method. Our results show that the proposed method is more efficient and accurate than existing methods in numerical analysis.","Write an abstract for a paper called Convergence analysis for a nonlocal gradient descent method via
  directional Gaussian smoothing about Numerical Analysis"
2112.025,"Jie Qin, Peng Zheng, Yichao Yan, Rong Quan, Xiaogang Cheng, Bingbing
  Ni",MovieNet-PS: A Large-Scale Person Search Dataset in the Wild,['cs.CV'],"  Person search aims to jointly localize and identify a query person from
natural, uncropped images, which has been actively studied over the past few
years. In this paper, we delve into the rich context information globally and
locally surrounding the target person, which we refer to as scene and group
context, respectively. Unlike previous works that treat the two types of
context individually, we exploit them in a unified global-local context network
(GLCNet) with the intuitive aim of feature enhancement. Specifically, re-ID
embeddings and context features are simultaneously learned in a multi-stage
fashion, ultimately leading to enhanced, discriminative features for person
search. We conduct the experiments on two person search benchmarks (i.e.,
CUHK-SYSU and PRW) as well as extend our approach to a more challenging setting
(i.e., character search on MovieNet). Extensive experimental results
demonstrate the consistent improvement of the proposed GLCNet over the
state-of-the-art methods on all three datasets. Our source codes, pre-trained
models, and the new dataset are publicly available at:
https://github.com/ZhengPeng7/GLCNet.
","[{'version': 'v1', 'created': 'Sun, 5 Dec 2021 07:38:53 GMT'}, {'version': 'v2', 'created': 'Fri, 25 Mar 2022 11:11:26 GMT'}, {'version': 'v3', 'created': 'Tue, 12 Apr 2022 13:20:39 GMT'}, {'version': 'v4', 'created': 'Tue, 28 Feb 2023 11:19:31 GMT'}]",2023-03-01,['Computer Vision and Pattern Recognition'],"This paper presents MovieNet-PS, a new large-scale person search dataset in the wild for computer vision and pattern recognition. MovieNet-PS contains over 1 million images of people from over 50,000 movies, TV shows, and other media sources. The dataset is annotated with bounding boxes, facial landmarks, and segmentation masks. We also present a suite of evaluation protocols and baseline results for person search and re-identification tasks. MovieNet-PS is the largest and most diverse person search dataset in the wild to date, and is suitable for a variety of applications including person search, re-identification, and face recognition.",Write an abstract for a paper called MovieNet-PS: A Large-Scale Person Search Dataset in the Wild about Computer Vision and Pattern Recognition
2302.04002,"Jun Cen, Di Luan, Shiwei Zhang, Yixuan Pei, Yingya Zhang, Deli Zhao,
  Shaojie Shen, Qifeng Chen","The Devil is in the Wrongly-classified Samples: Towards Unified Open-set
  Recognition",['cs.CV'],"  Open-set Recognition (OSR) aims to identify test samples whose classes are
not seen during the training process. Recently, Unified Open-set Recognition
(UOSR) has been proposed to reject not only unknown samples but also known but
wrongly classified samples, which tends to be more practical in real-world
applications. The UOSR draws little attention since it is proposed, but we find
sometimes it is even more practical than OSR in the real world applications, as
evaluation results of known but wrongly classified samples are also wrong like
unknown samples. In this paper, we deeply analyze the UOSR task under different
training and evaluation settings to shed light on this promising research
direction. For this purpose, we first evaluate the UOSR performance of several
OSR methods and show a significant finding that the UOSR performance
consistently surpasses the OSR performance by a large margin for the same
method. We show that the reason lies in the known but wrongly classified
samples, as their uncertainty distribution is extremely close to unknown
samples rather than known and correctly classified samples. Second, we analyze
how the two training settings of OSR (i.e., pre-training and outlier exposure)
influence the UOSR. We find although they are both beneficial for
distinguishing known and correctly classified samples from unknown samples,
pre-training is also helpful for identifying known but wrongly classified
samples while outlier exposure is not. In addition to different training
settings, we also formulate a new evaluation setting for UOSR which is called
few-shot UOSR, where only one or five samples per unknown class are available
during evaluation to help identify unknown samples. We propose FS-KNNS for the
few-shot UOSR to achieve state-of-the-art performance under all settings.
","[{'version': 'v1', 'created': 'Wed, 8 Feb 2023 11:34:04 GMT'}]",2023-02-09,['Computer Vision and Pattern Recognition'],"Open-set recognition is a challenging problem in the field of computer vision and pattern recognition. The goal of open-set recognition is to accurately classify objects and reject unrecognized samples. However, existing open-set recognition methods suffer from the problem of wrongly-classified samples, which can lead to misclassification and poor performance. This paper proposes a unified open-set recognition approach that addresses the issue of wrongly-classified samples. Specifically, the proposed approach utilizes a novel learning technique to train a model that can accurately classify objects and reject unrecognized samples. Additionally, the proposed approach is able to detect and reject wrongly-classified samples, thus reducing the risk of misclassification. Experimental results demonstrate the effectiveness of the proposed approach on a variety of datasets. The results show that the proposed approach achieves better performance than existing open-set recognition methods.","Write an abstract for a paper called The Devil is in the Wrongly-classified Samples: Towards Unified Open-set
  Recognition about Computer Vision and Pattern Recognition"
2202.13564,"Junchi Yan, Xianglong Lyu, Ruoyu Cheng, Yibo Lin","Towards Machine Learning for Placement and Routing in Chip Design: a
  Methodological Overview","['cs.LG', 'cs.AR']","  Placement and routing are two indispensable and challenging (NP-hard) tasks
in modern chip design flows. Compared with traditional solvers using heuristics
or expert-well-designed algorithms, machine learning has shown promising
prospects by its data-driven nature, which can be of less reliance on knowledge
and priors, and potentially more scalable by its advanced computational
paradigms (e.g. deep networks with GPU acceleration). This survey starts with
the introduction of basics of placement and routing, with a brief description
on classic learning-free solvers. Then we present detailed review on recent
advance in machine learning for placement and routing. Finally we discuss the
challenges and opportunities for future research.
","[{'version': 'v1', 'created': 'Mon, 28 Feb 2022 06:28:44 GMT'}]",2022-03-01,"['Machine Learning', 'Hardware Architecture']",", and Applications

This paper provides an overview of the use of machine learning for placement and routing in chip design, focusing on the methodology, hardware architecture, and applications of this technology. We discuss the challenges of machine learning for placement and routing, including the need for efficient hardware architectures and the need to address the complexity of the design process. We then provide an overview of the current state-of-the-art in machine learning for placement and routing, including its applications in the areas of power optimization, timing optimization, and thermal optimization. We also provide a comparison of different machine learning algorithms and their performance in different design scenarios. Finally, we discuss the potential of machine learning for placement and routing, and its implications for the future of chip design.","Write an abstract for a paper called Towards Machine Learning for Placement and Routing in Chip Design: a
  Methodological Overview about Machine Learning, Hardware Architecture"
2203.0925,"Irina Higgins, S\'ebastien Racani\`ere, Danilo Rezende","Symmetry-Based Representations for Artificial and Biological General
  Intelligence","['q-bio.NC', 'cs.AI', 'cs.LG', 'cs.NE', 'stat.ML']","  Biological intelligence is remarkable in its ability to produce complex
behaviour in many diverse situations through data efficient, generalisable and
transferable skill acquisition. It is believed that learning ""good"" sensory
representations is important for enabling this, however there is little
agreement as to what a good representation should look like. In this review
article we are going to argue that symmetry transformations are a fundamental
principle that can guide our search for what makes a good representation. The
idea that there exist transformations (symmetries) that affect some aspects of
the system but not others, and their relationship to conserved quantities has
become central in modern physics, resulting in a more unified theoretical
framework and even ability to predict the existence of new particles. Recently,
symmetries have started to gain prominence in machine learning too, resulting
in more data efficient and generalisable algorithms that can mimic some of the
complex behaviours produced by biological intelligence. Finally, first
demonstrations of the importance of symmetry transformations for representation
learning in the brain are starting to arise in neuroscience. Taken together,
the overwhelming positive effect that symmetries bring to these disciplines
suggest that they may be an important general framework that determines the
structure of the universe, constrains the nature of natural tasks and
consequently shapes both biological and artificial intelligence.
","[{'version': 'v1', 'created': 'Thu, 17 Mar 2022 11:18:34 GMT'}]",2022-03-18,"['Artificial Intelligence', 'Machine Learning', 'Neural and Evolutionary Computing']","This paper explores the use of symmetry-based representations for artificial and biological general intelligence. It investigates the potential for symmetry-based representations to be used for artificial intelligence, machine learning, neural computing, and evolutionary computing. It examines the advantages of symmetry-based representations, such as their ability to capture complex patterns, their scalability, and their potential for generalization. It also discusses the challenges of using symmetry-based representations, such as computational complexity, and the need for a better understanding of the relation between symmetries and representations. Finally, the paper discusses potential applications of symmetry-based representations, such as in robotics, natural language processing, and computer vision.","Write an abstract for a paper called Symmetry-Based Representations for Artificial and Biological General
  Intelligence about Artificial Intelligence, Machine Learning, Neural and Evolutionary Computing"
2209.06557,Ahmed Fraz Baig and Sigurd Eskeland,"A Generic Privacy-Preserving Protocol For Keystroke Dynamics-Based
  Continuous Authentication",['cs.CR'],"  Continuous authentication utilizes automatic recognition of certain user
features for seamless and passive authentication without requiring user
attention. Such features can be divided into categories of physiological
biometrics and behavioral biometrics. Keystroke dynamics is proposed for
behavioral biometrics-oriented authentication by recognizing users by means of
their typing patterns. However, it has been pointed out that continuous
authentication using physiological biometrics and behavior biometrics incur
privacy risks, revealing personal characteristics and activities. In this
paper, we consider a previously proposed keystroke dynamics-based
authentication scheme that has no privacy-preserving properties. In this
regard, we propose a generic privacy-preserving version of this authentication
scheme in which all user features are encrypted -- preventing disclosure of
those to the authentication server. Our scheme is generic in the sense that it
assumes homomorphic cryptographic primitives. Authentication is conducted on
the basis of encrypted data due to the homomorphic cryptographic properties of
our protocol.
","[{'version': 'v1', 'created': 'Wed, 14 Sep 2022 11:27:53 GMT'}]",2022-09-15,['Cryptography and Security'],"This paper proposes a generic privacy-preserving protocol for keystroke dynamics-based continuous authentication. The proposed protocol incorporates the use of cryptographic primitives to protect the user's authentication data from unauthorized access. The protocol is designed to provide a secure and efficient authentication mechanism that can be used for a variety of applications, such as online banking, online shopping, and access control. The protocol is evaluated using a set of security metrics, such as resistance to brute-force attacks, resistance to replay attacks, and resistance to man-in-the-middle attacks. The results show that the proposed protocol is secure and provides a robust authentication mechanism. The paper also provides an analysis of the security and privacy implications of the proposed protocol.","Write an abstract for a paper called A Generic Privacy-Preserving Protocol For Keystroke Dynamics-Based
  Continuous Authentication about Cryptography and Security"
2203.07993,"Kai Chen, Ye Wang, Yitong Li and Aiping Li","RotateQVS: Representing Temporal Information as Rotations in Quaternion
  Vector Space for Temporal Knowledge Graph Completion",['cs.AI'],"  Temporal factors are tied to the growth of facts in realistic applications,
such as the progress of diseases and the development of political situation,
therefore, research on Temporal Knowledge Graph (TKG) attracks much attention.
In TKG, relation patterns inherent with temporality are required to be studied
for representation learning and reasoning across temporal facts. However,
existing methods can hardly model temporal relation patterns, nor can capture
the intrinsic connections between relations when evolving over time, lacking of
interpretability. In this paper, we propose a novel temporal modeling method
which represents temporal entities as Rotations in Quaternion Vector Space
(RotateQVS) and relations as complex vectors in Hamilton's quaternion space. We
demonstrate our method can model key patterns of relations in TKG, such as
symmetry, asymmetry, inverse, and can further capture time-evolved relations by
theory. Empirically, we show that our method can boost the performance of link
prediction tasks over four temporal knowledge graph benchmarks.
","[{'version': 'v1', 'created': 'Tue, 15 Mar 2022 15:27:23 GMT'}, {'version': 'v2', 'created': 'Thu, 17 Mar 2022 03:31:46 GMT'}]",2022-03-18,['Artificial Intelligence'],"This paper presents RotateQVS, a novel approach for representing temporal information as rotations in quaternion vector space for temporal knowledge graph completion in Artificial Intelligence (AI). RotateQVS uses quaternions to represent temporal information, which allows for a more efficient and accurate representation of temporal information compared to traditional methods. The proposed approach is evaluated using a temporal knowledge graph completion task on a real-world AI dataset. Results show that RotateQVS outperforms the baseline methods in terms of accuracy and efficiency. Additionally, experiments demonstrate that RotateQVS can effectively capture temporal relationships between different entities in the graph. The paper concludes by discussing the potential applications of RotateQVS in AI, such as temporal reasoning and temporal event prediction.","Write an abstract for a paper called RotateQVS: Representing Temporal Information as Rotations in Quaternion
  Vector Space for Temporal Knowledge Graph Completion about Artificial Intelligence"
2302.08984,"Aruna Jayasena, Prabhat Mishra",DETER: Design for Trust utilizing Rareness Reduction,"['cs.CR', 'cs.AR', 'cs.LO']","  Increasing design complexity and reduced time-to-market have motivated
manufacturers to outsource some parts of the System-on-Chip (SoC) design flow
to third-party vendors. This provides an opportunity for attackers to introduce
hardware Trojans by constructing stealthy triggers consisting of rare events
(e.g., rare signals, states, and transitions). There are promising test
generation-based hardware Trojan detection techniques that rely on the
activation of rare events. In this paper, we investigate rareness reduction as
a design-for-trust solution to make it harder for an adversary to hide Trojans
(easier for Trojan detection). Specifically, we analyze different avenues to
reduce the potential rare trigger cases, including design diversity and area
optimization. While there is a good understanding of the relationship between
area, power, energy, and performance, this research provides a better insight
into the dependency between area and security. Our experimental evaluation
demonstrates that area reduction leads to a reduction in rareness. It also
reveals that reducing rareness leads to faster Trojan detection as well as
improved coverage by Trojan detection methods.
","[{'version': 'v1', 'created': 'Fri, 17 Feb 2023 16:42:11 GMT'}]",2023-02-20,"['Cryptography and Security', 'Hardware Architecture', 'Logic in Computer Science']","This paper presents DETER, a novel design approach to reduce the risk of security vulnerabilities in computing systems. DETER utilizes rareness reduction in cryptography and security, hardware architecture, and logic in computer science to create a secure system design. This approach is based on the concept of trust, which is defined as the assurance that a user or system can trust a given system to protect its data and resources. By reducing the rareness of security vulnerabilities, DETER can help create a secure system architecture that is more resilient to malicious attacks. The paper will discuss the various components of DETER, including its use of cryptography, hardware architecture, and logic. Additionally, the paper will evaluate the effectiveness of the DETER approach for reducing the risk of security vulnerabilities and discuss the potential implications for future system designs.","Write an abstract for a paper called DETER: Design for Trust utilizing Rareness Reduction about Cryptography and Security, Hardware Architecture, Logic in Computer Science"
2209.12142,"Junhao Guo, Zhijian Ji, and Yungang Liu",The topology in the game controllability of multiagent systems,"['math.OC', 'cs.MA']","  In this paper, the graph based condition for the controllability of game
based control system is presented when the control of regulator is not zero. A
control framework which can describe realism well expressed as the game based
control system (GBCS), was obtained in 2019, which, unfortunately, is not graph
theoretically verifiable, and the regulator control input is assumed to be
zero. However, based on a new established notion, strategy matrix, we propose a
graph theory condition to judge the controllability of GBCS, instead of using
algebraic conditions for complex mathematical calculations. More specifically,
to tackle these issues, one needs to study the expression of Nash equilibrium
actions when regulators control is not zero first. Based on this expression,
the general formula of game controllability matrix is obtained, which provides
theoretical support for studying the essential influence of topology on game
based control system. The general formula is always affected by the specific
matrix strategy matrix, composed of Nash equilibrium actions, and the matrix
can not only be obtained by matrix calculation, but also can be directly
written through the topology, which is the specific influence of the topology
on the GBCS. Finally, we obtain the result of judging the controllability of
the system directly according to the topological structure, and put forward the
conjecture that there is no limitation of equivalent partition in GBCS.
Arguably, this is a surprising conjecture on the equivalent partition of
graphs, because only the limitation of equivalent partition in fivenode graphs
has been solved so far
","[{'version': 'v1', 'created': 'Sun, 25 Sep 2022 04:33:56 GMT'}]",2022-09-27,['Multiagent Systems'],"This paper investigates the game controllability of multiagent systems from the perspective of topology. We analyze the controllability of multiagent systems with different topological structures, such as star, ring, and complete graphs. We consider the controllability of multiagent systems under various control strategies, including linear control, distributed control, and decentralized control. We also analyze the impact of different topological structures on the controllability of multiagent systems. Finally, we discuss the implications of our findings on the design of multiagent systems.",Write an abstract for a paper called The topology in the game controllability of multiagent systems about Multiagent Systems
2303.09113,"Lucianna Kiffer, Joachim Neu, Srivatsan Sridhar, Aviv Zohar, David Tse",Security of Blockchains at Capacity,"['cs.CR', 'cs.DC']","  Given a network of nodes with certain communication and computation
capacities, what is the maximum rate at which a blockchain can run securely? We
study this question for proof-of-work (PoW) and proof-of-stake (PoS) longest
chain protocols under a 'bounded bandwidth' model which captures queuing and
processing delays due to high block rate relative to capacity, bursty release
of adversarial blocks, and in PoS, spamming due to equivocations.
  We demonstrate that security of both PoW and PoS longest chain, when
operating at capacity, requires carefully designed scheduling policies that
correctly prioritize which blocks are processed first, as we show attack
strategies tailored to such policies. In PoS, we show an attack exploiting
equivocations, which highlights that the throughput of the PoS longest chain
protocol with a broad class of scheduling policies must decrease as the desired
security error probability decreases. At the same time, through an improved
analysis method, our work is the first to identify block production rates under
which PoW longest chain is secure in the bounded bandwidth setting. We also
present the first PoS longest chain protocol, SaPoS, which is secure with a
block production rate independent of the security error probability, by using
an 'equivocation removal' policy to prevent equivocation spamming.
","[{'version': 'v1', 'created': 'Thu, 16 Mar 2023 07:00:34 GMT'}]",2023-03-17,"['Cryptography and Security', 'Distributed, Parallel, and Cluster Computing']","This paper investigates the security of blockchains at capacity, focusing on cryptography and security, distributed, parallel, and cluster computing. It explores the potential risks associated with blockchain technology and how these risks can be mitigated through the use of distributed, parallel, and cluster computing. It looks at the security of blockchains at different levels, from the protocol level to the application level, and examines how the use of distributed, parallel, and cluster computing can improve the security of the system. It also examines the scalability of blockchains and how the use of distributed, parallel, and cluster computing can help to increase the scalability of the system. Finally, the paper discusses the implications of the security of blockchains at capacity and the potential benefits of distributed, parallel, and cluster computing.","Write an abstract for a paper called Security of Blockchains at Capacity about Cryptography and Security, Distributed, Parallel, and Cluster Computing"
2211.01142,"Yongzhi Su, Yan Di, Fabian Manhardt, Guangyao Zhai, Jason Rambach,
  Benjamin Busam, Didier Stricker, Federico Tombari","OPA-3D: Occlusion-Aware Pixel-Wise Aggregation for Monocular 3D Object
  Detection",['cs.CV'],"  Despite monocular 3D object detection having recently made a significant leap
forward thanks to the use of pre-trained depth estimators for pseudo-LiDAR
recovery, such two-stage methods typically suffer from overfitting and are
incapable of explicitly encapsulating the geometric relation between depth and
object bounding box. To overcome this limitation, we instead propose OPA-3D, a
single-stage, end-to-end, Occlusion-Aware Pixel-Wise Aggregation network that
to jointly estimate dense scene depth with depth-bounding box residuals and
object bounding boxes, allowing a two-stream detection of 3D objects, leading
to significantly more robust detections. Thereby, the geometry stream denoted
as the Geometry Stream, combines visible depth and depth-bounding box residuals
to recover the object bounding box via explicit occlusion-aware optimization.
In addition, a bounding box based geometry projection scheme is employed in an
effort to enhance distance perception. The second stream, named as the Context
Stream, directly regresses 3D object location and size. This novel two-stream
representation further enables us to enforce cross-stream consistency terms
which aligns the outputs of both streams, improving the overall performance.
Extensive experiments on the public benchmark demonstrate that OPA-3D
outperforms state-of-the-art methods on the main Car category, whilst keeping a
real-time inference speed. We plan to release all codes and trained models
soon.
","[{'version': 'v1', 'created': 'Wed, 2 Nov 2022 14:19:13 GMT'}]",2022-11-03,['Computer Vision and Pattern Recognition'],"This paper introduces OPA-3D, a novel occlusion-aware pixel-wise aggregation method for monocular 3D object detection. OPA-3D leverages a deep convolutional neural network (CNN) to extract image features and a novel pixel-wise aggregation module to aggregate pixels in the context of occlusions. Our method is able to detect 3D objects in complex scenes and handle occlusions that are common in real-world scenarios. Experiments on the KITTI and nuScenes datasets demonstrate that OPA-3D outperforms state-of-the-art methods in terms of accuracy, speed, and robustness. The results show that our method is able to significantly improve 3D object detection performance in challenging scenarios.","Write an abstract for a paper called OPA-3D: Occlusion-Aware Pixel-Wise Aggregation for Monocular 3D Object
  Detection about Computer Vision and Pattern Recognition"
2205.03571,Vincent Le Guen,"Deep learning for spatio-temporal forecasting -- application to solar
  energy","['stat.ML', 'cs.AI', 'cs.LG']","  This thesis tackles the subject of spatio-temporal forecasting with deep
learning. The motivating application at Electricity de France (EDF) is
short-term solar energy forecasting with fisheye images. We explore two main
research directions for improving deep forecasting methods by injecting
external physical knowledge. The first direction concerns the role of the
training loss function. We show that differentiable shape and temporal criteria
can be leveraged to improve the performances of existing models. We address
both the deterministic context with the proposed DILATE loss function and the
probabilistic context with the STRIPE model. Our second direction is to augment
incomplete physical models with deep data-driven networks for accurate
forecasting. For video prediction, we introduce the PhyDNet model that
disentangles physical dynamics from residual information necessary for
prediction, such as texture or details. We further propose a learning framework
(APHYNITY) that ensures a principled and unique linear decomposition between
physical and data-driven components under mild assumptions, leading to better
forecasting performances and parameter identification.
","[{'version': 'v1', 'created': 'Sat, 7 May 2022 06:42:48 GMT'}]",2022-05-10,"['Artificial Intelligence', 'Machine Learning']","This paper examines the potential of deep learning for spatio-temporal forecasting of solar energy. Deep learning is a powerful Artificial Intelligence (AI) and Machine Learning (ML) technique that has been used in a variety of applications such as computer vision, natural language processing, and robotics. This paper investigates how deep learning can be used to forecast solar energy at a regional level. Specifically, the paper focuses on the development of a deep learning model that can be used to predict solar energy output in both space and time. The model is evaluated using a case study of solar energy output in the United States. The results show that the deep learning model is able to accurately forecast solar energy output and can be used to inform policy makers and energy providers. The paper concludes by discussing potential applications of deep learning for spatio-temporal forecasting of solar energy.","Write an abstract for a paper called Deep learning for spatio-temporal forecasting -- application to solar
  energy about Artificial Intelligence, Machine Learning"
2303.15041,Amanda Lenzi and Haavard Rue,Towards black-box parameter estimation,"['stat.ML', 'cs.LG']","  Deep learning algorithms have recently shown to be a successful tool in
estimating parameters of statistical models for which simulation is easy, but
likelihood computation is challenging. But the success of these approaches
depends on simulating parameters that sufficiently reproduce the observed data,
and, at present, there is a lack of efficient methods to produce these
simulations. We develop new black-box procedures to estimate parameters of
statistical models based only on weak parameter structure assumptions. For
well-structured likelihoods with frequent occurrences, such as in time series,
this is achieved by pre-training a deep neural network on an extensive
simulated database that covers a wide range of data sizes. For other types of
complex dependencies, an iterative algorithm guides simulations to the correct
parameter region in multiple rounds. These approaches can successfully estimate
and quantify the uncertainty of parameters from non-Gaussian models with
complex spatial and temporal dependencies. The success of our methods is a
first step towards a fully flexible automatic black-box estimation framework.
","[{'version': 'v1', 'created': 'Mon, 27 Mar 2023 09:39:38 GMT'}]",2023-03-28,['Machine Learning'],This paper presents a novel approach to black-box parameter estimation for machine learning models. We introduce a general framework for parameter estimation that is applicable to a variety of model architectures and types of data. We then apply this framework to two distinct machine learning tasks: image classification and natural language processing. We evaluate our approach against existing methods and demonstrate that it achieves superior performance in both tasks. Our results suggest that our approach is a promising alternative to existing methods for parameter estimation in machine learning.,Write an abstract for a paper called Towards black-box parameter estimation about Machine Learning
2002.00863,"Hazem Fahmy, Fabrizio Pastore, Mojtaba Bagherzadeh, Lionel Briand","Supporting DNN Safety Analysis and Retraining through Heatmap-based
  Unsupervised Learning","['cs.SE', 'cs.LG']","  Deep neural networks (DNNs) are increasingly important in safety-critical
systems, for example in their perception layer to analyze images.
Unfortunately, there is a lack of methods to ensure the functional safety of
DNN-based components. We observe three major challenges with existing practices
regarding DNNs in safety-critical systems: (1) scenarios that are
underrepresented in the test set may lead to serious safety violation risks,
but may, however, remain unnoticed; (2) characterizing such high-risk scenarios
is critical for safety analysis; (3) retraining DNNs to address these risks is
poorly supported when causes of violations are difficult to determine. To
address these problems in the context of DNNs analyzing images, we propose
HUDD, an approach that automatically supports the identification of root causes
for DNN errors. HUDD identifies root causes by applying a clustering algorithm
to heatmaps capturing the relevance of every DNN neuron on the DNN outcome.
Also, HUDD retrains DNNs with images that are automatically selected based on
their relatedness to the identified image clusters. We evaluated HUDD with DNNs
from the automotive domain. HUDD was able to identify all the distinct root
causes of DNN errors, thus supporting safety analysis. Also, our retraining
approach has shown to be more effective at improving DNN accuracy than existing
approaches.
","[{'version': 'v1', 'created': 'Mon, 3 Feb 2020 16:16:05 GMT'}, {'version': 'v2', 'created': 'Fri, 29 May 2020 17:29:55 GMT'}, {'version': 'v3', 'created': 'Wed, 21 Oct 2020 09:09:58 GMT'}, {'version': 'v4', 'created': 'Thu, 22 Apr 2021 12:10:27 GMT'}]",2022-10-14,"['Software Engineering', 'Machine Learning']",", and Security

This paper presents a novel approach to support the safety analysis and retraining of deep neural networks (DNNs) through heatmap-based unsupervised learning. It combines the fields of software engineering, machine learning, and security to develop a method to identify potential vulnerabilities in DNNs. The proposed technique utilizes heatmaps to visualize the output of a DNN and identify anomalies and suspicious patterns. We then employ unsupervised learning algorithms to further analyze the heatmap and detect potential security issues. The results demonstrate the effectiveness of the proposed approach in identifying security issues in DNNs. This paper provides a valuable contribution to the field of DNN safety analysis and retraining and can be used to improve the security of DNNs.","Write an abstract for a paper called Supporting DNN Safety Analysis and Retraining through Heatmap-based
  Unsupervised Learning about Software Engineering, Machine Learning"
2112.1108,"Paola Francesca Antonietti, Stefano Berrone, Martina Busetto and Marco
  Verani","Agglomeration-based geometric multigrid schemes for the Virtual Element
  Method","['math.NA', 'cs.NA']","  In this paper we analyse the convergence properties of two-level, W-cycle and
V-cycle agglomeration-based geometric multigrid schemes for the numerical
solution of the linear system of equations stemming from the lowest order
$C^0$-conforming Virtual Element discretization of two-dimensional second-order
elliptic partial differential equations. The sequence of agglomerated
tessellations are nested, but the corresponding multilevel virtual discrete
spaces are generally non-nested thus resulting into non-nested multigrid
algorithms. We prove the uniform convergence of the two-level method with
respect to the mesh size and the uniform convergence of the W-cycle and the
V-cycle multigrid algorithms with respect to the mesh size and the number of
levels. Numerical experiments confirm the theoretical findings.
","[{'version': 'v1', 'created': 'Tue, 21 Dec 2021 10:27:36 GMT'}, {'version': 'v2', 'created': 'Sun, 27 Feb 2022 11:39:23 GMT'}]",2022-03-01,['Numerical Analysis'],"This paper presents an agglomeration-based geometric multigrid (GMG) scheme for the Virtual Element Method (VEM). In particular, we focus on the numerical solution of the Poisson equation in two dimensions, which is a fundamental problem in numerical analysis. The proposed GMG scheme is derived from a multilevel agglomeration strategy that is based on an appropriate refinement of the computational mesh and the definition of a suitable coarse space. The numerical experiments conducted in this paper demonstrate that the proposed GMG scheme is effective and efficient for the numerical solution of the Poisson equation. The results also show that the proposed GMG scheme is robust and converges rapidly.","Write an abstract for a paper called Agglomeration-based geometric multigrid schemes for the Virtual Element
  Method about Numerical Analysis"
2302.0883,Daniel Obmann and Markus Haltmeier,Convergence rates for critical point regularization,"['math.OC', 'cs.NA', 'math.NA']","  Tikhonov regularization involves minimizing the combination of a data
discrepancy term and a regularizing term, and is the standard approach for
solving inverse problems. The use of non-convex regularizers, such as those
defined by trained neural networks, has been shown to be effective in many
cases. However, finding global minimizers in non-convex situations can be
challenging, making existing theory inapplicable. A recent development in
regularization theory relaxes this requirement by providing convergence based
on critical points instead of strict minimizers. This paper investigates
convergence rates for the regularization with critical points using Bregman
distances. Furthermore, we show that when implementing near-minimization
through an iterative algorithm, a finite number of iterations is sufficient
without affecting convergence rates.
","[{'version': 'v1', 'created': 'Fri, 17 Feb 2023 12:03:03 GMT'}]",2023-02-20,['Numerical Analysis'],"This paper studies the convergence rates of critical point regularization for numerical analysis. We consider a family of algorithms that use a critical point as an approximate solution to a given problem. We analyze the convergence rate of these algorithms, and how it depends on the properties of the problem. We also discuss the convergence properties of the algorithms when the problem is ill-conditioned, and the effect of the regularization parameter on the convergence rate. The results of this paper provide insight into the behavior of critical point regularization for numerical analysis, and can be used to guide the choice of regularization parameters for a given problem.",Write an abstract for a paper called Convergence rates for critical point regularization about Numerical Analysis
2211.14144,"Can Chen, Scott T. Weiss, Yang-Yu Liu","Graph Convolutional Network-based Feature Selection for High-dimensional
  and Low-sample Size Data",['cs.LG'],"  Feature selection is a powerful dimension reduction technique which selects a
subset of relevant features for model construction. Numerous feature selection
methods have been proposed, but most of them fail under the high-dimensional
and low-sample size (HDLSS) setting due to the challenge of overfitting. In
this paper, we present a deep learning-based method - GRAph Convolutional
nEtwork feature Selector (GRACES) - to select important features for HDLSS
data. We demonstrate empirical evidence that GRACES outperforms other feature
selection methods on both synthetic and real-world datasets.
","[{'version': 'v1', 'created': 'Fri, 25 Nov 2022 14:46:36 GMT'}]",2022-11-28,['Machine Learning'],"This paper aims to discuss the use of graph convolutional networks (GCNs) for feature selection in high-dimensional and low-sample size data about machine learning. Feature selection is a critical step in machine learning, as it eliminates irrelevant or redundant features, thereby improving the performance of the model. GCNs are a type of deep learning model that has been shown to be effective in many tasks, including feature selection. In this paper, we propose a novel approach to feature selection using GCNs. We evaluate the proposed method on a variety of datasets and show that it outperforms traditional methods. Additionally, we discuss the implications of our results and provide a comparison of our method to existing methods. Finally, we discuss future directions for research.","Write an abstract for a paper called Graph Convolutional Network-based Feature Selection for High-dimensional
  and Low-sample Size Data about Machine Learning"
2303.07875,"E. Subramanian, M. Mithun Karthik, G Prem Krishna, D. Vaisnav Prasath,
  V. Sukesh Kumar",Solar Power Prediction Using Machine Learning,['cs.LG'],"  This paper presents a machine learning-based approach for predicting solar
power generation with high accuracy using a 99% AUC (Area Under the Curve)
metric. The approach includes data collection, pre-processing, feature
selection, model selection, training, evaluation, and deployment. High-quality
data from multiple sources, including weather data, solar irradiance data, and
historical solar power generation data, are collected and pre-processed to
remove outliers, handle missing values, and normalize the data. Relevant
features such as temperature, humidity, wind speed, and solar irradiance are
selected for model training. Support Vector Machines (SVM), Random Forest, and
Gradient Boosting are used as machine learning algorithms to produce accurate
predictions. The models are trained on a large dataset of historical solar
power generation data and other relevant features. The performance of the
models is evaluated using AUC and other metrics such as precision, recall, and
F1-score. The trained machine learning models are then deployed in a production
environment, where they can be used to make real-time predictions about solar
power generation. The results show that the proposed approach achieves a 99%
AUC for solar power generation prediction, which can help energy companies
better manage their solar power systems, reduce costs, and improve energy
efficiency.
","[{'version': 'v1', 'created': 'Sat, 11 Mar 2023 06:31:46 GMT'}]",2023-03-15,['Machine Learning'],"This paper presents a machine learning-based approach for predicting solar power output. The proposed model is based on a multi-layer perceptron (MLP) neural network and is trained using a dataset of historical solar power output, meteorological data and other relevant features. The model is evaluated using a cross-validation approach on a real-world dataset, and the results show that the proposed model is capable of accurately predicting solar power output. The paper also discusses several potential applications of the proposed model, such as energy management and forecasting. Finally, the paper provides a discussion of the potential challenges and future directions for solar power prediction using machine learning.",Write an abstract for a paper called Solar Power Prediction Using Machine Learning about Machine Learning
2212.10787,"Naoki Wake, Atsushi Kanehira, Kazuhiro Sasabuchi, Jun Takamatsu,
  Katsushi Ikeuchi",Interactive Task Encoding System for Learning-from-Observation,['cs.RO'],"  We introduce a practical pipeline that interactively encodes multimodal human
demonstrations for robot teaching. This pipeline is designed as an input system
for a framework called Learning-from-Observation (LfO), which aims to program
household robots with manipulative tasks through few-shots human demonstration
without coding. While most previous LfO systems run with visual demonstration,
recent research on robot teaching has shown the effectiveness of verbal
instruction in making recognition robust and teaching interactive. To the best
of our knowledge, however, no LfO system has yet been proposed that utilizes
both verbal instruction and interaction, namely \textit{multimodal LfO}. This
paper proposes the interactive task encoding system (ITES) as an input pipeline
for multimodal LfO. ITES assumes that the user teaches step-by-step, pausing
hand movements in order to match the granularity of human instructions with the
granularity of robot execution. ITES recognizes tasks based on step-by-step
verbal instructions that accompany the hand movements. Additionally, the
recognition is made robust through interactions with the user. We test ITES on
a real robot and show that the user can successfully teach multiple operations
through multimodal demonstrations. The results suggest the usefulness of ITES
for multimodal LfO. The source code is available at
https://github.com/microsoft/symbolic-robot-teaching-interface.
","[{'version': 'v1', 'created': 'Wed, 21 Dec 2022 06:11:28 GMT'}, {'version': 'v2', 'created': 'Wed, 25 Jan 2023 06:16:30 GMT'}]",2023-01-26,['Robotics'],"This paper presents an interactive task encoding system for robots that enables them to learn from observation. The proposed system uses a combination of computer vision and reinforcement learning algorithms to enable robots to learn from their environment and to understand tasks by observing demonstrations of them. The system consists of a task encoding layer and a reinforcement learning layer. The task encoding layer consists of a set of task-specific features which are used to encode a task, and the reinforcement learning layer uses these features to learn from the observations. Experiments with a simulated robot demonstrate the effectiveness of the proposed system in learning from observation. The results show that the proposed system is able to learn from observation and can successfully encode tasks to enable robots to learn from their environment.",Write an abstract for a paper called Interactive Task Encoding System for Learning-from-Observation about Robotics
2211.15613,"Yang Chen, Chao Jiang, Alan Ritter, Wei Xu",Frustratingly Easy Label Projection for Cross-lingual Transfer,"['cs.CL', 'cs.AI']","  Translating training data into many languages has emerged as a practical
solution for improving cross-lingual transfer. For tasks that involve
span-level annotations, such as information extraction or question answering,
an additional label projection step is required to map annotated spans onto the
translated texts. Recently, a few efforts have utilized a simple
mark-then-translate method to jointly perform translation and projection by
inserting special markers around the labeled spans in the original sentence.
However, as far as we are aware, no empirical analysis has been conducted on
how this approach compares to traditional annotation projection based on word
alignment. In this paper, we present an extensive empirical study across 42
languages and three tasks (QA, NER, and Event Extraction) to evaluate the
effectiveness and limitations of both methods, filling an important gap in the
literature. Experimental results show that our optimized version of
mark-then-translate, which we call EasyProject, is easily applied to many
languages and works surprisingly well, outperforming the more complex word
alignment-based methods. We analyze several key factors that affect end-task
performance, and show EasyProject works well because it can accurately preserve
label span boundaries after translation. We will publicly release all our code
and data.
","[{'version': 'v1', 'created': 'Mon, 28 Nov 2022 18:11:48 GMT'}, {'version': 'v2', 'created': 'Sun, 4 Dec 2022 17:34:56 GMT'}]",2022-12-06,"['Computation and Language', 'Artificial Intelligence']","This paper examines the problem of cross-lingual transfer in the field of Computation and Language, Artificial Intelligence (AI). It proposes a novel approach to label projection, which is a method of transferring labels from one language to another. The proposed approach is based on a combination of existing techniques, including the use of machine learning algorithms and natural language processing (NLP) tools. The proposed approach is evaluated on multiple datasets and compared to existing approaches. Results show that the proposed approach is able to achieve better performance than existing methods, with significant improvements in accuracy and efficiency. The proposed approach also offers a promising solution to the problem of cross-lingual transfer in AI and Computation and Language.","Write an abstract for a paper called Frustratingly Easy Label Projection for Cross-lingual Transfer about Computation and Language, Artificial Intelligence"
2212.03422,Weidong Wang and Jing Yang,"Generalized Companion Subresultants of Several Univariate Polynomials in
  Newton Basis","['cs.SC', 'math.AC']","  In this paper, the concept of companion subresultant for polynomials in power
basis is extended to that for polynomials in Newton basis and an explicit
formula in the form of determinental polynomial is developed for the
generalized companion subresultant. It is noted that the resulting subresultant
is expressed with the same Newton basis as the input polynomials and it shares
the essential property with the subresultant in power basis.
","[{'version': 'v1', 'created': 'Wed, 7 Dec 2022 03:09:23 GMT'}]",2022-12-08,['Symbolic Computation'],"This paper presents a new algorithm for computing the generalized companion subresultants of several univariate polynomials in Newton basis. The algorithm is based on the symbolic computation of the coefficients of the polynomials, which are used to construct a matrix of subresultants. The generalized companion subresultants are then computed from the matrix. The algorithm is implemented in the Maple computer algebra system and its performance is evaluated. The results demonstrate that the algorithm is faster than existing methods for computing generalized companion subresultants. Furthermore, the algorithm is applicable to polynomials of any degree and can be used for any number of polynomials.","Write an abstract for a paper called Generalized Companion Subresultants of Several Univariate Polynomials in
  Newton Basis about Symbolic Computation"
2203.09268,"Stefano B. Blumberg and Hongxiang Lin and Francesco Grussu and Yukun
  Zhou and Matteo Figini and Daniel C. Alexander","Progressive Subsampling for Oversampled Data - Application to
  Quantitative MRI","['eess.IV', 'cs.CV', 'cs.LG', 'q-bio.NC']","  We present PROSUB: PROgressive SUBsampling, a deep learning based, automated
methodology that subsamples an oversampled data set (e.g. multi-channeled 3D
images) with minimal loss of information. We build upon a recent dual-network
approach that won the MICCAI MUlti-DIffusion (MUDI) quantitative MRI
measurement sampling-reconstruction challenge, but suffers from deep learning
training instability, by subsampling with a hard decision boundary. PROSUB uses
the paradigm of recursive feature elimination (RFE) and progressively
subsamples measurements during deep learning training, improving optimization
stability. PROSUB also integrates a neural architecture search (NAS) paradigm,
allowing the network architecture hyperparameters to respond to the subsampling
process. We show PROSUB outperforms the winner of the MUDI MICCAI challenge,
producing large improvements >18% MSE on the MUDI challenge sub-tasks and
qualitative improvements on downstream processes useful for clinical
applications. We also show the benefits of incorporating NAS and analyze the
effect of PROSUB's components. As our method generalizes to other problems
beyond MRI measurement selection-reconstruction, our code is
https://github.com/sbb-gh/PROSUB
","[{'version': 'v1', 'created': 'Thu, 17 Mar 2022 11:44:07 GMT'}, {'version': 'v2', 'created': 'Fri, 8 Apr 2022 18:16:59 GMT'}, {'version': 'v3', 'created': 'Tue, 5 Jul 2022 17:07:16 GMT'}, {'version': 'v4', 'created': 'Sun, 9 Oct 2022 12:42:10 GMT'}, {'version': 'v5', 'created': 'Tue, 11 Oct 2022 09:21:56 GMT'}]",2022-10-12,"['Computer Vision and Pattern Recognition', 'Machine Learning']","This paper presents a novel progressive subsampling technique for oversampled data, with a particular focus on its application to quantitative magnetic resonance imaging (qMRI). We propose a progressive subsampling strategy that is based on a combination of computer vision and pattern recognition methods, as well as machine learning techniques. The proposed technique is applied to qMRI data to reduce the amount of data required for analysis. We demonstrate that the proposed technique can produce accurate results with minimal subsampling, and that it can be used to improve the accuracy of quantitative MRI analysis. We also present results that show the effectiveness of our proposed strategy in comparison to existing methods. Finally, we discuss potential future directions for the application of progressive subsampling to other types of data.","Write an abstract for a paper called Progressive Subsampling for Oversampled Data - Application to
  Quantitative MRI about Computer Vision and Pattern Recognition, Machine Learning"
2202.11632,"Nuri Mert Vural, Lu Yu, Krishnakumar Balasubramanian, Stanislav
  Volgushev, Murat A. Erdogdu","Mirror Descent Strikes Again: Optimal Stochastic Convex Optimization
  under Infinite Noise Variance","['stat.ML', 'cs.LG', 'math.OC']","  We study stochastic convex optimization under infinite noise variance.
Specifically, when the stochastic gradient is unbiased and has uniformly
bounded $(1+\kappa)$-th moment, for some $\kappa \in (0,1]$, we quantify the
convergence rate of the Stochastic Mirror Descent algorithm with a particular
class of uniformly convex mirror maps, in terms of the number of iterations,
dimensionality and related geometric parameters of the optimization problem.
Interestingly this algorithm does not require any explicit gradient clipping or
normalization, which have been extensively used in several recent empirical and
theoretical works. We complement our convergence results with
information-theoretic lower bounds showing that no other algorithm using only
stochastic first-order oracles can achieve improved rates. Our results have
several interesting consequences for devising online/streaming stochastic
approximation algorithms for problems arising in robust statistics and machine
learning.
","[{'version': 'v1', 'created': 'Wed, 23 Feb 2022 17:08:40 GMT'}]",2022-02-24,['Machine Learning'],"This paper presents a novel approach to stochastic convex optimization in machine learning. We explore the use of Mirror Descent (MD) for optimization under infinite noise variance. We analyze the theoretical properties of MD and compare it to existing optimization algorithms. We further demonstrate its effectiveness on a variety of machine learning tasks, including classification, regression, and clustering. Our results show that MD can achieve better performance than existing methods in terms of both accuracy and speed. Furthermore, we provide a range of practical insights on how to optimize MD for different scenarios. We conclude that MD is an effective optimization algorithm for machine learning applications and can be used to achieve optimal performance.","Write an abstract for a paper called Mirror Descent Strikes Again: Optimal Stochastic Convex Optimization
  under Infinite Noise Variance about Machine Learning"
2206.13156,"Yushan Zheng, Jun Li, Jun Shi, Fengying Xie, Zhiguo Jiang","Kernel Attention Transformer (KAT) for Histopathology Whole Slide Image
  Classification",['cs.CV'],"  Transformer has been widely used in histopathology whole slide image (WSI)
classification for the purpose of tumor grading, prognosis analysis, etc.
However, the design of token-wise self-attention and positional embedding
strategy in the common Transformer limits the effectiveness and efficiency in
the application to gigapixel histopathology images. In this paper, we propose a
kernel attention Transformer (KAT) for histopathology WSI classification. The
information transmission of the tokens is achieved by cross-attention between
the tokens and a set of kernels related to a set of positional anchors on the
WSI. Compared to the common Transformer structure, the proposed KAT can better
describe the hierarchical context information of the local regions of the WSI
and meanwhile maintains a lower computational complexity. The proposed method
was evaluated on a gastric dataset with 2040 WSIs and an endometrial dataset
with 2560 WSIs, and was compared with 6 state-of-the-art methods. The
experimental results have demonstrated the proposed KAT is effective and
efficient in the task of histopathology WSI classification and is superior to
the state-of-the-art methods. The code is available at
https://github.com/zhengyushan/kat.
","[{'version': 'v1', 'created': 'Mon, 27 Jun 2022 10:00:12 GMT'}]",2022-06-28,['Computer Vision and Pattern Recognition'],"This paper presents a novel Kernel Attention Transformer (KAT) for histopathology whole slide image (WSI) classification. KAT is a deep learning architecture that combines the advantages of convolutional neural networks (CNNs) and self-attention mechanisms. It is designed to capture both global and local features from WSIs and to improve the classification accuracy of the model. The proposed KAT model consists of a convolutional layer, a self-attention layer, and a fully-connected layer. Experiments conducted on two publicly available datasets show that the proposed KAT model outperforms existing CNN-based models in terms of accuracy and F1-score. Furthermore, the results demonstrate that the proposed KAT model is capable of capturing global and local features from WSIs, thereby improving the performance of the model. The results of this study have implications for computer vision and pattern recognition applications in medical imaging.","Write an abstract for a paper called Kernel Attention Transformer (KAT) for Histopathology Whole Slide Image
  Classification about Computer Vision and Pattern Recognition"
2203.16421,"Hanxiao Jiang, Yongsen Mao, Manolis Savva, Angel X. Chang",OPD: Single-view 3D Openable Part Detection,['cs.CV'],"  We address the task of predicting what parts of an object can open and how
they move when they do so. The input is a single image of an object, and as
output we detect what parts of the object can open, and the motion parameters
describing the articulation of each openable part. To tackle this task, we
create two datasets of 3D objects: OPDSynth based on existing synthetic
objects, and OPDReal based on RGBD reconstructions of real objects. We then
design OPDRCNN, a neural architecture that detects openable parts and predicts
their motion parameters. Our experiments show that this is a challenging task
especially when considering generalization across object categories, and the
limited amount of information in a single image. Our architecture outperforms
baselines and prior work especially for RGB image inputs. Short video summary
at https://www.youtube.com/watch?v=P85iCaD0rfc
","[{'version': 'v1', 'created': 'Wed, 30 Mar 2022 16:02:19 GMT'}]",2022-03-31,['Computer Vision and Pattern Recognition'],"This paper presents a novel method for detecting 3D openable parts from a single view image using computer vision and pattern recognition. The proposed method, called OPD, is capable of detecting 3D openable parts from a single view image without the need for multiple views or the manual annotation of the parts. OPD utilizes a convolutional neural network (CNN) to extract the features of the 3D openable parts from the single view image and then a support vector machine (SVM) to classify the parts. The OPD method is evaluated on a dataset of 3D openable parts from a variety of objects and is shown to achieve a high accuracy in detecting the parts. The results of the experiments demonstrate the effectiveness of the proposed method for 3D openable part detection from a single view image.",Write an abstract for a paper called OPD: Single-view 3D Openable Part Detection about Computer Vision and Pattern Recognition
2202.0045,"Liang Liao, Sen Lin, Lun Li, Xiuwei Zhang, Song Zhao, Yan Wang,
  Xinqiang Wang, Qi Gao, Jingyu Wang","Approximation of Images via Generalized Higher Order Singular Value
  Decomposition over Finite-dimensional Commutative Semisimple Algebra","['cs.LG', 'cs.IT', 'math.AC', 'math.IT', 'math.RT', 'math.SP']","  Low-rank approximation of images via singular value decomposition is
well-received in the era of big data. However, singular value decomposition
(SVD) is only for order-two data, i.e., matrices. It is necessary to flatten a
higher order input into a matrix or break it into a series of order-two slices
to tackle higher order data such as multispectral images and videos with the
SVD. Higher order singular value decomposition (HOSVD) extends the SVD and can
approximate higher order data using sums of a few rank-one components. We
consider the problem of generalizing HOSVD over a finite dimensional
commutative algebra. This algebra, referred to as a t-algebra, generalizes the
field of complex numbers. The elements of the algebra, called t-scalars, are
fix-sized arrays of complex numbers. One can generalize matrices and tensors
over t-scalars and then extend many canonical matrix and tensor algorithms,
including HOSVD, to obtain higher-performance versions. The generalization of
HOSVD is called THOSVD. Its performance of approximating multi-way data can be
further improved by an alternating algorithm. THOSVD also unifies a wide range
of principal component analysis algorithms. To exploit the potential of
generalized algorithms using t-scalars for approximating images, we use a pixel
neighborhood strategy to convert each pixel to ""deeper-order"" t-scalar.
Experiments on publicly available images show that the generalized algorithm
over t-scalars, namely THOSVD, compares favorably with its canonical
counterparts.
","[{'version': 'v1', 'created': 'Tue, 1 Feb 2022 15:01:12 GMT'}, {'version': 'v2', 'created': 'Thu, 3 Feb 2022 02:48:54 GMT'}, {'version': 'v3', 'created': 'Tue, 29 Mar 2022 05:33:29 GMT'}, {'version': 'v4', 'created': 'Wed, 4 May 2022 13:49:54 GMT'}, {'version': 'v5', 'created': 'Thu, 5 May 2022 15:37:07 GMT'}, {'version': 'v6', 'created': 'Wed, 18 May 2022 16:11:38 GMT'}, {'version': 'v7', 'created': 'Fri, 3 Jun 2022 03:25:42 GMT'}, {'version': 'v8', 'created': 'Thu, 25 Aug 2022 09:46:45 GMT'}]",2022-08-26,"['Machine Learning', 'Information Theory']",", and Algebra

This paper presents a novel approach to approximating images using Generalized Higher Order Singular Value Decomposition (GHOSVD) over finite-dimensional commutative semisimple algebra. The proposed method is based on the combination of machine learning, information theory and algebra. The paper focuses on the theoretical foundations of GHOSVD and its applications to image approximation. Firstly, we discuss the mathematical background of GHOSVD and its properties. Then, we analyze the advantages of GHOSVD over other existing methods. We also discuss the implementation of the proposed method and its results on a variety of image datasets. Finally, we discuss the potential applications of GHOSVD in image processing and other related fields. The results of our experiments demonstrate that GHOSVD is an effective approach for approximating images with high accuracy.","Write an abstract for a paper called Approximation of Images via Generalized Higher Order Singular Value
  Decomposition over Finite-dimensional Commutative Semisimple Algebra about Machine Learning, Information Theory"
2104.05169,"Wenjun Jiang, Mingyang Yue, Xiaojun Yuan, and Yong Zuo","MIMO-OFDM-Based Massive Connectivity With Frequency Selectivity
  Compensation","['cs.IT', 'math.IT']","  In this paper, we study how to efficiently and reliably detect active devices
and estimate their channels in a multiple-input multiple-output (MIMO)
orthogonal frequency-division multiplexing (OFDM) based grant-free
non-orthogonal multiple access (NOMA) system to enable massive machine-type
communications (mMTC). First, by exploiting the correlation of the channel
frequency responses in narrow-band mMTC, we propose a block-wise linear channel
model. Specifically, the continuous OFDM subcarriers in the narrow-band are
divided into several sub-blocks and a linear function with only two variables
(mean and slope) is used to approximate the frequency-selective channel in each
sub-block. This significantly reduces the number of variables to be determined
in channel estimation and the sub-block number can be adjusted to reliably
compensate the channel frequency-selectivity. Second, we formulate the joint
active device detection and channel estimation in the block-wise linear system
as a Bayesian inference problem. By exploiting the block-sparsity of the
channel matrix, we develop an efficient turbo message passing (Turbo-MP)
algorithm to resolve the Bayesian inference problem with near-linear
complexity. We further incorporate machine learning approaches into Turbo-MP to
learn unknown prior parameters. Numerical results demonstrate the superior
performance of the proposed algorithm over state-of-the-art algorithms.
","[{'version': 'v1', 'created': 'Mon, 12 Apr 2021 02:53:38 GMT'}]",2022-11-14,['Information Theory'],"This paper presents a novel approach to increase the connectivity of Massive Multiple-Input Multiple-Output Orthogonal Frequency Division Multiplexing (MIMO-OFDM) systems. The proposed approach utilizes a frequency selectivity compensation technique to compensate for the frequency-selectivity of the channel and enable massive connectivity. The paper also presents a theoretical analysis of the proposed approach, which shows that the proposed technique can effectively increase the connectivity of the MIMO-OFDM system. Furthermore, the paper provides simulation results to demonstrate the performance of the proposed approach. The results show that the proposed approach can effectively increase the connectivity of the MIMO-OFDM system and provide better performance than existing approaches.","Write an abstract for a paper called MIMO-OFDM-Based Massive Connectivity With Frequency Selectivity
  Compensation about Information Theory"
2302.10093,"Don Kurian Dennis, Abhishek Shetty, Anish Sevekari, Kazuhito Koishida,
  Virginia Smith","Progressive Knowledge Distillation: Building Ensembles for Efficient
  Inference",['cs.LG'],"  We study the problem of progressive distillation: Given a large, pre-trained
teacher model $g$, we seek to decompose the model into an ensemble of smaller,
low-inference cost student models $f_i$. The resulting ensemble allows for
flexibly tuning accuracy vs. inference cost, which is useful for a number of
applications in on-device inference. The method we propose, B-DISTIL, relies on
an algorithmic procedure that uses function composition over intermediate
activations to construct expressive ensembles with similar performance as $g$,
but with much smaller student models. We demonstrate the effectiveness of \algA
by decomposing pretrained models across standard image, speech, and sensor
datasets. We also provide theoretical guarantees for our method in terms of
convergence and generalization.
","[{'version': 'v1', 'created': 'Mon, 20 Feb 2023 16:57:44 GMT'}]",2023-02-21,['Machine Learning'],"This paper presents a novel method for building ensembles for efficient inference about machine learning. The method, called progressive knowledge distillation, is based on a combination of model compression and model ensembling techniques. It involves training multiple models on the same data, and then progressively distilling the knowledge from the models into a single, more efficient model. This method has the potential to improve the accuracy, speed, and scalability of machine learning models. The paper evaluates the performance of this approach on a range of real-world datasets, and demonstrates its effectiveness in improving the accuracy and speed of inference. The results show that progressive knowledge distillation can significantly reduce the computational resources required to perform inference, while still preserving accuracy. This paper provides a valuable new approach to efficient inference about machine learning, and has the potential to significantly improve the scalability of machine learning models.","Write an abstract for a paper called Progressive Knowledge Distillation: Building Ensembles for Efficient
  Inference about Machine Learning"
2210.0592,"Zhichun Guo, Chunhui Zhang, Yujie Fan, Yijun Tian, Chuxu Zhang, Nitesh
  Chawla",Boosting Graph Neural Networks via Adaptive Knowledge Distillation,['cs.LG'],"  Graph neural networks (GNNs) have shown remarkable performance on diverse
graph mining tasks. Although different GNNs can be unified as the same message
passing framework, they learn complementary knowledge from the same graph.
Knowledge distillation (KD) is developed to combine the diverse knowledge from
multiple models. It transfers knowledge from high-capacity teachers to a
lightweight student. However, to avoid oversmoothing, GNNs are often shallow,
which deviates from the setting of KD. In this context, we revisit KD by
separating its benefits from model compression and emphasizing its power of
transferring knowledge. To this end, we need to tackle two challenges: how to
transfer knowledge from compact teachers to a student with the same capacity;
and, how to exploit student GNN's own strength to learn knowledge. In this
paper, we propose a novel adaptive KD framework, called BGNN, which
sequentially transfers knowledge from multiple GNNs into a student GNN. We also
introduce an adaptive temperature module and a weight boosting module. These
modules guide the student to the appropriate knowledge for effective learning.
Extensive experiments have demonstrated the effectiveness of BGNN. In
particular, we achieve up to 3.05% improvement for node classification and
6.35% improvement for graph classification over vanilla GNNs.
","[{'version': 'v1', 'created': 'Wed, 12 Oct 2022 04:48:50 GMT'}, {'version': 'v2', 'created': 'Wed, 5 Apr 2023 02:00:19 GMT'}]",2023-04-06,['Machine Learning'],"This paper presents a novel approach to boost the performance of Graph Neural Networks (GNNs) via Adaptive Knowledge Distillation (AKD). AKD is an effective knowledge distillation strategy which dynamically adjusts the knowledge distillation rate according to the performance of the student model. We demonstrate the effectiveness of AKD on three GNNs: Graph Convolutional Network (GCN), Graph Attention Network (GAT) and Relational Graph Convolutional Network (RGCN). Experiments on several public datasets show that our approach can effectively boost the performance of GNNs. Furthermore, we provide an in-depth analysis of the effectiveness of AKD on different GNNs. The results show that AKD can improve the generalization performance of GNNs, especially when the amount of training data is limited. We also discuss the limitations of our approach and propose potential directions for future work.",Write an abstract for a paper called Boosting Graph Neural Networks via Adaptive Knowledge Distillation about Machine Learning
2205.05611,"Truc Nguyen, Phuc Thai, Tre' R. Jeter, Thang N. Dinh, My T. Thai",Blockchain-based Secure Client Selection in Federated Learning,"['cs.CR', 'cs.LG']","  Despite the great potential of Federated Learning (FL) in large-scale
distributed learning, the current system is still subject to several privacy
issues due to the fact that local models trained by clients are exposed to the
central server. Consequently, secure aggregation protocols for FL have been
developed to conceal the local models from the server. However, we show that,
by manipulating the client selection process, the server can circumvent the
secure aggregation to learn the local models of a victim client, indicating
that secure aggregation alone is inadequate for privacy protection. To tackle
this issue, we leverage blockchain technology to propose a verifiable client
selection protocol. Owing to the immutability and transparency of blockchain,
our proposed protocol enforces a random selection of clients, making the server
unable to control the selection process at its discretion. We present security
proofs showing that our protocol is secure against this attack. Additionally,
we conduct several experiments on an Ethereum-like blockchain to demonstrate
the feasibility and practicality of our solution.
","[{'version': 'v1', 'created': 'Wed, 11 May 2022 16:28:12 GMT'}]",2022-05-12,"['Cryptography and Security', 'Machine Learning']","This paper investigates the use of blockchain-based secure client selection in federated learning for cryptography and security, machine learning. Federated learning is a distributed machine learning approach that enables multiple clients to collaboratively learn a shared model while keeping their training data private. In federated learning, secure client selection is critical for ensuring the accuracy and security of the model. This paper proposes a blockchain-based secure client selection scheme for federated learning. The scheme utilizes a blockchain network to securely select clients for federated learning and to ensure the accuracy and security of the model. The proposed scheme is evaluated using a real-world dataset, and the results demonstrate its effectiveness in selecting secure clients for federated learning. The paper also provides insights into the potential of blockchain-based secure client selection for cryptography and security, machine learning.","Write an abstract for a paper called Blockchain-based Secure Client Selection in Federated Learning about Cryptography and Security, Machine Learning"
2107.0314,Bahram Sadeghi Bigham,Minimum Constraint Removal Problem for Line Segments is NP-hard,"['cs.CG', 'cs.RO']","  In the minimum constraint removal ($MCR$), there is no feasible path to move
from the starting point towards the goal and, the minimum constraints should be
removed in order to find a collision-free path. It has been proved that $MCR$
problem is $NP-hard$ when constraints have arbitrary shapes or even they are in
shape of convex polygons. However, it has a simple linear solution when
constraints are lines and the problem is open for other cases yet. In this
paper, using a reduction from Subset Sum problem, in three steps, we show that
the problem is NP-hard for both weighted and unweighted line segments.
","[{'version': 'v1', 'created': 'Wed, 7 Jul 2021 10:57:22 GMT'}]",2023-02-21,"['Computational Geometry', 'Robotics']","This paper presents a new NP-hard problem in computational geometry and robotics, called the Minimum Constraint Removal Problem for Line Segments (MCRLSP). The MCRLSP is defined as the problem of determining the minimum number of line segments that must be removed from a given set of line segments in order to make the remaining line segments non-intersecting. We prove that the MCRLSP is NP-hard by providing a polynomial-time reduction from the 3-dimensional matching problem. We also provide an algorithm for solving the MCRLSP in polynomial time in the special case when the given set of line segments is planar. Finally, we discuss the implications of our results on the application of MCRLSP in robotics.","Write an abstract for a paper called Minimum Constraint Removal Problem for Line Segments is NP-hard about Computational Geometry, Robotics"
2202.11079,"Mirco Mutti, Stefano Del Col, Marcello Restelli",Reward-Free Policy Space Compression for Reinforcement Learning,['cs.LG'],"  In reinforcement learning, we encode the potential behaviors of an agent
interacting with an environment into an infinite set of policies, the policy
space, typically represented by a family of parametric functions. Dealing with
such a policy space is a hefty challenge, which often causes sample and
computation inefficiencies. However, we argue that a limited number of policies
are actually relevant when we also account for the structure of the environment
and of the policy parameterization, as many of them would induce very similar
interactions, i.e., state-action distributions. In this paper, we seek for a
reward-free compression of the policy space into a finite set of representative
policies, such that, given any policy $\pi$, the minimum R\'enyi divergence
between the state-action distributions of the representative policies and the
state-action distribution of $\pi$ is bounded. We show that this compression of
the policy space can be formulated as a set cover problem, and it is inherently
NP-hard. Nonetheless, we propose a game-theoretic reformulation for which a
locally optimal solution can be efficiently found by iteratively stretching the
compressed space to cover an adversarial policy. Finally, we provide an
empirical evaluation to illustrate the compression procedure in simple domains,
and its ripple effects in reinforcement learning.
","[{'version': 'v1', 'created': 'Tue, 22 Feb 2022 18:11:57 GMT'}]",2022-02-23,['Machine Learning'],"This paper presents a reward-free policy space compression approach for reinforcement learning in machine learning. The proposed approach is based on the idea of compressing the space of policies by learning from a dataset of past observations without the need for rewards. The approach is demonstrated on two reinforcement learning tasks, a two-player game and a robotic manipulation task. Results show that the reward-free policy space compression approach can reduce the number of policies needed to solve the tasks, while still achieving the same level of performance. The paper further discusses the implications of the proposed approach, such as its potential to reduce the computational complexity of reinforcement learning, and its ability to enable the exploration of more complex policies.",Write an abstract for a paper called Reward-Free Policy Space Compression for Reinforcement Learning about Machine Learning
2303.04,"Johannes Janicka, Paulo Debiagi, Arne Scholtissek, Andreas Dreizler,
  Bernd Epple, Reiner Pawellek, Alexander Maltsev, Christian Hasse","The potential of retrofitting existing coal power plants: a case study
  for operation with green iron","['eess.SY', 'cs.SY', 'physics.flu-dyn']","  Storing electrical energy for long periods and transporting it over long
distances is an essential task of the necessary transition to a CO$_2$-free
energy economy. An oxidation-reduction cycle based on iron and its oxides
represents a very promising technology in this regard. The present work
assesses the potential of converting an existing modern coal-fired power plant
to operation with iron. For this purpose, a systematic retrofit study is
carried out, employing a model that balances all material and energy fluxes in
a state-of-the-art coal-fired power plant. Particular attention is given to
components of the burner system and the system$'$s heat exchanger. The analysis
provides evidence that main components such as the steam generator and steam
cycle can be reused with moderate modifications. Major modifications are
related to the larger amounts of solids produced during iron combustion, for
instance in the particle feeding and removal systems. Since the high particle
densities and lower demand for auxiliary systems improve the heat transfer, the
net efficiencies of iron operation can be one to two percentage points better
than coal operation, depending on operating conditions. This new insight can
significantly accelerate the introduction of this innovative technology by
guiding future research and the development of the retrofit option.
","[{'version': 'v1', 'created': 'Tue, 7 Mar 2023 16:00:05 GMT'}, {'version': 'v2', 'created': 'Mon, 27 Mar 2023 14:05:03 GMT'}]",2023-03-28,['Systems and Control'],"This paper investigates the potential of retrofitting existing coal power plants to operate with green iron. The study focuses on the application of Systems and Control theory to understand the challenges and opportunities of retrofitting existing coal power plants. The paper presents a case study of a retrofitting project in a large coal power plant in the United States. The case study examines the feasibility of retrofitting the plant's existing infrastructure and operation, and the potential for the plant to operate with green iron. The paper also discusses the challenges and opportunities associated with retrofitting existing coal power plants, and provides recommendations for future research in this area. The findings of this study suggest that retrofitting existing coal power plants is a viable option for transitioning to green iron, and that Systems and Control theory can play a key role in helping to ensure successful retrofitting projects.","Write an abstract for a paper called The potential of retrofitting existing coal power plants: a case study
  for operation with green iron about Systems and Control"
2302.11527,"K\'evin Planolles, Marc Chaumont, Fr\'ed\'eric Comby","A study on the invariance in security whatever the dimension of images
  for the steganalysis by deep-learning","['cs.CV', 'cs.CR']","  In this paper, we study the performance invariance of convolutional neural
networks when confronted with variable image sizes in the context of a more
""wild steganalysis"". First, we propose two algorithms and definitions for a
fine experimental protocol with datasets owning ""similar difficulty"" and
""similar security"". The ""smart crop 2"" algorithm allows the introduction of the
Nearly Nested Image Datasets (NNID) that ensure ""a similar difficulty"" between
various datasets, and a dichotomous research algorithm allows a ""similar
security"". Second, we show that invariance does not exist in state-of-the-art
architectures. We also exhibit a difference in behavior depending on whether we
test on images larger or smaller than the training images. Finally, based on
the experiments, we propose to use the dilated convolution which leads to an
improvement of a state-of-the-art architecture.
","[{'version': 'v1', 'created': 'Wed, 22 Feb 2023 18:09:15 GMT'}]",2023-02-23,"['Computer Vision and Pattern Recognition', 'Cryptography and Security']","This paper presents a study on the invariance of security in steganalysis by deep-learning for computer vision and pattern recognition, cryptography and security, regardless of the dimension of images. We conducted experiments on a variety of image sizes and used a convolutional neural network (CNN) for steganalysis. The results showed that the accuracy of the CNN model was not affected by the size of the images, indicating that the security of steganalysis was invariant regardless of the size of the images. Our findings suggest that steganalysis by deep-learning is a reliable security measure and can be used for a wide range of image sizes.","Write an abstract for a paper called A study on the invariance in security whatever the dimension of images
  for the steganalysis by deep-learning about Computer Vision and Pattern Recognition, Cryptography and Security"
2303.10772,"Xuqian Ren, Saihui Hou, Chunshui Cao, Xu Liu and Yongzhen Huang",Unsupervised Gait Recognition with Selective Fusion,['cs.CV'],"  Previous gait recognition methods primarily trained on labeled datasets,
which require painful labeling effort. However, using a pre-trained model on a
new dataset without fine-tuning can lead to significant performance
degradation. So to make the pre-trained gait recognition model able to be
fine-tuned on unlabeled datasets, we propose a new task: Unsupervised Gait
Recognition (UGR). We introduce a new cluster-based baseline to solve UGR with
cluster-level contrastive learning. But we further find more challenges this
task meets. First, sequences of the same person in different clothes tend to
cluster separately due to the significant appearance changes. Second, sequences
taken from 0 and 180 views lack walking postures and do not cluster with
sequences taken from other views. To address these challenges, we propose a
Selective Fusion method, which includes Selective Cluster Fusion (SCF) and
Selective Sample Fusion (SSF). With SCF, we merge matched clusters of the same
person wearing different clothes by updating the cluster-level memory bank with
a multi-cluster update strategy. And in SSF, we merge sequences taken from
front/back views gradually with curriculum learning. Extensive experiments show
the effectiveness of our method in improving the rank-1 accuracy in walking
with different coats condition and front/back views conditions.
","[{'version': 'v1', 'created': 'Sun, 19 Mar 2023 21:34:20 GMT'}]",2023-03-21,['Computer Vision and Pattern Recognition'],"This paper presents a novel unsupervised gait recognition method with selective fusion for computer vision and pattern recognition. The proposed method uses a combination of multiple gait features to improve the accuracy of gait recognition. It utilizes a selective fusion approach to combine multiple gait features, which allows for an improved recognition performance. The proposed method is evaluated on two publicly available datasets, and the results show that it outperforms existing methods. Additionally, the paper provides insights into the effectiveness of the proposed method and its potential applications.",Write an abstract for a paper called Unsupervised Gait Recognition with Selective Fusion about Computer Vision and Pattern Recognition
2210.03288,"Jiashun Liu, Zhe Xue, Ang Li","Scientific Paper Classification Based on Graph Neural Network with
  Hypergraph Self-attention Mechanism",['cs.IR'],"  The number of scientific papers has increased rapidly in recent years. How to
make good use of scientific papers for research is very important. Through the
high-quality classification of scientific papers, researchers can quickly find
the resource content they need from the massive scientific resources. The
classification of scientific papers will effectively help researchers filter
redundant information, obtain search results quickly and accurately, and
improve the search quality, which is necessary for scientific resource
management. This paper proposed a science-technique paper classification method
based on hypergraph neural network(SPHNN). In the heterogeneous information
network of scientific papers, the repeated high-order subgraphs are modeled as
hyperedges composed of multiple related nodes. Then the whole heterogeneous
information network is transformed into a hypergraph composed of different
hyperedges. The graph convolution operation is carried out on the hypergraph
structure, and the hyperedges self-attention mechanism is introduced to
aggregate different types of nodes in the hypergraph, so that the final node
representation can effectively maintain high-order nearest neighbor
relationships and complex semantic information. Finally, by comparing with
other methods, we proved that the model proposed in this paper has improved its
performance.
","[{'version': 'v1', 'created': 'Fri, 7 Oct 2022 02:38:53 GMT'}]",2022-10-10,['Information Retrieval'],"This paper proposes a novel graph neural network (GNN) based approach for scientific paper classification. The proposed approach is based on a GNN with hypergraph self-attention mechanism, which is used to capture both the local and global relationships between papers. The proposed approach is evaluated on a scientific paper dataset, and the results show that the proposed approach outperforms existing methods in terms of accuracy and precision. Furthermore, the proposed approach is able to provide interpretable results, which can be used to improve the performance of information retrieval systems.","Write an abstract for a paper called Scientific Paper Classification Based on Graph Neural Network with
  Hypergraph Self-attention Mechanism about Information Retrieval"
2106.07767,"Jiong Zhu, Junchen Jin, Donald Loveland, Michael T. Schaub, Danai
  Koutra","How does Heterophily Impact the Robustness of Graph Neural Networks?
  Theoretical Connections and Practical Implications","['cs.LG', 'stat.ML']","  We bridge two research directions on graph neural networks (GNNs), by
formalizing the relation between heterophily of node labels (i.e., connected
nodes tend to have dissimilar labels) and the robustness of GNNs to adversarial
attacks. Our theoretical and empirical analyses show that for homophilous graph
data, impactful structural attacks always lead to reduced homophily, while for
heterophilous graph data the change in the homophily level depends on the node
degrees. These insights have practical implications for defending against
attacks on real-world graphs: we deduce that separate aggregators for ego- and
neighbor-embeddings, a design principle which has been identified to
significantly improve prediction for heterophilous graph data, can also offer
increased robustness to GNNs. Our comprehensive experiments show that GNNs
merely adopting this design achieve improved empirical and certifiable
robustness compared to the best-performing unvaccinated model. Additionally,
combining this design with explicit defense mechanisms against adversarial
attacks leads to an improved robustness with up to 18.33% performance increase
under attacks compared to the best-performing vaccinated model.
","[{'version': 'v1', 'created': 'Mon, 14 Jun 2021 21:39:36 GMT'}, {'version': 'v2', 'created': 'Wed, 20 Oct 2021 01:17:11 GMT'}, {'version': 'v3', 'created': 'Sat, 11 Jun 2022 02:36:42 GMT'}, {'version': 'v4', 'created': 'Sat, 23 Jul 2022 02:42:48 GMT'}]",2022-07-26,['Machine Learning'],"This paper explores how heterophily, defined as the tendency of individuals to associate with similar others, affects the robustness of graph neural networks (GNNs) in machine learning. We discuss the theoretical connections between heterophily and robustness, and evaluate the practical implications of these connections. We use a variety of datasets to analyze the effects of heterophily on the performance of GNNs and compare the results with other machine learning models. Our findings suggest that heterophily can have a positive effect on the robustness of GNNs, and that the magnitude of this effect depends on the underlying dataset. We also discuss strategies for improving the robustness of GNNs in the presence of heterophily. Our work provides insights into the relationship between heterophily and robustness in GNNs, and has implications for the development of more robust machine learning models.","Write an abstract for a paper called How does Heterophily Impact the Robustness of Graph Neural Networks?
  Theoretical Connections and Practical Implications about Machine Learning"
2204.00934,Steven Oud and Koen van der Pool,"The Effects of the Environment and Linear Actuators on Robot
  Morphologies","['cs.RO', 'cs.NE']","  The field of evolutionary robotics uses principles of natural evolution to
design robots. In this paper, we study the effect of adding a new module
inspired by the skeletal muscle to the existing RoboGen framework: the linear
actuator. Additionally, we investigate how robots evolved in a plain
environment differ from robots evolved in a rough environment. We consider the
task of directed locomotion for comparing evolved robot morphologies. The
results show that the addition of the linear actuator does not have a
significant impact on the performance and morphologies of robots evolved in a
plain environment. However, we find significant differences in the morphologies
of robots evolved in a plain environment and robots evolved in a rough
environment. We find that more complex behavior and morphologies emerge when we
change the terrain of the environment.
","[{'version': 'v1', 'created': 'Sat, 2 Apr 2022 20:03:32 GMT'}, {'version': 'v2', 'created': 'Tue, 5 Apr 2022 09:19:32 GMT'}, {'version': 'v3', 'created': 'Fri, 26 Aug 2022 15:07:12 GMT'}]",2022-08-29,"['Robotics', 'Neural and Evolutionary Computing']","This paper explores the effects of the environment and linear actuators on robot morphologies using robotics, neural, and evolutionary computing. The study focuses on the design of robots with different morphologies and how the environment and actuators affect the design. The paper will include a review of existing literature on the effects of the environment and actuators on robot morphologies, as well as a discussion of the implications of the findings. The paper will also provide a discussion of potential applications of the results. Finally, the paper will offer suggestions for future research in this area.","Write an abstract for a paper called The Effects of the Environment and Linear Actuators on Robot
  Morphologies about Robotics, Neural and Evolutionary Computing"
2212.10744,"Kai Li, Fenghua Xie, Hang Chen, Kexin Yuan, Xiaolin Hu","An Audio-Visual Speech Separation Model Inspired by
  Cortico-Thalamo-Cortical Circuits","['cs.SD', 'cs.CV']","  Audio-visual approaches involving visual inputs have laid the foundation for
recent progress in speech separation. However, the optimization of the
concurrent usage of auditory and visual inputs is still an active research
area. Inspired by the cortico-thalamo-cortical circuit, in which the sensory
processing mechanisms of different modalities modulate one another via the
non-lemniscal sensory thalamus, we propose a novel cortico-thalamo-cortical
neural network (CTCNet) for audio-visual speech separation (AVSS). First, the
CTCNet learns hierarchical auditory and visual representations in a bottom-up
manner in separate auditory and visual subnetworks, mimicking the functions of
the auditory and visual cortical areas. Then, inspired by the large number of
connections between cortical regions and the thalamus, the model fuses the
auditory and visual information in a thalamic subnetwork through top-down
connections. Finally, the model transmits this fused information back to the
auditory and visual subnetworks, and the above process is repeated several
times. The results of experiments on three speech separation benchmark datasets
show that CTCNet remarkably outperforms existing AVSS methods with
considerablely fewer parameters. These results suggest that mimicking the
anatomical connectome of the mammalian brain has great potential for advancing
the development of deep neural networks. Project repo is
https://github.com/JusperLee/CTCNet.
","[{'version': 'v1', 'created': 'Wed, 21 Dec 2022 03:28:30 GMT'}]",2022-12-22,"['Sound', 'Computer Vision and Pattern Recognition']","This paper presents a novel audio-visual speech separation model inspired by the cortico-thalamo-cortical (CTC) circuit. The proposed model combines sound, computer vision, and pattern recognition techniques to accurately separate speech from noise and other audio sources. The model utilizes a convolutional neural network (CNN) to extract visual features from the video frames and a recurrent neural network (RNN) to extract audio features from the audio signal. The extracted features are then combined in a multi-modal fusion layer, which is used to generate a speech mask. The model is evaluated on the AV-Speech dataset and is shown to outperform existing audio-only and audio-visual speech separation models. The results demonstrate the effectiveness of combining sound, computer vision, and pattern recognition techniques for speech separation.","Write an abstract for a paper called An Audio-Visual Speech Separation Model Inspired by
  Cortico-Thalamo-Cortical Circuits about Sound, Computer Vision and Pattern Recognition"
2303.01994,"Philipp Scharpf and Moritz Schubotz and Howard S. Cohl and Corinna
  Breitinger and Bela Gipp",Discovery and Recognition of Formula Concepts using Machine Learning,"['cs.IR', 'cs.LG']","  Citation-based Information Retrieval (IR) methods for scientific documents
have proven effective for IR applications, such as Plagiarism Detection or
Literature Recommender Systems in academic disciplines that use many
references. In science, technology, engineering, and mathematics, researchers
often employ mathematical concepts through formula notation to refer to prior
knowledge. Our long-term goal is to generalize citation-based IR methods and
apply this generalized method to both classical references and mathematical
concepts. In this paper, we suggest how mathematical formulas could be cited
and define a Formula Concept Retrieval task with two subtasks: Formula Concept
Discovery (FCD) and Formula Concept Recognition (FCR). While FCD aims at the
definition and exploration of a 'Formula Concept' that names bundled equivalent
representations of a formula, FCR is designed to match a given formula to a
prior assigned unique mathematical concept identifier. We present machine
learning-based approaches to address the FCD and FCR tasks. We then evaluate
these approaches on a standardized test collection (NTCIR arXiv dataset). Our
FCD approach yields a precision of 68% for retrieving equivalent
representations of frequent formulas and a recall of 72% for extracting the
formula name from the surrounding text. FCD and FCR enable the citation of
formulas within mathematical documents and facilitate semantic search and
question answering as well as document similarity assessments for plagiarism
detection or recommender systems.
","[{'version': 'v1', 'created': 'Fri, 3 Mar 2023 15:06:03 GMT'}, {'version': 'v2', 'created': 'Sun, 19 Mar 2023 20:22:39 GMT'}]",2023-03-21,"['Information Retrieval', 'Machine Learning']","This paper presents a novel approach to information retrieval and machine learning using the discovery and recognition of formula concepts. The proposed method is based on the application of machine learning to a large set of formula-based documents. The method uses a combination of supervised and unsupervised learning techniques to detect and recognize formula concepts. Experiments are conducted on a dataset of formula-based documents, showing that the proposed method can accurately detect and recognize formula concepts. Furthermore, the results show that the proposed method outperforms existing methods in terms of accuracy and efficiency. Finally, the paper discusses the implications of the proposed approach and its potential applications.","Write an abstract for a paper called Discovery and Recognition of Formula Concepts using Machine Learning about Information Retrieval, Machine Learning"
2205.09068,"Kennard Ng, Ser-Nam Lim, Gim Hee Lee",VRAG: Region Attention Graphs for Content-Based Video Retrieval,"['cs.CV', 'cs.MM']","  Content-based Video Retrieval (CBVR) is used on media-sharing platforms for
applications such as video recommendation and filtering. To manage databases
that scale to billions of videos, video-level approaches that use fixed-size
embeddings are preferred due to their efficiency. In this paper, we introduce
Video Region Attention Graph Networks (VRAG) that improves the state-of-the-art
of video-level methods. We represent videos at a finer granularity via
region-level features and encode video spatio-temporal dynamics through
region-level relations. Our VRAG captures the relationships between regions
based on their semantic content via self-attention and the permutation
invariant aggregation of Graph Convolution. In addition, we show that the
performance gap between video-level and frame-level methods can be reduced by
segmenting videos into shots and using shot embeddings for video retrieval. We
evaluate our VRAG over several video retrieval tasks and achieve a new
state-of-the-art for video-level retrieval. Furthermore, our shot-level VRAG
shows higher retrieval precision than other existing video-level methods, and
closer performance to frame-level methods at faster evaluation speeds. Finally,
our code will be made publicly available.
","[{'version': 'v1', 'created': 'Wed, 18 May 2022 16:50:45 GMT'}]",2022-05-19,"['Computer Vision and Pattern Recognition', 'Multimedia']","This paper presents VRAG, a novel Region Attention Graphs (RAG) approach for content-based video retrieval. VRAG is a graph-based representation of video content that leverages the power of computer vision and pattern recognition techniques to detect and represent salient regions in a video. The proposed approach is evaluated on two popular video retrieval datasets and achieves superior performance compared to existing state-of-the-art methods. The results demonstrate that VRAG can effectively capture the semantic content of videos and can be used for efficient and accurate content-based video retrieval. Furthermore, the proposed approach is generalizable to other multimedia retrieval tasks.","Write an abstract for a paper called VRAG: Region Attention Graphs for Content-Based Video Retrieval about Computer Vision and Pattern Recognition, Multimedia"
2207.12697,Marco T. Moraz\'an (Seton Hall University),Design of Classes I,"['cs.CY', 'cs.PL']","  The use of functional programming languages in the first programming course
at many universities is well-established and effective. Invariably, however,
students must progress to study object-oriented programming. This article
presents how the first steps of this transition have been successfully
implemented at Seton Hall University. The developed methodology builds on the
students' experience with type-based design acquired in their previous
introduction to programming courses. The transition is made smooth by
explicitly showing students that the design lessons they have internalized are
relevant in object-oriented programming. This allows for new abstractions
offered by object-oriented programming languages to be more easily taught and
used by students. Empirical evidence collected from students in the course
suggests that the approach developed is effective and that the transition is
smooth.
","[{'version': 'v1', 'created': 'Tue, 26 Jul 2022 07:44:24 GMT'}]",2022-08-15,"['Computers and Society', 'Programming Languages']","This paper presents a design of classes to explore the relationship between computers and society, and programming languages. It will discuss how the design of classes can be used to understand the impact of computers on society, as well as how programming languages can be used to create innovative applications. The paper will focus on the use of object-oriented programming languages for the design of classes, and will explore the implications of using different programming languages for different types of applications. In addition, it will discuss how the design of classes can be used to create applications that are more user-friendly, efficient, and secure. Finally, it will examine the implications of using different programming languages for different types of applications, and the impact of computer technology on society.","Write an abstract for a paper called Design of Classes I about Computers and Society, Programming Languages"
2105.10598,Coen D. Needell and Wilma A. Bainbridge,"Embracing New Techniques in Deep Learning for Estimating Image
  Memorability","['cs.CV', 'cs.AI', 'cs.LG']","  Various work has suggested that the memorability of an image is consistent
across people, and thus can be treated as an intrinsic property of an image.
Using computer vision models, we can make specific predictions about what
people will remember or forget. While older work has used now-outdated deep
learning architectures to predict image memorability, innovations in the field
have given us new techniques to apply to this problem. Here, we propose and
evaluate five alternative deep learning models which exploit developments in
the field from the last five years, largely the introduction of residual neural
networks, which are intended to allow the model to use semantic information in
the memorability estimation process. These new models were tested against the
prior state of the art with a combined dataset built to optimize both
within-category and across-category predictions. Our findings suggest that the
key prior memorability network had overstated its generalizability and was
overfit on its training set. Our new models outperform this prior model,
leading us to conclude that Residual Networks outperform simpler convolutional
neural networks in memorability regression. We make our new state-of-the-art
model readily available to the research community, allowing memory researchers
to make predictions about memorability on a wider range of images.
","[{'version': 'v1', 'created': 'Fri, 21 May 2021 23:05:23 GMT'}, {'version': 'v2', 'created': 'Tue, 23 Nov 2021 22:16:47 GMT'}, {'version': 'v3', 'created': 'Sat, 8 Jan 2022 22:57:41 GMT'}]",2022-01-11,"['Computer Vision and Pattern Recognition', 'Artificial Intelligence', 'Machine Learning']","This paper presents a comprehensive review of the latest advances in deep learning for estimating image memorability. We discuss the current state of the art of deep learning methods, and the potential of these methods to improve the accuracy of image memorability estimation. We focus on computer vision and pattern recognition, artificial intelligence, and machine learning, and discuss the advantages of deep learning in each domain. We also analyze the challenges faced by deep learning in the estimation of image memorability and present possible solutions for overcoming them. We conclude by highlighting the potential of deep learning for estimating image memorability and discuss future research directions.","Write an abstract for a paper called Embracing New Techniques in Deep Learning for Estimating Image
  Memorability about Computer Vision and Pattern Recognition, Artificial Intelligence, Machine Learning"
2211.15235,"Shaolei Liu, Siqi Yin, Linhao Qu, Manning Wang","Reducing Domain Gap in Frequency and Spatial domain for Cross-modality
  Domain Adaptation on Medical Image Segmentation",['cs.CV'],"  Unsupervised domain adaptation (UDA) aims to learn a model trained on source
domain and performs well on unlabeled target domain. In medical image
segmentation field, most existing UDA methods depend on adversarial learning to
address the domain gap between different image modalities, which is ineffective
due to its complicated training process. In this paper, we propose a simple yet
effective UDA method based on frequency and spatial domain transfer uner
multi-teacher distillation framework. In the frequency domain, we first
introduce non-subsampled contourlet transform for identifying domain-invariant
and domain-variant frequency components (DIFs and DVFs), and then keep the DIFs
unchanged while replacing the DVFs of the source domain images with that of the
target domain images to narrow the domain gap. In the spatial domain, we
propose a batch momentum update-based histogram matching strategy to reduce the
domain-variant image style bias. Experiments on two cross-modality medical
image segmentation datasets (cardiac, abdominal) show that our proposed method
achieves superior performance compared to state-of-the-art methods.
","[{'version': 'v1', 'created': 'Mon, 28 Nov 2022 11:35:39 GMT'}]",2022-11-29,['Computer Vision and Pattern Recognition'],This paper presents a novel approach to reduce the domain gap in frequency and spatial domain for cross-modality domain adaptation on medical image segmentation. The proposed approach utilizes a combination of frequency domain and spatial domain feature extraction techniques to reduce the domain gap between different modalities of medical images. The frequency domain features are extracted using a Convolutional Neural Network (CNN) and the spatial domain features are extracted using a Support Vector Machine (SVM). The performance of the proposed approach is evaluated on a publicly available medical image segmentation dataset. The results demonstrate that the proposed approach is able to reduce the domain gap between different modalities of medical images and achieve a higher accuracy in segmentation compared to the baseline methods. The paper also provides an in-depth analysis of the results and discusses the implications of the proposed approach for cross-modality domain adaptation on medical image segmentation.,"Write an abstract for a paper called Reducing Domain Gap in Frequency and Spatial domain for Cross-modality
  Domain Adaptation on Medical Image Segmentation about Computer Vision and Pattern Recognition"
2211.11425,"Tuomas Varanka, Yante Li, Wei Peng and Guoying Zhao",Data Leakage and Evaluation Issues in Micro-Expression Analysis,['cs.CV'],"  Micro-expressions have drawn increasing interest lately due to various
potential applications. The task is, however, difficult as it incorporates many
challenges from the fields of computer vision, machine learning and emotional
sciences. Due to the spontaneous and subtle characteristics of
micro-expressions, the available training and testing data are limited, which
make evaluation complex. We show that data leakage and fragmented evaluation
protocols are issues among the micro-expression literature. We find that fixing
data leaks can drastically reduce model performance, in some cases even making
the models perform similarly to a random classifier. To this end, we go through
common pitfalls, propose a new standardized evaluation protocol using facial
action units with over 2000 micro-expression samples, and provide an open
source library that implements the evaluation protocols in a standardized
manner. Code is publicly available in \url{https://github.com/tvaranka/meb}.
","[{'version': 'v1', 'created': 'Mon, 21 Nov 2022 13:12:07 GMT'}, {'version': 'v2', 'created': 'Mon, 6 Mar 2023 07:37:49 GMT'}]",2023-03-07,['Computer Vision and Pattern Recognition'],"This paper explores the data leakage and evaluation issues associated with micro-expression analysis in computer vision and pattern recognition. Micro-expression analysis is a relatively new field of computer vision and pattern recognition that focuses on the study of facial expressions that occur in a fraction of a second. This paper examines the current state of research in this field, highlighting the challenges associated with data leakage and evaluation. The paper also examines the potential solutions to these issues, such as the use of cross-validation, data augmentation, and the application of machine learning algorithms. Finally, the paper discusses the implications of these solutions in terms of the advancement of the field.",Write an abstract for a paper called Data Leakage and Evaluation Issues in Micro-Expression Analysis about Computer Vision and Pattern Recognition
2206.07376,"Xiaoteng Ma, Shuai Ma, Li Xia, Qianchuan Zhao","Mean-Semivariance Policy Optimization via Risk-Averse Reinforcement
  Learning","['cs.LG', 'cs.AI']","  Keeping risk under control is often more crucial than maximizing expected
rewards in real-world decision-making situations, such as finance, robotics,
autonomous driving, etc. The most natural choice of risk measures is variance,
which penalizes the upside volatility as much as the downside part. Instead,
the (downside) semivariance, which captures the negative deviation of a random
variable under its mean, is more suitable for risk-averse proposes. This paper
aims at optimizing the mean-semivariance (MSV) criterion in reinforcement
learning w.r.t. steady reward distribution. Since semivariance is
time-inconsistent and does not satisfy the standard Bellman equation, the
traditional dynamic programming methods are inapplicable to MSV problems
directly. To tackle this challenge, we resort to Perturbation Analysis (PA)
theory and establish the performance difference formula for MSV. We reveal that
the MSV problem can be solved by iteratively solving a sequence of RL problems
with a policy-dependent reward function. Further, we propose two on-policy
algorithms based on the policy gradient theory and the trust region method.
Finally, we conduct diverse experiments from simple bandit problems to
continuous control tasks in MuJoCo, which demonstrate the effectiveness of our
proposed methods.
","[{'version': 'v1', 'created': 'Wed, 15 Jun 2022 08:32:53 GMT'}, {'version': 'v2', 'created': 'Sun, 16 Oct 2022 01:31:43 GMT'}, {'version': 'v3', 'created': 'Wed, 8 Mar 2023 09:47:11 GMT'}]",2023-03-09,"['Machine Learning', 'Artificial Intelligence']","and Robotics

This paper presents an approach to policy optimization in machine learning, artificial intelligence, and robotics systems by combining mean-semivariance risk-averse reinforcement learning with a novel risk-averse optimization algorithm. The proposed method enables the system to learn a policy that maximizes the expected return while minimizing the risk associated with the decision-making process. The paper explores the potential of the proposed method in a variety of robotic tasks, and provides an empirical evaluation of its performance. We show that the proposed method is able to achieve significantly better performance compared to existing methods in terms of both mean return and risk reduction. The results demonstrate the potential of the proposed approach to provide a more robust and reliable decision-making system for robotics applications.","Write an abstract for a paper called Mean-Semivariance Policy Optimization via Risk-Averse Reinforcement
  Learning about Machine Learning, Artificial Intelligence"
2212.11685,"Zhenghao Lin, Yeyun Gong, Yelong Shen, Tong Wu, Zhihao Fan, Chen Lin,
  Nan Duan, Weizhu Chen","Text Generation with Diffusion Language Models: A Pre-training Approach
  with Continuous Paragraph Denoise","['cs.CL', 'cs.LG']","  In this paper, we introduce a novel dIffusion language modEl pre-training
framework for text generation, which we call GENIE. GENIE is a large-scale
pretrained diffusion language model that consists of an encoder and a
diffusion-based decoder, which can generate text by gradually transforming a
random noise sequence into a coherent text sequence. To pre-train GENIE on a
large-scale language corpus, we design a new continuous paragraph denoise
objective, which encourages the diffusion-decoder to reconstruct a clean text
paragraph from a corrupted version, while preserving the semantic and syntactic
coherence. We evaluate GENIE on four downstream text generation benchmarks,
namely XSum, CNN/DailyMail, Gigaword, and CommonGen. Our experimental results
show that GENIE achieves comparable performance with the state-of-the-art
autoregressive models on these benchmarks, and generates more diverse text
samples. The code and models of GENIE are available at
https://github.com/microsoft/ProphetNet/tree/master/GENIE.
","[{'version': 'v1', 'created': 'Thu, 22 Dec 2022 13:17:11 GMT'}, {'version': 'v2', 'created': 'Fri, 17 Feb 2023 17:14:52 GMT'}]",2023-02-20,"['Computation and Language', 'Machine Learning']","This paper presents a novel pre-training approach for text generation with diffusion language models (DLMs) and continuous paragraph denoising. We explore the application of DLMs to the task of text generation, and the potential of pre-training with the continuous paragraph denoising task to improve the performance of text generation models. We evaluate our approach on a range of popular datasets for computation and language, machine learning, and natural language processing tasks. The results show that our pre-training approach significantly outperforms the baseline models in terms of both perplexity and text generation quality. The findings suggest that our approach can be used to improve the performance of text generation models, and to facilitate the development of advanced text generation systems.","Write an abstract for a paper called Text Generation with Diffusion Language Models: A Pre-training Approach
  with Continuous Paragraph Denoise about Computation and Language, Machine Learning"
2302.06172,"Charilaos Efthymiou, Weiming Feng","On the Mixing Time of Glauber Dynamics for the Hard-core and Related
  Models on G(n,d/n)","['cs.DM', 'cs.DS', 'math.PR']","  We study the single-site Glauber dynamics for the fugacity $\lambda$,
Hard-core model on the random graph $G(n, d/n)$. We show that for the typical
instances of the random graph $G(n,d/n)$ and for fugacity $\lambda <
\frac{d^d}{(d-1)^{d+1}}$, the mixing time of Glauber dynamics is $n^{1 +
O(1/\log \log n)}$.
  Our result improves on the recent elegant algorithm in [Bezakova, Galanis,
Goldberg Stefankovic; ICALP'22]. The algorithm there is a MCMC based sampling
algorithm, but it is not the Glauber dynamics. Our algorithm here is simpler,
as we use the classic Glauber dynamics. Furthermore, the bounds on mixing time
we prove are smaller than those in Bezakova et al. paper, hence our algorithm
is also faster.
  The main challenge in our proof is handling vertices with unbounded degrees.
We provide stronger results with regard the spectral independence via branching
values and show that the our Gibbs distributions satisfy the approximate
tensorisation of the entropy. We conjecture that the bounds we have here are
optimal for $G(n,d/n)$.
  As corollary of our analysis for the Hard-core model, we also get bounds on
the mixing time of the Glauber dynamics for the Monomer-dimer model on
$G(n,d/n)$. The bounds we get for this model are slightly better than those we
have for the Hard-core model
","[{'version': 'v1', 'created': 'Mon, 13 Feb 2023 08:15:17 GMT'}]",2023-02-14,"['Discrete Mathematics', 'Data Structures and Algorithms']","This paper explores the mixing time of Glauber Dynamics for the Hard-core and related models on G(n,d/n). We analyze the mixing time of Glauber Dynamics for these models, using the spectral gap of the Markov chain as a measure of the speed of convergence. We prove that the mixing time of the Glauber Dynamics for the Hard-core model on G(n,d/n) is O(n^2 log n). Furthermore, we show that the mixing time of the Glauber Dynamics for the related models on G(n,d/n) is at most O(n^2 log n). We also discuss the implications of our results for the design and analysis of discrete mathematics, data structures, and algorithms.","Write an abstract for a paper called On the Mixing Time of Glauber Dynamics for the Hard-core and Related
  Models on G(n,d/n) about Discrete Mathematics, Data Structures and Algorithms"
2210.12752,"Wanyi Zhuang, Qi Chu, Zhentao Tan, Qiankun Liu, Haojie Yuan, Changtao
  Miao, Zixiang Luo, Nenghai Yu","UIA-ViT: Unsupervised Inconsistency-Aware Method based on Vision
  Transformer for Face Forgery Detection",['cs.CV'],"  Intra-frame inconsistency has been proved to be effective for the
generalization of face forgery detection. However, learning to focus on these
inconsistency requires extra pixel-level forged location annotations. Acquiring
such annotations is non-trivial. Some existing methods generate large-scale
synthesized data with location annotations, which is only composed of real
images and cannot capture the properties of forgery regions. Others generate
forgery location labels by subtracting paired real and fake images, yet such
paired data is difficult to collected and the generated label is usually
discontinuous. To overcome these limitations, we propose a novel Unsupervised
Inconsistency-Aware method based on Vision Transformer, called UIA-ViT, which
only makes use of video-level labels and can learn inconsistency-aware feature
without pixel-level annotations. Due to the self-attention mechanism, the
attention map among patch embeddings naturally represents the consistency
relation, making the vision Transformer suitable for the consistency
representation learning. Based on vision Transformer, we propose two key
components: Unsupervised Patch Consistency Learning (UPCL) and Progressive
Consistency Weighted Assemble (PCWA). UPCL is designed for learning the
consistency-related representation with progressive optimized pseudo
annotations. PCWA enhances the final classification embedding with previous
patch embeddings optimized by UPCL to further improve the detection
performance. Extensive experiments demonstrate the effectiveness of the
proposed method.
","[{'version': 'v1', 'created': 'Sun, 23 Oct 2022 15:24:47 GMT'}]",2022-10-25,['Computer Vision and Pattern Recognition'],"This paper presents UIA-ViT, an unsupervised inconsistency-aware method based on Vision Transformer (ViT) for face forgery detection. UIA-ViT is designed to detect face forgeries with minimal human supervision and is capable of recognizing inconsistencies between different parts of a face image. The proposed method consists of two main components: a vision transformer-based feature extractor and an inconsistency detector. The vision transformer-based feature extractor is used to extract features from face images, while the inconsistency detector uses the extracted features to identify inconsistencies between different parts of the face. Experimental results show that UIA-ViT outperforms existing methods on several face forgery detection datasets. The proposed method can be used in a variety of computer vision and pattern recognition applications.","Write an abstract for a paper called UIA-ViT: Unsupervised Inconsistency-Aware Method based on Vision
  Transformer for Face Forgery Detection about Computer Vision and Pattern Recognition"
2207.07646,"Rui Qian, Yeqing Li, Zheng Xu, Ming-Hsuan Yang, Serge Belongie, Yin
  Cui","Multimodal Open-Vocabulary Video Classification via Pre-Trained Vision
  and Language Models","['cs.CV', 'cs.LG']","  Utilizing vision and language models (VLMs) pre-trained on large-scale
image-text pairs is becoming a promising paradigm for open-vocabulary visual
recognition. In this work, we extend this paradigm by leveraging motion and
audio that naturally exist in video. We present \textbf{MOV}, a simple yet
effective method for \textbf{M}ultimodal \textbf{O}pen-\textbf{V}ocabulary
video classification. In MOV, we directly use the vision encoder from
pre-trained VLMs with minimal modifications to encode video, optical flow and
audio spectrogram. We design a cross-modal fusion mechanism to aggregate
complimentary multimodal information. Experiments on Kinetics-700 and VGGSound
show that introducing flow or audio modality brings large performance gains
over the pre-trained VLM and existing methods. Specifically, MOV greatly
improves the accuracy on base classes, while generalizes better on novel
classes. MOV achieves state-of-the-art results on UCF and HMDB zero-shot video
classification benchmarks, significantly outperforming both traditional
zero-shot methods and recent methods based on VLMs. Code and models will be
released.
","[{'version': 'v1', 'created': 'Fri, 15 Jul 2022 17:59:11 GMT'}]",2022-07-18,"['Computer Vision and Pattern Recognition', 'Machine Learning']","This paper presents a novel approach to multimodal open-vocabulary video classification using pre-trained vision and language models. We propose a two-stage approach that first extracts a set of features from the video frames and then uses a pre-trained language model to encode the semantic content of the video. We evaluate our approach on a dataset of videos from the YouTube-8M dataset and show that our model outperforms other state-of-the-art methods for video classification. Additionally, we show that our model is able to process videos with open-vocabulary labels, which is a challenging task for existing video classification models. Our results demonstrate that pre-trained vision and language models can be used to accurately classify videos with open-vocabulary labels.","Write an abstract for a paper called Multimodal Open-Vocabulary Video Classification via Pre-Trained Vision
  and Language Models about Computer Vision and Pattern Recognition, Machine Learning"
2303.13027,"Shoichi Koyama, Keisuke Kimura, Natsuki Ueno","Weighted Pressure and Mode Matching for Sound Field Reproduction:
  Theoretical and Experimental Comparisons","['eess.AS', 'cs.SD']","  Two sound field reproduction methods, weighted pressure matching and weighted
mode matching, are theoretically and experimentally compared. The weighted
pressure and mode matching are a generalization of conventional pressure and
mode matching, respectively. Both methods are derived by introducing a
weighting matrix in the pressure and mode matching. The weighting matrix in the
weighted pressure matching is defined on the basis of the kernel interpolation
of the sound field from pressure at a discrete set of control points. In the
weighted mode matching, the weighting matrix is defined by a regional
integration of spherical wavefunctions. It is theoretically shown that the
weighted pressure matching is a special case of the weighted mode matching by
infinite-dimensional harmonic analysis for estimating expansion coefficients
from pressure observations. The difference between the two methods are
discussed through experiments.
","[{'version': 'v1', 'created': 'Thu, 23 Mar 2023 04:26:06 GMT'}]",2023-03-24,['Sound'],"Field Reproduction

This paper presents a theoretical and experimental comparison of weighted pressure and mode matching for sound field reproduction. Weighted pressure is a method of sound field reproduction that uses a weighted sum of loudspeakers’ individual sound fields to approximate the desired sound field. Mode matching is a method of sound field reproduction that employs a set of loudspeakers and filters to match the modal characteristics of the desired sound field. The paper will compare the two methods in terms of their accuracy, efficiency, and practicality. Experimental results will be presented to demonstrate the effectiveness of each method and to compare the two methods. The paper will also discuss the advantages and disadvantages of each method, as well as potential improvements to the methods. Finally, the paper will provide recommendations for sound field reproduction systems that use weighted pressure and mode matching.","Write an abstract for a paper called Weighted Pressure and Mode Matching for Sound Field Reproduction:
  Theoretical and Experimental Comparisons about Sound"
2301.12321,"Jang-Hyun Kim, Sangdoo Yun, Hyun Oh Song",Neural Relation Graph for Identifying Problematic Data,['cs.LG'],"  Diagnosing and cleaning datasets are crucial for building robust machine
learning systems. However, identifying problems within large-scale datasets
with real-world distributions is difficult due to the presence of complex
issues, such as label errors or under-representation of certain types. In this
paper, we propose a novel approach for identifying problematic data by
utilizing a largely ignored source of information: a relational structure of
data in the feature-embedded space. We develop an efficient algorithm for
detecting label errors and outlier data points based on the relational graph
structure of the dataset. We further introduce a visualization tool for
contextualizing data points, which can serve as an effective tool for
interactively diagnosing datasets. We evaluate label error and
out-of-distribution detection performances on large-scale image and language
domain tasks, including ImageNet and GLUE benchmarks, and demonstrate the
effectiveness of our approach for debugging datasets and building robust
machine learning systems.
","[{'version': 'v1', 'created': 'Sun, 29 Jan 2023 02:09:13 GMT'}]",2023-01-31,['Machine Learning'],"This paper presents a novel approach for identifying problematic data in machine learning applications. The proposed Neural Relation Graph (NRG) is a graph-based model that uses neural networks to capture the relationships between data points. NRG is capable of learning complex relationships between data points and can be used to identify data points that are likely to be problematic. The proposed approach is evaluated on a real-world dataset and results show that NRG can effectively identify problematic data points. Furthermore, the paper also discusses the implications of using NRG for improving the performance of machine learning models.",Write an abstract for a paper called Neural Relation Graph for Identifying Problematic Data about Machine Learning
2109.12584,"Mirza Yusuf, Praatibh Surana, Gauri Gupta and Krithika Ramesh","Curb Your Carbon Emissions: Benchmarking Carbon Emissions in Machine
  Translation","['cs.CL', 'cs.AI', 'cs.LG']","  In recent times, there has been definitive progress in the field of NLP, with
its applications growing as the utility of our language models increases with
advances in their performance. However, these models require a large amount of
computational power and data to train, consequently leading to large carbon
footprints. Therefore, it is imperative that we study the carbon efficiency and
look for alternatives to reduce the overall environmental impact of training
models, in particular large language models. In our work, we assess the
performance of models for machine translation, across multiple language pairs
to assess the difference in computational power required to train these models
for each of these language pairs and examine the various components of these
models to analyze aspects of our pipeline that can be optimized to reduce these
carbon emissions.
","[{'version': 'v1', 'created': 'Sun, 26 Sep 2021 12:30:10 GMT'}, {'version': 'v2', 'created': 'Mon, 4 Oct 2021 20:49:20 GMT'}, {'version': 'v3', 'created': 'Mon, 11 Oct 2021 09:11:43 GMT'}, {'version': 'v4', 'created': 'Fri, 1 Apr 2022 20:02:50 GMT'}]",2022-04-05,"['Computation and Language', 'Artificial Intelligence', 'Machine Learning']","This paper explores the impact of machine translation on carbon emissions in the context of computation and language, artificial intelligence, and machine learning. It uses a benchmarking approach to measure the current carbon emissions associated with machine translation and to identify potential areas for improvement. The paper considers the effects of machine translation on energy consumption, the potential for reducing carbon emissions through optimization of algorithms, and the potential for reducing electricity use through hardware and software optimization. The paper also evaluates the potential for using renewable energy sources to power machine translation systems. Finally, the paper offers suggestions for future research and policy initiatives to reduce carbon emissions associated with machine translation.","Write an abstract for a paper called Curb Your Carbon Emissions: Benchmarking Carbon Emissions in Machine
  Translation about Computation and Language, Artificial Intelligence, Machine Learning"
2110.10984,"Telikepalli Kavitha, Tam\'as Kir\'aly, Jannik Matuschke, Ildik\'o
  Schlotter, Ulrike Schmidt-Kraepelin","The popular assignment problem: when cardinality is more important than
  popularity","['cs.DS', 'cs.GT']","  We consider a matching problem in a bipartite graph $G=(A\cup B,E)$ where
each node in $A$ is an agent having preferences in partial order over her
neighbors, while nodes in $B$ are objects with no preferences. The size of our
matching is more important than node preferences; thus, we are interested in
maximum matchings only. Any pair of maximum matchings in $G$ (equivalently,
perfect matchings or assignments) can be compared by holding a head-to-head
election between them where agents are voters. The goal is to compute an
assignment $M$ such that there is no better or ""more popular"" assignment. This
is the popular assignment problem and it generalizes the well-studied popular
matching problem.
  Popular assignments need not always exist. We show a polynomial-time
algorithm that decides if the given instance admits one or not, and computes
one, if so. In instances with no popular assignment, we consider the problem of
finding an almost popular assignment, i.e., an assignment with minimum
unpopularity margin. We show an $O^*(|E|^k)$ time algorithm for deciding if
there exists an assignment with unpopularity margin at most $k$. We show that
this algorithm is essentially optimal by proving that the problem is
$\mathsf{W}_l[1]$-hard with parameter $k$.
  We also consider the minimum-cost popular assignment problem when there are
edge costs, and show its $\mathsf{NP}$-hardness even when all edge costs are in
$\{0,1\}$ and agents have strict preferences. By contrast, we propose a
polynomial-time algorithm to the problem of deciding if there exists a popular
assignment with a given set of forced/forbidden edges (this tractability holds
even for partially ordered preferences). Our algorithms are combinatorial and
based on LP duality. They search for an appropriate witness or dual
certificate, and when a certificate cannot be found, we prove that the desired
assignment does not exist in $G$.
","[{'version': 'v1', 'created': 'Thu, 21 Oct 2021 08:56:49 GMT'}, {'version': 'v2', 'created': 'Mon, 28 Mar 2022 10:08:38 GMT'}]",2022-03-29,"['Data Structures and Algorithms', 'Computer Science and Game Theory']","This paper explores the popular assignment problem, a problem of assigning objects to agents in a manner that maximizes overall popularity. The paper begins by introducing the popular assignment problem and its applications in the fields of computer science, data structures and algorithms, and game theory. It then examines the implications of the problem when cardinality, the number of objects assigned to each agent, is more important than popularity. The paper then provides a survey of existing solutions to the problem, including the use of greedy algorithms, simulated annealing, and genetic algorithms. Finally, the paper concludes with a discussion of future research directions and potential applications. The paper provides a comprehensive overview of the popular assignment problem and its applications, and offers insight into when cardinality should be prioritized over popularity.","Write an abstract for a paper called The popular assignment problem: when cardinality is more important than
  popularity about Data Structures and Algorithms, Computer Science and Game Theory"
2207.02552,"Rajen Kumar, Prashant Kumar Srivastava and Sudhan Majhi","A Construction of Type-II ZCCS of Arbitrary Sequence Length with Low
  PMEPR","['cs.IT', 'math.IT']","  In this letter, we propose a novel construction of type-II $Z$-complementary
code set (ZCCS) having arbitrary sequence length. The proposed construction
features type-II $(K,K,NP-P+1,NP)$-ZCCS using Kronecker product between
$(K,K,N)$-complete complementary code (CCC) set and unimodular sequence. In
this construction, for the first time, Barker sequences are used to reduce row
sequence peak-to-mean envelope power ratio (PMEPR) for some specific lengths
sequence and column sequence PMEPR for some specific size of codes. The column
sequence PMEPR of the proposed type-II ZCCS is upper bounded by a number
smaller than $2$, which is lower than the bounds reported in existing
literature for such codes. The proposed construction also contributes new
lengths of type-II $Z$-complementary pair (ZCP) and type-II $Z$-complementary
set (ZCS), which are also not reported before in literature. Furthermore, the
PMEPR of the obtained type-II ZCP is also lower than some of the existing work.
The PMEPR of type-II ZCS can also be ensured to be low.
","[{'version': 'v1', 'created': 'Wed, 6 Jul 2022 10:05:55 GMT'}, {'version': 'v2', 'created': 'Mon, 22 Aug 2022 09:10:59 GMT'}]",2022-08-23,['Information Theory'],"This paper presents a novel construction of Type-II Zero-Correlation Coded Sequences (ZCCS) of arbitrary sequence length with low Peak-to-Mean-Energy-Per-Realization (PMEPR). The proposed construction uses a combination of linear algebra and coding theory to design a low PMEPR ZCCS with arbitrary length. The proposed ZCCS is constructed using a combination of linear algebra and coding theory, and is constructed in a way that ensures the low PMEPR condition is satisfied. The paper also provides a theoretical analysis of the proposed construction and its performance. Finally, simulation results are provided to demonstrate the effectiveness of the proposed construction.","Write an abstract for a paper called A Construction of Type-II ZCCS of Arbitrary Sequence Length with Low
  PMEPR about Information Theory"
2205.11743,"Jia Cui, Mingze Gao, Xiaoming Zhou, Yang Li, Wei Liu, Jiazheng Tian,
  Ximing Zhang","Demand Response Method Considering Multiple Types of Flexible Loads in
  Industrial Parks","['eess.SY', 'cs.LG', 'cs.SY']","  With the rapid development of the energy internet, the proportion of flexible
loads in smart grid is getting much higher than before. It is highly important
to model flexible loads based on demand response. Therefore, a new demand
response method considering multiple flexible loads is proposed in this paper
to character the integrated demand response (IDR) resources. Firstly, a
physical process analytical deduction (PPAD) model is proposed to improve the
classification of flexible loads in industrial parks. Scenario generation, data
point augmentation, and smooth curves under various operating conditions are
considered to enhance the applicability of the model. Secondly, in view of the
strong volatility and poor modeling effect of Wasserstein-generative
adversarial networks (WGAN), an improved WGAN-gradient penalty (IWGAN-GP) model
is developed to get a faster convergence speed than traditional WGAN and
generate a higher quality samples. Finally, the PPAD and IWGAN-GP models are
jointly implemented to reveal the degree of correlation between flexible loads.
Meanwhile, an intelligent offline database is built to deal with the impact of
nonlinear factors in different response scenarios. Numerical examples have been
performed with the results proving that the proposed method is significantly
better than the existing technologies in reducing load modeling deviation and
improving the responsiveness of park loads.
","[{'version': 'v1', 'created': 'Tue, 24 May 2022 03:18:06 GMT'}]",2023-03-13,"['Machine Learning', 'Systems and Control']","This paper presents a novel method for demand response in industrial parks using machine learning and systems and control. The proposed method focuses on the coordination of multiple types of flexible loads, such as energy storage, electric vehicles, and demand-side management. The proposed method utilizes machine learning techniques to identify the optimal control strategies for each flexible load and to coordinate the operation of multiple flexible loads. The proposed method is tested on a simulated industrial park and the results demonstrate the effectiveness of the proposed method. The paper further discusses the potential of the proposed method to improve the efficiency and reliability of the power system.","Write an abstract for a paper called Demand Response Method Considering Multiple Types of Flexible Loads in
  Industrial Parks about Machine Learning, Systems and Control"
2206.121,"Zahra Ghodsi, Mojan Javaheripi, Nojan Sheybani, Xinqiao Zhang, Ke
  Huang, Farinaz Koushanfar",zPROBE: Zero Peek Robustness Checks for Federated Learning,"['cs.LG', 'cs.CR']","  Privacy-preserving federated learning allows multiple users to jointly train
a model with coordination of a central server. The server only learns the final
aggregation result, thus the users' (private) training data is not leaked from
the individual model updates. However, keeping the individual updates private
allows malicious users to perform Byzantine attacks and degrade the accuracy
without being detected. Best existing defenses against Byzantine workers rely
on robust rank-based statistics, e.g., median, to find malicious updates.
However, implementing privacy-preserving rank-based statistics is nontrivial
and not scalable in the secure domain, as it requires sorting all individual
updates. We establish the first private robustness check that uses high break
point rank-based statistics on aggregated model updates. By exploiting
randomized clustering, we significantly improve the scalability of our defense
without compromising privacy. We leverage our statistical bounds in
zero-knowledge proofs to detect and remove malicious updates without revealing
the private user updates. Our novel framework, zPROBE, enables Byzantine
resilient and secure federated learning. Empirical evaluations demonstrate that
zPROBE provides a low overhead solution to defend against state-of-the-art
Byzantine attacks while preserving privacy.
","[{'version': 'v1', 'created': 'Fri, 24 Jun 2022 06:20:37 GMT'}, {'version': 'v2', 'created': 'Tue, 25 Oct 2022 19:42:48 GMT'}]",2022-10-27,"['Machine Learning', 'Cryptography and Security']","This paper presents zPROBE, a new approach for providing robustness checks for federated learning in the context of machine learning, cryptography and security. zPROBE is a zero-peek protocol which uses a combination of cryptographic techniques and machine learning algorithms to ensure that the data used in federated learning is not tampered with. The protocol is designed to be secure against malicious actors and provide a high level of privacy for the data. The paper describes the design of zPROBE, its security and privacy guarantees, and an evaluation of its performance. The results of the evaluation demonstrate that zPROBE provides a secure and private federated learning environment, and that it performs better than existing protocols.","Write an abstract for a paper called zPROBE: Zero Peek Robustness Checks for Federated Learning about Machine Learning, Cryptography and Security"
2202.12245,"Laurence Likforman-Sulem, Anna Esposito, Marcos Faundez-Zanuy, Stephan
  Clemen\c{c}on, Gennaro Cordasco","EMOTHAW: A novel database for emotional state recognition from
  handwriting","['cs.CV', 'cs.LG']","  The detection of negative emotions through daily activities such as
handwriting is useful for promoting well-being. The spread of human-machine
interfaces such as tablets makes the collection of handwriting samples easier.
In this context, we present a first publicly available handwriting database
which relates emotional states to handwriting, that we call EMOTHAW. This
database includes samples of 129 participants whose emotional states, namely
anxiety, depression and stress, are assessed by the Depression Anxiety Stress
Scales (DASS) questionnaire. Seven tasks are recorded through a digitizing
tablet: pentagons and house drawing, words copied in handprint, circles and
clock drawing, and one sentence copied in cursive writing. Records consist in
pen positions, on-paper and in-air, time stamp, pressure, pen azimuth and
altitude. We report our analysis on this database. From collected data, we
first compute measurements related to timing and ductus. We compute separate
measurements according to the position of the writing device: on paper or
in-air. We analyse and classify this set of measurements (referred to as
features) using a random forest approach. This latter is a machine learning
method [2], based on an ensemble of decision trees, which includes a feature
ranking process. We use this ranking process to identify the features which
best reveal a targeted emotional state.
  We then build random forest classifiers associated to each emotional state.
Our results, obtained from cross-validation experiments, show that the targeted
emotional states can be identified with accuracies ranging from 60% to 71%.
","[{'version': 'v1', 'created': 'Wed, 23 Feb 2022 15:15:44 GMT'}]",2022-02-25,"['Computer Vision and Pattern Recognition', 'Machine Learning']","This paper presents EMOTHAW, a novel database for emotional state recognition from handwriting. The database consists of handwriting samples from over 200 participants, collected from a variety of sources, including online surveys, physical surveys, and interviews. The handwriting samples are then analyzed using computer vision and pattern recognition techniques, as well as machine learning algorithms, in order to classify the emotional states. The results of the experiments show that the database is capable of recognizing emotional states with an accuracy of up to 90%. Furthermore, the database is able to differentiate between different emotional states with an accuracy of up to 80%. The paper concludes by discussing the potential applications of the database and the implications of the results.","Write an abstract for a paper called EMOTHAW: A novel database for emotional state recognition from
  handwriting about Computer Vision and Pattern Recognition, Machine Learning"
2208.04076,"Heng Cong, Rongyu Zhang, Jiarong He, Jin Gao","Multi-Frames Temporal Abnormal Clues Learning Method for Face
  Anti-Spoofing",['cs.CV'],"  Face anti-spoofing researches are widely used in face recognition and has
received more attention from industry and academics. In this paper, we propose
the EulerNet, a new temporal feature fusion network in which the differential
filter and residual pyramid are used to extract and amplify abnormal clues from
continuous frames, respectively. A lightweight sample labeling method based on
face landmarks is designed to label large-scale samples at a lower cost and has
better results than other methods such as 3D camera. Finally, we collect 30,000
live and spoofing samples using various mobile ends to create a dataset that
replicates various forms of attacks in a real-world setting. Extensive
experiments on public OULU-NPU show that our algorithm is superior to the state
of art and our solution has already been deployed in real-world systems
servicing millions of users.
","[{'version': 'v1', 'created': 'Mon, 8 Aug 2022 11:54:36 GMT'}]",2022-08-09,['Computer Vision and Pattern Recognition'],"This paper presents a novel Multi-Frames Temporal Abnormal Clues Learning (MFTACL) method for face anti-spoofing, which is a task of computer vision and pattern recognition. The proposed method utilizes a deep learning model to extract temporal abnormal clues from multiple frames of video sequences, which are then used to detect spoofing attacks. The proposed method is evaluated on two public datasets, Replay-Attack and OULU-NPU, and the experimental results demonstrate its superiority over the state-of-the-art methods. The results show that the proposed method achieves the highest accuracy of 98.5% and 97.2% on Replay-Attack and OULU-NPU datasets, respectively. Furthermore, the proposed method has a significant improvement in the detection of subtle spoofing attacks. The results of this paper suggest that the proposed MFTACL method is an effective and promising approach for face anti-spoofing.","Write an abstract for a paper called Multi-Frames Temporal Abnormal Clues Learning Method for Face
  Anti-Spoofing about Computer Vision and Pattern Recognition"
2301.12504,"Ruizhe Zhang, Qingyao Ai, Yueyue Wu, Yixiao Ma and Yiqun Liu",Diverse legal case search,['cs.IR'],"  In last decades, legal case search has received more and more attention.
Legal practitioners need to work or enhance their efficiency by means of class
case search. In the process of searching, legal practitioners often need the
search results under several different causes of cases as reference. However,
existing work tends to focus on the relevance of the judgments themselves,
without considering the connection between the causes of action. Several
well-established diversity search techniques already exist in open-field search
efforts. However, these techniques do not take into account the specificity of
legal search scenarios, e.g., the subtopic may not be independent of each
other, but somehow connected. Therefore, we construct a diversity legal
retrieval model. This model takes into account both diversity and relevance,
and is well adapted to this scenario. At the same time, considering the lack of
dataset with diversity labels, we constructed a diversity legal retrieval
dataset and obtained labels by manual labeling. experiments confirmed that our
model is effective.
","[{'version': 'v1', 'created': 'Sun, 29 Jan 2023 17:50:58 GMT'}]",2023-01-31,['Information Retrieval'],"This paper examines the application of Information Retrieval (IR) techniques to the task of legal case search. We discuss the challenges of legal case search, such as the need for a diverse set of search results, and how IR can be used to address them. We present a case study of a legal case search engine developed using IR techniques, and discuss the results of our experiments. We conclude that IR techniques are an effective way to search legal cases, and can be used to provide a more diverse set of results than traditional search methods.",Write an abstract for a paper called Diverse legal case search about Information Retrieval
2302.07512,"Ryutaro Kodama, Yoshitaka Arahori, and Kathuhiko Gondow","Path-sensitive Type Analysis with Backward Analysis for Quality
  Assurance of Dynamic Typed Language Code",['cs.SE'],"  Precise and fast static type analysis for dynamically typed language is very
difficult. This is mainly because the lack of static type information makes it
difficult to approximate all possible values of a variable. Actually, the
existing static type analysis methods are imprecise or slow. In this paper, we
propose a novel method to improve the precision of static type analysis for
Python code, where a backward analysis is used to obtain the path-sensitivity.
By doing so, our method aims to obtain more precise static type information,
which contributes to the overall improvement of static analysis. To show the
effectiveness of our method, we conducted a preliminary experiment to compare
our method implementation and the existing analysis tool with respect to
precision and time efficiency. The result shows our method provides more
precise type analysis with fewer false positives than the existing static type
analysis tool. Also it shows our proposed method increases the analysis time,
but it is still within the range of practical use.
","[{'version': 'v1', 'created': 'Wed, 15 Feb 2023 08:04:23 GMT'}]",2023-02-16,['Software Engineering'],"This paper presents a novel path-sensitive type analysis system for dynamic typed language code to ensure the quality of software engineering. This system combines backward analysis with type inference techniques to detect type errors in dynamic typed language code. The proposed system is applied to a variety of programming languages and is evaluated using a set of real-world programs. The results show that our system can detect a wide range of type errors with high accuracy and low false-positive rates. Furthermore, the system is able to accurately detect type errors in programs with complex control flows, and it is able to detect type errors that are not detected by other existing type analysis systems. The paper concludes with a discussion of the implications of the proposed system and its potential applications.","Write an abstract for a paper called Path-sensitive Type Analysis with Backward Analysis for Quality
  Assurance of Dynamic Typed Language Code about Software Engineering"
2303.14562,"Daniel Nakhimovich, Yinglong Miao, Kostas E. Bekris",Resolution Complete In-Place Object Retrieval given Known Object Models,['cs.RO'],"  This work proposes a robot task planning framework for retrieving a target
object in a confined workspace among multiple stacked objects that obstruct the
target. The robot can use prehensile picking and in-workspace placing actions.
The method assumes access to 3D models for the visible objects in the scene.
The key contribution is in achieving desirable properties, i.e., to provide (a)
safety, by avoiding collisions with sensed obstacles, objects, and occluded
regions, and (b) resolution completeness (RC) - or probabilistic completeness
(PC) depending on implementation - which indicates a solution will be
eventually found (if it exists) as the resolution of algorithmic parameters
increases. A heuristic variant of the basic RC algorithm is also proposed to
solve the task more efficiently while retaining the desirable properties.
Simulation results compare using random picking and placing operations against
the basic RC algorithm that reasons about object dependency as well as its
heuristic variant. The success rate is higher for the RC approaches given the
same amount of time. The heuristic variant is able to solve the problem even
more efficiently than the basic approach. The integration of the RC algorithm
with perception, where an RGB-D sensor detects the objects as they are being
moved, enables real robot demonstrations of safely retrieving target objects
from a cluttered shelf.
","[{'version': 'v1', 'created': 'Sat, 25 Mar 2023 21:08:09 GMT'}]",2023-03-28,['Robotics'],This paper presents an approach to in-place object retrieval that is robust and efficient. We propose a method that combines the use of known object models with a resolution complete approach to object retrieval. Our approach is demonstrated using a robotic system that is capable of recognizing objects in a cluttered environment. The method is tested in a variety of scenarios to evaluate its accuracy and efficiency. Results show that our approach is able to successfully retrieve objects with a high degree of accuracy and in a timely manner. This approach is expected to be of great benefit to robotic systems that are required to recognize objects in cluttered environments.,Write an abstract for a paper called Resolution Complete In-Place Object Retrieval given Known Object Models about Robotics
2204.00291,Rodolfo Zevallos,Text-To-Speech Data Augmentation for Low Resource Speech Recognition,"['cs.CL', 'cs.SD', 'eess.AS']","  Nowadays, the main problem of deep learning techniques used in the
development of automatic speech recognition (ASR) models is the lack of
transcribed data. The goal of this research is to propose a new data
augmentation method to improve ASR models for agglutinative and low-resource
languages. This novel data augmentation method generates both synthetic text
and synthetic audio. Some experiments were conducted using the corpus of the
Quechua language, which is an agglutinative and low-resource language. In this
study, a sequence-to-sequence (seq2seq) model was applied to generate synthetic
text, in addition to generating synthetic speech using a text-to-speech (TTS)
model for Quechua. The results show that the new data augmentation method works
well to improve the ASR model for Quechua. In this research, an 8.73%
improvement in the word-error-rate (WER) of the ASR model is obtained using a
combination of synthetic text and synthetic speech.
","[{'version': 'v1', 'created': 'Fri, 1 Apr 2022 08:53:44 GMT'}]",2022-04-04,"['Computation and Language', 'Sound']","This paper presents a novel text-to-speech data augmentation technique for low resource speech recognition. We propose a method to generate synthetic speech data using a recurrent neural network (RNN) based text-to-speech model. This approach is applied to a low resource speech recognition task on a dataset of English digits. We evaluate the performance of our proposed method and compare it to existing augmentation techniques. Our results demonstrate significant improvement in speech recognition accuracy, suggesting that our proposed method is a viable solution for low resource speech recognition. Additionally, we discuss the implications of our findings for the field of computation and language, sound.","Write an abstract for a paper called Text-To-Speech Data Augmentation for Low Resource Speech Recognition about Computation and Language, Sound"
2209.02189,"Maria Naumcheva, Sophie Ebersold, Alexandr Naumchev, Jean-Michel
  Bruel, Florian Galinier, Bertrand Meyer","Object-Oriented Requirements: a Unified Framework for Specifications,
  Scenarios and Tests",['cs.SE'],"  A paradox of requirements specifications as dominantly practiced in the
industry is that they often claim to be object-oriented (OO) but largely rely
on procedural (non-OO) techniques. Use cases and user stories describe
functional flows, not object types. To gain the benefits provided by object
technology (such as extendibility, reusability, reliability), requirements
should instead take advantage of the same data abstraction concepts -- classes,
inheritance, information hiding -- as OO design and OO programs.
  Many people find use cases and user stories appealing because of the
simplicity and practicality of the concepts. Can we reconcile requirements with
object-oriented principles and get the best of both worlds?
  This article proposes a unified framework. It shows that the concept of class
is general enough to describe not only ""objects"" in a narrow sense but also
scenarios such as use cases and user stories and other important artifacts such
as test cases and oracles.
  Having a single framework opens the way to requirements that enjoy the
benefits of both approaches: like use cases and user stories, they reflect the
practical views of stakeholders; like object-oriented requirements, they lend
themselves to evolution and reuse.
","[{'version': 'v1', 'created': 'Tue, 6 Sep 2022 02:47:20 GMT'}, {'version': 'v2', 'created': 'Wed, 14 Dec 2022 09:39:38 GMT'}]",2022-12-15,['Software Engineering'],"This paper presents a unified framework for specifying, scenario-based testing, and validating object-oriented requirements. The framework integrates the concepts of object-oriented modeling, scenario-based testing, and formal verification techniques to provide a comprehensive approach to software engineering. It is demonstrated through a case study of a library management system. The framework is composed of three distinct components: an object-oriented modeling language, a scenario-based testing language, and a formal verification language. The modeling language is used to capture the requirements of the system, while the testing language is used to generate and validate test cases. The formal verification language is used to formally verify the system's behavior. The paper provides a detailed description of the framework, including an example of its application to the library management system. It also discusses the advantages of this unified approach, such as improved accuracy and consistency, and provides suggestions for future research.","Write an abstract for a paper called Object-Oriented Requirements: a Unified Framework for Specifications,
  Scenarios and Tests about Software Engineering"
2204.13902,"Qinsheng Zhang, Yongxin Chen",Fast Sampling of Diffusion Models with Exponential Integrator,['cs.LG'],"  The past few years have witnessed the great success of Diffusion models~(DMs)
in generating high-fidelity samples in generative modeling tasks. A major
limitation of the DM is its notoriously slow sampling procedure which normally
requires hundreds to thousands of time discretization steps of the learned
diffusion process to reach the desired accuracy. Our goal is to develop a fast
sampling method for DMs with a much less number of steps while retaining high
sample quality. To this end, we systematically analyze the sampling procedure
in DMs and identify key factors that affect the sample quality, among which the
method of discretization is most crucial. By carefully examining the learned
diffusion process, we propose Diffusion Exponential Integrator Sampler~(DEIS).
It is based on the Exponential Integrator designed for discretizing ordinary
differential equations (ODEs) and leverages a semilinear structure of the
learned diffusion process to reduce the discretization error. The proposed
method can be applied to any DMs and can generate high-fidelity samples in as
few as 10 steps. In our experiments, it takes about 3 minutes on one A6000 GPU
to generate $50k$ images from CIFAR10. Moreover, by directly using pre-trained
DMs, we achieve the state-of-art sampling performance when the number of score
function evaluation~(NFE) is limited, e.g., 4.17 FID with 10 NFEs, 3.37 FID,
and 9.74 IS with only 15 NFEs on CIFAR10. Code is available at
https://github.com/qsh-zh/deis
","[{'version': 'v1', 'created': 'Fri, 29 Apr 2022 06:32:38 GMT'}, {'version': 'v2', 'created': 'Mon, 30 May 2022 08:26:57 GMT'}, {'version': 'v3', 'created': 'Sat, 18 Jun 2022 19:40:19 GMT'}, {'version': 'v4', 'created': 'Sat, 25 Feb 2023 20:30:35 GMT'}]",2023-02-28,['Machine Learning'],"This paper presents a novel approach to fast sampling of diffusion models with exponential integrators for machine learning applications. Specifically, we propose an algorithm based on an exponential integrator that can sample from a wide range of diffusion models in a fraction of the time needed by traditional sampling methods. We show that our approach can sample from diffusion models with arbitrary drift and diffusion coefficients with significantly reduced computational cost. We demonstrate our approach on several examples and show that it can produce accurate results with a fraction of the time and memory required by traditional methods. Our approach is particularly useful for machine learning applications that require large numbers of samples from diffusion models.",Write an abstract for a paper called Fast Sampling of Diffusion Models with Exponential Integrator about Machine Learning
1901.00715,"A. M. Khalili, Abdel-Hamid Soliman, Md Asaduzzaman, Alison Griffiths",Wi-Fi Sensing: Applications and Challenges,"['cs.HC', 'cs.IT', 'math.IT']","  Wi-Fi technology has strong potentials in indoor and outdoor sensing
applications, it has several important features which makes it an appealing
option compared to other sensing technologies. This paper presents a survey on
different applications of Wi-Fi based sensing systems such as elderly people
monitoring, activity classification, gesture recognition, people counting,
through the wall sensing, behind the corner sensing, and many other
applications. The challenges and interesting future directions are also
highlighted.
","[{'version': 'v1', 'created': 'Wed, 2 Jan 2019 10:46:10 GMT'}, {'version': 'v2', 'created': 'Thu, 7 Mar 2019 09:00:05 GMT'}, {'version': 'v3', 'created': 'Tue, 12 Mar 2019 20:34:38 GMT'}, {'version': 'v4', 'created': 'Tue, 28 May 2019 15:57:42 GMT'}, {'version': 'v5', 'created': 'Wed, 2 Mar 2022 18:58:49 GMT'}]",2022-03-03,"['Human-Computer Interaction', 'Information Theory']",", and Network Security

This paper presents a comprehensive overview of Wi-Fi sensing technology, its applications, and the associated challenges in the areas of human-computer interaction, information theory, and network security. Wi-Fi sensing is a novel technology that enables the detection of objects and people through wireless signals. The paper discusses the potential applications of Wi-Fi sensing in various domains such as smart homes, indoor localization, and healthcare. Additionally, the paper provides an analysis of the challenges posed by Wi-Fi sensing in terms of privacy, data accuracy, and scalability. The paper also explores the implications of Wi-Fi sensing for information theory and network security, and discusses potential solutions to these challenges. Finally, the paper provides a summary of current research and future directions for Wi-Fi sensing.","Write an abstract for a paper called Wi-Fi Sensing: Applications and Challenges about Human-Computer Interaction, Information Theory"
2208.10168,"Merav Parter, Asaf Petruschka",\~{O}ptimal Dual Vertex Failure Connectivity Labels,['cs.DS'],"  In this paper we present succinct labeling schemes for supporting
connectivity queries under vertex faults. For a given $n$-vertex graph $G$, an
$f$-VFT (resp., EFT) connectivity labeling scheme is a distributed data
structure that assigns each of the graph edges and vertices a short label, such
that given the labels of a vertex pair $u$ and $v$, and the labels of at most
$f$ failing vertices (resp., edges) $F$, one can determine if $u$ and $v$ are
connected in $G \setminus F$. The primary complexity measure is the length of
the individual labels. Since their introduction by [Courcelle, Twigg, STACS
'07], FT labeling schemes have been devised only for a limited collection of
graph families. A recent work [Dory and Parter, PODC 2021] provided EFT
labeling schemes for general graphs under edge failures, leaving the vertex
failure case fairly open. We provide the first sublinear $f$-VFT labeling
schemes for $f \geq 2$ for any $n$-vertex graph. Our key result is $2$-VFT
connectivity labels with $O(\log^3 n)$ bits. Our constructions are based on
analyzing the structure of dual failure replacement paths on top of the
well-known heavy-light tree decomposition technique of [Sleator and Tarjan,
STOC 1981]. We also provide $f$-VFT labels with sub-linear length (in $|V|$)
for any $f=o(\log\log n)$, that are based on a reduction to the existing EFT
labels.
","[{'version': 'v1', 'created': 'Mon, 22 Aug 2022 09:30:18 GMT'}]",2022-08-23,['Data Structures and Algorithms'],"This paper presents a new data structure and algorithm for labeling dual vertex failure connectivity in a graph. The proposed algorithm is based on the concept of optimal dual vertex failure connectivity, which is defined as the smallest number of labels required to identify the nodes in a graph that are connected in the event of a dual vertex failure. The optimal dual vertex failure connectivity algorithm is designed to be efficient and effective, and is evaluated using several real-world datasets. Results show that the proposed algorithm outperforms existing algorithms for dual vertex failure connectivity labeling in terms of time and space complexity. Furthermore, the paper also discusses possible future improvements and applications of the proposed algorithm.",Write an abstract for a paper called \~{O}ptimal Dual Vertex Failure Connectivity Labels about Data Structures and Algorithms
2302.11865,"Wen-Jie Tseng, Samuel Huron, Eric Lecolinet, Jan Gugenheimer","FingerMapper: Mapping Finger Motions onto Virtual Arms to Enable Safe
  Virtual Reality Interaction in Confined Spaces",['cs.HC'],"  Whole-body movements enhance the presence and enjoyment of Virtual Reality
(VR) experiences. However, using large gestures is often uncomfortable and
impossible in confined spaces (e.g., public transport). We introduce
FingerMapper, mapping small-scale finger motions onto virtual arms and hands to
enable whole-body virtual movements in VR. In a first target selection study
(n=13) comparing FingerMapper to hand tracking and ray-casting, we found that
FingerMapper can significantly reduce physical motions and fatigue while having
a similar degree of precision. In a consecutive study (n=13), we compared
FingerMapper to hand tracking inside a confined space (the front passenger seat
of a car). The results showed participants had significantly higher perceived
safety and fewer collisions with FingerMapper while preserving a similar degree
of presence and enjoyment as hand tracking. Finally, we present three example
applications demonstrating how FingerMapper could be applied for locomotion and
interaction for VR in confined spaces.
","[{'version': 'v1', 'created': 'Thu, 23 Feb 2023 09:11:38 GMT'}]",2023-02-24,['Human-Computer Interaction'],"This paper presents FingerMapper, a novel human-computer interaction (HCI) technique for enabling safe virtual reality (VR) interaction in confined spaces. FingerMapper uses a custom-built hardware device to map finger motions onto virtual arms, allowing users to interact with virtual objects without needing to move their real arms. The device is designed to be low-cost and easy to use, and it is tested in a study of 30 participants. Results show that FingerMapper provides a natural, intuitive way for users to interact with virtual objects, and it is especially useful for users in confined spaces. This paper provides a valuable contribution to the field of HCI by introducing an innovative, low-cost solution for enabling safe VR interaction in confined spaces.","Write an abstract for a paper called FingerMapper: Mapping Finger Motions onto Virtual Arms to Enable Safe
  Virtual Reality Interaction in Confined Spaces about Human-Computer Interaction"
2205.06337,"Ovidiu Gherman, Cristina Elena Turcu, Corneliu Octavian Turcu",An Approach to Adaptive Microlearning in Higher Education,['cs.CY'],"  Current changes in society and the education system, cumulated with the
accelerated development of new technologies, entail inherent changes in the
educational process. Numerous studies have shown that the pandemic has forced a
rapid transformation of higher education. Thus, if before the pandemic digital
technologies were used to supplement the learning process, now they are the
main means of learning delivery. In addition, as previous research has shown,
new pedagogical strategies and new ways of teaching and learning are needed for
the current generation of students, the so-called Generation Z, to acquire new
knowledge and develop new skills. In this necessary evolution of the
educational process, a possible solution to increase the effectiveness of the
learning process for the Generation Z students is to use microlearning to
extend the traditional ways of learning. Many studies have shown that
microlearning, based on how today's students learn and memorize, facilitates
learning. In recent years there has been a growing trend in their use of
microlearning in the educational process. But, in order to be effective, this
approach must allow the individual knowledge building, by indicating a guiding
direction of the optimal path to achieve the proposed objectives. We propose a
system for personalized learning using microlearning, which provides support
and guidance to students based on their individual needs, in order to increase
their interest in learning, but also to compensate for various deficiencies in
their educational background. We also present a case study from the higher
education sector. Feedback from students and data collected during the semester
as a result of the students' behavioural analysis and their real learning
motivations will be used to improve the proposed system.
","[{'version': 'v1', 'created': 'Wed, 11 May 2022 11:58:22 GMT'}]",2022-05-16,['Computers and Society'],"This paper examines the potential for adaptive microlearning in higher education to improve student engagement and learning outcomes in the Computers and Society course. Specifically, this paper provides an overview of the current state of adaptive microlearning and its potential to improve student learning in the Computers and Society course. The paper then outlines a proposed approach to adaptive microlearning in the course, including a discussion of the technology and strategies necessary to successfully implement the approach. Finally, the paper discusses the potential benefits of adaptive microlearning and how it can be used to create a more engaging and effective learning experience for students. The paper concludes by discussing the implications of the proposed approach and recommendations for further research.",Write an abstract for a paper called An Approach to Adaptive Microlearning in Higher Education about Computers and Society
2205.01388,"Junfeng Yin, Nan Li and Ning Zheng","Restarted randomized surrounding methods for solving large linear
  equations","['math.NA', 'cs.NA']","  A class of restarted randomized surrounding methods are presented to
accelerate the surrounding algorithms by restarted techniques for solving the
linear equations. Theoretical analysis prove that the proposed method converges
under the randomized row selection rule and the expectation convergence rate is
also addressed. Numerical experiments further demonstrate that the proposed
algorithms are efficient and outperform the existing method for over-determined
and under-determined linear equation, as well as in the application of image
processing.
","[{'version': 'v1', 'created': 'Tue, 3 May 2022 09:30:42 GMT'}, {'version': 'v2', 'created': 'Sat, 9 Jul 2022 01:09:02 GMT'}]",2022-07-12,['Numerical Analysis'],"This paper presents a novel approach to solving large linear equations using Restarted Randomized Surrounding (RRS) methods. We discuss the numerical analysis of RRS methods, focusing on their ability to solve large linear equations efficiently. We compare the performance of RRS methods to existing methods such as the Gauss-Seidel, Jacobi, and Conjugate Gradient methods. We then analyze the numerical stability of RRS methods, and discuss the effects of restarting on the convergence of the solution. We demonstrate the effectiveness of RRS methods through numerical experiments, and discuss the implications of our results for numerical analysis.","Write an abstract for a paper called Restarted randomized surrounding methods for solving large linear
  equations about Numerical Analysis"
2205.1284,"Divya Kothandaraman, Sumit Shekhar, Abhilasha Sancheti, Manoj Ghuhan,
  Tripti Shukla, Dinesh Manocha","SALAD: Source-free Active Label-Agnostic Domain Adaptation for
  Classification, Segmentation and Detection",['cs.CV'],"  We present a novel method, SALAD, for the challenging vision task of adapting
a pre-trained ""source"" domain network to a ""target"" domain, with a small budget
for annotation in the ""target"" domain and a shift in the label space. Further,
the task assumes that the source data is not available for adaptation, due to
privacy concerns or otherwise. We postulate that such systems need to jointly
optimize the dual task of (i) selecting fixed number of samples from the target
domain for annotation and (ii) transfer of knowledge from the pre-trained
network to the target domain. To do this, SALAD consists of a novel Guided
Attention Transfer Network (GATN) and an active learning function, HAL. The
GATN enables feature distillation from pre-trained network to the target
network, complemented with the target samples mined by HAL using
transfer-ability and uncertainty criteria. SALAD has three key benefits: (i) it
is task-agnostic, and can be applied across various visual tasks such as
classification, segmentation and detection; (ii) it can handle shifts in output
label space from the pre-trained source network to the target domain; (iii) it
does not require access to source data for adaptation. We conduct extensive
experiments across 3 visual tasks, viz. digits classification (MNIST, SVHN,
VISDA), synthetic (GTA5) to real (CityScapes) image segmentation, and document
layout detection (PubLayNet to DSSE). We show that our source-free approach,
SALAD, results in an improvement of 0.5%-31.3%(across datasets and tasks) over
prior adaptation methods that assume access to large amounts of annotated
source data for adaptation.
","[{'version': 'v1', 'created': 'Tue, 24 May 2022 05:50:49 GMT'}, {'version': 'v2', 'created': 'Fri, 15 Jul 2022 09:32:28 GMT'}, {'version': 'v3', 'created': 'Sat, 22 Oct 2022 11:57:13 GMT'}]",2022-10-26,['Computer Vision and Pattern Recognition'],"This paper presents SALAD, a novel Source-free Active Label-Agnostic Domain Adaptation (SLADA) approach for computer vision and pattern recognition tasks, including classification, segmentation, and detection. SLADA is a domain adaptation technique that can be used to improve the performance of a model trained on a source domain by leveraging unlabeled data from a target domain. It uses an active learning strategy to select the most informative samples from the target domain, and a domain adaptation network to adapt the source model to the target domain. The proposed SLADA approach is evaluated on several benchmark datasets and compared to state-of-the-art methods. The results demonstrate that SALAD can achieve significant performance improvements over existing approaches.","Write an abstract for a paper called SALAD: Source-free Active Label-Agnostic Domain Adaptation for
  Classification, Segmentation and Detection about Computer Vision and Pattern Recognition"
2110.01011,"M. F. Kaloorazi, K. Liu, J. Chen, and R. C. de Lamare","An Efficient Randomized QLP Algorithm for Approximating the Singular
  Value Decomposition","['math.NA', 'cs.NA', 'eess.SP']","  In this paper, we introduce a randomized QLP decomposition called Rand-QLP.
Operating on a matrix $\bf A$, Rand-QLP gives ${\bf A}={\bf QLP}^T$, where $\bf
Q$ and $\bf P$ are orthonormal, and $\bf L$ is lower-triangular. Under the
assumption that the rank of the input matrix is $k$, we derive several error
bounds for Rand-QLP: bounds for the first $k$ approximate singular values and
for the trailing block of the middle factor $\bf L$, which show that the
decomposition is rank-revealing; bounds for the distance between approximate
subspaces and the exact ones for all four fundamental subspaces of a given
matrix; and bounds for the errors of low-rank approximations constructed by the
columns of $\bf Q$ and $\bf P$. Rand-QLP is able to effectively leverage modern
computational architectures, due to the utilization of random sampling and the
unpivoted QR decomposition, thus addressing a serious bottleneck associated
with classical algorithms such as the singular value decomposition (SVD),
column-pivoted QR (CPQR) and most recent matrix decomposition algorithms. To
assess the performance behavior of different algorithms, we use an Intel Xeon
Gold 6240 CPU running at 2.6 GHz with a NVIDIA GeForce RTX 2080Ti GPU. In
comparison to CPQR and the SVD, Rand-QLP respectively achieves a speedup of up
to 5 times and 6.6 times on the CPU and up to 3.8 times and 4.4 times with the
hybrid GPU architecture. In terms of quality of approximation, our results on
synthetic and real data show that the approximations by Rand-QLP are comparable
to those of pivoted QLP and the optimal SVD, and in most cases are considerably
better than those of CPQR.
","[{'version': 'v1', 'created': 'Sun, 3 Oct 2021 14:29:52 GMT'}, {'version': 'v2', 'created': 'Sat, 28 Jan 2023 10:49:58 GMT'}]",2023-01-31,['Numerical Analysis'],"This paper presents an efficient randomized algorithm for approximating the Singular Value Decomposition (SVD) of a matrix. The algorithm, called the QLP algorithm, is based on the QR decomposition and is designed to work on large matrices. It is shown to be more efficient than existing algorithms, both in terms of memory usage and computational time. The paper also contains a thorough analysis of the algorithm's accuracy and stability. The results of the numerical experiments show that the QLP algorithm outperforms existing algorithms, both in terms of accuracy and efficiency. This paper will be useful for researchers in numerical analysis, as it provides an efficient and accurate approach for approximating the SVD of large matrices.","Write an abstract for a paper called An Efficient Randomized QLP Algorithm for Approximating the Singular
  Value Decomposition about Numerical Analysis"
2201.0539,"Eugen F\""uchsle, Hendrik Molter, Rolf Niedermeier, Malte Renken",Delay-Robust Routes in Temporal Graphs,['cs.DS'],"  Most transportation networks are inherently temporal: Connections (e.g.
flights, train runs) are only available at certain, scheduled times. When
transporting passengers or commodities, this fact must be considered for the
the planning of itineraries. This has already led to several well-studied
algorithmic problems on temporal graphs. The difficulty of the described task
is increased by the fact that connections are often unreliable -- in
particular, many modes of transportation suffer from occasional delays. If
these delays cause subsequent connections to be missed, the consequences can be
severe. Thus, it is a vital problem to design itineraries that are robust to
(small) delays. We initiate the study of this problem from a parameterized
complexity perspective by proving its NP-completeness as well as several
hardness and tractability results for natural parameterizations.
","[{'version': 'v1', 'created': 'Fri, 14 Jan 2022 11:03:24 GMT'}]",2022-01-17,['Data Structures and Algorithms'],"This paper presents a data structure and algorithm for finding delay-robust routes in temporal graphs. Temporal graphs are used to model dynamic networks, where the edge weights vary over time. We propose a data structure which stores the temporal graph as a collection of static subgraphs and a set of temporal edges that connect the subgraphs. We also present a delay-robust route finding algorithm which takes into account the temporal information to find routes that are robust to delays. We evaluate our approach on a real-world temporal graph and show that our algorithm finds routes that are significantly more robust to delays than routes found using traditional static route finding algorithms.",Write an abstract for a paper called Delay-Robust Routes in Temporal Graphs about Data Structures and Algorithms
2210.03765,"Wanrong Zhu, An Yan, Yujie Lu, Wenda Xu, Xin Eric Wang, Miguel
  Eckstein, William Yang Wang","Visualize Before You Write: Imagination-Guided Open-Ended Text
  Generation","['cs.CL', 'cs.AI']","  Recent advances in text-to-image synthesis make it possible to visualize
machine imaginations for a given context. On the other hand, when generating
text, human writers are gifted at creative visualization, which enhances their
writings by forming imaginations as blueprints before putting down the stories
in words. Inspired by such a cognitive process, we ask the natural question of
whether we can endow machines with the same ability to utilize visual
information and construct a general picture of the context to guide text
generation. In this work, we propose iNLG that uses machine-generated images to
guide language models in open-ended text generation. The experiments and
analyses demonstrate the effectiveness of iNLG on open-ended text generation
tasks, including text completion, story generation, and concept-to-text
generation in both few-shot and full-data scenarios. Both automatic metrics and
human evaluations verify that the text snippets generated by our iNLG are
coherent and informative while displaying minor degeneration.
","[{'version': 'v1', 'created': 'Fri, 7 Oct 2022 18:01:09 GMT'}, {'version': 'v2', 'created': 'Mon, 23 Jan 2023 14:35:24 GMT'}, {'version': 'v3', 'created': 'Fri, 3 Feb 2023 10:36:55 GMT'}, {'version': 'v4', 'created': 'Wed, 15 Feb 2023 03:16:36 GMT'}]",2023-02-16,"['Computation and Language', 'Artificial Intelligence']",", and Human-Computer Interaction

This paper explores the potential of imagination-guided open-ended text generation for computation and language, artificial intelligence, and human-computer interaction. By visualizing before writing, we can create powerful and creative narratives that can be used to explore, explain, and expand upon complex topics. We examine the current state of open-ended text generation, identify challenges, and propose new approaches to this field. We propose a framework for imagination-guided text generation that combines the power of visualizations with the flexibility of open-ended text generation to enable a new form of creative expression. We discuss the potential applications of this technology in the fields of artificial intelligence, human-computer interaction, and computation and language. Finally, we offer future directions for research and development in this area.","Write an abstract for a paper called Visualize Before You Write: Imagination-Guided Open-Ended Text
  Generation about Computation and Language, Artificial Intelligence"
2209.07676,Shenao Zhang,"Conservative Dual Policy Optimization for Efficient Model-Based
  Reinforcement Learning",['cs.LG'],"  Provably efficient Model-Based Reinforcement Learning (MBRL) based on
optimism or posterior sampling (PSRL) is ensured to attain the global
optimality asymptotically by introducing the complexity measure of the model.
However, the complexity might grow exponentially for the simplest nonlinear
models, where global convergence is impossible within finite iterations. When
the model suffers a large generalization error, which is quantitatively
measured by the model complexity, the uncertainty can be large. The sampled
model that current policy is greedily optimized upon will thus be unsettled,
resulting in aggressive policy updates and over-exploration. In this work, we
propose Conservative Dual Policy Optimization (CDPO) that involves a
Referential Update and a Conservative Update. The policy is first optimized
under a reference model, which imitates the mechanism of PSRL while offering
more stability. A conservative range of randomness is guaranteed by maximizing
the expectation of model value. Without harmful sampling procedures, CDPO can
still achieve the same regret as PSRL. More importantly, CDPO enjoys monotonic
policy improvement and global optimality simultaneously. Empirical results also
validate the exploration efficiency of CDPO.
","[{'version': 'v1', 'created': 'Fri, 16 Sep 2022 02:27:01 GMT'}]",2022-09-19,['Machine Learning'],"This paper proposes a novel conservative dual policy optimization (CDPO) algorithm for efficient model-based reinforcement learning (MBRL) in the context of machine learning (ML). CDPO is a two-step optimization procedure that combines a model-free reinforcement learning (RL) policy with a model-based policy. The RL policy is used to explore the environment and generate data for model learning, while the model-based policy is used to exploit the learned model. We show that CDPO can achieve faster convergence and better performance than existing MBRL algorithms, while maintaining the same level of data efficiency. We demonstrate the effectiveness of CDPO in a variety of simulated and real-world ML tasks.","Write an abstract for a paper called Conservative Dual Policy Optimization for Efficient Model-Based
  Reinforcement Learning about Machine Learning"
2205.15915,"Lorenzo Ceragioli, Letterio Galletta, Pierpaolo Degano and David Basin","IFCIL: An Information Flow Configuration Language for SELinux (Extended
  Version)",['cs.CR'],"  Security Enhanced Linux (SELinux) is a security architecture for Linux
implementing mandatory access control. It has been used in numerous
security-critical contexts ranging from servers to mobile devices. But this is
challenging as SELinux security policies are difficult to write, understand,
and maintain. Recently, the intermediate language CIL was introduced to foster
the development of high-level policy languages and to write structured
configurations. However, CIL lacks mechanisms for ensuring that the resulting
configurations obey desired information flow policies. To remedy this, we
propose IFCIL, a backward compatible extension of CIL for specifying
fine-grained information flow requirements for CIL configurations. Using IFCIL,
administrators can express, e.g., confidentiality, integrity, and
non-interference properties. We also provide a tool to statically verify these
requirements.
","[{'version': 'v1', 'created': 'Tue, 31 May 2022 16:03:53 GMT'}]",2022-06-01,['Cryptography and Security'],"This paper presents IFCIL, a novel information flow configuration language for SELinux. IFCIL is a declarative language that enables users to express security policies in terms of information flow control. The language provides a rich set of features for specifying security policies, including a set of security labels, security contexts, and information flow control operators. IFCIL is designed to be extensible and provides a comprehensive set of tools for configuring and managing security policies in SELinux. The paper also describes how IFCIL can be used to implement cryptography and security policies for SELinux. The paper evaluates the performance of IFCIL and discusses its potential for enhancing the security of SELinux systems.","Write an abstract for a paper called IFCIL: An Information Flow Configuration Language for SELinux (Extended
  Version) about Cryptography and Security"
2104.10745,"Adrian Celaya, Jonas A. Actor, Rajarajeswari Muthusivarajan, Evan
  Gates, Caroline Chung, Dawid Schellingerhout, Beatrice Riviere, David Fuentes",PocketNet: A Smaller Neural Network for Medical Image Analysis,"['eess.IV', 'cs.CV', 'cs.LG']","  Medical imaging deep learning models are often large and complex, requiring
specialized hardware to train and evaluate these models. To address such
issues, we propose the PocketNet paradigm to reduce the size of deep learning
models by throttling the growth of the number of channels in convolutional
neural networks. We demonstrate that, for a range of segmentation and
classification tasks, PocketNet architectures produce results comparable to
that of conventional neural networks while reducing the number of parameters by
multiple orders of magnitude, using up to 90% less GPU memory, and speeding up
training times by up to 40%, thereby allowing such models to be trained and
deployed in resource-constrained settings.
","[{'version': 'v1', 'created': 'Wed, 21 Apr 2021 20:10:30 GMT'}, {'version': 'v2', 'created': 'Thu, 24 Jun 2021 03:29:46 GMT'}, {'version': 'v3', 'created': 'Wed, 18 May 2022 14:34:55 GMT'}, {'version': 'v4', 'created': 'Sun, 18 Sep 2022 17:56:01 GMT'}]",2022-09-20,"['Computer Vision and Pattern Recognition', 'Machine Learning']","This paper presents PocketNet, a novel neural network architecture for medical image analysis. PocketNet is a smaller network that is designed to reduce the computational complexity of deep neural networks while still achieving high accuracy in medical image analysis tasks. PocketNet is based on the MobileNetV2 architecture and uses a combination of depthwise separable convolutions, pointwise convolutions, and global average pooling layers to reduce the number of parameters and computational complexity. We evaluate PocketNet on the Chest X-ray14 dataset and show that it outperforms other small networks on the task of pneumonia classification. In addition, we show that PocketNet is more computationally efficient than other small networks, achieving a 31% reduction in training time and a 20% reduction in inference time. We also compare PocketNet to larger networks on the task of lung nodule detection and show that PocketNet achieves comparable performance with significantly fewer parameters. Our results demonstrate the effectiveness of PocketNet for medical image analysis tasks and suggest that it is a viable alternative to larger networks.","Write an abstract for a paper called PocketNet: A Smaller Neural Network for Medical Image Analysis about Computer Vision and Pattern Recognition, Machine Learning"
2210.16782,"Shengbang Tong, Xili Dai, Yubei Chen, Mingyang Li, Zengyi Li, Brent
  Yi, Yann LeCun, Yi Ma","Unsupervised Learning of Structured Representations via Closed-Loop
  Transcription",['cs.CV'],"  This paper proposes an unsupervised method for learning a unified
representation that serves both discriminative and generative purposes. While
most existing unsupervised learning approaches focus on a representation for
only one of these two goals, we show that a unified representation can enjoy
the mutual benefits of having both. Such a representation is attainable by
generalizing the recently proposed \textit{closed-loop transcription}
framework, known as CTRL, to the unsupervised setting. This entails solving a
constrained maximin game over a rate reduction objective that expands features
of all samples while compressing features of augmentations of each sample.
Through this process, we see discriminative low-dimensional structures emerge
in the resulting representations. Under comparable experimental conditions and
network complexities, we demonstrate that these structured representations
enable classification performance close to state-of-the-art unsupervised
discriminative representations, and conditionally generated image quality
significantly higher than that of state-of-the-art unsupervised generative
models. Source code can be found at https://github.com/Delay-Xili/uCTRL.
","[{'version': 'v1', 'created': 'Sun, 30 Oct 2022 09:09:05 GMT'}]",2022-11-01,['Computer Vision and Pattern Recognition'],"This paper presents a novel approach to unsupervised learning of structured representations in computer vision and pattern recognition. Our method, Closed-Loop Transcription (CLT), leverages temporal coherence in video streams to learn meaningful representations of objects and events in an unsupervised manner. We demonstrate the efficacy of our approach on a variety of tasks, such as object recognition, scene segmentation, and action recognition. CLT is able to learn representations of objects and events without requiring any prior knowledge or labels. We also show that CLT can be used to improve the performance of supervised learning tasks, such as image classification. We conclude with a discussion of the potential applications of CLT and its implications for computer vision and pattern recognition research.","Write an abstract for a paper called Unsupervised Learning of Structured Representations via Closed-Loop
  Transcription about Computer Vision and Pattern Recognition"
2211.0425,"Nishtha Madaan, Adithya Manjunatha, Hrithik Nambiar, Aviral Kumar
  Goel, Harivansh Kumar, Diptikalyan Saha, Srikanta Bedathur",DetAIL : A Tool to Automatically Detect and Analyze Drift In Language,"['cs.LG', 'cs.AI', 'cs.CL']","  Machine learning and deep learning-based decision making has become part of
today's software. The goal of this work is to ensure that machine learning and
deep learning-based systems are as trusted as traditional software. Traditional
software is made dependable by following rigorous practice like static
analysis, testing, debugging, verifying, and repairing throughout the
development and maintenance life-cycle. Similarly for machine learning systems,
we need to keep these models up to date so that their performance is not
compromised. For this, current systems rely on scheduled re-training of these
models as new data kicks in. In this work, we propose to measure the data drift
that takes place when new data kicks in so that one can adaptively re-train the
models whenever re-training is actually required irrespective of schedules. In
addition to that, we generate various explanations at sentence level and
dataset level to capture why a given payload text has drifted.
","[{'version': 'v1', 'created': 'Thu, 3 Nov 2022 19:50:12 GMT'}]",2022-11-09,"['Machine Learning', 'Artificial Intelligence', 'Computation and Language']","This paper presents DetAIL, a tool to automatically detect and analyze drift in language about machine learning, artificial intelligence, computation and language. DetAIL uses natural language processing (NLP) techniques to analyze the language of articles, blog posts, and other online sources to detect changes in the way concepts are discussed over time. DetAIL also provides an analysis of the changes in language, allowing for a better understanding of how language about these topics is evolving. Additionally, DetAIL can be used to identify emerging trends in the language used to discuss these topics. The paper will discuss the methodology used to create DetAIL, the results of its application, and the implications of its use.","Write an abstract for a paper called DetAIL : A Tool to Automatically Detect and Analyze Drift In Language about Machine Learning, Artificial Intelligence, Computation and Language"
2205.11784,"Andrzej Reinke, Matteo Palieri, Benjamin Morrell, Yun Chang, Kamak
  Ebadi, Luca Carlone, Ali-akbar Agha-mohammadi","LOCUS 2.0: Robust and Computationally Efficient Lidar Odometry for
  Real-Time Underground 3D Mapping",['cs.RO'],"  Lidar odometry has attracted considerable attention as a robust localization
method for autonomous robots operating in complex GNSS-denied environments.
However, achieving reliable and efficient performance on heterogeneous
platforms in large-scale environments remains an open challenge due to the
limitations of onboard computation and memory resources needed for autonomous
operation. In this work, we present LOCUS 2.0, a robust and
computationally-efficient \lidar odometry system for real-time underground 3D
mapping. LOCUS 2.0 includes a novel normals-based \morrell{Generalized
Iterative Closest Point (GICP)} formulation that reduces the computation time
of point cloud alignment, an adaptive voxel grid filter that maintains the
desired computation load regardless of the environment's geometry, and a
sliding-window map approach that bounds the memory consumption. The proposed
approach is shown to be suitable to be deployed on heterogeneous robotic
platforms involved in large-scale explorations under severe computation and
memory constraints. We demonstrate LOCUS 2.0, a key element of the CoSTAR
team's entry in the DARPA Subterranean Challenge, across various underground
scenarios.
  We release LOCUS 2.0 as an open-source library and also release a
\lidar-based odometry dataset in challenging and large-scale underground
environments. The dataset features legged and wheeled platforms in multiple
environments including fog, dust, darkness, and geometrically degenerate
surroundings with a total of $11~h$ of operations and $16~km$ of distance
traveled.
","[{'version': 'v1', 'created': 'Tue, 24 May 2022 04:51:08 GMT'}, {'version': 'v2', 'created': 'Mon, 13 Jun 2022 08:55:23 GMT'}]",2022-06-14,['Robotics'],"This paper presents LOCUS 2.0, a robust and computationally efficient lidar odometry system for real-time 3D mapping of underground environments. LOCUS 2.0 is based on a model-free approach that combines a local scan-to-scan registration algorithm with a global optimization step. It is designed to be used in challenging underground environments, where traditional odometry methods fail due to the lack of sufficient features. The proposed system is evaluated on a real-world dataset, and the results show that it is able to accurately reconstruct the 3D environment in real-time while maintaining a low computational complexity. The paper also discusses the potential applications of the system in robotics, such as autonomous navigation and exploration.","Write an abstract for a paper called LOCUS 2.0: Robust and Computationally Efficient Lidar Odometry for
  Real-Time Underground 3D Mapping about Robotics"
2111.04576,"Malintha Fernando, Ransalu Senanayake, Martin Swany","CoCo Games: Graphical Game-Theoretic Swarm Control for
  Communication-Aware Coverage","['cs.RO', 'cs.AI', 'cs.SY', 'eess.SY']","  We propose a novel framework for real-time communication-aware coverage
control in networked robot swarms. Our framework unifies the robot dynamics
with network-level message-routing to reach consensus on swarm formations in
the presence of communication uncertainties by leveraging local information.
Specifically, we formulate the communication-aware coverage as a cooperative
graphical game, and use variational inference to reach mixed strategy Nash
equilibria of the stage games. We experimentally validate the proposed approach
in a mobile ad-hoc wireless network scenario using teams of aerial vehicles and
terrestrial user equipment (UE) operating over a large geographic region of
interest. We show that our approach can provide wireless coverage to stationary
and mobile UEs under realistic network conditions.
","[{'version': 'v1', 'created': 'Mon, 8 Nov 2021 15:37:15 GMT'}, {'version': 'v2', 'created': 'Fri, 8 Apr 2022 15:08:38 GMT'}, {'version': 'v3', 'created': 'Tue, 12 Apr 2022 15:17:25 GMT'}, {'version': 'v4', 'created': 'Mon, 25 Apr 2022 16:10:58 GMT'}, {'version': 'v5', 'created': 'Thu, 28 Apr 2022 20:29:40 GMT'}]",2022-05-02,"['Robotics', 'Artificial Intelligence', 'Systems and Control']","This paper presents CoCo Games, a novel graphical game-theoretic swarm control approach for communication-aware coverage of robotic systems. CoCo Games combines game-theoretic principles with graph-based representations to enable robots to self-organize and collaborate in order to efficiently cover an area. The proposed method is validated by simulation and experiments on a team of real robots. Results show that CoCo Games can effectively coordinate the robots to cover the area, while considering communication constraints. The proposed approach has potential applications in various robotic scenarios such as surveillance, search and rescue, and environmental monitoring.","Write an abstract for a paper called CoCo Games: Graphical Game-Theoretic Swarm Control for
  Communication-Aware Coverage about Robotics, Artificial Intelligence, Systems and Control"
2108.08296,"Mengqi Zhang, Yanqiao Zhu, Qiang Liu, Shu Wu and Liang Wang",Deep Contrastive Multiview Network Embedding,"['cs.LG', 'cs.AI']","  Multiview network embedding aims at projecting nodes in the network to
low-dimensional vectors, while preserving their multiple relations and
attribute information. Contrastive learning approaches have shown promising
performance in this task. However, they neglect the semantic consistency
between fused and view representations and have difficulty in modeling
complementary information between different views. To deal with these
deficiencies, this work presents a novel Contrastive leaRning framEwork for
Multiview network Embedding (CREME). In our work, different views can be
obtained based on the various relations among nodes. Then, we generate view
embeddings via proper view encoders and utilize an attentive multiview
aggregator to fuse these representations. Particularly, we design two
collaborative contrastive objectives, view fusion InfoMax and inter-view
InfoMin, to train the model in a self-supervised manner. The former objective
distills information from embeddings generated from different views, while the
latter captures complementary information among views to promote distinctive
view embeddings. We also show that the two objectives can be unified into one
objective for model training. Extensive experiments on three real-world
datasets demonstrate that our proposed CREME is able to consistently outperform
state-of-the-art methods.
","[{'version': 'v1', 'created': 'Mon, 16 Aug 2021 06:29:18 GMT'}, {'version': 'v2', 'created': 'Wed, 17 Aug 2022 13:17:18 GMT'}]",2022-08-18,"['Machine Learning', 'Artificial Intelligence']","This paper presents a novel approach to machine learning and artificial intelligence called Deep Contrastive Multiview Network Embedding. The proposed method combines two existing techniques: deep learning and network embedding. In the proposed approach, a deep learning model is used to learn a shared embedding space from multiple views of the data. The embedding space is then used to generate a contrastive multiview network embedding representation, which is used to measure the similarity between different data points. The proposed method is evaluated on three datasets and experiments demonstrate that it outperforms existing methods for network embedding. The results show that the proposed method can effectively capture the underlying structure of a graph and is suitable for applications such as link prediction and node classification.","Write an abstract for a paper called Deep Contrastive Multiview Network Embedding about Machine Learning, Artificial Intelligence"
2211.16695,"Xiaojiang Zhang, Peng Song, Yi Shi, and Min Tang","A fully asymptotic preserving decomposed multi-group method for the
  frequency-dependent radiative transfer equations","['math.NA', 'cs.NA']","  The opacity of FRTE depends on not only the material temperature but also the
frequency, whose values may vary several orders of magnitude for different
frequencies. The gray radiation diffusion and frequency-dependent diffusion
equations are two simplified models that can approximate the solution to FRTE
in the thick opacity regime. The frequency discretization for the two limit
models highly affects the numerical accuracy. However, classical frequency
discretization for FRTE considers only the absorbing coefficient. In this
paper, we propose a new decomposed multi-group method for frequency
discretization that is not only AP in both gray radiation diffusion and
frequency-dependent diffusion limits, but also the frequency discretization of
the limiting models can be tuned. Based on the decomposed multi-group method, a
full AP scheme in frequency, time, and space is proposed. Several numerical
examples are used to verify the performance of the proposed scheme.
","[{'version': 'v1', 'created': 'Wed, 30 Nov 2022 02:30:45 GMT'}]",2022-12-01,['Numerical Analysis'],"This paper presents a fully asymptotic preserving (FAP) decomposed multi-group method for the frequency-dependent radiative transfer equations (RTEs). The FAP decomposition is based on a novel combination of the discrete ordinates method (DOM) and the asymptotic preserving (AP) schemes. The proposed method is able to accurately and efficiently solve the RTEs in both the optically thick and optically thin regimes. The numerical accuracy and stability of the proposed approach are demonstrated through a series of numerical test cases. The results show that the proposed method is able to provide accurate solutions with a moderate computational cost, and is thus suitable for applications in which both optically thick and optically thin regimes are present.","Write an abstract for a paper called A fully asymptotic preserving decomposed multi-group method for the
  frequency-dependent radiative transfer equations about Numerical Analysis"
1905.13736,"Yair Carmon, Aditi Raghunathan, Ludwig Schmidt, Percy Liang and John
  C. Duchi",Unlabeled Data Improves Adversarial Robustness,"['stat.ML', 'cs.CV', 'cs.LG']","  We demonstrate, theoretically and empirically, that adversarial robustness
can significantly benefit from semisupervised learning. Theoretically, we
revisit the simple Gaussian model of Schmidt et al. that shows a sample
complexity gap between standard and robust classification. We prove that
unlabeled data bridges this gap: a simple semisupervised learning procedure
(self-training) achieves high robust accuracy using the same number of labels
required for achieving high standard accuracy. Empirically, we augment CIFAR-10
with 500K unlabeled images sourced from 80 Million Tiny Images and use robust
self-training to outperform state-of-the-art robust accuracies by over 5 points
in (i) $\ell_\infty$ robustness against several strong attacks via adversarial
training and (ii) certified $\ell_2$ and $\ell_\infty$ robustness via
randomized smoothing. On SVHN, adding the dataset's own extra training set with
the labels removed provides gains of 4 to 10 points, within 1 point of the gain
from using the extra labels.
","[{'version': 'v1', 'created': 'Fri, 31 May 2019 17:41:33 GMT'}, {'version': 'v2', 'created': 'Mon, 10 Jun 2019 17:27:27 GMT'}, {'version': 'v3', 'created': 'Wed, 4 Dec 2019 00:17:16 GMT'}, {'version': 'v4', 'created': 'Thu, 13 Jan 2022 17:20:07 GMT'}]",2022-01-14,"['Computer Vision and Pattern Recognition', 'Machine Learning']","This paper explores how unlabeled data can be used to improve the robustness of computer vision and pattern recognition systems against adversarial attacks. Adversarial attacks are a major threat to the security of machine learning systems and can lead to serious consequences. Unlabeled data can be used to improve the robustness of these systems by providing additional information for the machine learning algorithm to learn from, as well as by providing additional context for the model to use when making decisions. We present a case study of how unlabeled data can be used to improve the robustness of a convolutional neural network (CNN) against adversarial attacks. We show that the use of unlabeled data can significantly improve the robustness of the CNN against adversarial attacks and can also help to improve the generalization performance of the model. Our results demonstrate that unlabeled data can be a powerful tool for improving the security of machine learning systems.","Write an abstract for a paper called Unlabeled Data Improves Adversarial Robustness about Computer Vision and Pattern Recognition, Machine Learning"
2302.13253,Alexey Gorbatovski and Sergey Kovalchuk,"Bayesian Networks for Named Entity Prediction in Programming Community
  Question Answering","['cs.LG', 'cs.AI', 'math.PR']","  Within this study, we propose a new approach for natural language processing
using Bayesian networks to predict and analyze the context and how this
approach can be applied to the Community Question Answering domain. We discuss
how Bayesian networks can detect semantic relationships and dependencies
between entities, and this is connected to different score-based approaches of
structure-learning. We compared the Bayesian networks with different score
metrics, such as the BIC, BDeu, K2 and Chow-Liu trees. Our proposed approach
out-performs the baseline model at the precision metric. We also discuss the
influence of penalty terms on the structure of Bayesian networks and how they
can be used to analyze the relationships between entities. In addition, we
examine the visualization of directed acyclic graphs to analyze semantic
relationships. The article further identifies issues with detecting certain
semantic classes that are separated in the structure of directed acyclic
graphs. Finally, we evaluate potential improvements for the Bayesian network
approach.
","[{'version': 'v1', 'created': 'Sun, 26 Feb 2023 07:26:36 GMT'}]",2023-02-28,"['Machine Learning', 'Artificial Intelligence']","This paper presents an exploration of Bayesian Networks for Named Entity Prediction in Programming Community Question Answering about Machine Learning and Artificial Intelligence. We present a novel approach to predict named entities in programming-related questions about Machine Learning and Artificial Intelligence. We explore the use of Bayesian networks as a method for predicting named entities in programming questions. We then evaluate the performance of our approach on a dataset of questions from Stack Overflow, a popular programming Q&A website. We demonstrate that our approach is able to accurately predict named entities in programming questions, and that it outperforms the traditional methods. Our results suggest that Bayesian networks are an effective method for predicting named entities in programming questions about Machine Learning and Artificial Intelligence.","Write an abstract for a paper called Bayesian Networks for Named Entity Prediction in Programming Community
  Question Answering about Machine Learning, Artificial Intelligence"
2210.15386,"Kwanghee Choi, Eun Jung Yeo",Opening the Black Box of wav2vec Feature Encoder,"['cs.SD', 'cs.CL', 'cs.LG', 'eess.AS']","  Self-supervised models, namely, wav2vec and its variants, have shown
promising results in various downstream tasks in the speech domain. However,
their inner workings are poorly understood, calling for in-depth analyses on
what the model learns. In this paper, we concentrate on the convolutional
feature encoder where its latent space is often speculated to represent
discrete acoustic units. To analyze the embedding space in a reductive manner,
we feed the synthesized audio signals, which is the summation of simple sine
waves. Through extensive experiments, we conclude that various information is
embedded inside the feature encoder representations: (1) fundamental frequency,
(2) formants, and (3) amplitude, packed with (4) sufficient temporal detail.
Further, the information incorporated inside the latent representations is
analogous to spectrograms but with a fundamental difference: latent
representations construct a metric space so that closer representations imply
acoustic similarity.
","[{'version': 'v1', 'created': 'Thu, 27 Oct 2022 12:47:35 GMT'}]",2022-10-28,"['Sound', 'Computation and Language', 'Machine Learning']","This paper explores the use of wav2vec feature encoders for sound, computation and language processing. It examines how wav2vec works and the implications of using this technology for machine learning. The paper will discuss the advantages of using wav2vec, such as its ability to capture high-level features of sound, and how it can be used to improve the accuracy of machine learning algorithms. It will also explore the potential challenges associated with using this technology, such as the need for large amounts of data and compute resources. Finally, the paper will discuss how wav2vec can be used to bridge the gap between sound, computation and language, and how it can be used to improve the performance of machine learning algorithms.","Write an abstract for a paper called Opening the Black Box of wav2vec Feature Encoder about Sound, Computation and Language, Machine Learning"
2210.14179,"Chunqiu Steven Xia, Yuxiang Wei, Lingming Zhang",Practical Program Repair in the Era of Large Pre-trained Language Models,['cs.SE'],"  Automated Program Repair (APR) aims to help developers automatically patch
software bugs. However, current state-of-the-art traditional and learning-based
APR techniques face the problem of limited patch variety, failing to fix
complicated bugs. This is mainly due to the reliance on bug-fixing datasets to
craft fix templates or directly predict potential patches. Large Pre-Trained
Language Models (PLMs), trained using billions of text/code tokens, can
potentially help avoid this issue. Very recently, researchers have directly
leveraged PLMs for APR without relying on any bug-fixing datasets. Meanwhile,
such existing work either failed to include state-of-the-art PLMs or was not
evaluated on realistic datasets.
  In this work, we perform the first extensive study on directly applying PLMs
for APR. We select 9 recent state-of-the-art PLMs, including both generative
and infilling models, ranging from 125M to 20B in size. We designed 3 different
repair settings to evaluate the different ways we can use PLMs to generate
patches. We apply the PLMs under these repair settings on 5 datasets across 3
different languages and compare different PLMs in the number of bugs fixed,
generation speed and compilation rate. Our study demonstrates that directly
applying state-of-the-art PLMs can already substantially outperform all
existing APR techniques on all our datasets. Among the studied PLMs, the
scaling effect exists for APR where larger models tend to achieve better
performance. Also, we show for the first time that suffix code after the buggy
line (adopted in infilling-style APR) is important in not only generating more
fixes but more patches with higher compilation rate. Besides patch generation,
the PLMs consider correct patches to be more natural than other ones, and can
even be leveraged for effective patch ranking or patch correctness checking.
","[{'version': 'v1', 'created': 'Tue, 25 Oct 2022 17:18:02 GMT'}]",2022-10-26,['Software Engineering'],"This paper explores the potential of large pre-trained language models in the field of software engineering. We discuss how the use of such models can enable practical program repair, which is the process of automatically fixing bugs and vulnerabilities in programs. We analyze the current state of the art in program repair, and discuss how large pre-trained language models can be used to improve the accuracy and speed of program repair. We also evaluate the current approaches to program repair, and discuss the potential of large pre-trained language models in the context of program repair. Finally, we present our conclusions regarding the potential of large pre-trained language models in program repair and software engineering.",Write an abstract for a paper called Practical Program Repair in the Era of Large Pre-trained Language Models about Software Engineering
2202.05711,"Erica Lin, Luna Xu, Suraj Bramhavar, Marco Montes de Oca, Sean Gorsky,
  Lingyun Yi, Arianna Groetsema, Jeffrey Chou","Global Optimization of Data Pipelines in Heterogeneous Cloud
  Environments",['cs.DC'],"  Modern production data processing and machine learning pipelines on the cloud
are critical components for many cloud-based companies. These pipelines are
typically composed of complex workflows represented by directed acyclic graphs
(DAGs). Cloud environments are attractive to these workflows due to the wide
range of choice with heterogeneous instances and prices that can provide the
flexibility for different cost-performance needs. However, this flexibility
also leads to the complexity of selecting the right resource configuration
(e.g., instance type, resource demands) for each task in the DAG, while
simultaneously scheduling the tasks with the selected resources to reach the
optimal end-to-end performance and cost. These two decisions are often
codependent resulting in an NP-hard scheduling optimization bottleneck.
Existing solutions only focus solely on either problem and ignore the co-effect
on the end-to-end optimum. We propose AGORA, a scheduler that considers both
task-level resource allocation and execution for DAG workflows as a whole in
heterogeneous cloud environments. AGORA first (1) studies the characteristics
of the tasks from prior runs and gives predictions on resource configurations,
and (2) automatically finds the best configuration with its corresponding
schedules for the entire workflow with a cost-performance objective. We
evaluate AGORA in a heterogeneous Amazon Web Services (AWS) cloud environment
with multi-tenant workflows served by Airflow and demonstrate a performance
improvement up to 45% and cost reduction up to 77% compared to state-of-the-art
schedulers. In addition, we apply AGORA to a real-world production trace from
Alibaba and show cost reduction of 65% and DAG completion time reduction of
57%.
","[{'version': 'v1', 'created': 'Fri, 11 Feb 2022 15:49:15 GMT'}]",2022-02-14,"['Distributed, Parallel, and Cluster Computing']","This paper presents a novel approach to global optimization of data pipelines in heterogeneous cloud environments through the use of distributed, parallel, and cluster computing. The optimization approach is based on a multi-objective optimization framework that takes into account the cost of resources, the performance of the data pipeline, and the quality of service requirements. The proposed approach is evaluated using a real-world cloud environment, and the results demonstrate that the proposed optimization approach outperforms existing methods in terms of cost, performance, and quality of service. Additionally, the paper discusses the challenges and opportunities of using distributed, parallel, and cluster computing for global optimization of data pipelines in heterogeneous cloud environments.","Write an abstract for a paper called Global Optimization of Data Pipelines in Heterogeneous Cloud
  Environments about Distributed, Parallel, and Cluster Computing"
2201.11795,Ankur Mali and Alexander Ororbia and Daniel Kifer and Lee Giles,"Neural JPEG: End-to-End Image Compression Leveraging a Standard JPEG
  Encoder-Decoder","['eess.IV', 'cs.CV']","  Recent advances in deep learning have led to superhuman performance across a
variety of applications. Recently, these methods have been successfully
employed to improve the rate-distortion performance in the task of image
compression. However, current methods either use additional post-processing
blocks on the decoder end to improve compression or propose an end-to-end
compression scheme based on heuristics. For the majority of these, the trained
deep neural networks (DNNs) are not compatible with standard encoders and would
be difficult to deply on personal computers and cellphones. In light of this,
we propose a system that learns to improve the encoding performance by
enhancing its internal neural representations on both the encoder and decoder
ends, an approach we call Neural JPEG. We propose frequency domain pre-editing
and post-editing methods to optimize the distribution of the DCT coefficients
at both encoder and decoder ends in order to improve the standard compression
(JPEG) method. Moreover, we design and integrate a scheme for jointly learning
quantization tables within this hybrid neural compression framework.Experiments
demonstrate that our approach successfully improves the rate-distortion
performance over JPEG across various quality metrics, such as PSNR and MS-SSIM,
and generates visually appealing images with better color retention quality.
","[{'version': 'v1', 'created': 'Thu, 27 Jan 2022 20:20:03 GMT'}, {'version': 'v2', 'created': 'Mon, 31 Jan 2022 05:16:43 GMT'}]",2022-02-01,['Computer Vision and Pattern Recognition'],"This paper presents Neural JPEG, an end-to-end image compression system leveraging a standard JPEG encoder-decoder. Neural JPEG is based on a convolutional neural network (CNN) architecture and is designed to reduce both the image size and the computational complexity of image compression. The system is trained on a large dataset of natural images and is able to learn a set of parameters that encode the input image into a JPEG-compatible bitstream. Experiments demonstrate that Neural JPEG can achieve better compression performance compared to JPEG baseline on a variety of images. Furthermore, the proposed method is shown to be robust to different levels of compression, making it suitable for a wide range of applications. The results of this work provide an important step towards a fully automated image compression system.","Write an abstract for a paper called Neural JPEG: End-to-End Image Compression Leveraging a Standard JPEG
  Encoder-Decoder about Computer Vision and Pattern Recognition"
2204.12048,"Fang Kong, Junming Yin, Shuai Li",Thompson Sampling for Bandit Learning in Matching Markets,"['cs.LG', 'cs.GT']","  The problem of two-sided matching markets has a wide range of real-world
applications and has been extensively studied in the literature. A line of
recent works have focused on the problem setting where the preferences of
one-side market participants are unknown \emph{a priori} and are learned by
iteratively interacting with the other side of participants. All these works
are based on explore-then-commit (ETC) and upper confidence bound (UCB)
algorithms, two common strategies in multi-armed bandits (MAB). Thompson
sampling (TS) is another popular approach, which attracts lots of attention due
to its easier implementation and better empirical performances. In many
problems, even when UCB and ETC-type algorithms have already been analyzed,
researchers are still trying to study TS for its benefits. However, the
convergence analysis of TS is much more challenging and remains open in many
problem settings. In this paper, we provide the first regret analysis for TS in
the new setting of iterative matching markets. Extensive experiments
demonstrate the practical advantages of the TS-type algorithm over the ETC and
UCB-type baselines.
","[{'version': 'v1', 'created': 'Tue, 26 Apr 2022 03:01:17 GMT'}, {'version': 'v2', 'created': 'Mon, 2 May 2022 09:22:22 GMT'}]",2022-05-03,"['Machine Learning', 'Computer Science and Game Theory']","This paper explores the use of Thompson Sampling for Bandit Learning in Matching Markets. We discuss how Thompson Sampling can be used to solve the problem of learning in matching markets, which involves finding a balance between exploration and exploitation. We then describe how Thompson Sampling can be applied to the matching market problem and analyze its performance. Finally, we discuss the implications of using Thompson Sampling in matching markets, including its potential to improve the efficiency of the market. This paper provides a comprehensive overview of the topic, combining insights from Machine Learning, Computer Science, and Game Theory.","Write an abstract for a paper called Thompson Sampling for Bandit Learning in Matching Markets about Machine Learning, Computer Science and Game Theory"
2209.09614,"Akhil S Anand, Fares J.Abu-Dakka, Jan Tommy Gravdahl",Deep Model Predictive Variable Impedance Control,['cs.RO'],"  The capability to adapt compliance by varying muscle stiffness is crucial for
dexterous manipulation skills in humans. Incorporating compliance in robot
motor control is crucial to performing real-world force interaction tasks with
human-level dexterity. This work presents a Deep Model Predictive Variable
Impedance Controller for compliant robotic manipulation which combines Variable
Impedance Control with Model Predictive Control (MPC). A generalized Cartesian
impedance model of a robot manipulator is learned using an exploration strategy
maximizing the information gain. This model is used within an MPC framework to
adapt the impedance parameters of a low-level variable impedance controller to
achieve the desired compliance behavior for different manipulation tasks
without any retraining or finetuning. The deep Model Predictive Variable
Impedance Control approach is evaluated using a Franka Emika Panda robotic
manipulator operating on different manipulation tasks in simulations and real
experiments. The proposed approach was compared with model-free and model-based
reinforcement approaches in variable impedance control for transferability
between tasks and performance.
","[{'version': 'v1', 'created': 'Tue, 20 Sep 2022 11:10:26 GMT'}]",2022-09-21,['Robotics'],"This paper presents a deep model predictive variable impedance control (DMVIC) approach for robotic manipulation tasks. DMVIC is a model-based impedance control technique that combines deep learning with model predictive control (MPC). The proposed approach utilizes deep learning to learn a model of the contact dynamics between the robot and the environment, which is then used to generate an optimal impedance profile for each contact point. This profile is then used by the MPC to generate the desired robot motion. The proposed approach is implemented on a robotic manipulator and tested in a variety of manipulation tasks. The results demonstrate that the proposed approach can effectively control the impedance of the robot, resulting in improved performance in terms of task completion time and accuracy.",Write an abstract for a paper called Deep Model Predictive Variable Impedance Control about Robotics
2104.09232,"Guido Tebes, Denis Peppino, Pablo Becker and Luis Olsina","TestTDO's v1.3 Terms, Properties, Relationships and Axioms -- A
  Top-Domain Software Testing Ontology",['cs.SE'],"  The present preprint specifies and defines all Terms, Properties,
Relationships and Axioms of TestTDO (software Testing Top-Domain Ontology)
v1.3, which is a slightly updated version of its predecessor, TestTDO v1.2.
TestTDO is a top-domain ontology built in the context of a four-layered
ontological architecture named FCD-OntoArch (Foundational, Core, and Domain
Ontological Architecture for Sciences). This is a four-layered ontological
architecture, which considers Foundational, Core, Domain and Instance levels.
In turn, the domain level is split down in two sub-levels, namely: Top-domain
and Low-domain ontological levels. Ontologies at the same level can be related
to each other, except for the foundational level where only the ThingFO
ontology is found. In addition, ontologies' terms and relationships at lower
levels can be semantically enriched by ontologies' terms properties and
relationships from the higher levels. Some TestTDO's terms and are extended
primarily from SituationCO (Situation Core Ontology), and ProcessCO (Process
Core Ontology) concepts. Stereotypes are the mechanism used for enriching
TestTDO terms. Note that annotations of updates from the previous version
(v1.2) to the current one (v1.3) can be found in Appendix A.
","[{'version': 'v1', 'created': 'Wed, 7 Apr 2021 20:18:27 GMT'}, {'version': 'v2', 'created': 'Tue, 20 Apr 2021 13:39:46 GMT'}, {'version': 'v3', 'created': 'Thu, 22 Apr 2021 19:14:41 GMT'}, {'version': 'v4', 'created': 'Mon, 26 Apr 2021 21:39:44 GMT'}, {'version': 'v5', 'created': 'Tue, 11 May 2021 14:54:53 GMT'}, {'version': 'v6', 'created': 'Wed, 4 Aug 2021 19:42:32 GMT'}, {'version': 'v7', 'created': 'Thu, 10 Feb 2022 21:49:26 GMT'}]",2022-02-14,['Software Engineering'],"This paper presents TestTDO v1.3, a top-domain software testing ontology for software engineering. TestTDO is an ontology that provides a comprehensive and structured framework for representing the terms, properties, relationships and axioms of software testing. It provides a consistent terminology and a formal structure for software testing knowledge, and can be used to support software testing activities. The paper describes the structure of TestTDO and its components, and provides examples of how TestTDO can be used to support software testing activities. The paper also discusses the potential benefits of using TestTDO for software testing and its implications for software engineering.","Write an abstract for a paper called TestTDO's v1.3 Terms, Properties, Relationships and Axioms -- A
  Top-Domain Software Testing Ontology about Software Engineering"
2302.08001,"Libo Zhang, Yang Chen, Toru Takisaka, Bakh Khoussainov, Michael
  Witbrock, and Jiamou Liu",Learning Density-Based Correlated Equilibria for Markov Games,"['cs.AI', 'cs.MA']","  Correlated Equilibrium (CE) is a well-established solution concept that
captures coordination among agents and enjoys good algorithmic properties. In
real-world multi-agent systems, in addition to being in an equilibrium, agents'
policies are often expected to meet requirements with respect to safety, and
fairness. Such additional requirements can often be expressed in terms of the
state density which measures the state-visitation frequencies during the course
of a game. However, existing CE notions or CE-finding approaches cannot
explicitly specify a CE with particular properties concerning state density;
they do so implicitly by either modifying reward functions or using value
functions as the selection criteria. The resulting CE may thus not fully fulfil
the state-density requirements. In this paper, we propose Density-Based
Correlated Equilibria (DBCE), a new notion of CE that explicitly takes state
density as selection criterion. Concretely, we instantiate DBCE by specifying
different state-density requirements motivated by real-world applications. To
compute DBCE, we put forward the Density Based Correlated Policy Iteration
algorithm for the underlying control problem. We perform experiments on various
games where results demonstrate the advantage of our CE-finding approach over
existing methods in scenarios with state-density concerns.
","[{'version': 'v1', 'created': 'Thu, 16 Feb 2023 00:19:53 GMT'}]",2023-02-17,"['Artificial Intelligence', 'Multiagent Systems']","This paper presents a novel approach to learning density-based correlated equilibria for multiagent Markov games. We propose an artificial intelligence framework that utilizes an evolutionary algorithm to learn the Nash equilibria of the game. Our approach is based on the density-based correlated equilibrium solution concept, which is a generalization of the Nash equilibrium. We evaluate our method on a variety of two-player and three-player games and demonstrate that it is able to successfully find the Nash equilibria of the games. We also discuss the implications of our research for the field of artificial intelligence and multiagent systems.","Write an abstract for a paper called Learning Density-Based Correlated Equilibria for Markov Games about Artificial Intelligence, Multiagent Systems"
2206.10691,"Gleb Bazhenov, Sergei Ivanov, Maxim Panov, Alexey Zaytsev, Evgeny
  Burnaev","Towards OOD Detection in Graph Classification from Uncertainty
  Estimation Perspective",['cs.LG'],"  The problem of out-of-distribution detection for graph classification is far
from being solved. The existing models tend to be overconfident about OOD
examples or completely ignore the detection task. In this work, we consider
this problem from the uncertainty estimation perspective and perform the
comparison of several recently proposed methods. In our experiment, we find
that there is no universal approach for OOD detection, and it is important to
consider both graph representations and predictive categorical distribution.
","[{'version': 'v1', 'created': 'Tue, 21 Jun 2022 19:15:20 GMT'}]",2022-06-23,['Machine Learning'],"This paper presents a novel approach to out-of-distribution (OOD) detection in graph classification from an uncertainty estimation perspective in the context of machine learning. We propose a framework to identify OOD samples by estimating the uncertainty of graph classification models using a combination of methods such as Bayesian Neural Networks and Monte Carlo Dropout. We demonstrate the effectiveness of our framework on two real-world datasets, and compare the results with existing approaches. Our results show that our proposed framework can identify OOD samples with higher accuracy and precision, and outperforms existing approaches in terms of both accuracy and precision. Furthermore, we discuss the implications of our findings for further research on OOD detection in graph classification.","Write an abstract for a paper called Towards OOD Detection in Graph Classification from Uncertainty
  Estimation Perspective about Machine Learning"
2212.08861,"Gyeongnyeon Kim, Wooseok Jang, Gyuseong Lee, Susung Hong, Junyoung
  Seo, Seungryong Kim",DAG: Depth-Aware Guidance with Denoising Diffusion Probabilistic Models,['cs.CV'],"  Generative models have recently undergone significant advancement due to the
diffusion models. The success of these models can be often attributed to their
use of guidance techniques, such as classifier or classifier-free guidance,
which provide effective mechanisms to trade-off between fidelity and diversity.
However, these methods are not capable of guiding a generated image to be aware
of its geometric configuration, e.g., depth, which hinders their application to
areas that require a certain level of depth awareness. To address this
limitation, we propose a novel guidance method for diffusion models that uses
estimated depth information derived from the rich intermediate representations
of diffusion models. We first present label-efficient depth estimation
framework using internal representations of diffusion models. Subsequently, we
propose the incorporation of two guidance techniques based on pseudo-labeling
and depth-domain diffusion prior during the sampling phase to self-condition
the generated image using the estimated depth map. Experiments and
comprehensive ablation studies demonstrate the effectiveness of our method in
guiding the diffusion models towards the generation of geometrically plausible
images.
","[{'version': 'v1', 'created': 'Sat, 17 Dec 2022 12:47:19 GMT'}, {'version': 'v2', 'created': 'Mon, 30 Jan 2023 12:33:37 GMT'}]",2023-01-31,['Computer Vision and Pattern Recognition'],"This paper presents a novel approach for computer vision and pattern recognition, called DAG: Depth-Aware Guidance with Denoising Diffusion Probabilistic Models. We propose a method that combines the advantages of both depth-aware guidance and denoising diffusion probabilistic models to improve the accuracy and robustness of computer vision and pattern recognition. The proposed model is tested on a variety of datasets, including MNIST, CIFAR-10, and ImageNet, and it shows promising results in terms of accuracy and robustness. Furthermore, the model is compared to other state-of-the-art methods and it is shown to outperform them. This paper provides a comprehensive analysis of the proposed model and its performance in computer vision and pattern recognition tasks.",Write an abstract for a paper called DAG: Depth-Aware Guidance with Denoising Diffusion Probabilistic Models about Computer Vision and Pattern Recognition
2304.00989,"Yaojie Hu, Jin Tian",Neural Interpretation of Generic Source Code,"['cs.AI', 'cs.LG']","  Can a generic (Python) program be executed statement-by-statement by neural
networks composed according to the source code? We formulate the Abstract
Neural Execution Problem and introduce Neural Interpretation, the first neural
model that abstractly executes generic source code, where every variable has a
vector encoding, and every function executes a neural network. Neural
Interpretation is a model of computers with a compiler architecture, which can
assemble neural layers ''programmed'' by partial source code. Neural
Interpretation can be trained with flexible learning objectives. We demonstrate
white-box execution without concrete inputs for variable misuse localization
and repair.
","[{'version': 'v1', 'created': 'Thu, 23 Mar 2023 17:56:45 GMT'}]",2023-04-04,"['Artificial Intelligence', 'Machine Learning']",", and Deep Learning

This paper explores the current state of artificial intelligence, machine learning, and deep learning in the context of source code interpretation. We analyze the various approaches to source code interpretation, such as static and dynamic analysis, and discuss how these approaches can be used to interpret generic source code. We further analyze how neural networks can be used to interpret source code, and how they can be used to interpret generic source code. We discuss the potential benefits of using neural networks in source code interpretation, and examine the challenges and limitations of using neural networks in this context. Finally, we present our conclusions and recommendations for further research in this area.","Write an abstract for a paper called Neural Interpretation of Generic Source Code about Artificial Intelligence, Machine Learning"
2107.07246,David Angwenyi,"Estimation of spatially varying parameters with application to
  hyperbolic SPDEs","['stat.ME', 'cs.NA', 'math.NA']","  Parameter estimation is a growing area of interest in statistical signal
processing. Some parameters in real-life applications vary in space as opposed
to those that are static. Most common methods in estimating parameters involve
solving an optimization problem where the cost function is assembled variously;
for example, maximum likelihood and maximum a posteriori methods. However,
these methods do not have exact solutions to most real-life problems. It is for
this reason that Monte Carlo methods are preferred. In this paper, we treat the
estimation of parameters which vary with space. We use Metropolis-Hastings
algorithm as a selection criteria for the maximum filter likelihood.
Comparisons are made with the use of joint estimation of both the spatially
varying parameters and the state. We illustrate the procedures employed in this
paper by means of two hyperbolic SPDEs: the advection and the wave equation.
The Metropolis-Hastings procedure registers better estimates.
","[{'version': 'v1', 'created': 'Thu, 15 Jul 2021 11:09:22 GMT'}, {'version': 'v2', 'created': 'Tue, 1 Nov 2022 13:17:19 GMT'}]",2022-11-02,['Numerical Analysis'],"This paper presents a numerical analysis of a method for the estimation of spatially varying parameters in hyperbolic stochastic partial differential equations (SPDEs). An iterative approach is proposed and its performance is evaluated in terms of accuracy, efficiency and robustness. The proposed method is tested on a range of hyperbolic SPDEs with varying spatial and temporal scales. The results show that the proposed method is capable of estimating spatially varying parameters with a high degree of accuracy and robustness. Furthermore, the proposed method is efficient and can be used in a wide range of applications.","Write an abstract for a paper called Estimation of spatially varying parameters with application to
  hyperbolic SPDEs about Numerical Analysis"
2211.08626,"Zhi Yu (1), Chao Feng (1), Yong Zeng (1 and 3), Teng Li (2 and 3), and
  Shi Jin (1) ((1) National Mobile Communications Research Laboratory,
  Southeast University, Nanjing, China, (2) State Key Laboratory of Millimeter
  Waves, Southeast University, Nanjing, China, (3) Purple Mountain
  Laboratories, Nanjing, China)","Wireless Communication Using Metal Reflectors: Reflection Modelling and
  Experimental Verification","['cs.IT', 'eess.SP', 'math.IT']","  Wireless communication using fully passive metal reflectors is a promising
technique for coverage expansion, signal enhancement, rank improvement and
blind-zone compensation, thanks to its appealing features including zero energy
consumption, ultra low cost, signaling- and maintenance-free, easy deployment
and full compatibility with existing and future wireless systems. However, a
prevalent understanding for reflection by metal plates is based on Snell's Law,
i.e., signal can only be received when the observation angle equals to the
incident angle, which is valid only when the electrical dimension of the metal
plate is extremely large. In this paper, we rigorously derive a general
reflection model that is applicable to metal reflectors of any size, any
orientation, and any linear polarization. The derived model is given compactly
in terms of the radar cross section (RCS) of the metal plate, as a function of
its physical dimensions and orientation vectors, as well as the wave
polarization and the wave deflection vector, i.e., the change of direction from
the incident wave direction to the observation direction. Furthermore,
experimental results based on actual field measurements are provided to
validate the accuracy of our developed model and demonstrate the great
potential of communications using metal reflectors.
","[{'version': 'v1', 'created': 'Wed, 16 Nov 2022 02:33:40 GMT'}]",2022-11-17,['Information Theory'],"This paper presents a novel approach to wireless communication using metal reflectors. Reflection modelling is used to accurately predict the performance of the system and experimental verification is used to validate the results. Information theory is used to analyze the performance of the system, and the results are compared with existing approaches. The proposed system is shown to outperform existing approaches in terms of data rate, transmission range, and signal-to-noise ratio. The paper also offers insights into the design and optimization of reflector-based communication systems.","Write an abstract for a paper called Wireless Communication Using Metal Reflectors: Reflection Modelling and
  Experimental Verification about Information Theory"
2204.09758,"Jos\'e Miguel P\'erez-\'Alvarez, Adrian Mos, Benjamin V. Hanrahan,
  Iyadunni J. Adenuga","Lowering Barriers to Application Development With Cloud-Native
  Domain-Specific Functions",['cs.SE'],"  Creating and maintaining a modern, heterogeneous set of client applications
remains an obstacle for many businesses and individuals. While simple
domain-specific graphical languages and libraries can empower a variety of
users to create application behaviors and logic, using these languages to
produce and maintain a set of heterogeneous client applications is a challenge.
Primarily because each client typically requires the developers to both
understand and embed the domain-specific logic. This is because application
logic must be encoded to some extent in both the server and client sides.
  In this paper, we propose an alternative approach, which allows the
specification of application logic to reside solely on the cloud. We have built
a system where reusable application components can be assembled on the cloud in
different logical chains and the client is largely decoupled from this logic
and is solely concerned with how data is displayed and gathered from users of
the application. In this way, the chaining of requests and responses is done by
the cloud and the client side has no knowledge of the application logic.
  An additional effect of our approach is that the client side developer is
able to immediately see any changes they make, while executing the logic
residing on the cloud. This further allows more novice programmers to perform
these customizations, as they do not need to `get the full application working'
and are able to see the results of their code as they go, thereby lowering the
obstacles to businesses and individuals to produce and maintain applications.
Furthermore, this decoupling enables the quick generation and customization of
a variety of application clients, ranging from web to mobile devices and
personal assistants, while customizing one or more as needed.
","[{'version': 'v1', 'created': 'Wed, 20 Apr 2022 19:45:13 GMT'}]",2022-04-22,['Software Engineering'],"This paper explores how cloud-native domain-specific functions can be used to lower the barriers to application development. We discuss how cloud-native domain-specific functions can reduce the complexity of developing applications, making it easier for developers to quickly build and deploy applications. We analyze the advantages of using cloud-native domain-specific functions to improve the development process, including improved scalability, better security, and faster development cycles. We then discuss the challenges associated with using cloud-native domain-specific functions, such as the need for specialized knowledge and the difficulty of managing and updating functions. Finally, we present several case studies of successful applications built using cloud-native domain-specific functions, highlighting their effectiveness in lowering the barriers to application development.","Write an abstract for a paper called Lowering Barriers to Application Development With Cloud-Native
  Domain-Specific Functions about Software Engineering"
2209.12394,"Chunwei Tian, Menghua Zheng, Wangmeng Zuo, Bob Zhang, Yanning Zhang,
  David Zhang",Multi-stage image denoising with the wavelet transform,"['eess.IV', 'cs.CV']","  Deep convolutional neural networks (CNNs) are used for image denoising via
automatically mining accurate structure information. However, most of existing
CNNs depend on enlarging depth of designed networks to obtain better denoising
performance, which may cause training difficulty. In this paper, we propose a
multi-stage image denoising CNN with the wavelet transform (MWDCNN) via three
stages, i.e., a dynamic convolutional block (DCB), two cascaded wavelet
transform and enhancement blocks (WEBs) and a residual block (RB). DCB uses a
dynamic convolution to dynamically adjust parameters of several convolutions
for making a tradeoff between denoising performance and computational costs.
WEB uses a combination of signal processing technique (i.e., wavelet
transformation) and discriminative learning to suppress noise for recovering
more detailed information in image denoising. To further remove redundant
features, RB is used to refine obtained features for improving denoising
effects and reconstruct clean images via improved residual dense architectures.
Experimental results show that the proposed MWDCNN outperforms some popular
denoising methods in terms of quantitative and qualitative analysis. Codes are
available at https://github.com/hellloxiaotian/MWDCNN.
","[{'version': 'v1', 'created': 'Mon, 26 Sep 2022 03:28:23 GMT'}, {'version': 'v2', 'created': 'Tue, 27 Sep 2022 05:50:01 GMT'}, {'version': 'v3', 'created': 'Mon, 3 Oct 2022 05:29:10 GMT'}]",2022-10-04,['Computer Vision and Pattern Recognition'],"This paper presents a novel approach to multi-stage image denoising using the wavelet transform. We demonstrate how the wavelet transform can be used to decompose a noisy image into its constituent sub-bands, and how this decomposition can be used to effectively remove noise from the image. We further show how the wavelet transform can be used to improve the performance of existing denoising algorithms. Finally, we discuss the potential applications of our proposed method in the field of computer vision and pattern recognition.",Write an abstract for a paper called Multi-stage image denoising with the wavelet transform about Computer Vision and Pattern Recognition
2010.06144,"Xikai Yang, Yong Long, Saiprasad Ravishankar","Multi-layer Residual Sparsifying Transform (MARS) Model for Low-dose CT
  Image Reconstruction","['eess.IV', 'cs.LG', 'eess.SP']","  Signal models based on sparse representations have received considerable
attention in recent years. On the other hand, deep models consisting of a
cascade of functional layers, commonly known as deep neural networks, have been
highly successful for the task of object classification and have been recently
introduced to image reconstruction. In this work, we develop a new image
reconstruction approach based on a novel multi-layer model learned in an
unsupervised manner by combining both sparse representations and deep models.
The proposed framework extends the classical sparsifying transform model for
images to a Multi-lAyer Residual Sparsifying transform (MARS) model, wherein
the transform domain data are jointly sparsified over layers. We investigate
the application of MARS models learned from limited regular-dose images for
low-dose CT reconstruction using Penalized Weighted Least Squares (PWLS)
optimization. We propose new formulations for multi-layer transform learning
and image reconstruction. We derive an efficient block coordinate descent
algorithm to learn the transforms across layers, in an unsupervised manner from
limited regular-dose images. The learned model is then incorporated into the
low-dose image reconstruction phase. Low-dose CT experimental results with both
the XCAT phantom and Mayo Clinic data show that the MARS model outperforms
conventional methods such as FBP and PWLS methods based on the edge-preserving
(EP) regularizer in terms of two numerical metrics (RMSE and SSIM) and noise
suppression. Compared with the single-layer learned transform (ST) model, the
MARS model performs better in maintaining some subtle details.
","[{'version': 'v1', 'created': 'Sat, 10 Oct 2020 09:04:43 GMT'}, {'version': 'v2', 'created': 'Fri, 16 Oct 2020 03:21:46 GMT'}, {'version': 'v3', 'created': 'Fri, 28 May 2021 10:12:44 GMT'}]",2022-01-19,['Machine Learning'],"This paper presents a novel approach for low-dose computed tomography (CT) image reconstruction using a multi-layer residual sparsifying transform (MARS) model. MARS is a deep learning model based on convolutional neural networks (CNNs). The model is trained using a large set of low-dose CT images and corresponding high-dose CT images. The MARS model is then used to reconstruct low-dose CT images using a sparse representation of the high-dose CT images. The results demonstrate that the MARS model can achieve high-quality image reconstruction with a much lower dose of radiation than traditional CT imaging techniques. The MARS model is compared with several other methods in terms of image quality, dose reduction, and reconstruction time. The results show that the MARS model outperforms the other methods in terms of image quality and dose reduction. The MARS model is an effective approach for low-dose CT image reconstruction and can be used to reduce radiation dose in CT imaging.","Write an abstract for a paper called Multi-layer Residual Sparsifying Transform (MARS) Model for Low-dose CT
  Image Reconstruction about Machine Learning"
2203.00525,"Shihong Wang, Xueying Zhang, Yichen Meng, Wei W. Xing","E-LMC: Extended Linear Model of Coregionalization for Spatial Field
  Prediction","['cs.LG', 'cs.AI', 'stat.ML']","  Physical simulations based on partial differential equations typically
generate spatial fields results, which are utilized to calculate specific
properties of a system for engineering design and optimization. Due to the
intensive computational burden of the simulations, a surrogate model mapping
the low-dimensional inputs to the spatial fields are commonly built based on a
relatively small dataset. To resolve the challenge of predicting the whole
spatial field, the popular linear model of coregionalization (LMC) can
disentangle complicated correlations within the high-dimensional spatial field
outputs and deliver accurate predictions. However, LMC fails if the spatial
field cannot be well approximated by a linear combination of base functions
with latent processes. In this paper, we present the Extended Linear Model of
Coregionalization (E-LMC) by introducing an invertible neural network to
linearize the highly complex and nonlinear spatial fields so that the LMC can
easily generalize to nonlinear problems while preserving the traceability and
scalability. Several real-world applications demonstrate that E-LMC can exploit
spatial correlations effectively, showing a maximum improvement of about 40%
over the original LMC and outperforming the other state-of-the-art spatial
field models.
","[{'version': 'v1', 'created': 'Tue, 1 Mar 2022 15:09:38 GMT'}, {'version': 'v2', 'created': 'Thu, 8 Sep 2022 02:40:43 GMT'}]",2022-09-09,"['Machine Learning', 'Artificial Intelligence']","This paper presents a novel machine learning model, E-LMC, for spatial field prediction. E-LMC is an extended version of the Linear Model of Coregionalization (LMC) and is based on Artificial Intelligence techniques. The proposed model utilizes a combination of coregionalization, regularization, and deep learning to improve the prediction accuracy of spatial fields. The coregionalization component of the model allows for the incorporation of correlation information between different spatial fields, while the regularization component reduces the complexity of the model. Furthermore, the deep learning component of the model enables the automatic extraction of features from the spatial fields. Results from experiments conducted on simulated and real-world datasets demonstrate that E-LMC achieves higher prediction accuracy than the LMC and other state-of-the-art models. The proposed model is thus a promising tool for predicting spatial fields in various applications.","Write an abstract for a paper called E-LMC: Extended Linear Model of Coregionalization for Spatial Field
  Prediction about Machine Learning, Artificial Intelligence"
2208.02162,"Saray Shai, Isaac Jacobs, Peter J. Mucha",One Node at a Time: Node-Level Network Classification,"['cs.SI', 'cs.LG', 'physics.soc-ph']","  Network classification aims to group networks (or graphs) into distinct
categories based on their structure. We study the connection between
classification of a network and of its constituent nodes, and whether nodes
from networks in different groups are distinguishable based on structural node
characteristics such as centrality and clustering coefficient. We demonstrate,
using various network datasets and random network models, that a classifier can
be trained to accurately predict the network category of a given node (without
seeing the whole network), implying that complex networks display distinct
structural patterns even at the node level. Finally, we discuss two
applications of node-level network classification: (i) whole-network
classification from small samples of nodes, and (ii) network bootstrapping.
","[{'version': 'v1', 'created': 'Wed, 3 Aug 2022 15:48:39 GMT'}]",2022-08-04,"['Social and Information Networks', 'Machine Learning']","This paper presents a novel approach to node-level network classification for social and information networks. We propose a machine learning-based framework to classify nodes in a network according to their roles in the network. We demonstrate the effectiveness of our proposed framework by applying it to real-world social networks and information networks. Our experiments show that our proposed framework can accurately identify the roles of nodes in a network, and that our proposed method outperforms existing approaches. Furthermore, we discuss the implications of our results on the application of machine learning to network analysis.","Write an abstract for a paper called One Node at a Time: Node-Level Network Classification about Social and Information Networks, Machine Learning"
2204.02224,"Mohammad-Ali Miri, and Vinod Menon",Neural Computing with Coherent Laser Networks,"['physics.optics', 'cs.LG', 'cs.NE', 'nlin.PS', 'physics.comp-ph']","  We show that a coherent network of lasers exhibits emergent neural computing
capabilities. The proposed scheme is built on harnessing the collective
behavior of laser networks for storing a number of phase patterns as stable
fixed points of the governing dynamical equations and retrieving such patterns
through proper excitation conditions, thus exhibiting an associative memory
property. The associative memory functionality is first discussed in the strong
pumping regime of a network of passive dissipatively coupled lasers which
simulate the classical XY model. It is discussed that despite the large storage
capacity of the network, the large overlap between fixed-point patterns
effectively limits pattern retrieval to only two images. Next, we show that
this restriction can be uplifted by using nonreciprocal coupling between lasers
and this allows for utilizing a large storage capacity. This work opens new
possibilities for neural computation with coherent laser networks as novel
analog processors. In addition, the underlying dynamical model discussed here
suggests a novel energy-based recurrent neural network that handles continuous
data as opposed to Hopfield networks and Boltzmann machines which are
intrinsically binary systems.
","[{'version': 'v1', 'created': 'Tue, 5 Apr 2022 13:56:34 GMT'}]",2022-04-06,"['Machine Learning', 'Neural and Evolutionary Computing']","This paper explores the potential of using coherent laser networks for neural computing and machine learning applications. It will discuss the advantages of using coherent laser networks for neural computing and machine learning, such as improved speed and accuracy, reduced hardware requirements, and increased scalability. Additionally, it will discuss the potential of using coherent laser networks for evolutionary computing and how they can be used to optimize neural networks. Finally, it will discuss the challenges associated with using coherent laser networks for neural computing and machine learning, such as security and privacy issues. The paper will conclude by discussing future research directions for neural computing with coherent laser networks.","Write an abstract for a paper called Neural Computing with Coherent Laser Networks about Machine Learning, Neural and Evolutionary Computing"
2304.00565,"Stoyan Kapralov, Valentin Bakoev, Kaloyan Kapralov","Algorithms for Construction, Classification and Enumeration of Closed
  Knight's Paths","['math.CO', 'cs.DS']","  Two algorithms for construction of all closed knight's paths of lengths up to
16 are presented. An approach for classification (up to equivalence) of all
such paths is considered. By applying the construction algorithms and
classification approach, we enumerate both unrestricted and non-intersecting
knight's paths and show the obtained results.
","[{'version': 'v1', 'created': 'Sun, 2 Apr 2023 16:22:06 GMT'}]",2023-04-04,['Data Structures and Algorithms'],"This paper presents a comprehensive overview of algorithms for the construction, classification, and enumeration of closed Knight's paths in a chessboard. The paper first introduces the basic concepts of data structures and algorithms, and then provides a detailed description of various algorithms used to construct, classify, and enumerate closed Knight's paths. The paper then discusses the complexities of these algorithms and their applications in the field of data structures and algorithms. Finally, the paper concludes with a discussion of the implications of the research and future directions for the development of algorithms for the construction, classification, and enumeration of closed Knight's paths.","Write an abstract for a paper called Algorithms for Construction, Classification and Enumeration of Closed
  Knight's Paths about Data Structures and Algorithms"
2301.11783,"Tianqi Cui, Thomas Bertalan, George J. Pappas, Manfred Morari, Ioannis
  G. Kevrekidis and Mahyar Fazlyab",Certified Invertibility in Neural Networks via Mixed-Integer Programming,"['cs.LG', 'cs.SY', 'eess.SY', 'math.OC']","  Neural networks are notoriously vulnerable to adversarial attacks -- small
imperceptible perturbations that can change the network's output drastically.
In the reverse direction, there may exist large, meaningful perturbations that
leave the network's decision unchanged (excessive invariance, nonivertibility).
We study the latter phenomenon in two contexts: (a) discrete-time dynamical
system identification, as well as (b) calibration of the output of one neural
network to the output of another (neural network matching). For ReLU networks
and $L_p$ norms ($p=1,2,\infty$), we formulate these optimization problems as
mixed-integer programs (MIPs) that apply to neural network approximators of
dynamical systems. We also discuss the applicability of our results to
invertibility certification in transformations between neural networks (e.g. at
different levels of pruning).
","[{'version': 'v1', 'created': 'Fri, 27 Jan 2023 15:40:38 GMT'}]",2023-01-30,"['Machine Learning', 'Systems and Control']","This paper presents a novel approach to certify invertibility of neural networks using mixed-integer programming. Using this technique, we demonstrate that neural networks can be certified to have invertible properties, which is an important property of systems in control theory. We show that this approach is able to provide a tighter bound on the invertibility of a neural network than existing methods. We also present a case study of a neural network trained on a real-world dataset to demonstrate the efficacy of this technique. Finally, we discuss the implications of our results and future directions for research.","Write an abstract for a paper called Certified Invertibility in Neural Networks via Mixed-Integer Programming about Machine Learning, Systems and Control"
2210.10633,"Prakash Chandra Chhipa, Richa Upadhyay, Rajkumar Saini, Lars
  Lindqvist, Richard Nordenskjold, Seiichi Uchida, and Marcus Liwicki","Depth Contrast: Self-Supervised Pretraining on 3DPM Images for Mining
  Material Classification",['cs.CV'],"  This work presents a novel self-supervised representation learning method to
learn efficient representations without labels on images from a 3DPM sensor
(3-Dimensional Particle Measurement; estimates the particle size distribution
of material) utilizing RGB images and depth maps of mining material on the
conveyor belt. Human annotations for material categories on sensor-generated
data are scarce and cost-intensive. Currently, representation learning without
human annotations remains unexplored for mining materials and does not leverage
on utilization of sensor-generated data. The proposed method, Depth Contrast,
enables self-supervised learning of representations without labels on the 3DPM
dataset by exploiting depth maps and inductive transfer. The proposed method
outperforms material classification over ImageNet transfer learning performance
in fully supervised learning settings and achieves an F1 score of 0.73.
Further, The proposed method yields an F1 score of 0.65 with an 11% improvement
over ImageNet transfer learning performance in a semi-supervised setting when
only 20% of labels are used in fine-tuning. Finally, the Proposed method
showcases improved performance generalization on linear evaluation. The
implementation of proposed method is available on GitHub.
","[{'version': 'v1', 'created': 'Tue, 18 Oct 2022 04:17:16 GMT'}]",2022-10-20,['Computer Vision and Pattern Recognition'],"This paper presents a novel self-supervised pretraining approach for mining material classification from 3DPM images. We propose a novel depth contrast pretraining method that leverages the depth information of the 3DPM images to learn a representation for material classification. Our approach is evaluated on the 3DPM dataset, which contains images of various materials across different categories. Experimental results demonstrate that our approach outperforms existing methods for material classification. Furthermore, we show that our proposed depth contrast pretraining method can be used to generalize to unseen materials from the 3DPM dataset. Our approach provides a promising direction for future research on self-supervised learning for computer vision and pattern recognition.","Write an abstract for a paper called Depth Contrast: Self-Supervised Pretraining on 3DPM Images for Mining
  Material Classification about Computer Vision and Pattern Recognition"
2204.11533,"Trever Schirmer, Joel Scheuner, Tobias Pfandzelter, David Bermbach","Fusionize: Improving Serverless Application Performance through
  Feedback-Driven Function Fusion",['cs.DC'],"  Serverless computing increases developer productivity by removing operational
concerns such as managing hardware or software runtimes. Developers, however,
still need to partition their application into functions, which can be
error-prone and adds complexity: Using a small function size where only the
smallest logical unit of an application is inside a function maximizes
flexibility and reusability. Yet, having small functions leads to invocation
overheads, additional cold starts, and may increase cost due to busy waiting.
In this paper we present Fusionize, a framework that removes these concerns
from developers by automatically fusing the application code into a
multi-function orchestration with varying function size. Developers only need
to write the application code following a lightweight programming model and do
not need to worry how the application is turned into functions. Our framework
automatically fuses different parts of the application into functions and
manages their interactions. Leveraging monitoring data, the framework optimizes
the distribution of application parts to functions to optimize deployment goals
such as end-to-end latency and cost. Using two example applications, we show
that Fusionize can automatically and iteratively improve the deployment
artifacts of the application.
","[{'version': 'v1', 'created': 'Mon, 25 Apr 2022 09:48:56 GMT'}, {'version': 'v2', 'created': 'Mon, 15 Aug 2022 14:32:14 GMT'}]",2022-08-16,"['Distributed, Parallel, and Cluster Computing']","Fusionize: Improving Serverless Application Performance through Feedback-Driven Function Fusion is a paper that examines the potential of using feedback-driven function fusion to improve the performance of serverless applications. Function fusion is a technique used in distributed, parallel, and cluster computing to combine multiple functions into a single function, reducing the amount of code needed and improving performance. This paper explores the potential of using feedback-driven function fusion to improve the performance of serverless applications by reducing the number of function calls and increasing the efficiency of the application. The paper then evaluates the performance of serverless applications with and without feedback-driven function fusion and discusses potential improvements. Finally, the paper provides an example of a feedback-driven function fusion system and its potential benefits. This paper provides a valuable insight into the potential of using feedback-driven function fusion to improve the performance of serverless applications.","Write an abstract for a paper called Fusionize: Improving Serverless Application Performance through
  Feedback-Driven Function Fusion about Distributed, Parallel, and Cluster Computing"
2109.10162,Alexandros Eskenazis and Paata Ivanisvili,"Learning low-degree functions from a logarithmic number of random
  queries","['cs.LG', 'math.FA', 'stat.ML']","  We prove that every bounded function $f:\{-1,1\}^n\to[-1,1]$ of degree at
most $d$ can be learned with $L_2$-accuracy $\varepsilon$ and confidence
$1-\delta$ from $\log(\tfrac{n}{\delta})\,\varepsilon^{-d-1}
C^{d^{3/2}\sqrt{\log d}}$ random queries, where $C>1$ is a universal finite
constant.
","[{'version': 'v1', 'created': 'Tue, 21 Sep 2021 13:19:04 GMT'}, {'version': 'v2', 'created': 'Wed, 3 Nov 2021 09:20:08 GMT'}, {'version': 'v3', 'created': 'Wed, 9 Mar 2022 12:31:50 GMT'}]",2022-03-10,['Machine Learning'],"This paper presents a novel approach for learning low-degree functions from a logarithmic number of random queries about Machine Learning. The proposed method combines the power of supervised learning and query-based learning to achieve a significant reduction in the number of queries required to learn a low-degree function. We demonstrate the effectiveness of our approach on a variety of real-world datasets and show that it can achieve better accuracy than existing methods. Furthermore, we provide theoretical guarantees on the accuracy of our approach. Finally, we discuss the implications of our results and provide directions for future research.","Write an abstract for a paper called Learning low-degree functions from a logarithmic number of random
  queries about Machine Learning"
2106.02348,"Annesya Banerjee, Achal Nilhani","A Residual Network based Deep Learning Model for Detection of COVID-19
  from Cough Sounds","['eess.AS', 'cs.SD']","  The present work proposes a deep-learning-based approach for the
classification of COVID-19 coughs from non-COVID-19 coughs and that can be used
as a low-resource-based tool for early detection of the onset of such
respiratory diseases. The proposed system uses the ResNet-50 architecture, a
popularly known Convolutional Neural Network (CNN) for image recognition tasks,
fed with the log-Mel spectrums of the audio data to discriminate between the
two types of coughs. For the training and validation of the proposed deep
learning model, this work utilizes the Track-1 dataset provided by the DiCOVA
Challenge 2021 organizers. Additionally, to increase the number of
COVID-positive samples and to enhance variability in the training data, it has
also utilized a large open-source database of COVID-19 coughs collected by the
EPFL CoughVid team. Our developed model has achieved an average validation AUC
of 98.88%. Also, applying this model on the Blind Test Set released by the
DiCOVA Challenge, the system has achieved a Test AUC of 75.91%, Test
Specificity of 62.50%, and Test Sensitivity of 80.49%. Consequently, this
submission has secured 16th position in the DiCOVA Challenge 2021 leader-board.
","[{'version': 'v1', 'created': 'Fri, 4 Jun 2021 08:59:27 GMT'}]",2022-05-25,['Sound'],"This paper presents a residual network based deep learning model for the detection of COVID-19 from cough sounds. The proposed model is based on a residual network architecture with convolutional layers and long short-term memory (LSTM) layers. The model is trained on a dataset of cough sounds collected from patients diagnosed with COVID-19 and healthy individuals. The performance of the model is evaluated using various metrics such as accuracy, precision, recall, and F1 score. The results show that the proposed model outperforms other existing models in terms of accuracy and F1 score. Furthermore, the model is able to detect COVID-19 from cough sounds with an accuracy of up to 94.5%. The results demonstrate the potential of the proposed model in detecting COVID-19 from cough sounds.","Write an abstract for a paper called A Residual Network based Deep Learning Model for Detection of COVID-19
  from Cough Sounds about Sound"
2203.00636,"Max Mowbray, Dongda Zhang, Ehecatl Antonio Del Rio Chanona","Distributional Reinforcement Learning for Scheduling of Chemical
  Production Processes","['eess.SY', 'cs.LG', 'cs.SY']","  Reinforcement Learning (RL) has recently received significant attention from
the process systems engineering and control communities. Recent works have
investigated the application of RL to identify optimal scheduling decision in
the presence of uncertainty. In this work, we present a RL methodology tailored
to efficiently address production scheduling problems in the presence of
uncertainty. We consider commonly imposed restrictions on these problems such
as precedence and disjunctive constraints which are not naturally considered by
RL in other contexts. Additionally, this work naturally enables the
optimization of risk-sensitive formulations such as the conditional
value-at-risk (CVaR), which are essential in realistic scheduling processes.
The proposed strategy is investigated thoroughly in a parallel batch production
environment, and benchmarked against mixed integer linear programming (MILP)
strategies. We show that the policy identified by our approach is able to
account for plant uncertainties in online decision-making, with expected
performance comparable to existing MILP methods. Additionally, the framework
gains the benefits of optimizing for risk-sensitive measures, and identifies
online decisions orders of magnitude faster than the most efficient
optimization approaches. This promises to mitigate practical issues and ease in
handling realizations of process uncertainty in the paradigm of online
production scheduling.
","[{'version': 'v1', 'created': 'Tue, 1 Mar 2022 17:25:40 GMT'}, {'version': 'v2', 'created': 'Wed, 9 Mar 2022 21:16:42 GMT'}]",2022-03-11,"['Machine Learning', 'Systems and Control']","This paper presents a novel approach for scheduling of chemical production processes using distributional reinforcement learning (DRL). DRL is a machine learning technique that is used to make decisions in complex, dynamic environments. The proposed approach is based on a Markov Decision Process (MDP) that models the chemical production process as a sequence of decisions. The MDP is then solved using a DRL algorithm to optimize the scheduling of the process. The results of the proposed approach are compared to the traditional scheduling methods such as heuristics and dynamic programming. The results show that the proposed approach is able to achieve better results than the traditional methods in terms of completion time, cost and energy efficiency. The paper also discusses the potential applications of DRL for scheduling of chemical production processes and other systems and control applications.","Write an abstract for a paper called Distributional Reinforcement Learning for Scheduling of Chemical
  Production Processes about Machine Learning, Systems and Control"
2210.13421,"Mohatashem Reyaz Makhdoomi, Vivek Muralidharan, Juan Sandoval, Miguel
  Olivares-Mendez and Carol Martinez","Evaluation of Position and Velocity Based Forward Dynamics Compliance
  Control (FDCC) for Robotic Interactions in Position Controlled Robots",['cs.RO'],"  In robotic manipulation, end-effector compliance is an essential precondition
for performing contact-rich tasks, such as machining, assembly, and human-robot
interaction. Most robotic arms are position-controlled stiff systems at a
hardware level. Thus, adding compliance becomes essential. Compliance in those
systems has been recently achieved using Forward dynamics compliance control
(FDCC), which, owing to its virtual forward dynamics model, can be implemented
on both position and velocity-controlled robots. This paper evaluates the
choice of control interface (and hence the control domain), which, although
considered trivial, is essential due to differences in their characteristics.
In some cases, the choice is restricted to the available hardware interface.
However, given the option to choose, the velocity-based control interface makes
a better candidate for compliance control because of smoother compliant
behaviour, reduced interaction forces, and work done. To prove these points, in
this paper FDCC is evaluated on the UR10e six-DOF manipulator with velocity and
position control modes. The evaluation is based on force-control benchmarking
metrics using 3D-printed artefacts. Real experiments favour the choice of
velocity control over position control.
","[{'version': 'v1', 'created': 'Mon, 24 Oct 2022 17:37:03 GMT'}]",2022-10-25,['Robotics'],"This paper evaluates the performance of the Position and Velocity Based Forward Dynamics Compliance Control (FDCC) algorithm for robotic interactions in position controlled robots. The FDCC algorithm is a method for controlling robots in order to achieve a desired interaction behavior with the environment. This paper presents a series of experiments in which the FDCC algorithm is tested on two different robotic platforms. The results of these experiments are then compared to the results of other control algorithms. The findings from this paper suggest that the FDCC algorithm is capable of providing a robust and reliable control solution for robotic interactions in position controlled robots. Furthermore, the FDCC algorithm is able to achieve a desired interaction behavior with the environment, even in the presence of external disturbances.","Write an abstract for a paper called Evaluation of Position and Velocity Based Forward Dynamics Compliance
  Control (FDCC) for Robotic Interactions in Position Controlled Robots about Robotics"
2209.01839,Lucien Grillet and Juan Souto,Effective Estimation of the Dimensions of a Manifold from Random Samples,"['cs.CG', 'math.GT']","  We give explicit theoretical and heuristical bounds for how big does a data
set sampled from a reach-1 submanifold M of euclidian space need to be, to be
able to estimate the dimension of M with 90% confidence.
","[{'version': 'v1', 'created': 'Mon, 5 Sep 2022 08:54:04 GMT'}, {'version': 'v2', 'created': 'Wed, 7 Sep 2022 08:16:54 GMT'}]",2022-09-08,['Computational Geometry'],"This paper presents a novel approach for the effective estimation of the dimensions of a manifold from random samples. A manifold is a generalization of a surface in higher-dimensional spaces, and its dimension is a fundamental property of the manifold. The proposed approach is based on the concept of persistent homology, a powerful tool for analyzing the topology of a dataset. It is shown that the proposed approach is effective at estimating the dimension of a manifold from a random sample of points, and it is also more accurate than existing methods. Furthermore, the proposed approach is computationally efficient, making it suitable for applications in computational geometry.",Write an abstract for a paper called Effective Estimation of the Dimensions of a Manifold from Random Samples about Computational Geometry
2302.11082,"Guoli Wang, Pingping Wang, Jinyu Cong, Kunmeng Liu, Benzheng Wei","BB-GCN: A Bi-modal Bridged Graph Convolutional Network for Multi-label
  Chest X-Ray Recognition",['cs.CV'],"  Multi-label chest X-ray (CXR) recognition involves simultaneously diagnosing
and identifying multiple labels for different pathologies. Since pathological
labels have rich information about their relationship to each other, modeling
the co-occurrence dependencies between pathological labels is essential to
improve recognition performance. However, previous methods rely on state
variable coding and attention mechanisms-oriented to model local label
information, and lack learning of global co-occurrence relationships between
labels. Furthermore, these methods roughly integrate image features and label
embedding, ignoring the alignment and compactness problems in cross-modal
vector fusion.To solve these problems, a Bi-modal Bridged Graph Convolutional
Network (BB-GCN) model is proposed. This model mainly consists of a backbone
module, a pathology Label Co-occurrence relationship Embedding (LCE) module,
and a Transformer Bridge Graph (TBG) module. Specifically, the backbone module
obtains image visual feature representation. The LCE module utilizes a graph to
model the global co-occurrence relationship between multiple labels and employs
graph convolutional networks for learning inference. The TBG module bridges the
cross-modal vectors more compactly and efficiently through the GroupSum
method.We have evaluated the effectiveness of the proposed BB-GCN in two
large-scale CXR datasets (ChestX-Ray14 and CheXpert). Our model achieved
state-of-the-art performance: the mean AUC scores for the 14 pathologies were
0.835 and 0.813, respectively.The proposed LCE and TBG modules can jointly
effectively improve the recognition performance of BB-GCN. Our model also
achieves satisfactory results in multi-label chest X-ray recognition and
exhibits highly competitive generalization performance.
","[{'version': 'v1', 'created': 'Wed, 22 Feb 2023 01:03:53 GMT'}]",2023-02-23,['Computer Vision and Pattern Recognition'],"This paper presents a novel Bi-modal Bridged Graph Convolutional Network (BB-GCN) for multi-label chest X-ray recognition. BB-GCN is designed to integrate the information from both the image and the associated text of chest X-rays. The proposed network consists of two main components: a convolutional neural network (CNN) for image feature extraction and a graph convolutional network (GCN) for text feature extraction. The two components are connected by a bridging layer, which combines the two types of features in a unified learning framework. The proposed BB-GCN is evaluated on two publicly available datasets and achieves competitive results compared to the state-of-the-art methods for multi-label chest X-ray recognition. The results demonstrate the effectiveness of the proposed method in the field of computer vision and pattern recognition.","Write an abstract for a paper called BB-GCN: A Bi-modal Bridged Graph Convolutional Network for Multi-label
  Chest X-Ray Recognition about Computer Vision and Pattern Recognition"
2302.13869,"Yiman Liu, Xiaoxiang Han, Tongtong Liang, Qiaohong Liu, Qingli Li,
  Yuqi Zhang","EDMAE: An Efficient Decoupled Masked Autoencoder for Standard View
  Identification in Pediatric Echocardiography","['eess.IV', 'cs.CV']","  An efficient decoupled masked autoencoder (EDMAE), which is a novel
self-supervised method is proposed for standard view recognition in pediatric
echocardiography in this paper. The proposed EDMAE based on the encoder-decoder
structure forms a new proxy task. The decoder of EDMAE consists of a teacher
encoder and a student encoder, in which the teacher encoder extracts the latent
representation of the masked image blocks, while the student encoder extracts
the latent representation of the visible image blocks. A loss is calculated
between the feature maps output from two encoders to ensure consistency in the
latent representations they extracted. EDMAE replaces the VIT structure in the
encoder of traditional MAE with pure convolution operation to improve training
efficiency. EDMAE is pre-trained in a self-supervised manner on a large-scale
private dataset of pediatric echocardiography, and then fine-tuned on the
downstream task of standard view recognition. The high classification accuracy
is achieved in 27 standard views of pediatric echocardiography. To further
validate the effectiveness of the proposed method, another downstream task of
cardiac ultrasound segmentation is performed on a public dataset CAMUS. The
experiments show that the proposed method not only can surpass some recent
supervised methods but also has more competitiveness on different downstream
tasks.
","[{'version': 'v1', 'created': 'Mon, 27 Feb 2023 15:17:01 GMT'}, {'version': 'v2', 'created': 'Fri, 17 Mar 2023 13:25:21 GMT'}]",2023-03-20,['Computer Vision and Pattern Recognition'],"This paper presents EDMAE, an efficient decoupled masked autoencoder (DMAE) for standard view identification in pediatric echocardiography. EDMAE utilizes a deep convolutional neural network (CNN) to extract features from echocardiogram images and a decoupled masked autoencoder (DMAE) to identify standard views. The proposed method is evaluated on a public pediatric echocardiography dataset and compared with several state-of-the-art methods. Experimental results show that EDMAE outperforms existing methods in terms of accuracy, precision, recall and F1-score, demonstrating its effectiveness in recognizing standard views in pediatric echocardiography. Furthermore, the proposed method is computationally efficient and can be implemented in real-time clinical applications.","Write an abstract for a paper called EDMAE: An Efficient Decoupled Masked Autoencoder for Standard View
  Identification in Pediatric Echocardiography about Computer Vision and Pattern Recognition"
2301.05901,"Pietro Talli, Francesco Pase, Federico Chiariotti, Andrea Zanella,
  Michele Zorzi","Semantic and Effective Communication for Remote Control Tasks with
  Dynamic Feature Compression","['cs.LG', 'cs.AI', 'cs.IT', 'math.IT']","  The coordination of robotic swarms and the remote wireless control of
industrial systems are among the major use cases for 5G and beyond systems: in
these cases, the massive amounts of sensory information that needs to be shared
over the wireless medium can overload even high-capacity connections.
Consequently, solving the effective communication problem by optimizing the
transmission strategy to discard irrelevant information can provide a
significant advantage, but is often a very complex task. In this work, we
consider a prototypal system in which an observer must communicate its sensory
data to an actor controlling a task (e.g., a mobile robot in a factory). We
then model it as a remote Partially Observable Markov Decision Process (POMDP),
considering the effect of adopting semantic and effective
communication-oriented solutions on the overall system performance. We split
the communication problem by considering an ensemble Vector Quantized
Variational Autoencoder (VQ-VAE) encoding, and train a Deep Reinforcement
Learning (DRL) agent to dynamically adapt the quantization level, considering
both the current state of the environment and the memory of past messages. We
tested the proposed approach on the well-known CartPole reference control
problem, obtaining a significant performance increase over traditional
approaches
","[{'version': 'v1', 'created': 'Sat, 14 Jan 2023 11:43:56 GMT'}]",2023-01-18,"['Machine Learning', 'Artificial Intelligence', 'Information Theory']","This paper presents a novel approach to remote control tasks with dynamic feature compression using semantic and effective communication between machine learning and artificial intelligence systems. We propose a new framework that combines information theory with machine learning and artificial intelligence to enable dynamic feature compression for remote control tasks. The proposed framework utilizes semantic and effective communication to reduce the communication overhead and improve the efficiency of remote control tasks. We also present experimental results that demonstrate the effectiveness of the proposed framework in terms of communication overhead and task performance. Finally, we discuss potential applications of the proposed framework in various fields, such as autonomous robotics and internet of things.","Write an abstract for a paper called Semantic and Effective Communication for Remote Control Tasks with
  Dynamic Feature Compression about Machine Learning, Artificial Intelligence, Information Theory"
2206.07707,"Towaki Takikawa and Alex Evans and Jonathan Tremblay and Thomas
  M\""uller and Morgan McGuire and Alec Jacobson and Sanja Fidler",Variable Bitrate Neural Fields,"['cs.CV', 'cs.GR', 'cs.LG', 'cs.MM']","  Neural approximations of scalar and vector fields, such as signed distance
functions and radiance fields, have emerged as accurate, high-quality
representations. State-of-the-art results are obtained by conditioning a neural
approximation with a lookup from trainable feature grids that take on part of
the learning task and allow for smaller, more efficient neural networks.
Unfortunately, these feature grids usually come at the cost of significantly
increased memory consumption compared to stand-alone neural network models. We
present a dictionary method for compressing such feature grids, reducing their
memory consumption by up to 100x and permitting a multiresolution
representation which can be useful for out-of-core streaming. We formulate the
dictionary optimization as a vector-quantized auto-decoder problem which lets
us learn end-to-end discrete neural representations in a space where no direct
supervision is available and with dynamic topology and structure. Our source
code will be available at https://github.com/nv-tlabs/vqad.
","[{'version': 'v1', 'created': 'Wed, 15 Jun 2022 17:58:34 GMT'}]",2022-06-16,"['Computer Vision and Pattern Recognition', 'Graphics', 'Machine Learning', 'Multimedia']","This paper presents a novel approach to Computer Vision and Pattern Recognition, Graphics, Machine Learning, and Multimedia, called Variable Bitrate Neural Fields (VBR-NF). Using a VBR-NF, the paper proposes a new way of representing visual information and improving the accuracy of pattern recognition. The paper also introduces a new learning algorithm for VBR-NFs, which improves the accuracy of pattern recognition by using a variable bitrate representation of the input data. The paper also presents experiments that demonstrate the effectiveness of VBR-NFs in Computer Vision and Pattern Recognition, Graphics, Machine Learning, and Multimedia. The results show that VBR-NFs can significantly improve the accuracy of pattern recognition compared to traditional methods.","Write an abstract for a paper called Variable Bitrate Neural Fields about Computer Vision and Pattern Recognition, Graphics, Machine Learning, Multimedia"
2302.06854,"Shreya Saxena, Raj Sangani, Siva Prasad, Shubham Kumar, Mihir Athale,
  Rohan Awhad, Vishal Vaddina","Large-Scale Knowledge Synthesis and Complex Information Retrieval from
  Biomedical Documents","['cs.IR', 'cs.AI']","  Recent advances in the healthcare industry have led to an abundance of
unstructured data, making it challenging to perform tasks such as efficient and
accurate information retrieval at scale. Our work offers an all-in-one scalable
solution for extracting and exploring complex information from large-scale
research documents, which would otherwise be tedious. First, we briefly explain
our knowledge synthesis process to extract helpful information from
unstructured text data of research documents. Then, on top of the knowledge
extracted from the documents, we perform complex information retrieval using
three major components- Paragraph Retrieval, Triplet Retrieval from Knowledge
Graphs, and Complex Question Answering (QA). These components combine lexical
and semantic-based methods to retrieve paragraphs and triplets and perform
faceted refinement for filtering these search results. The complexity of
biomedical queries and documents necessitates using a QA system capable of
handling queries more complex than factoid queries, which we evaluate
qualitatively on the COVID-19 Open Research Dataset (CORD-19) to demonstrate
the effectiveness and value-add.
","[{'version': 'v1', 'created': 'Tue, 14 Feb 2023 06:03:43 GMT'}]",2023-02-15,"['Information Retrieval', 'Artificial Intelligence']",", and Natural Language Processing

This paper presents a novel approach to large-scale knowledge synthesis and complex information retrieval from biomedical documents. The proposed approach combines information retrieval, artificial intelligence, and natural language processing to extract and analyze relevant information from biomedical documents. Specifically, we propose a deep learning-based approach for document retrieval, a knowledge graph-based approach for entity extraction and relation extraction, and a natural language processing-based approach for semantic analysis. The proposed approach is evaluated on a large-scale biomedical dataset consisting of over one million documents. The results demonstrate the effectiveness of the proposed approach in extracting and synthesizing relevant knowledge from biomedical documents. We also discuss the challenges and opportunities of applying large-scale knowledge synthesis and complex information retrieval to biomedical documents.","Write an abstract for a paper called Large-Scale Knowledge Synthesis and Complex Information Retrieval from
  Biomedical Documents about Information Retrieval, Artificial Intelligence"
2105.04976,"Maya Raifer, Guy Rotman, Reut Apel, Moshe Tennenholtz, Roi Reichart","Designing an Automatic Agent for Repeated Language based Persuasion
  Games",['cs.CL'],"  Persuasion games are fundamental in economics and AI research and serve as
the basis for important applications. However, work on this setup assumes
communication with stylized messages that do not consist of rich human
language. In this paper we consider a repeated sender (expert) -- receiver
(decision maker) game, where the sender is fully informed about the state of
the world and aims to persuade the receiver to accept a deal by sending one of
several possible natural language reviews. We design an automatic expert that
plays this repeated game, aiming to achieve the maximal payoff. Our expert is
implemented within the Monte Carlo Tree Search (MCTS) algorithm, with deep
learning models that exploit behavioral and linguistic signals in order to
predict the next action of the decision maker, and the future payoff of the
expert given the state of the game and a candidate review. We demonstrate the
superiority of our expert over strong baselines, its adaptability to different
decision makers, and that its selected reviews are nicely adapted to the
proposed deal.
","[{'version': 'v1', 'created': 'Tue, 11 May 2021 12:25:57 GMT'}, {'version': 'v2', 'created': 'Fri, 31 Dec 2021 09:00:56 GMT'}]",2022-01-03,['Computation and Language'],"This paper presents a novel approach to designing an automatic agent for repeated language-based persuasion games about computation and language. The proposed agent utilizes reinforcement learning and natural language processing techniques to identify persuasive strategies and generate persuasive messages. We evaluate the agent's performance using simulations of a repeated language-based persuasion game and compare it to a human baseline. Results show that the agent can learn to generate persuasive messages that are more effective than those generated by humans. Furthermore, the agent's performance improves with more training data and more iterations of the game. This work provides an important step towards the development of agents that can engage in persuasive conversations with humans.","Write an abstract for a paper called Designing an Automatic Agent for Repeated Language based Persuasion
  Games about Computation and Language"
2203.13031,"Su Zhang, Ruyi An, Yi Ding, Cuntai Guan","Continuous Emotion Recognition using Visual-audio-linguistic
  information: A Technical Report for ABAW3","['cs.MM', 'cs.CV']","  We propose a cross-modal co-attention model for continuous emotion
recognition using visual-audio-linguistic information. The model consists of
four blocks. The visual, audio, and linguistic blocks are used to learn the
spatial-temporal features of the multi-modal input. A co-attention block is
designed to fuse the learned features with the multi-head co-attention
mechanism. The visual encoding from the visual block is concatenated with the
attention feature to emphasize the visual information. To make full use of the
data and alleviate over-fitting, cross-validation is carried out on the
training and validation set. The concordance correlation coefficient (CCC)
centering is used to merge the results from each fold. The achieved CCC on the
test set is $0.520$ for valence and $0.602$ for arousal, which significantly
outperforms the baseline method with the corresponding CCC of 0.180 and 0.170
for valence and arousal, respectively. The code is available at
https://github.com/sucv/ABAW3.
","[{'version': 'v1', 'created': 'Thu, 24 Mar 2022 12:18:06 GMT'}, {'version': 'v2', 'created': 'Wed, 30 Mar 2022 09:31:29 GMT'}]",2022-03-31,"['Multimedia', 'Computer Vision and Pattern Recognition']","This technical report provides an overview of the current state of continuous emotion recognition using visual-audio-linguistic information. It reviews existing research in the field and proposes a new method for emotion recognition. The proposed method combines visual, audio, and linguistic information to accurately detect and classify emotions in real-time. The report also presents a comprehensive evaluation of the proposed method, showing its effectiveness in recognizing emotions in real-world scenarios. Finally, the report outlines future research directions and potential applications of the proposed method.","Write an abstract for a paper called Continuous Emotion Recognition using Visual-audio-linguistic
  information: A Technical Report for ABAW3 about Multimedia, Computer Vision and Pattern Recognition"
2202.06736,"Charly Robinson La Rocca, Emma Frejinger, Jean-Fran\c{c}ois Cordeau","Minimizing Entropy to Discover Good Solutions to Recurrent Mixed Integer
  Programs","['math.OC', 'cs.LG']","  Current state-of-the-art solvers for mixed-integer programming (MIP) problems
are designed to perform well on a wide range of problems. However, for many
real-world use cases, problem instances come from a narrow distribution. This
has motivated the development of specialized methods that can exploit the
information in historical datasets to guide the design of heuristics. Recent
works have shown that machine learning (ML) can be integrated with an MIP
solver to inject domain knowledge and efficiently close the optimality gap.
This hybridization is usually done with deep learning (DL), which requires a
large dataset and extensive hyperparameter tuning to perform well. This paper
proposes an online heuristic that uses the notion of entropy to efficiently
build a model with minimal training data and tuning. We test our method on the
locomotive assignment problem (LAP), a recurring real-world problem that is
challenging to solve at scale. Experimental results show a speed up of an order
of magnitude compared to a general purpose solver (CPLEX) with a relative gap
of less than 2%. We also observe that for some instances our method can
discover better solutions than CPLEX within the time limit.
","[{'version': 'v1', 'created': 'Mon, 7 Feb 2022 18:52:56 GMT'}]",2022-02-15,['Machine Learning'],"This paper presents a novel approach to finding good solutions to Recurrent Mixed Integer Programs (RMIPs) in the context of Machine Learning (ML). By minimizing the entropy of the RMIPs, the proposed method is able to identify the best solutions in a shorter amount of time. The paper provides an in-depth analysis of the entropy minimization approach, its complexity, and its performance on a set of benchmark RMIPs related to ML. The results show that the proposed approach is able to discover good solutions in a fraction of the time needed by existing methods.","Write an abstract for a paper called Minimizing Entropy to Discover Good Solutions to Recurrent Mixed Integer
  Programs about Machine Learning"
2203.07639,Kai Wu and J. Andrew Zhang and Y. Jay Guo,"Fast and Accurate Linear Fitting for Incompletely Sampled Gaussian
  Function With a Long Tail","['eess.SP', 'cs.NA', 'math.NA', 'physics.ins-det']","  Fitting experiment data onto a curve is a common signal processing technique
to extract data features and establish the relationship between variables.
Often, we expect the curve to comply with some analytical function and then
turn data fitting into estimating the unknown parameters of a function. Among
analytical functions for data fitting, Gaussian function is the most widely
used one due to its extensive applications in numerous science and engineering
fields. To name just a few, Gaussian function is highly popular in statistical
signal processing and analysis, thanks to the central limit theorem [1];
Gaussian function frequently appears in the quantum harmonic oscillator,
quantum field theory, optics, lasers, and many other theories and models in
Physics [2]; moreover, Gaussian function is widely applied in chemistry for
depicting molecular orbitals, in computer science for imaging processing and in
artificial intelligence for defining neural networks.
","[{'version': 'v1', 'created': 'Tue, 15 Mar 2022 04:30:45 GMT'}, {'version': 'v2', 'created': 'Mon, 25 Jul 2022 12:10:52 GMT'}]",2022-11-23,['Numerical Analysis'],"This paper presents a novel numerical analysis algorithm for fast and accurate linear fitting of incompletely sampled Gaussian functions with a long tail. The proposed method utilizes a two-step approach to reduce the computational complexity and improve the accuracy of the linear fitting process. First, a fast Fourier transform (FFT) is used to estimate the parameters of the Gaussian function. Second, an iterative least-squares fitting algorithm is employed to refine the parameters obtained from the FFT. The proposed algorithm is evaluated on a variety of test cases and is shown to significantly reduce the computational complexity and improve the accuracy of linear fitting for incompletely sampled Gaussian functions with a long tail. The results demonstrate the effectiveness of the proposed algorithm, and suggest that it could be applied to a wide range of numerical analysis problems.","Write an abstract for a paper called Fast and Accurate Linear Fitting for Incompletely Sampled Gaussian
  Function With a Long Tail about Numerical Analysis"
2303.15556,"Robert M. Alaniz and Josh Brunner and Michael Coulombe and Erik D.
  Demaine and Yevhenii Diomidov and Ryan Knobel and Timothy Gomez and Elise
  Grizzell and Jayson Lynch and Andrew Rodriguez and Robert Schweller and Tim
  Wylie",Complexity of Reconfiguration in Surface Chemical Reaction Networks,['cs.CC'],"  We analyze the computational complexity of basic reconfiguration problems for
the recently introduced surface Chemical Reaction Networks (sCRNs), where
ordered pairs of adjacent species nondeterministically transform into a
different ordered pair of species according to a predefined set of allowed
transition rules (chemical reactions). In particular, two questions that are
fundamental to the simulation of sCRNs are whether a given configuration of
molecules can ever transform into another given configuration, and whether a
given cell can ever contain a given species, given a set of transition rules.
We show that these problems can be solved in polynomial time, are NP-complete,
or are PSPACE-complete in a variety of different settings, including when
adjacent species just swap instead of arbitrary transformation (swap sCRNs),
and when cells can change species a limited number of times (k-burnout). Most
problems turn out to be at least NP-hard except with very few distinct species
(2 or 3).
","[{'version': 'v1', 'created': 'Mon, 27 Mar 2023 19:14:50 GMT'}]",2023-03-29,['Computational Complexity'],"This paper explores the computational complexity of reconfiguring a surface chemical reaction network (SCRN). It examines the complexity of reconfiguring a SCRN by changing the set of catalysts, substrates, and products, and by changing the reaction rate constants. The paper also discusses the computational complexity of finding the optimal SCRN configuration for a given set of reaction conditions. Finally, the paper presents an algorithm for solving the reconfiguration problem for SCRNs and evaluates its performance. The results of this paper provide insight into the complexity of reconfiguring SCRNs and will be useful for researchers interested in designing and optimizing SCRNs.",Write an abstract for a paper called Complexity of Reconfiguration in Surface Chemical Reaction Networks about Computational Complexity
2211.06605,"Weichen Zhao, Chenguang Wang, Congying Han, Tiande Guo","Comprehensive Analysis of Over-smoothing in Graph Neural Networks from
  Markov Chains Perspective",['cs.LG'],"  The over-smoothing problem is an obstacle of developing deep graph neural
network (GNN). Although many approaches to improve the over-smoothing problem
have been proposed, there is still a lack of comprehensive understanding and
conclusion of this problem. In this work, we analyze the over-smoothing problem
from the Markov chain perspective. We focus on message passing of GNN and first
establish a connection between GNNs and Markov chains on the graph. GNNs are
divided into two classes of operator-consistent and operator-inconsistent based
on whether the corresponding Markov chains are time-homogeneous. Next we
attribute the over-smoothing problem to the convergence of an arbitrary initial
distribution to a stationary distribution. Based on this, we prove that
although the previously proposed methods can alleviate over-smoothing, but
these methods cannot avoid the over-smoothing problem. In addition, we give the
conclusion of the over-smoothing problem in two types of GNNs in the Markovian
sense. On the one hand, operator-consistent GNN cannot avoid over-smoothing at
an exponential rate. On the other hand, operator-inconsistent GNN is not always
over-smoothing. Further, we investigate the existence of the limiting
distribution of the time-inhomogeneous Markov chain, from which we derive a
sufficient condition for operator-inconsistent GNN to avoid over-smoothing.
Finally, we design experiments to verify our findings. Results show that our
proposed sufficient condition can effectively improve over-smoothing problem in
operator-inconsistent GNN and enhance the performance of the model.
","[{'version': 'v1', 'created': 'Sat, 12 Nov 2022 08:03:57 GMT'}, {'version': 'v2', 'created': 'Wed, 1 Mar 2023 15:16:15 GMT'}]",2023-03-02,['Machine Learning'],"This paper presents a comprehensive analysis of over-smoothing in graph neural networks (GNNs) from a Markov chains perspective. The paper first reviews the existing literature on GNNs and over-smoothing, and then provides an in-depth analysis of the phenomenon using Markov chains. The analysis covers the impact of over-smoothing on the performance of GNNs, the selection of hyperparameters, and the development of strategies to mitigate over-smoothing. Additionally, the paper provides an overview of the current research on over-smoothing in GNNs and suggests possible directions for future research. The findings of this paper can help researchers better understand and address the issue of over-smoothing in GNNs to improve the performance of machine learning models.","Write an abstract for a paper called Comprehensive Analysis of Over-smoothing in Graph Neural Networks from
  Markov Chains Perspective about Machine Learning"
2210.17012,Jiaming Liang and Chao Xu and Shengze Cai,"GotFlow3D: Recurrent Graph Optimal Transport for Learning 3D Flow Motion
  in Particle Tracking","['physics.flu-dyn', 'cs.AI']","  Flow visualization technologies such as particle tracking velocimetry (PTV)
are broadly used in understanding the all-pervasiveness three-dimensional (3D)
turbulent flow from nature and industrial processes. Despite the advances in 3D
acquisition techniques, the developed motion estimation algorithms in particle
tracking remain great challenges of large particle displacements, dense
particle distributions and high computational cost. By introducing a novel deep
neural network based on recurrent Graph Optimal Transport, called GotFlow3D, we
present an end-to-end solution to learn the 3D fluid flow motion from
double-frame particle sets. The proposed network constructs two graphs in the
geometric and feature space and further enriches the original particle
representations with the fused intrinsic and extrinsic features learnt from a
graph neural network. The extracted deep features are subsequently utilized to
make optimal transport plans indicating the correspondences of particle pairs,
which are then iteratively and adaptively retrieved to guide the recurrent flow
learning. Experimental evaluations, including assessments on numerical
experiments and validations on real-world experiments, demonstrate that the
proposed GotFlow3D achieves state-of-the-art performance against both
recently-developed scene flow learners and particle tracking algorithms, with
impressive accuracy, robustness and generalization ability, which can provide
deeper insight into the complex dynamics of broad physical and biological
systems.
","[{'version': 'v1', 'created': 'Mon, 31 Oct 2022 02:05:58 GMT'}]",2022-11-01,['Artificial Intelligence'],"GotFlow3D is a novel Recurrent Graph Optimal Transport (RGOT) approach for learning 3D flow motion in particle tracking. This paper presents a novel application of RGOT for learning 3D flow motion in particle tracking. It proposes a new method for learning 3D flow motion from particle trajectories, which can be used for particle tracking and analysis. The proposed method is evaluated on two datasets, and the results demonstrate that it can effectively capture the 3D flow motion from particle trajectories. Furthermore, the proposed method is compared to existing methods, and it outperforms them in terms of accuracy and efficiency. This paper provides an efficient and accurate method for learning 3D flow motion in particle tracking, and it is expected to have a great impact on artificial intelligence applications.","Write an abstract for a paper called GotFlow3D: Recurrent Graph Optimal Transport for Learning 3D Flow Motion
  in Particle Tracking about Artificial Intelligence"
2201.08078,Martin Waltz and Ostap Okhrin,Two-Sample Testing in Reinforcement Learning,['cs.LG'],"  Value-based reinforcement-learning algorithms have shown strong performances
in games, robotics, and other real-world applications. The most popular
sample-based method is $Q$-Learning. It subsequently performs updates by
adjusting the current $Q$-estimate towards the observed reward and the maximum
of the $Q$-estimates of the next state. The procedure introduces maximization
bias with approaches like Double $Q$-Learning. We frame the bias problem
statistically and consider it an instance of estimating the maximum expected
value (MEV) of a set of random variables. We propose the $T$-Estimator (TE)
based on two-sample testing for the mean, that flexibly interpolates between
over- and underestimation by adjusting the significance level of the underlying
hypothesis tests. A generalization, termed $K$-Estimator (KE), obeys the same
bias and variance bounds as the TE while relying on a nearly arbitrary kernel
function. We introduce modifications of $Q$-Learning and the Bootstrapped Deep
$Q$-Network (BDQN) using the TE and the KE. Furthermore, we propose an adaptive
variant of the TE-based BDQN that dynamically adjusts the significance level to
minimize the absolute estimation bias. All proposed estimators and algorithms
are thoroughly tested and validated on diverse tasks and environments,
illustrating the bias control and performance potential of the TE and KE.
","[{'version': 'v1', 'created': 'Thu, 20 Jan 2022 09:22:43 GMT'}, {'version': 'v2', 'created': 'Wed, 6 Jul 2022 06:22:57 GMT'}]",2022-07-07,['Machine Learning'],"This paper presents an overview of two-sample testing in reinforcement learning (RL) and its application to machine learning. The paper begins by discussing the concept of two-sample testing and its use in RL. It then describes the various methods and techniques that can be used to perform two-sample testing in RL, including Monte Carlo simulations, bootstrapping, and importance sampling. The paper then presents a case study of two-sample testing in RL as applied to a machine learning problem. This case study demonstrates how two-sample testing can be used to compare the performance of different machine learning algorithms and to identify the best algorithm for a given task. Finally, the paper discusses the potential applications of two-sample testing in RL and machine learning, as well as its limitations.",Write an abstract for a paper called Two-Sample Testing in Reinforcement Learning about Machine Learning
2201.12352,"Swapnil Bhosale, Rupayan Chakraborty, Sunil Kumar Kopparapu","Automatic Audio Captioning using Attention weighted Event based
  Embeddings","['cs.SD', 'cs.LG', 'eess.AS']","  Automatic Audio Captioning (AAC) refers to the task of translating audio into
a natural language that describes the audio events, source of the events and
their relationships. The limited samples in AAC datasets at present, has set up
a trend to incorporate transfer learning with Audio Event Detection (AED) as a
parent task. Towards this direction, in this paper, we propose an
encoder-decoder architecture with light-weight (i.e. with lesser learnable
parameters) Bi-LSTM recurrent layers for AAC and compare the performance of two
state-of-the-art pre-trained AED models as embedding extractors. Our results
show that an efficient AED based embedding extractor combined with temporal
attention and augmentation techniques is able to surpass existing literature
with computationally intensive architectures. Further, we provide evidence of
the ability of the non-uniform attention weighted encoding generated as a part
of our model to facilitate the decoder glance over specific sections of the
audio while generating each token.
","[{'version': 'v1', 'created': 'Fri, 28 Jan 2022 05:54:19 GMT'}]",2022-02-01,"['Sound', 'Machine Learning']","and Natural Language Processing

This paper presents a novel approach to audio captioning using attention-weighted event-based embeddings. We propose a machine learning model that utilizes natural language processing techniques to generate captions for audio files. Our model is based on sound event recognition and uses attention-weighted event-based embeddings to capture the temporal context of the audio. We evaluate our model on a public dataset and compare it to existing approaches. The results show that our model outperforms existing approaches in terms of accuracy and speed. We also discuss the challenges and potential applications of our model.","Write an abstract for a paper called Automatic Audio Captioning using Attention weighted Event based
  Embeddings about Sound, Machine Learning"
2210.03582,"Yigit Yildirim, Emre Ugur","Learning Social Navigation from Demonstrations with Conditional Neural
  Processes","['cs.RO', 'cs.LG']","  Sociability is essential for modern robots to increase their acceptability in
human environments. Traditional techniques use manually engineered utility
functions inspired by observing pedestrian behaviors to achieve social
navigation. However, social aspects of navigation are diverse, changing across
different types of environments, societies, and population densities, making it
unrealistic to use hand-crafted techniques in each domain. This paper presents
a data-driven navigation architecture that uses state-of-the-art neural
architectures, namely Conditional Neural Processes, to learn global and local
controllers of the mobile robot from observations. Additionally, we leverage a
state-of-the-art, deep prediction mechanism to detect situations not similar to
the trained ones, where reactive controllers step in to ensure safe navigation.
Our results demonstrate that the proposed framework can successfully carry out
navigation tasks regarding social norms in the data. Further, we showed that
our system produces fewer personal-zone violations, causing less discomfort.
","[{'version': 'v1', 'created': 'Fri, 7 Oct 2022 14:37:56 GMT'}, {'version': 'v2', 'created': 'Mon, 26 Dec 2022 15:58:46 GMT'}]",2022-12-27,"['Robotics', 'Machine Learning']",", and Artificial Intelligence

This paper presents a novel method for robots to learn social navigation strategies from demonstrations using conditional neural processes. We propose a framework that combines a generative model with reinforcement learning to enable robots to learn from demonstrations and adapt to environmental changes. We evaluate our framework on a simulated robot navigation task, demonstrating its ability to learn from demonstrations and generalize to unseen scenarios. Our results show that robots can learn social navigation strategies from demonstrations with a high degree of success. Furthermore, we discuss how our proposed approach can be applied to a variety of robotics and artificial intelligence tasks.","Write an abstract for a paper called Learning Social Navigation from Demonstrations with Conditional Neural
  Processes about Robotics, Machine Learning"
2110.06786,"Yuantong Zhang, Huairui Wang, Han Zhu, Zhenzhong Chen","Optical Flow Reusing for High-Efficiency Space-Time Video Super
  Resolution",['cs.CV'],"  In this paper, we consider the task of space-time video super-resolution
(ST-VSR), which can increase the spatial resolution and frame rate for a given
video simultaneously. Despite the remarkable progress of recent methods, most
of them still suffer from high computational costs and inefficient long-range
information usage. To alleviate these problems, we propose a Bidirectional
Recurrence Network (BRN) with the optical-flow-reuse strategy to better use
temporal knowledge from long-range neighboring frames for high-efficiency
reconstruction. Specifically, an efficient and memory-saving multi-frame motion
utilization strategy is proposed by reusing the intermediate flow of adjacent
frames, which considerably reduces the computation burden of frame alignment
compared with traditional LSTM-based designs. In addition, the proposed hidden
state in BRN is updated by the reused optical flow and refined by the Feature
Refinement Module (FRM) for further optimization. Moreover, by utilizing
intermediate flow estimation, the proposed method can inference non-linear
motion and restore details better. Extensive experiments demonstrate that our
optical-flow-reuse-based bidirectional recurrent network (OFR-BRN) is superior
to state-of-the-art methods in accuracy and efficiency.
","[{'version': 'v1', 'created': 'Wed, 13 Oct 2021 15:21:30 GMT'}, {'version': 'v2', 'created': 'Sat, 19 Mar 2022 12:37:35 GMT'}, {'version': 'v3', 'created': 'Wed, 16 Nov 2022 13:03:32 GMT'}]",2022-11-17,['Computer Vision and Pattern Recognition'],"This paper presents a novel approach for high-efficiency space-time video super resolution (STVSR) based on optical flow reusing. Traditional STVSR algorithms require large amounts of computation for optical flow estimation and feature extraction. To reduce the computational complexity, we propose a method that reuses the optical flow from the low-resolution frames to generate the high-resolution frames. The proposed method is evaluated on a publicly available dataset and compared with the state-of-the-art methods. Experimental results demonstrate that our method achieves better performance in terms of both speed and accuracy. The proposed approach can be used to improve the performance of computer vision and pattern recognition tasks.","Write an abstract for a paper called Optical Flow Reusing for High-Efficiency Space-Time Video Super
  Resolution about Computer Vision and Pattern Recognition"
2203.16757,Keyu An and Ji Xiao and Zhijian Ou,"Exploiting Single-Channel Speech for Multi-Channel End-to-End Speech
  Recognition: A Comparative Study","['eess.AS', 'cs.CL']","  Recently, the end-to-end training approach for multi-channel ASR has shown
its effectiveness, which usually consists of a beamforming front-end and a
recognition back-end. However, the end-to-end training becomes more difficult
due to the integration of multiple modules, particularly considering that
multi-channel speech data recorded in real environments are limited in size.
This raises the demand to exploit the single-channel data for multi-channel
end-to-end ASR. In this paper, we systematically compare the performance of
three schemes to exploit external single-channel data for multi-channel
end-to-end ASR, namely back-end pre-training, data scheduling, and data
simulation, under different settings such as the sizes of the single-channel
data and the choices of the front-end. Extensive experiments on CHiME-4 and
AISHELL-4 datasets demonstrate that while all three methods improve the
multi-channel end-to-end speech recognition performance, data simulation
outperforms the other two, at the cost of longer training time. Data scheduling
outperforms back-end pre-training marginally but nearly consistently,
presumably because that in the pre-training stage, the back-end tends to
overfit on the single-channel data, especially when the single-channel data
size is small.
","[{'version': 'v1', 'created': 'Thu, 31 Mar 2022 02:28:11 GMT'}, {'version': 'v2', 'created': 'Sat, 8 Oct 2022 09:57:25 GMT'}]",2022-10-11,['Computation and Language'],"Model

This paper presents a comparative study of two approaches to exploiting single-channel speech for multi-channel end-to-end speech recognition. The first approach is based on a computation-based model, while the second approach is based on a language model. The paper analyzes the performance of both approaches in terms of accuracy, speed, and robustness to noise. The results of the study show that the language model approach outperforms the computation-based approach in all three categories. The paper concludes that the language model approach is an effective way to exploit single-channel speech for multi-channel end-to-end speech recognition.","Write an abstract for a paper called Exploiting Single-Channel Speech for Multi-Channel End-to-End Speech
  Recognition: A Comparative Study about Computation and Language"
2212.08979,"Koustuv Sinha, Jon Gauthier, Aaron Mueller, Kanishka Misra, Keren
  Fuentes, Roger Levy, Adina Williams",Language model acceptability judgements are not always robust to context,"['cs.CL', 'cs.LG']","  Targeted syntactic evaluations of language models ask whether models show
stable preferences for syntactically acceptable content over minimal-pair
unacceptable inputs. Most targeted syntactic evaluation datasets ask models to
make these judgements with just a single context-free sentence as input. This
does not match language models' training regime, in which input sentences are
always highly contextualized by the surrounding corpus. This mismatch raises an
important question: how robust are models' syntactic judgements in different
contexts? In this paper, we investigate the stability of language models'
performance on targeted syntactic evaluations as we vary properties of the
input context: the length of the context, the types of syntactic phenomena it
contains, and whether or not there are violations of grammaticality. We find
that model judgements are generally robust when placed in randomly sampled
linguistic contexts. However, they are substantially unstable for contexts
containing syntactic structures matching those in the critical test content.
Among all tested models (GPT-2 and five variants of OPT), we significantly
improve models' judgements by providing contexts with matching syntactic
structures, and conversely significantly worsen them using unacceptable
contexts with matching but violated syntactic structures. This effect is
amplified by the length of the context, except for unrelated inputs. We show
that these changes in model performance are not explainable by simple features
matching the context and the test inputs, such as lexical overlap and
dependency overlap. This sensitivity to highly specific syntactic features of
the context can only be explained by the models' implicit in-context learning
abilities.
","[{'version': 'v1', 'created': 'Sun, 18 Dec 2022 00:11:06 GMT'}]",2022-12-20,"['Computation and Language', 'Machine Learning']","This paper presents an empirical investigation into the robustness of language model acceptability judgements to context. We show that language models trained on large-scale datasets are not always robust to changes in context. We use a variety of machine learning techniques to evaluate the acceptability of a given sentence in two different contexts. We find that language models trained on large-scale datasets are not always able to correctly predict the acceptability of a given sentence in both contexts. We also find that language models trained on smaller datasets are more robust to context than those trained on large-scale datasets. Our findings suggest that language model acceptability judgements are not always robust to context, and that further research is needed to improve the robustness of these models.","Write an abstract for a paper called Language model acceptability judgements are not always robust to context about Computation and Language, Machine Learning"
2202.03665,"S{\o}ren Taverniers, Svyatoslav Korneev, Kyle M. Pietrzyk, Morad
  Behandish","Accelerating Part-Scale Simulation in Liquid Metal Jet Additive
  Manufacturing via Operator Learning","['physics.flu-dyn', 'cs.AI', 'cs.LG', 'physics.comp-ph', 'physics.data-an']","  Predicting part quality for additive manufacturing (AM) processes requires
high-fidelity numerical simulation of partial differential equations (PDEs)
governing process multiphysics on a scale of minimum manufacturable features.
This makes part-scale predictions computationally demanding, especially when
they require many small-scale simulations. We consider drop-on-demand liquid
metal jetting (LMJ) as an illustrative example of such computational
complexity. A model describing droplet coalescence for LMJ may include coupled
incompressible fluid flow, heat transfer, and phase change equations.
Numerically solving these equations becomes prohibitively expensive when
simulating the build process for a full part consisting of thousands to
millions of droplets. Reduced-order models (ROMs) based on neural networks (NN)
or k-nearest neighbor (kNN) algorithms have been built to replace the original
physics-based solver and are computationally tractable for part-level
simulations. However, their quick inference capabilities often come at the
expense of accuracy, robustness, and generalizability. We apply an operator
learning (OL) approach to learn a mapping between initial and final states of
the droplet coalescence process for enabling rapid and accurate part-scale
build simulation. Preliminary results suggest that OL requires
order-of-magnitude fewer data points than a kNN approach and is generalizable
beyond the training set while achieving similar prediction error.
","[{'version': 'v1', 'created': 'Wed, 2 Feb 2022 17:24:16 GMT'}]",2022-05-16,"['Artificial Intelligence', 'Machine Learning']",", and Computer Vision

This paper explores the potential of Artificial Intelligence (AI), Machine Learning (ML), and Computer Vision (CV) to accelerate part-scale simulation in liquid metal jet additive manufacturing (LMJAM). The paper investigates the use of AI, ML, and CV to enable the operator to quickly and accurately detect and diagnose problems in LMJAM processes, enabling faster and more reliable production. The paper also discusses the challenges and opportunities associated with integrating AI, ML, and CV into LMJAM processes. Finally, the paper presents a case study to demonstrate the potential of AI, ML, and CV for accelerating part-scale simulation in LMJAM.","Write an abstract for a paper called Accelerating Part-Scale Simulation in Liquid Metal Jet Additive
  Manufacturing via Operator Learning about Artificial Intelligence, Machine Learning"
2212.0615,"Shuo Yang, Yang Jiao, Shaoyu Dou, Mana Zheng, Chen Zhu","CPMLHO:Hyperparameter Tuning via Cutting Plane and Mixed-Level
  Optimization",['cs.LG'],"  The hyperparameter optimization of neural network can be expressed as a
bilevel optimization problem. The bilevel optimization is used to automatically
update the hyperparameter, and the gradient of the hyperparameter is the
approximate gradient based on the best response function. Finding the best
response function is very time consuming. In this paper we propose CPMLHO, a
new hyperparameter optimization method using cutting plane method and
mixed-level objective function.The cutting plane is added to the inner layer to
constrain the space of the response function. To obtain more accurate
hypergradient,the mixed-level can flexibly adjust the loss function by using
the loss of the training set and the verification set. Compared to existing
methods, the experimental results show that our method can automatically update
the hyperparameters in the training process, and can find more superior
hyperparameters with higher accuracy and faster convergence.
","[{'version': 'v1', 'created': 'Sun, 11 Dec 2022 07:46:19 GMT'}]",2022-12-14,['Machine Learning'],"This paper presents CPMLHO, a novel hyperparameter tuning method for machine learning. CPMLHO combines the cutting plane and mixed-level optimization techniques to achieve a more efficient and accurate tuning of hyperparameters. Experiments on several datasets demonstrate that CPMLHO outperforms existing approaches in terms of accuracy and runtime. Furthermore, CPMLHO is robust and can be applied to a variety of machine learning models and datasets. The paper also provides insights into how the proposed method works and how it can be applied to other machine learning tasks.","Write an abstract for a paper called CPMLHO:Hyperparameter Tuning via Cutting Plane and Mixed-Level
  Optimization about Machine Learning"
2203.13854,"Arash Bahari Kordabad, Hossein Nejatbakhsh Esfahani, Wenqi Cai,
  Sebastien Gros",Quasi-Newton Iteration in Deterministic Policy Gradient,"['cs.LG', 'cs.SY', 'eess.SY']","  This paper presents a model-free approximation for the Hessian of the
performance of deterministic policies to use in the context of Reinforcement
Learning based on Quasi-Newton steps in the policy parameters. We show that the
approximate Hessian converges to the exact Hessian at the optimal policy, and
allows for a superlinear convergence in the learning, provided that the policy
parametrization is rich. The natural policy gradient method can be interpreted
as a particular case of the proposed method. We analytically verify the
formulation in a simple linear case and compare the convergence of the proposed
method with the natural policy gradient in a nonlinear example.
","[{'version': 'v1', 'created': 'Fri, 25 Mar 2022 18:38:57 GMT'}]",2022-03-29,"['Machine Learning', 'Systems and Control']","This paper presents a novel approach to applying the deterministic policy gradient algorithm to the problem of reinforcement learning. The approach, called Quasi-Newton Iteration in Deterministic Policy Gradient (QNDPG), is based on an iterative optimization of the policy parameters using a quasi-Newton method. The proposed approach is demonstrated on a simulated robotic arm and a simulated inverted pendulum. The results show that the proposed approach outperforms traditional policy gradient methods in terms of convergence speed, learning accuracy, and sample complexity. Furthermore, the proposed approach is compared to existing reinforcement learning algorithms, showing that it is capable of achieving similar or better performance. Finally, the paper discusses the potential applications of the proposed method to control and system design in the context of machine learning.","Write an abstract for a paper called Quasi-Newton Iteration in Deterministic Policy Gradient about Machine Learning, Systems and Control"
2003.1055,"Amrit Singh Bedi, Dheeraj Peddireddy, Vaneet Aggarwal, Brian M.
  Sadler, and Alec Koppel","Regret and Belief Complexity Trade-off in Gaussian Process Bandits via
  Information Thresholding","['cs.LG', 'cs.IT', 'math.IT', 'stat.ML']","  Bayesian optimization is a framework for global search via maximum a
posteriori updates rather than simulated annealing, and has gained prominence
for decision-making under uncertainty. In this work, we cast Bayesian
optimization as a multi-armed bandit problem, where the payoff function is
sampled from a Gaussian process (GP). Further, we focus on action selections
via upper confidence bound (UCB) or expected improvement (EI) due to their
prevalent use in practice. Prior works using GPs for bandits cannot allow the
iteration horizon $T$ to be large, as the complexity of computing the posterior
parameters scales cubically with the number of past observations. To circumvent
this computational burden, we propose a simple statistical test: only
incorporate an action into the GP posterior when its conditional entropy
exceeds an $\epsilon$ threshold. Doing so permits us to precisely characterize
the trade-off between regret bounds of GP bandit algorithms and complexity of
the posterior distributions depending on the compression parameter $\epsilon$
for both discrete and continuous action sets. To best of our knowledge, this is
the first result which allows us to obtain sublinear regret bounds while still
maintaining sublinear growth rate of the complexity of the posterior which is
linear in the existing literature. Moreover, a provably finite bound on the
complexity could be achieved but the algorithm would result in
$\epsilon$-regret which means $\textbf{Reg}_T/T \rightarrow
\mathcal{O}(\epsilon)$ as $T\rightarrow \infty$. Experimentally, we observe
state of the art accuracy and complexity trade-offs for GP bandit algorithms
applied to global optimization, suggesting the merits of compressed GPs in
bandit settings.
","[{'version': 'v1', 'created': 'Mon, 23 Mar 2020 21:05:15 GMT'}, {'version': 'v2', 'created': 'Wed, 9 Dec 2020 01:16:04 GMT'}, {'version': 'v3', 'created': 'Mon, 21 Mar 2022 21:12:54 GMT'}]",2022-03-24,"['Machine Learning', 'Information Theory']","and Optimization

This paper investigates the trade-off between regret and belief complexity in Gaussian Process Bandits (GPB) through the lens of information theory and optimization. We propose a novel information thresholding approach which uses a convex optimization framework to balance the trade-off between regret and belief complexity. We evaluate the proposed approach on a variety of synthetic and real-world datasets, and demonstrate that our approach outperforms existing methods in terms of both regret and belief complexity. Furthermore, we provide theoretical analysis to support our findings. Our results suggest that information thresholding can be an effective and promising approach for managing regret and belief complexity in GPB.","Write an abstract for a paper called Regret and Belief Complexity Trade-off in Gaussian Process Bandits via
  Information Thresholding about Machine Learning, Information Theory"
2205.00202,"Mariana C. A. Clare and Maike Sonnewald and Redouane Lguensat and
  Julie Deshayes and Venkatramani Balaji","Explainable Artificial Intelligence for Bayesian Neural Networks:
  Towards trustworthy predictions of ocean dynamics","['physics.ao-ph', 'cs.LG']","  The trustworthiness of neural networks is often challenged because they lack
the ability to express uncertainty and explain their skill. This can be
problematic given the increasing use of neural networks in high stakes
decision-making such as in climate change applications. We address both issues
by successfully implementing a Bayesian Neural Network (BNN), where parameters
are distributions rather than deterministic, and applying novel implementations
of explainable AI (XAI) techniques. The uncertainty analysis from the BNN
provides a comprehensive overview of the prediction more suited to
practitioners' needs than predictions from a classical neural network. Using a
BNN means we can calculate the entropy (i.e. uncertainty) of the predictions
and determine if the probability of an outcome is statistically significant. To
enhance trustworthiness, we also spatially apply the two XAI techniques of
Layer-wise Relevance Propagation (LRP) and SHapley Additive exPlanation (SHAP)
values. These XAI methods reveal the extent to which the BNN is suitable and/or
trustworthy. Using two techniques gives a more holistic view of BNN skill and
its uncertainty, as LRP considers neural network parameters, whereas SHAP
considers changes to outputs. We verify these techniques using comparison with
intuition from physical theory. The differences in explanation identify
potential areas where new physical theory guided studies are needed.
","[{'version': 'v1', 'created': 'Sat, 30 Apr 2022 08:35:57 GMT'}]",2022-12-07,['Machine Learning'],"This paper explores the use of Explainable Artificial Intelligence (XAI) for Bayesian Neural Networks (BNNs) to improve the trustworthiness of predictions of ocean dynamics about Machine Learning (ML). We discuss the challenges of applying XAI to BNNs, including the need for interpretability, explainability, and trustworthiness. We then propose a novel XAI approach for BNNs, which combines existing XAI methods with probabilistic inference to provide more accurate, reliable, and interpretable predictions. We evaluate this approach on a simulated ocean dynamics dataset and demonstrate its ability to accurately predict ocean dynamics while providing explainable insight into the underlying ML model. Finally, we discuss the implications of this research and potential applications in the field of ocean dynamics.","Write an abstract for a paper called Explainable Artificial Intelligence for Bayesian Neural Networks:
  Towards trustworthy predictions of ocean dynamics about Machine Learning"
2012.02334,"Yaofeng Desmond Zhong, Biswadip Dey, Amit Chakraborty","Benchmarking Energy-Conserving Neural Networks for Learning Dynamics
  from Data","['cs.LG', 'cs.AI', 'cs.SY', 'eess.SY', 'math.DS']","  The last few years have witnessed an increased interest in incorporating
physics-informed inductive bias in deep learning frameworks. In particular, a
growing volume of literature has been exploring ways to enforce energy
conservation while using neural networks for learning dynamics from observed
time-series data. In this work, we survey ten recently proposed
energy-conserving neural network models, including HNN, LNN, DeLaN, SymODEN,
CHNN, CLNN and their variants. We provide a compact derivation of the theory
behind these models and explain their similarities and differences. Their
performance are compared in 4 physical systems. We point out the possibility of
leveraging some of these energy-conserving models to design energy-based
controllers.
","[{'version': 'v1', 'created': 'Thu, 3 Dec 2020 23:53:08 GMT'}, {'version': 'v2', 'created': 'Wed, 30 Dec 2020 18:34:04 GMT'}, {'version': 'v3', 'created': 'Fri, 26 Feb 2021 18:13:21 GMT'}, {'version': 'v4', 'created': 'Tue, 18 May 2021 19:24:55 GMT'}, {'version': 'v5', 'created': 'Tue, 11 Jan 2022 20:14:55 GMT'}]",2022-01-13,"['Machine Learning', 'Artificial Intelligence', 'Systems and Control']","This paper presents a benchmarking study of energy-conserving neural networks for learning dynamics from data about machine learning, artificial intelligence, systems and control. We leverage a comprehensive dataset of real-world system dynamics to evaluate the performance of energy-conserving neural networks in predicting system behavior. We compare the performance of energy-conserving neural networks to traditional neural networks, and other state-of-the-art models. We also discuss the implications of our findings for the development of energy-conserving neural networks in machine learning, artificial intelligence, systems and control.","Write an abstract for a paper called Benchmarking Energy-Conserving Neural Networks for Learning Dynamics
  from Data about Machine Learning, Artificial Intelligence, Systems and Control"
2304.04091,"Yuhang Wu, Zeyu Zheng, Tingyu Zhu",Best Arm Identification with Fairness Constraints on Subpopulations,"['cs.LG', 'cs.CY', 'stat.ML']","  We formulate, analyze and solve the problem of best arm identification with
fairness constraints on subpopulations (BAICS). Standard best arm
identification problems aim at selecting an arm that has the largest expected
reward where the expectation is taken over the entire population. The BAICS
problem requires that an selected arm must be fair to all subpopulations (e.g.,
different ethnic groups, age groups, or customer types) by satisfying
constraints that the expected reward conditional on every subpopulation needs
to be larger than some thresholds. The BAICS problem aims at correctly
identify, with high confidence, the arm with the largest expected reward from
all arms that satisfy subpopulation constraints. We analyze the complexity of
the BAICS problem by proving a best achievable lower bound on the sample
complexity with closed-form representation. We then design an algorithm and
prove that the algorithm's sample complexity matches with the lower bound in
terms of order. A brief account of numerical experiments are conducted to
illustrate the theoretical findings.
","[{'version': 'v1', 'created': 'Sat, 8 Apr 2023 19:41:02 GMT'}]",2023-04-11,"['Machine Learning', 'Computers and Society']","This paper examines machine learning algorithms to identify the best arm in a multi-armed bandit problem while ensuring fairness constraints on subpopulations. We propose two approaches to address this problem: a fairness-aware algorithm that incorporates fairness constraints into the selection of the best arm, and a fairness-aware post-processing algorithm that adjusts the arm selection results to ensure fairness. We evaluate our approaches on a synthetic dataset and demonstrate the effectiveness of our algorithms in achieving fairness. Furthermore, we discuss the implications of our results for the use of machine learning algorithms in decision-making systems.","Write an abstract for a paper called Best Arm Identification with Fairness Constraints on Subpopulations about Machine Learning, Computers and Society"
2109.12525,Chaemin Lee and Jongho Park,Preconditioning for finite element methods with strain smoothing,"['math.NA', 'cs.NA']","  Strain smoothing methods such as the smoothed finite element methods (S-FEMs)
and the strain-smoothed element~(SSE) method have successfully improved the
performance of finite elements, and there have been numerous applications of
them in finite element analysis. For the sake of efficient applications to
large-scale problems, it is important to develop a mathematically and
numerically well-elaborated iterative solver for the strain smoothing methods.
In this paper, inspired by the spectral properties of the strain smoothing
methods, we propose efficient ways of preconditioning for the methods. First,
we analyze the spectrums of the stiffness matrices of the edge-based S-FEM and
the SSE method. Then, we propose an improved two-level additive Schwarz
preconditioner for the strain smoothing methods by modifying local solvers
appropriately. For the sake of convenience of implementation, an alternative
form of the preconditioner is also proposed by defining the coarse-scale
operation in terms of the standard FEM. We verify our theoretical results
through numerical experiments.
","[{'version': 'v1', 'created': 'Sun, 26 Sep 2021 08:19:08 GMT'}, {'version': 'v2', 'created': 'Mon, 21 Nov 2022 06:32:37 GMT'}]",2022-12-01,['Numerical Analysis'],"This paper focuses on the numerical analysis of preconditioning for finite element methods with strain smoothing. Strain smoothing is a technique used to reduce the effects of numerical errors. The paper examines the effects of preconditioning on the convergence of finite element methods with strain smoothing. It also investigates the stability of the numerical solution and the impact of the preconditioning parameters on the convergence rate. The paper will present numerical results on various examples to validate the proposed preconditioning technique. The results will be compared with other existing methods and the advantages of the proposed technique will be discussed. Finally, the paper will discuss the implications of the proposed technique and its potential applications.",Write an abstract for a paper called Preconditioning for finite element methods with strain smoothing about Numerical Analysis
2206.11076,Sam Clarke and Jess Whittlestone,A Survey of the Potential Long-term Impacts of AI,['cs.CY'],"  It is increasingly recognised that advances in artificial intelligence could
have large and long-lasting impacts on society. However, what form those
impacts will take, just how large and long-lasting they will be, and whether
they will ultimately be positive or negative for humanity, is far from clear.
Based on surveying literature on the societal impacts of AI, we identify and
discuss five potential long-term impacts of AI: how AI could lead to long-term
changes in science, cooperation, power, epistemics, and values. We review the
state of existing research in each of these areas and highlight priority
questions for future research.
","[{'version': 'v1', 'created': 'Wed, 22 Jun 2022 13:42:28 GMT'}]",2022-06-23,['Computers and Society'],"This paper examines the potential long-term impacts of Artificial Intelligence (AI) on computers and society. It begins with a discussion of the current state of AI and its potential applications, followed by an overview of the potential long-term impacts of AI on computers and society. The paper then reviews the literature on the potential long-term impacts of AI, focusing on the potential for AI to improve the efficiency of computers and society, the potential for AI to create new jobs and industries, and potential ethical considerations. Finally, the paper concludes with a discussion of the implications of the potential long-term impacts of AI for computer and society.",Write an abstract for a paper called A Survey of the Potential Long-term Impacts of AI about Computers and Society
2212.00188,"Logan E Beaver, Max Sokolich, Suhail Alsalehi, Ron Weiss, Sambeeta
  Das, Calin Belta",Learning for Control of Rolling ubots,['cs.RO'],"  Micron-scale robots (ubots) have recently shown great promise for emerging
medical applications, and accurate control of ubots is a critical next step to
deploying them in real systems. In this work, we develop the idea of a
nonlinear mismatch controller to compensate for the mismatch between the
disturbed unicycle model of a rolling ubot and trajectory data collected during
an experiment. We exploit the differential flatness property of the rolling
ubot model to generate a mapping from the desired state trajectory to nominal
control actions. Due to model mismatch and parameter estimation error, the
nominal control actions will not exactly reproduce the desired state
trajectory. We employ a Gaussian Process (GP) to learn the model mismatch as a
function of the desired control actions, and correct the nominal control
actions using a least-squares optimization. We demonstrate the performance of
our online learning algorithm in simulation, where we show that the model
mismatch makes some desired states unreachable. Finally, we validate our
approach in an experiment and show that the error metrics are reduced by up to
40%.
","[{'version': 'v1', 'created': 'Thu, 1 Dec 2022 00:20:28 GMT'}]",2022-12-02,['Robotics'],"This paper presents an approach to learning for control of rolling robots. The proposed approach combines learning from demonstration (LfD) with reinforcement learning (RL). The LfD technique is used to generate an initial policy, which is then refined using the RL approach. The approach is evaluated on a real-world robotic system and the results demonstrate that the proposed approach is able to successfully learn the desired control policy. Furthermore, the approach is shown to be robust to disturbances and able to successfully adapt to changing environments. The results demonstrate the potential of the proposed approach for controlling rolling robots in a variety of scenarios.",Write an abstract for a paper called Learning for Control of Rolling ubots about Robotics
2302.09891,"Yu Shi, Ning Xu, Hua Yuan and Xin Geng",Unreliable Partial Label Learning with Recursive Separation,['cs.AI'],"  Partial label learning (PLL) is a typical weakly supervised learning problem
in which each instance is associated with a candidate label set, and among
which only one is true. However, the assumption that the ground-truth label is
always among the candidate label set would be unrealistic, as the reliability
of the candidate label sets in real-world applications cannot be guaranteed by
annotators. Therefore, a generalized PLL named Unreliable Partial Label
Learning (UPLL) is proposed, in which the true label may not be in the
candidate label set. Due to the challenges posed by unreliable labeling,
previous PLL methods will experience a marked decline in performance when
applied to UPLL. To address the issue, we propose a two-stage framework named
Unreliable Partial Label Learning with Recursive Separation (UPLLRS). In the
first stage, the self-adaptive recursive separation strategy is proposed to
separate the training set into a reliable subset and an unreliable subset. In
the second stage, a disambiguation strategy is employed to progressively
identify the ground-truth labels in the reliable subset. Simultaneously,
semi-supervised learning methods are adopted to extract valuable information
from the unreliable subset. Our method demonstrates state-of-the-art
performance as evidenced by experimental results, particularly in situations of
high unreliability.
","[{'version': 'v1', 'created': 'Mon, 20 Feb 2023 10:39:31 GMT'}]",2023-02-21,['Artificial Intelligence'],"This paper presents a novel approach to the problem of partial label learning in the context of Artificial Intelligence (AI). We propose a recursive separation technique to identify and separate the reliable and unreliable label sets. We evaluate the proposed approach on two benchmark datasets and compare it to existing methods. Our results show that our method outperforms the existing methods in terms of accuracy, precision, and recall, offering a promising solution to the partial label learning problem. Additionally, we discuss potential applications of our proposed approach and provide an analysis of its potential for further research.",Write an abstract for a paper called Unreliable Partial Label Learning with Recursive Separation about Artificial Intelligence
1909.00155,"Shengwen Liang, Ying Wang, Cheng Liu, Lei He, Huawei Li, and Xiaowei
  Li","EnGN: A High-Throughput and Energy-Efficient Accelerator for Large Graph
  Neural Networks",['cs.DC'],"  Graph neural networks (GNNs) emerge as a powerful approach to process
non-euclidean data structures and have been proved powerful in various
application domains such as social networks and e-commerce. While such graph
data maintained in real-world systems can be extremely large and sparse, thus
employing GNNs to deal with them requires substantial computational and memory
overhead, which induces considerable energy and resource cost on CPUs and GPUs.
In this work, we present a specialized accelerator architecture, EnGN, to
enable high-throughput and energy-efficient processing of large-scale GNNs. The
proposed EnGN is designed to accelerate the three key stages of GNN
propagation, which is abstracted as common computing patterns shared by typical
GNNs. To support the key stages simultaneously, we propose the
ring-edge-reduce(RER) dataflow that tames the poor locality of
sparsely-and-randomly connected vertices, and the RER PE-array to practice RER
dataflow. In addition, we utilize a graph tiling strategy to fit large graphs
into EnGN and make good use of the hierarchical on-chip buffers through
adaptive computation reordering and tile scheduling. Overall, EnGN achieves
performance speedup by 1802.9X, 19.75X, and 2.97X and energy efficiency by
1326.35X, 304.43X, and 6.2X on average compared to CPU, GPU, and a
state-of-the-art GCN accelerator HyGCN, respectively.
","[{'version': 'v1', 'created': 'Sat, 31 Aug 2019 07:12:59 GMT'}, {'version': 'v2', 'created': 'Sat, 30 Nov 2019 02:08:40 GMT'}, {'version': 'v3', 'created': 'Tue, 7 Apr 2020 11:34:10 GMT'}]",2023-04-06,"['Distributed, Parallel, and Cluster Computing']","This paper presents EnGN, a high-throughput and energy-efficient accelerator for large graph neural networks (GNNs). GNNs are a powerful tool for analyzing complex, large-scale graph datasets, and are increasingly being used in distributed, parallel, and cluster computing applications. EnGN is a novel accelerator design that utilizes an optimized dataflow to achieve high throughput and energy efficiency. To evaluate the performance of EnGN, we deploy and benchmark it on a distributed cluster of machines. We demonstrate that EnGN achieves significant performance gains over existing GNN accelerators, with a throughput up to 3.5x higher and energy efficiency up to 3x higher. Our results show that EnGN is an effective accelerator for distributed, parallel, and cluster computing applications, and can be used to enable large-scale GNNs.","Write an abstract for a paper called EnGN: A High-Throughput and Energy-Efficient Accelerator for Large Graph
  Neural Networks about Distributed, Parallel, and Cluster Computing"
2302.06563,"Luca G. Tallini, Nawaf Alqwaifly and Bella Bose","Efficient Systematic Deletions/Insertions of $0$'s Error Control Codes
  and the $L_{1}$ Metric (Extended version)","['cs.IT', 'math.CO', 'math.IT']","  This paper gives some theory and efficient design of binary block systematic
codes capable of controlling the deletions of the symbol ``$0$'' (referred to
as $0$-deletions) and/or the insertions of the symbol ``$0$'' (referred to as
$0$-insertions). The problem of controlling $0$-deletions and/or $0$-insertions
(referred to as $0$-errors) is known to be equivalent to the efficient design
of $L_{1}$ metric asymmetric error control codes over the natural alphabet,
$\mathbb{N}$. So, $t$ $0$-insertion correcting codes can actually correct $t$
$0$-errors, detect $(t+1)$ $0$-errors and, simultaneously, detect all
occurrences of only $0$-deletions or only $0$-insertions in every received word
(briefly, they are $t$-Symmetric $0$-Error Correcting/$(t+1)$-Symmetric
$0$-Error Detecting/All Unidirectional $0$-Error Detecting
($t$-Sy$0$EC/$(t+1)$-Sy$0$ED/AU$0$ED) codes). From the relations with the
$L_{1}$ distance, optimal systematic code designs are given. In general, for
all $t,k\in\mathbb{N}$, a recursive method is presented to encode $k$
information bits into efficient systematic $t$-Sy$0$EC/$(t+1)$-Sy$0$ED/AU$0$ED
codes of length $$ n\leq k+t\log_{2}k+o(t\log n) $$ as $n\in\mathbb{N}$
increases. Decoding can be efficiently performed by algebraic means using the
Extended Euclidean Algorithm (EEA).
","[{'version': 'v1', 'created': 'Mon, 13 Feb 2023 18:06:38 GMT'}]",2023-02-14,['Information Theory'],"This paper presents an efficient systematic deletion/insertion algorithm for error control codes and the L1 metric. The algorithm is based on information theory principles and is capable of achieving a low error rate with a low complexity. The algorithm uses a combination of the Hamming distance and the L1 metric to identify and correct errors in the data stream. The algorithm is extended to include a new metric, the extended L1 metric, which provides improved error correction performance. The paper provides a detailed description of the algorithm and its performance evaluation. Simulation results demonstrate the effectiveness of the proposed algorithm in comparison to existing methods. The results show that the proposed algorithm has better error correction performance and lower complexity compared to existing methods.","Write an abstract for a paper called Efficient Systematic Deletions/Insertions of $0$'s Error Control Codes
  and the $L_{1}$ Metric (Extended version) about Information Theory"
2303.02412,Uwe D. Hanebeck,"Progressive Bayesian Particle Flows based on Optimal Transport Map
  Sequences","['stat.ML', 'cs.SY', 'eess.SY']","  We propose a method for optimal Bayesian filtering with deterministic
particles. In order to avoid particle degeneration, the filter step is not
performed at once. Instead, the particles progressively flow from prior to
posterior. This is achieved by splitting the filter step into a series of
sub-steps. In each sub-step, optimal resampling is done by a map that replaces
non-equally weighted particles with equally weighted ones. Inversions of the
maps or monotonicity constraints are not required, greatly simplifying the
procedure. The parameters of the mapping network are optimized w.r.t.\ to a
particle set distance. This distance is differentiable, and compares
non-equally and equally weighted particles. Composition of the map sequence
provides a final mapping from prior to posterior particles. Radial basis
function neural networks are used as maps. It is important that no intermediate
continuous density representation is required. The entire flow works directly
with particle representations. This avoids costly density estimation.
","[{'version': 'v1', 'created': 'Sat, 4 Mar 2023 13:12:43 GMT'}]",2023-03-07,['Systems and Control'],"This paper presents a novel approach to systems and control using progressive Bayesian particle flows based on optimal transport map sequences. By leveraging the power of optimal transport maps, this method allows for efficient and accurate estimation of the state of a system over time. Specifically, the proposed method uses a Bayesian particle filter to progressively refine the optimal transport maps and generate a sequence of increasingly accurate estimates of the state of the system. The effectiveness of this approach is demonstrated through numerical simulations and comparison to existing methods. The results show that the proposed method can provide more accurate estimates of the system state than existing methods, and is thus a promising tool for systems and control.","Write an abstract for a paper called Progressive Bayesian Particle Flows based on Optimal Transport Map
  Sequences about Systems and Control"
2210.13209,Vikash Pandey,"Response to ""Comment on 'Origin of the Curie--von Schweidler law and the
  fractional capacitor from time-varying capacitance [J. Pow. Sources 532
  (2022) 231309]' ""","['cond-mat.mtrl-sci', 'cs.SY', 'eess.SY']","  We welcome Allagui et al.'s discussions about our recent paper that has
proposed revisions to the existing theory of capacitors. It gives us an
opportunity to emphasize on the physical underpinnings of the mathematical
expressions that are relevant for modeling using fractional derivatives. The
concerns raised by Allagui et al. are found to be quite questionable when
examined in light of the established standard results of fractional calculus.
Consequently, the inferences that they have drawn are not true. Finally, we
would like to thank Allagui et al. because this subsequent Response to their
Comment has actually led to a further consolidation of our results that are
supposed to be significant for materials science as well as for fractional
control systems and engineering.
","[{'version': 'v1', 'created': 'Sat, 17 Sep 2022 03:53:32 GMT'}, {'version': 'v2', 'created': 'Wed, 26 Oct 2022 08:17:54 GMT'}]",2022-10-27,['Systems and Control'],"This paper provides an in-depth response to the article ""Comment on 'Origin of the Curie--von Schweidler law and the fractional capacitor from time-varying capacitance [J. Pow. Sources 532 (2022) 231309]'"" by discussing the implications of the article for systems and control. The paper begins by summarizing the original article and outlining its main findings. It then explains the importance of the article for systems and control, by discussing the implications of the article for systems modeling, control theory, and design. The paper then describes the potential applications of the article's findings to systems and control, and suggests further research directions in this area. Finally, the paper provides a conclusion summarizing the main contributions of the article to systems and control.","Write an abstract for a paper called Response to ""Comment on 'Origin of the Curie--von Schweidler law and the
  fractional capacitor from time-varying capacitance [J. Pow. Sources 532
  (2022) 231309]' "" about Systems and Control"
2109.05327,"Francesco Sovrano, Fabio Vitali","An Objective Metric for Explainable AI: How and Why to Estimate the
  Degree of Explainability","['cs.AI', 'cs.CL']","  Explainable AI was born as a pathway to allow humans to explore and
understand the inner working of complex systems. However, establishing what is
an explanation and objectively evaluating explainability are not trivial tasks.
This paper presents a new model-agnostic metric to measure the Degree of
Explainability of information in an objective way. We exploit a specific
theoretical model from Ordinary Language Philosophy called the Achinstein's
Theory of Explanations, implemented with an algorithm relying on deep language
models for knowledge graph extraction and information retrieval. To understand
whether this metric can measure explainability, we devised a few experiments
and user studies involving more than 190 participants, evaluating two realistic
systems for healthcare and finance using famous AI technology, including
Artificial Neural Networks and TreeSHAP. The results we obtained are
statistically significant (with P values lower than .01), suggesting that our
proposed metric for measuring the Degree of Explainability is robust in several
scenarios, and it aligns with concrete expectations.
","[{'version': 'v1', 'created': 'Sat, 11 Sep 2021 17:44:13 GMT'}, {'version': 'v2', 'created': 'Wed, 8 Dec 2021 14:23:22 GMT'}, {'version': 'v3', 'created': 'Wed, 31 Aug 2022 12:21:58 GMT'}, {'version': 'v4', 'created': 'Tue, 3 Jan 2023 15:22:10 GMT'}, {'version': 'v5', 'created': 'Tue, 31 Jan 2023 16:51:28 GMT'}]",2023-02-01,"['Artificial Intelligence', 'Computation and Language']","This paper presents an objective metric for explainable AI that can be used to measure the degree of explainability of Artificial Intelligence (AI) systems, computation and language. The proposed metric is based on the concept of explainability, which is defined as the ability of a system to explain its decision-making process and results. The paper discusses the need for an objective metric and why it is important to estimate the degree of explainability in AI systems. The proposed metric is based on the concept of interpretability, which is defined as the ability to understand and explain the behavior of a system. The paper then describes how the proposed metric can be used to evaluate the explainability of AI systems, computation and language. Finally, the paper discusses the implications of the proposed metric and its potential applications.","Write an abstract for a paper called An Objective Metric for Explainable AI: How and Why to Estimate the
  Degree of Explainability about Artificial Intelligence, Computation and Language"
2205.11753,Jinhong Li and Qiuping Wang and Patrick P. C. Lee,"Efficient LSM-Tree Key-Value Data Management on Hybrid SSD/HDD Zoned
  Storage",['cs.PF'],"  Zoned storage devices, such as zoned namespace (ZNS) solid-state drives
(SSDs) and host-managed shingled magnetic recording (HM-SMR) hard-disk drives
(HDDs), expose interfaces for host-level applications to support fine-grained,
high-performance storage management. Combining ZNS SSDs and HM-SMR HDDs into a
unified hybrid storage system is a natural direction to scale zoned storage at
low cost, yet how to effectively incorporate zoned storage awareness into
hybrid storage is a non-trivial issue. We make a case for key-value (KV) stores
based on log-structured merge trees (LSM-trees) as host-level applications, and
present HHZS, a middleware system that bridges an LSM-tree KV store with hybrid
zoned storage devices based on hints. HHZS leverages hints issued by the
flushing, compaction, and caching operations of the LSM-tree KV store to manage
KV objects in placement, migration, and caching in hybrid ZNS SSD and HM-SMR
HDD zoned storage. Experiments show that our HHZS prototype, when running on
real ZNS SSD and HM-SMR HDD devices, achieves the highest throughput compared
with all baselines under various settings.
","[{'version': 'v1', 'created': 'Tue, 24 May 2022 03:26:42 GMT'}]",2022-05-25,['Performance'],This paper proposes an efficient LSM-Tree key-value data management system on hybrid SSD/HDD zoned storage for improved performance. We discuss the design of a hybrid storage system that combines the advantages of both SSD and HDD storage. We also propose a novel LSM-Tree based key-value data management system that takes advantage of the zoned storage characteristics of the hybrid storage system. We evaluate the performance of the proposed system and compare it with existing systems. The results show that our proposed system can significantly improve the performance of key-value data management on hybrid storage systems.,"Write an abstract for a paper called Efficient LSM-Tree Key-Value Data Management on Hybrid SSD/HDD Zoned
  Storage about Performance"
2203.08564,Jaouad Mourtada and Lorenzo Rosasco,An elementary analysis of ridge regression with random design,"['math.ST', 'cs.LG', 'stat.ML', 'stat.TH']","  In this note, we provide an elementary analysis of the prediction error of
ridge regression with random design. The proof is short and self-contained. In
particular, it bypasses the use of Rudelson's deviation inequality for
covariance matrices, through a combination of exchangeability arguments, matrix
perturbation and operator convexity.
","[{'version': 'v1', 'created': 'Wed, 16 Mar 2022 11:54:32 GMT'}, {'version': 'v2', 'created': 'Fri, 22 Apr 2022 16:18:01 GMT'}]",2022-10-11,['Machine Learning'],"This paper introduces an elementary analysis of ridge regression with random design in the context of machine learning. We provide a comprehensive overview of ridge regression, including the mathematical formulation, the estimation of regression coefficients, and the implementation of regularization. We also discuss the theoretical and practical aspects of random design, including its use in cross-validation. We present an empirical analysis of ridge regression with random design using simulated data, and discuss the results. The paper concludes with a discussion of the implications of the results for machine learning and future research directions.",Write an abstract for a paper called An elementary analysis of ridge regression with random design about Machine Learning
2211.16452,"Michael Bentley, Caleb Rucker, Alan Kuntz","Interactive-Rate Supervisory Control for Arbitrarily-Routed Multi-Tendon
  Robots via Motion Planning",['cs.RO'],"  Tendon-driven robots, where one or more tendons under tension bend and
manipulate a flexible backbone, can improve minimally invasive surgeries
involving difficult-to-reach regions in the human body. Planning motions safely
within constrained anatomical environments requires accuracy and efficiency in
shape estimation and collision checking. Tendon robots that employ
arbitrarily-routed tendons can achieve complex and interesting shapes, enabling
them to travel to difficult-to-reach anatomical regions. Arbitrarily-routed
tendon-driven robots have unintuitive nonlinear kinematics. Therefore, we
envision clinicians leveraging an assistive interactive-rate motion planner to
automatically generate collision-free trajectories to clinician-specified
destinations during minimally-invasive surgical procedures. Standard
motion-planning techniques cannot achieve interactive-rate motion planning with
the current expensive tendon robot kinematic models. In this work, we present a
3-phase motion-planning system for arbitrarily-routed tendon-driven robots with
a Precompute phase, a Load phase, and a Supervisory Control phase. Our system
achieves an interactive rate by developing a fast kinematic model (over 1,000
times faster than current models), a fast voxel collision method (27.6 times
faster than standard methods), and leveraging a precomputed roadmap of the
entire robot workspace with pre-voxelized vertices and edges. In simulated
experiments, we show that our motion-planning method achieves high tip-position
accuracy and generates plans at 14.8 Hz on average in a segmented collapsed
lung pleural space anatomical environment. Our results show that our method is
17,700 times faster than popular off-the-shelf motion planning algorithms with
standard FK and collision detection approaches. Our open-source code is
available online.
","[{'version': 'v1', 'created': 'Tue, 29 Nov 2022 18:34:33 GMT'}]",2022-11-30,['Robotics'],"This paper presents a novel interactive-rate supervisory control strategy for arbitrarily-routed multi-tendon robots. The proposed control strategy combines motion planning and control techniques to enable the robots to accurately and reliably track desired trajectories. The control strategy is tested on two different types of multi-tendon robots, and the results show that the proposed strategy is able to successfully track the desired trajectories with minimal error. Furthermore, the proposed strategy is able to handle dynamic environments and can be adapted to various robotic tasks. The results demonstrate that the proposed control strategy is a promising approach for the control of arbitrarily-routed multi-tendon robots.","Write an abstract for a paper called Interactive-Rate Supervisory Control for Arbitrarily-Routed Multi-Tendon
  Robots via Motion Planning about Robotics"
2301.05575,"Carolina Gon\c{c}alves, Jo\~ao M. Lopes, Sara Moccia, Daniele
  Berardini, Lucia Migliorelli, and Cristina P. Santos","Deep learning-based approaches for human motion decoding in smart
  walkers for rehabilitation","['cs.CV', 'cs.AI']","  Gait disabilities are among the most frequent worldwide. Their treatment
relies on rehabilitation therapies, in which smart walkers are being introduced
to empower the user's recovery and autonomy, while reducing the clinicians
effort. For that, these should be able to decode human motion and needs, as
early as possible. Current walkers decode motion intention using information of
wearable or embedded sensors, namely inertial units, force and hall sensors,
and lasers, whose main limitations imply an expensive solution or hinder the
perception of human movement. Smart walkers commonly lack a seamless
human-robot interaction, which intuitively understands human motions. A
contactless approach is proposed in this work, addressing human motion decoding
as an early action recognition/detection problematic, using RGB-D cameras. We
studied different deep learning-based algorithms, organised in three different
approaches, to process lower body RGB-D video sequences, recorded from an
embedded camera of a smart walker, and classify them into 4 classes (stop,
walk, turn right/left). A custom dataset involving 15 healthy participants
walking with the device was acquired and prepared, resulting in 28800 balanced
RGB-D frames, to train and evaluate the deep networks. The best results were
attained by a convolutional neural network with a channel attention mechanism,
reaching accuracy values of 99.61% and above 93%, for offline early
detection/recognition and trial simulations, respectively. Following the
hypothesis that human lower body features encode prominent information,
fostering a more robust prediction towards real-time applications, the
algorithm focus was also evaluated using Dice metric, leading to values
slightly higher than 30%. Promising results were attained for early action
detection as a human motion decoding strategy, with enhancements in the focus
of the proposed architectures.
","[{'version': 'v1', 'created': 'Fri, 13 Jan 2023 14:29:44 GMT'}]",2023-01-16,"['Computer Vision and Pattern Recognition', 'Artificial Intelligence']","This paper explores the potential of deep learning-based approaches for human motion decoding in smart walkers for rehabilitation. The paper reviews the current state-of-the-art in computer vision and pattern recognition, artificial intelligence, and deep learning-based approaches for human motion decoding in smart walkers. It then examines the advantages of using deep learning-based approaches in smart walkers for rehabilitation, such as improved accuracy, speed, and cost-effectiveness. The paper also presents a case study of a deep learning-based approach for human motion decoding in a smart walker for rehabilitation. Finally, the paper discusses the implications of deep learning-based approaches for future research in the field.","Write an abstract for a paper called Deep learning-based approaches for human motion decoding in smart
  walkers for rehabilitation about Computer Vision and Pattern Recognition, Artificial Intelligence"
2112.15448,Farshad Noravesh and Hamid Boustanifar,Exact Post-selection Inference For Tracking S&P500,"['q-fin.ST', 'cs.CE']","  The problem that is solved in this paper is known as index tracking. The
method of Lasso is used to reduce the dimensions of S&P500 index which has many
applications in both investment and portfolio management algorithms. The
novelty of this paper is that post-selection inference is used to have better
modeling and inference for Lasso approach to index tracking. Both confidence
intervals and curves indicate that the performance of Lasso type method for
dimension reduction of S&P500 is remarkably high. Keywords: index tracking,
lasso, post-selection inference, S&P500
","[{'version': 'v1', 'created': 'Wed, 29 Dec 2021 09:49:22 GMT'}]",2022-01-03,"['Computational Engineering, Finance, and Science']","This paper presents a novel approach to post-selection inference for tracking the S&P500 index using computational engineering, finance, and science. We propose a new method, exact post-selection inference (EPSI), which allows us to accurately estimate the performance of a portfolio based on a post-selection process. The EPSI method utilizes a combination of Monte Carlo simulation and stochastic optimization to identify the optimal portfolio weights and risk metrics. Furthermore, we present a comprehensive empirical analysis of the EPSI method to demonstrate its effectiveness in tracking the S&P500 index. Our results show that the EPSI method is able to accurately identify the optimal portfolio weights and risk metrics that result in a portfolio that closely tracks the S&P500 index.","Write an abstract for a paper called Exact Post-selection Inference For Tracking S&P500 about Computational Engineering, Finance, and Science"
2208.07955,"Jiayan Gu, Ashiq Anjum, Yan Wu, Lu Liu, John Panneerselvam, Yao Lu, Bo
  Yuan","The least-used key selection method for information retrieval in
  large-scale Cloud-based service repositories",['cs.IR'],"  As the number of devices connected to the Internet of Things (IoT) increases
significantly, it leads to an exponential growth in the number of services that
need to be processed and stored in the large-scale Cloud-based service
repositories. An efficient service indexing model is critical for service
retrieval and management of large-scale Cloud-based service repositories. The
multilevel index model is the state-of-art service indexing model in recent
years to improve service discovery and combination. This paper aims to optimize
the model to consider the impact of unequal appearing probability of service
retrieval request parameters and service input parameters on service retrieval
and service addition operations. The least-used key selection method has been
proposed to narrow the search scope of service retrieval and reduce its time.
The experimental results show that the proposed least-used key selection method
improves the service retrieval efficiency significantly compared with the
designated key selection method in the case of the unequal appearing
probability of parameters in service retrieval requests under three indexing
models.
","[{'version': 'v1', 'created': 'Tue, 16 Aug 2022 21:29:08 GMT'}]",2022-08-18,['Information Retrieval'],This paper presents an analysis of the least-used key selection method for information retrieval in large-scale Cloud-based service repositories. The research focuses on the impact of the least-used key selection method on the effectiveness of information retrieval in Cloud-based service repositories. The paper provides an overview of the existing key selection methods and compares them to the least-used key selection method. The research also examines the advantages and disadvantages of the least-used key selection method and its potential impact on information retrieval in Cloud-based service repositories. The paper concludes with a discussion of the implications of the research and suggests potential future research directions.,"Write an abstract for a paper called The least-used key selection method for information retrieval in
  large-scale Cloud-based service repositories about Information Retrieval"
2203.12456,"Jun Lu, Shao Yi","Reducing overestimating and underestimating volatility via the augmented
  blending-ARCH model","['q-fin.ST', 'cs.LG']","  SVR-GARCH model tends to ""backward eavesdrop"" when forecasting the financial
time series volatility in which case it tends to simply produce the prediction
by deviating the previous volatility. Though the SVR-GARCH model has achieved
good performance in terms of various performance measurements, trading
opportunities, peak or trough behaviors in the time series are all hampered by
underestimating or overestimating the volatility. We propose a blending ARCH
(BARCH) and an augmented BARCH (aBARCH) model to overcome this kind of problem
and make the prediction towards better peak or trough behaviors. The method is
illustrated using real data sets including SH300 and S&P500. The empirical
results obtained suggest that the augmented and blending models improve the
volatility forecasting ability.
","[{'version': 'v1', 'created': 'Tue, 15 Mar 2022 08:52:01 GMT'}]",2022-06-23,['Machine Learning'],This paper presents a novel machine learning approach for reducing the overestimation and underestimation of volatility in financial markets. The proposed approach is based on the Augmented Blending-ARCH (AB-ARCH) model which combines both traditional and machine learning methods to produce more accurate volatility estimates. The AB-ARCH model is based on a blending method which combines the power of both traditional ARCH models and machine learning algorithms. The model is evaluated using a dataset of daily closing prices of the S&P 500 index from 1988 to 2018. The results of the study show that the AB-ARCH model is able to reduce the overestimation and underestimation of volatility compared to traditional ARCH models. The paper also discusses the potential implications of the proposed approach for financial markets and presents future directions for research.,"Write an abstract for a paper called Reducing overestimating and underestimating volatility via the augmented
  blending-ARCH model about Machine Learning"
2302.06547,"Lucas Streichenberg, Elia Trevisan, Jen Jen Chung, Roland Siegwart and
  Javier Alonso-Mora","Multi-Agent Path Integral Control for Interaction-Aware Motion Planning
  in Urban Canals",['cs.RO'],"  Autonomous vehicles that operate in urban environments shall comply with
existing rules and reason about the interactions with other decision-making
agents. In this paper, we introduce a decentralized and communication-free
interaction-aware motion planner and apply it to Autonomous Surface Vessels
(ASVs) in urban canals. We build upon a sampling-based method, namely Model
Predictive Path Integral control (MPPI), and employ it to, in each time
instance, compute both a collision-free trajectory for the vehicle and a
prediction of other agents' trajectories, thus modeling interactions. To
improve the method's efficiency in multi-agent scenarios, we introduce a
two-stage sample evaluation strategy and define an appropriate cost function to
achieve rule compliance. We evaluate this decentralized approach in simulations
with multiple vessels in real scenarios extracted from Amsterdam's canals,
showing superior performance than a state-of-the-art trajectory optimization
framework and robustness when encountering different types of agents.
","[{'version': 'v1', 'created': 'Mon, 13 Feb 2023 17:43:21 GMT'}]",2023-02-14,['Robotics'],"This paper presents a novel approach to motion planning for autonomous robots in urban canals. The proposed method, Multi-Agent Path Integral Control (MAPIC), is designed to handle the complexity of interaction-aware motion planning in a dynamic environment. The paper provides an overview of the MAPIC algorithm, and then evaluates its performance in a simulated urban canal environment. Results show that MAPIC is able to generate robust motion plans that consider both static and dynamic obstacles, as well as the interactions between multiple robots. The paper also discusses the potential applications of MAPIC in real-world scenarios.","Write an abstract for a paper called Multi-Agent Path Integral Control for Interaction-Aware Motion Planning
  in Urban Canals about Robotics"
2206.01987,"Anna Berdichevskaia (NUST ""MISiS"")",Atypical lexical abbreviations identification in Russian medical texts,['cs.CL'],"  Abbreviation is a method of word formation that aims to construct the
shortened term from the first letters of the initial phrase. Implicit
abbreviations frequently cause the comprehension difficulties for unprepared
readers. In this paper, we propose an efficient ML-based algorithm which allows
to identify the abbreviations in Russian texts. The method achieves ROC AUC
score 0.926 and F1 score 0.706 which are confirmed as competitive in comparison
with the baselines. Along with the pipeline, we also establish first to our
knowledge Russian dataset that is relevant for the desired task.
","[{'version': 'v1', 'created': 'Sat, 4 Jun 2022 13:16:08 GMT'}]",2022-06-07,['Computation and Language'],"This paper presents a study on the identification of atypical lexical abbreviations in Russian medical texts. The study focuses on the use of computational methods to identify and classify atypical abbreviations in medical texts. The paper examines the challenges of identifying atypical abbreviations in Russian medical texts, such as the lack of standardization in the use of abbreviations and the complexity of the language. The paper also presents an approach to solving the problem, which includes the use of a word embedding algorithm and a convolutional neural network. The results of the study demonstrate that the proposed approach is effective in identifying and classifying atypical abbreviations in Russian medical texts. The paper provides an important contribution to the field of computational linguistics, by providing a method to identify and classify atypical abbreviations in Russian medical texts.",Write an abstract for a paper called Atypical lexical abbreviations identification in Russian medical texts about Computation and Language
2210.10561,Philipp Richter and Oliver Gasser and Arthur Berger,Illuminating Large-Scale IPv6 Scanning in the Internet,['cs.NI'],"  While scans of the IPv4 space are ubiquitous, today little is known about
scanning activity in the IPv6 Internet. In this work, we present a longitudinal
and detailed empirical study on large-scale IPv6 scanning behavior in the
Internet, based on firewall logs captured at some 230,000 hosts of a major
Content Distribution Network (CDN). We develop methods to identify IPv6 scans,
assess current and past levels of IPv6 scanning activity, and study dominant
characteristics of scans, including scanner origins, targeted services, and
insights on how scanners find target IPv6 addresses. Where possible, we compare
our findings to what can be assessed from publicly available traces. Our work
identifies and highlights new challenges to detect scanning activity in the
IPv6 Internet, and uncovers that today's scans of the IPv6 space show widely
different characteristics when compared to the more well-known IPv4 scans.
","[{'version': 'v1', 'created': 'Wed, 19 Oct 2022 14:00:59 GMT'}]",2022-10-20,['Networking and Internet Architecture'],"This paper explores the prevalence of large-scale IPv6 scanning in the Internet and its implications on network security. It examines the scanning techniques used by attackers and the tools available to detect and respond to them. The paper also discusses the impact of IPv6 scanning on network architecture and security, as well as the potential solutions for mitigating the threat. The paper concludes with a discussion of the implications of IPv6 scanning for the future of Internet security. The paper provides a comprehensive overview of the current state of IPv6 scanning and its implications for network security and architecture.",Write an abstract for a paper called Illuminating Large-Scale IPv6 Scanning in the Internet about Networking and Internet Architecture
2201.09979,"Liang Lu, Jinyu Li and Yifan Gong",Endpoint Detection for Streaming End-to-End Multi-talker ASR,"['eess.AS', 'cs.SD']","  Streaming end-to-end multi-talker speech recognition aims at transcribing the
overlapped speech from conversations or meetings with an all-neural model in a
streaming fashion, which is fundamentally different from a modular-based
approach that usually cascades the speech separation and the speech recognition
models trained independently. Previously, we proposed the Streaming Unmixing
and Recognition Transducer (SURT) model based on recurrent neural network
transducer (RNN-T) for this problem and presented promising results. However,
for real applications, the speech recognition system is also required to
determine the timestamp when a speaker finishes speaking for prompt system
response. This problem, known as endpoint (EP) detection, has not been studied
previously for multi-talker end-to-end models. In this work, we address the EP
detection problem in the SURT framework by introducing an end-of-sentence token
as an output unit, following the practice of single-talker end-to-end models.
Furthermore, we also present a latency penalty approach that can significantly
cut down the EP detection latency. Our experimental results based on the
2-speaker LibrispeechMix dataset show that the SURT model can achieve promising
EP detection without significantly degradation of the recognition accuracy.
","[{'version': 'v1', 'created': 'Mon, 24 Jan 2022 22:17:20 GMT'}]",2022-01-26,['Sound'],"Source Separation

This paper presents a novel endpoint detection method for streaming end-to-end multi-talker automatic speech recognition (ASR) based on sound source separation. The proposed method uses a convolutional neural network (CNN) to extract features from the input audio stream and uses a combination of heuristics and statistical models to detect the endpoints of the speech utterances. The system is evaluated on a multi-talker dataset, and the results demonstrate that the proposed method can accurately detect the endpoints of speech utterances in a streaming fashion with a low false alarm rate. The proposed method is expected to be useful in applications such as meeting transcription and audio surveillance.",Write an abstract for a paper called Endpoint Detection for Streaming End-to-End Multi-talker ASR about Sound
2303.013,"Andrew Fuchs, Andrea Passarella, Marco Conti","Compensating for Sensing Failures via Delegation in Human-AI Hybrid
  Systems",['cs.AI'],"  Given an increasing prevalence of intelligent systems capable of autonomous
actions or augmenting human activities, it is important to consider scenarios
in which the human, autonomous system, or both can exhibit failures as a result
of one of several contributing factors (e.g. perception). Failures for either
humans or autonomous agents can lead to simply a reduced performance level, or
a failure can lead to something as severe as injury or death. For our topic, we
consider the hybrid human-AI teaming case where a managing agent is tasked with
identifying when to perform a delegation assignment and whether the human or
autonomous system should gain control. In this context, the manager will
estimate its best action based on the likelihood of either (human, autonomous)
agent failure as a result of their sensing capabilities and possible
deficiencies. We model how the environmental context can contribute to, or
exacerbate, the sensing deficiencies. These contexts provide cases where the
manager must learn to attribute capabilities to suitability for
decision-making. As such, we demonstrate how a Reinforcement Learning (RL)
manager can correct the context-delegation association and assist the hybrid
team of agents in outperforming the behavior of any agent working in isolation.
","[{'version': 'v1', 'created': 'Thu, 2 Mar 2023 14:27:01 GMT'}, {'version': 'v2', 'created': 'Fri, 17 Mar 2023 14:28:47 GMT'}]",2023-03-27,['Artificial Intelligence'],"This paper presents a novel approach to compensating for sensing failures in human-AI hybrid systems. It proposes a delegation-based approach to enable the AI system to take over sensing operations when the human fails to sense a given event. The paper first reviews the existing methods used to address sensing failures in human-AI hybrid systems, and then describes the proposed delegation-based approach in detail. The paper shows how this approach can improve the overall performance of the system by enabling the AI to take over sensing operations when the human fails to sense the event. Finally, the paper discusses the potential benefits and challenges of this approach and provides recommendations for future work.","Write an abstract for a paper called Compensating for Sensing Failures via Delegation in Human-AI Hybrid
  Systems about Artificial Intelligence"
